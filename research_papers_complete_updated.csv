cite_key,title,authors,year,Downloaded,Relevancy,Relevancy Justification,Insights,TL;DR,Summary,Research Question,Methodology,Key Findings,Primary Outcomes,Limitations,Conclusion,Research Gaps,Future Work,Implementation Insights,url,DOI,Tags
abdallah_2021,Towards a GML-Enabled Knowledge Graph Platform,"Hussein Abdallah, Essam Mansour",2021,Yes,High,"This paper is highly relevant as it proposes KGNet, a platform for on-demand graph machine learning (GML) as a service on top of RDF engines. This directly addresses the HDM project's need for integrating machine learning with knowledge graphs to enable intelligent reasoning over personal data. The focus on scalability, automated model training, and a GML-enabled query language (SPARQLML) provides a strong architectural blueprint for the HDM system.","The paper introduces KGNet, a platform that automates the training of GML models on knowledge graphs by using task-specific subgraphs. This approach improves scalability and accuracy for tasks like node classification and link prediction. It also proposes SPARQLML, a SPARQL-like query language that allows users to query and perform inference over KGs using the trained GML models.","This vision paper proposes KGNet, a platform that provides on-demand graph machine learning (GML) as a service on top of RDF engines. It aims to bridge the gap between GML frameworks and RDF data stores by automating the training of GML models on task-specific subgraphs of a knowledge graph. The platform introduces SPARQLML, a GML-enabled query language, to allow for querying and inferencing over KGs using the trained models, thereby improving scalability, accuracy, and accessibility of GML on knowledge graphs.","How can we seamlessly integrate graph machine learning (GML) models with RDF engines to enable scalable, on-demand training and querying of knowledge graphs for various prediction tasks?","The paper proposes the KGNet platform, which consists of two main components: GML-as-a-service (GMLaaS) and SPARQLML as a Service. GMLaaS automates the GML training pipeline by using a meta-sampling approach to extract task-specific subgraphs, selecting the optimal GML method based on budget constraints, and managing the trained models. SPARQLML as a Service provides a query interface that allows users to train, delete, and query GML models using a SPARQL-like syntax.","The experimental evaluation shows that training GML models on task-specific subgraphs identified by KGNet's meta-sampling approach significantly reduces training time and memory usage while maintaining comparable or even improved accuracy compared to training on the entire knowledge graph. For instance, on the DBLP dataset, KGNet achieved up to an 11% improvement in accuracy with at least a 22% reduction in memory and 27% reduction in training time.","The primary outcome is the proposal of the KGNet platform, a vision for a fully-fledged GML-enabled knowledge graph platform. The paper outlines the architecture, key components, and research challenges, and provides a proof-of-concept evaluation that demonstrates the feasibility and benefits of the proposed approach.","The paper is a vision paper, so the implementation is a prototype and not a fully mature system. The query optimization for SPARQLML is still an open research problem. The meta-sampling approach has been evaluated on a limited number of scenarios.","The integration of GML frameworks with RDF engines is a critical step towards building scalable and intelligent knowledge graph applications. By automating the GML pipeline and providing a high-level query language, platforms like KGNet can significantly lower the barrier for data scientists and developers to apply advanced machine learning techniques to knowledge graphs.","The paper identifies several research gaps, including the need for more advanced meta-sampling techniques, better methods for SPARQLML query optimization, and the development of comprehensive benchmarks for evaluating GML-enabled KG engines. There is also a need for more seamless integration between GML models and RDF engines to avoid the use of UDFs.","Future work includes developing more sophisticated meta-sampling approaches, creating advanced query optimization techniques for SPARQLML, and building comprehensive benchmarks to evaluate the performance of GML-enabled KG platforms. There is also an opportunity to explore the use of KGNet in various application domains beyond the ones presented in the paper.","The paper provides valuable insights into the architecture of a GML-enabled KG platform. The use of a meta-sampler to extract task-specific subgraphs is a key technique for improving scalability. The KGMeta graph, which stores metadata about trained models, is a clever way to enable seamless integration and query optimization. The proposed SPARQLML language provides a user-friendly interface for interacting with the system.",https://doi.org/10.1145/3447772,10.1145/3447772,"Knowledge Graph, Graph Machine Learning, GML, SPARQL, RDF, GNN, AI, Heterogeneous, Integration, Semantic",
aburasheed_2023b,Building Contextual Knowledge Graphs for Personalized Learning Recommendations Using Text Mining and Semantic Graph Completion,"Hasan Abu-Rasheed, Mareike Dornhöfer, Christian Weber, Gábor Kismihók, Ulrike Buchmann, Madjid Fathi",2023,Yes,High,"This paper is highly relevant as it directly addresses the transformation of hierarchical learning object data models into knowledge graphs using text mining and semantic graph completion. This aligns with the HDM project's focus on heterogeneous data integration and building schemas for diverse data, as it demonstrates a method for converting structured but inflexible data into a semantically rich knowledge graph. The emphasis on personalized recommendations also aligns with the HDM's goal of user-centric systems.","Transforms hierarchical learning object data models into knowledge graphs using custom text mining pipelines to extract semantic relations, achieving semantically comparable results to domain expert definitions with improved graph communities and betweenness centrality.","This paper addresses the transformation of hierarchical learning object (LO) data models into knowledge graphs to enable more sophisticated, context-aware learning recommendations. The research uses custom text mining pipelines to extract semantic relations between learning object elements and transforms hierarchical data models into knowledge graph models. The approach enables progression from basic ""remembering-level"" learning objectives to higher-order application and analysis objectives by representing learning contexts more comprehensively through knowledge graphs, addressing limitations in traditional hierarchical learning models.",How can hierarchical learning object data models be transformed into contextual knowledge graphs to enable more sophisticated context-aware personalized learning recommendations?,Custom text mining pipelines for semantic relation extraction; transformation from hierarchical data models to knowledge graph models; quality-control metrics and semantic similarity comparisons for evaluation; graph structure analysis using communities and betweenness centrality.,Knowledge graph relations semantically comparable to domain expert definitions; improved representation of learning object contexts; increased graph communities and betweenness centrality; successful transformation from hierarchical to graph-based learning models.,"The primary outcome is a demonstrated methodology for transforming hierarchical learning object data models into contextual knowledge graphs, enabling more sophisticated context-aware personalized learning recommendations. The paper shows that the resulting KG relations are semantically comparable to domain expert definitions and improve graph communities and betweenness centrality.",Limited evaluation on specific learning domains; dependency on quality of input hierarchical data models; computational complexity considerations for large-scale learning object datasets.,Successfully demonstrates transformation of hierarchical learning models into contextual knowledge graphs that enable progression from basic to higher-order learning objectives.,"The paper implicitly points to the need for: (1) broader evaluation across diverse educational domains and learning contexts, (2) investigation into scalability for very large learning object repositories, and (3) development of adaptive context modeling for dynamic learning environments.","Future work includes expanding evaluation to diverse educational domains and learning contexts, investigating scalability for large learning object repositories, and developing adaptive context modeling for dynamic learning environments.",Provides practical framework for implementing context-aware educational recommendation systems using knowledge graphs; demonstrates effective transformation from hierarchical to semantic learning models for HDM educational applications.,https://ieeexplore.ieee.org/document/10260850/,10.1109/ICALT58122.2023.00040,"Contextual Knowledge Graphs, Personalized Learning, Educational Technology, Semantic Graph Completion, Learning Object Modeling",
abusalih_2023,"Healthcare knowledge graph construction: A systematic review of the state-of-the-art, open issues, and opportunities","Bilal Abu-Salih, Muhammad AL-Qurishi, Mohammed Alweshah, Mohammad AL-Smadi, Reem Alfayez, Heba Saadeh",2023,Yes,High,"This paper is highly relevant to the HDM project as it provides a comprehensive systematic review of healthcare knowledge graph construction. The challenges identified, such as data heterogeneity, knowledge interoperability, and the need for robust evaluation, are directly applicable to the development of a personal knowledge graph for HDM. The proposed taxonomy and discussion of open issues offer a valuable roadmap for designing and implementing a high-quality, reliable PKG.","The paper's key insight is the urgent need for a more systematic and standardized approach to constructing healthcare knowledge graphs. It reveals that many existing methods are ad-hoc and lack rigorous evaluation, which compromises the quality and reliability of the resulting KGs. The authors emphasize the importance of addressing data heterogeneity, ensuring data quality, and incorporating temporal dynamics to build effective healthcare KGs.","This paper provides the first comprehensive systematic review of healthcare knowledge graph (KG) construction. It introduces a new taxonomy for the field, critically evaluates state-of-the-art techniques, and discusses open issues and future research opportunities. The authors highlight the inadequacy of many existing approaches and call for more rigorous methodologies to ensure the quality and robustness of healthcare KGs.","What is the current state-of-the-art in healthcare knowledge graph construction, what are the main challenges and open issues, and what are the opportunities for future research?","The authors conducted a systematic literature review following the PRISMA framework. They analyzed 101 papers published between 2018 and 2022, focusing on KG construction methodologies in various healthcare domains. Based on this analysis, they developed a taxonomy for healthcare KG construction and critically evaluated the existing techniques.","The study found that many healthcare KG construction approaches are ad-hoc and lack proper evaluation. Key challenges include handling data heterogeneity, ensuring data quality and privacy, achieving knowledge interoperability, and modeling the temporal nature of healthcare data. The paper also highlights a lack of publicly available healthcare KGs, which hinders research and development in the field.","The primary outcome is a comprehensive survey that provides a bird's-eye view of healthcare KG construction. It includes a novel taxonomy, an in-depth analysis of the state-of-the-art, and a discussion of open issues and future directions, serving as a valuable resource for researchers and practitioners.","The review is limited to papers published within a specific timeframe (2018-2022). The analysis is based on the information provided in the papers, and the authors note that many studies did not fully disclose their methodologies, which could affect the completeness of the review.","The construction of high-quality, robust, and interoperable healthcare knowledge graphs is crucial for advancing data-driven healthcare. This requires a shift from ad-hoc approaches to more systematic and standardized methodologies that address the unique challenges of the healthcare domain.","The paper identifies several research gaps, including the need for advanced techniques for integrating heterogeneous data sources, better methods for ensuring data quality and privacy, more effective approaches for knowledge interoperability, and the development of dynamic KGs that can capture the temporal aspects of healthcare data.","Future research should focus on developing sophisticated data collection and aggregation techniques, promoting semantic expansion and interoperability, establishing standardized KG construction and evaluation methodologies, and addressing data quality and privacy issues through advanced NLP and deep learning algorithms.","The paper offers valuable implementation insights, such as the importance of following a structured construction process, the need to integrate data from diverse sources to create a holistic view, and the necessity of rigorous evaluation to ensure the quality of the KG. The proposed taxonomy can be used as a practical guide for building healthcare KGs.",,,"Healthcare, Knowledge Graph, Systematic Review, Data Integration, Semantic Web, Ontology, Machine Learning",
abusalih_2024,A systematic literature review of knowledge graph construction and application in education,"Bilal Abu-Salih, Salihah Alotaibi",2024,Yes,High,"This paper provides a comprehensive systematic review of knowledge graph construction and applications in education. While the domain is education, the identified challenges and methodologies are highly relevant to the HDM project. The paper discusses challenges like data heterogeneity, lack of standardization, limited interoperability, and scalability, which are all core issues the HDM project aims to address. The structured review of KG construction techniques and applications provides valuable patterns and anti-patterns that can inform the design of the HDM's own data integration and knowledge representation architecture.","The key insight is that while KGs show immense promise in education, the field suffers from a lack of standardization, poor interoperability, and inadequate evaluation techniques. Many KG construction approaches are ad-hoc and not well-documented, hindering reproducibility and progress. This mirrors challenges in other domains and underscores the need for rigorous, systematic methodologies for building KGs, a core principle of the HDM project.","A systematic review of 120 papers reveals that Knowledge Graphs are increasingly used in education for personalized learning, curriculum design, and semantic search, but progress is hampered by a lack of standardization, poor data integration, and weak evaluation methods.","This paper conducts a systematic literature review (SLR) of 120 articles published between 2019 and 2023 on the construction and application of Knowledge Graphs (KGs) in education. It categorizes applications into five main domains: Adaptive and Personalised Learning, Curriculum Design and Planning, Concept Mapping and Visualization, Semantic Search and Question Answering, and miscellaneous applications. The review highlights the growing interest in using KGs to enhance educational experiences but also identifies significant limitations in the current state-of-the-art, including a lack of standardization, poor interoperability, sparse data, scalability issues, and inadequate evaluation techniques. The paper calls for more rigorous and standardized methodologies to advance the field.","What are the state-of-the-art methodologies for Knowledge Graph construction in education, what are their primary applications, and what are the key limitations and research gaps in the current literature?","The authors conducted a Systematic Literature Review (SLR) following the PRISMA framework. They searched multiple academic databases (Elsevier, ACM, MDPI, IEEE Xplore, Google Scholar) for papers published between 2019 and 2023. An initial set of 565 records was screened, and after a full-text eligibility assessment, 120 papers were included in the final review. The paper then synthesizes the findings from these papers, categorizing them by application domain and analyzing their methodologies, resources, and limitations.","The review found a rapidly growing interest in using KGs in education. The primary applications are in personalized learning, curriculum design, and semantic search. However, the study also identified major weaknesses in the existing research: (1) Lack of standardization in KG schemas and ontologies. (2) Limited interoperability between different educational KGs. (3) Many KGs are built on sparse and incomplete data. (4) Scalability is a major, often unaddressed, challenge. (5) Evaluation methods are often poor and subjective, lacking rigor. (6) Privacy and security are not well-studied.","The primary outcome is a comprehensive survey that maps the landscape of KG construction and application in education. It provides a structured overview of the state-of-the-art, a clear summary of the limitations and challenges, and a set of future research directions.","The paper itself identifies the limitations of the reviewed works. The authors of the survey note that their review is based on the information provided in the papers, and many studies did not fully disclose their methodologies. The review is also limited to papers published within a specific timeframe (2019-2023).","Knowledge Graphs have the potential to revolutionize education by enabling personalized, data-driven learning experiences. However, to realize this potential, the field needs to move beyond ad-hoc approaches and adopt more rigorous, standardized, and collaborative methodologies for KG construction, evaluation, and maintenance.",The paper explicitly identifies several research gaps: the need for standardized ontologies for education; methods for better data integration and handling of semantic heterogeneity; scalable KG construction and maintenance techniques; robust and objective evaluation metrics; and research into privacy-preserving techniques for educational KGs. The integration of LLMs with educational KGs is also highlighted as a key area for future research.,"Future work should focus on addressing the identified research gaps. This includes developing common ontologies for education, creating benchmarks and standardized evaluation metrics, exploring the use of LLMs for automated KG construction and maintenance, and designing privacy-preserving architectures for educational KGs.","The paper provides valuable implementation insights by highlighting common pitfalls to avoid. Key takeaways for the HDM project include: (1) The importance of adopting a standardized, ontology-driven approach from the outset. (2) The need to plan for scalability and real-time updates. (3) The necessity of developing a rigorous evaluation framework. (4) The critical importance of addressing privacy and security concerns when dealing with personal (student) data. The detailed tables summarizing various approaches also serve as a useful reference for specific techniques.",https://doi.org/10.1016/j.heliyon.2024.e25383,10.1016/j.heliyon.2024.e25383,"Knowledge Graph, Education, Systematic Literature Review, Survey, Data Integration, Ontology, Personalized Learning, Semantic Web"
aho_2023,A human digital twin for the M‑Machine,"Pertti Saariluoma, Mari Myllylä, Antero Karvonen, Mika Luimula, Jami Aho",2023,Yes,Medium,"This paper's focus on Human Digital Twins (HDTs) and cognitive mimetics is relevant to the HDM project's goal of creating a system that understands and augments human cognitive processes. While the paper is more conceptual, it provides a strong theoretical foundation for modeling human-technology interaction, which is central to the HDM's purpose. The discussion of how to model human information processing and intentionality can inform the design of the HDM's user model and its interaction paradigms.","The key insight of the paper is the necessity of shifting design focus from purely technical artifacts to a more holistic view that includes modeling human actions and intentions. The authors argue that Human Digital Twins, which are computational models of human interaction with technology, are essential tools for designing the intelligent systems of the future. They introduce ""cognitive mimetics"" as a method for imitating human information processing to create more effective and human-centered AI.","This paper introduces the concept of a Human Digital Twin (HDT) as a computational model of human actions involved in interacting with technical artifacts. The authors argue that as technology becomes more intelligent, designers must focus on modeling human interaction to create effective and human-centered systems. Using Minsky's M-Machine as a general model, they explore the conceptual foundations of HDTs, their design principles, and their role as a tool for designing future intelligent technologies.",How can we construct Human Digital Twins (HDTs) to effectively model human interaction with intelligent technologies and guide the design of future human-centered AI systems?,The paper uses a conceptual engineering approach. The authors define and elaborate on the concepts of Human Digital Twins and cognitive mimetics. They use Marvin Minsky's M-Machine as an abstract conceptual model to analyze the general properties of HDTs and their design. They also introduce the Ideal-Exception-Correction (IEC) model as a practical example of an HDT for process control tasks.,"The paper establishes that HDTs are a crucial tool for designing the next generation of intelligent technologies. It demonstrates that by modeling human information processing and intentionality, designers can create more holistic and effective systems. The use of the M-Machine as a general model allows for the analysis of HDT properties at an abstract level, applicable to any specific technology.","The primary outcome is a conceptual framework for Human Digital Twins. The paper defines the key components of an HDT (the user, the machine, and the Human Interaction Point), introduces the concept of cognitive mimetics, and proposes the IEC model as a concrete example of an HDT.","The paper is largely theoretical and does not present a concrete, implemented HDT for a real-world system beyond the conceptual IEC model. The discussion on the practical implementation of HDTs is limited, and the paper does not address the technical challenges of building such complex models in detail.","The design of future intelligent systems must be holistic, considering not just the technology itself but also how humans interact with it. Human Digital Twins, based on the principles of cognitive mimetics, provide a powerful conceptual tool for achieving this human-centered design approach.","The paper highlights the need for more research into the practical construction of HDTs. It also points to the challenge of integrating different types of models (e.g., causal models for machines and intentional models for humans) within a single HDT framework.",Future work should focus on developing practical methodologies and tools for building HDTs for specific application domains. There is also a need to explore how to integrate various cognitive architectures (like ACT-R and GOMS) into HDT models to better capture the complexities of human information processing.,"The paper provides valuable insights for implementation by breaking down the concept of an HDT into its core components: the user model, the machine model (M-Machine), and the Human Interaction Point (HIP). The IEC model serves as a practical example of how to model human control processes, offering a template for designing HDTs for similar tasks.",https://doi.org/10.1007/s44163-024-00164-x,10.1007/s44163-024-00164-x,"Human Digital Twin, Cognitive Mimetics, Human-Computer Interaction, AI Design, User Modeling, M-Machine, Design Science",
ai_2025,Zep: A Temporal Knowledge Graph Architecture for Agent Memory,Preston Rasmussen Zep AI,2025,Yes,High,Contains relevant concepts applicable to HDM systems,"Core component ""Graphiti"" - temporally-aware knowledge graph engine that dynamically synthesizes unstructured conversational and structured business data while maintaining historical relationships.",Temporal knowledge graph architecture that outperforms existing memory systems,"This paper presents Zep, a temporal knowledge graph architecture for AI agent memory that addresses limitations of static document retrieval in RAG frameworks by enabling dynamic knowledge integration from diverse sources including ongoing conversations and business data.",How can temporal knowledge graphs improve AI agent memory for enterprise applications requiring dynamic knowledge integration?,"Core component: ""Graphiti"" - temporally-aware knowledge graph engine; dynamically synthesizes unstructured conversational and structured business data; maintains historical relationships.",Outperforms MemGPT in Deep Memory Retrieval benchmark (94.8% vs 93.4%); 18.5% accuracy improvement in LongMemEval benchmark; 90% reduction in response latency.,"Enhanced cross-session information synthesis, improved long-term context maintenance, effective for enterprise AI applications.",Not explicitly stated in the abstract.,Demonstrates significant advancement in temporal knowledge graph applications for AI agent memory and enterprise knowledge integration.,Limitations of static document retrieval in RAG frameworks; need for dynamic knowledge integration in enterprise AI.,"Enhanced cross-session information synthesis, improved long-term context maintenance, effective for enterprise AI applications.",Provides practical implementation of temporal knowledge graphs for AI agent memory with focus on dynamic data synthesis and historical relationship maintenance.,https://arxiv.org/abs/2501.13956,10.48550/arXiv.2501.13956,"ai, integration, knowledge_graph, llm, machine_learning, memory, semantic, survey, temporal"
ain_2024,Learner Modeling and Recommendation of Learning Resources using Personal Knowledge Graphs,"Qurat Ul Ain, Rawaa Alatrash, Mohamed Amine Chatti, Shoeb Joarder, Paul Arthur Meteng Kamdem, Clara Siepmann",2024,Yes,High,"This paper is highly relevant as it directly implements and evaluates a Personal Knowledge Graph (PKG) system for a practical application (education). It addresses key HDM themes like user-in-the-loop control, semantic modeling, and personalized recommendations. The use of a scrutable, user-controlled PKG to drive recommendations is a core concept for the HDM project.","The key insight is that giving learners direct control over their knowledge model (by marking concepts they ""Did Not Understand"") significantly improves their satisfaction and intention to use a recommender system. This user-in-the-loop approach, creating a scrutable PKG, is more impactful on user perception than minor differences in algorithmic accuracy.","This paper presents an educational recommender system where students build their own Personal Knowledge Graphs (PKGs) by flagging concepts they don't understand. This PKG-based approach, which gives users control over their data model, was perceived as more accurate, novel, diverse, and useful than a traditional content-based system.","This paper proposes an educational recommender system that empowers students by allowing them to construct their own Personal Knowledge Graphs (PKGs). Students interact with learning materials and explicitly mark concepts they ""Did Not Understand"" (DNU). These DNU concepts form the basis of their PKG, which is then used to generate personalized recommendations for external learning resources (YouTube videos, Wikipedia articles). The system was evaluated through offline experiments and an online user study (N=31), comparing the PKG-based approach to a traditional content-based one. The results showed that while the content-based approach had slightly higher precision, the PKG-based approach was perceived more positively by users across several metrics, including accuracy, novelty, diversity, usefulness, and overall satisfaction. The authors conclude that user control over the learner model is a key factor in the acceptance and perceived quality of educational recommender systems.","How can we effectively leverage Personal Knowledge Graphs (PKGs) to model students' knowledge and recommend learning resources in a MOOC environment, and what is the impact of this approach on system accuracy and user perception?","The authors developed a PKG-based recommender system within their CourseMapper MOOC platform. Key components include: 1) A concept extraction pipeline using SingleRank and DBpedia Spotlight. 2) A user interface allowing students to mark concepts as ""Did Not Understand"" (DNU). 3) A learner model built from the student's PKG, using SBERT for concept embeddings and a modified LightGCN to incorporate graph structure. 4) A recommendation engine that computes cosine similarity between the learner model and candidate resources from YouTube and Wikipedia. The system was evaluated in an online user study (N=31) comparing four variants (PKG-based vs. content-based, each with keyphrase vs. document-level analysis) using quantitative metrics (Precision@k, MRR, MAP) and a qualitative user survey (ResQue framework).","The user study found that: (1) The PKG-based recommendation approach was perceived by users as more accurate, novel, diverse, and useful than the content-based approach. (2) Users were more satisfied with the PKG-based system and had a higher intention to use it. (3) Giving users control over their learner model (by marking DNU concepts) was a major contributor to their satisfaction. (4) Offline, the keyphrase-based variants achieved higher semantic similarity scores, but online, the document-based variants had slightly better ranking accuracy (MRR/MAP), highlighting the discrepancy between offline metrics and user perception.","The primary outcome is a novel, user-centric educational recommender system that demonstrates the practical benefits of using scrutable Personal Knowledge Graphs. The study also provides a set of validated findings on the importance of user control in personalized learning environments.","The authors acknowledge several limitations: (1) The concept extraction process was sometimes imprecise, leading to irrelevant concepts being included in the PKG. (2) The YouTube API's query length limit restricted the number of DNU concepts that could be used for candidate retrieval. (3) The user study was conducted with a relatively small sample size (N=31) from a single university course, which may limit the generalizability of the findings.","The paper concludes that a PKG-based approach, which gives students control over their own learner model, is a promising direction for building more effective and engaging educational recommender systems. The increased user satisfaction and perceived quality of the recommendations highlight the importance of transparency and scrutability in personalized learning environments.","The paper points to the need for improved concept extraction pipelines to create more accurate PKGs. It also highlights the need for further research into how to best balance global (all DNU concepts) and local (current slide's concepts) context when generating recommendations. Finally, it suggests investigating other user-centric evaluation metrics beyond the ResQue framework, such as trust and confidence.",Future work will focus on: (1) Improving the accuracy of the concept extraction pipeline. (2) Refining the recommendation algorithm to better handle the trade-off between global and local context. (3) Conducting larger-scale user studies to further validate the findings and explore other user-centric metrics like transparency and trust.,The paper provides a clear architectural blueprint for a PKG-based recommender system. Key implementation insights include: (1) The use of SBERT for generating semantic embeddings of concepts and documents. (2) The application of a GCN (LightGCN) to enrich concept embeddings with graph-structural information. (3) The design of a user interface that allows for explicit user feedback (marking DNU concepts). (4) The use of the ResQue framework for a comprehensive user-centric evaluation.,https://dl.acm.org/doi/10.1145/3636555.3636881,10.1145/3636555.3636881,"Personal Knowledge Graph, PKG, Educational Recommender System, ERS, Learner Modeling, Open Learner Model, User Control, Scrutability, GCN, SBERT, Semantic Similarity, User Study"
akroyd_2021,Universal Digital Twin - A Dynamic Knowledge Graph,"Jethro Akroyd, Sebastian Mosbach, Amit Bhave, Markus Kraft",2021,Yes,Super,"This paper is of super relevancy because it introduces a dynamic knowledge graph approach for digital twins, which is a core concept for the HDM project. The paper's focus on interoperability, real-time data integration, and the use of ontologies and agents aligns perfectly with the HDM's architectural goals. The concepts of a ""base world"" and ""parallel worlds"" for scenario analysis are directly applicable to the HDM's need to model and simulate personal data.","The key insight is that a dynamic knowledge graph, implemented with Semantic Web technologies, provides a robust and scalable foundation for creating a ""Universal Digital Twin."" This architecture, composed of ontologies, instances, and autonomous agents, can handle heterogeneous, cross-domain data while ensuring it remains connected, discoverable, and queryable.","The paper proposes a dynamic knowledge graph architecture for creating digital twins, demonstrating its ability to integrate diverse data sources, support real-time updates via computational agents, and enable complex scenario analysis through ""parallel worlds.""","This paper introduces a dynamic knowledge-graph approach for digital twins, arguing it is well-suited for realizing a Universal Digital Twin. The implementation uses Semantic Web technologies, with concepts and instances defined by ontologies, and computational agents that update the graph. This design is inherently distributed, supports cross-domain interoperability, and ensures data is connected, portable, discoverable, and queryable via a uniform interface. The paper introduces the notions of a ""base world"" for real-world representation maintained by real-time data agents, and ""parallel worlds"" for exploring alternative scenarios without affecting the base world. Use cases demonstrate the graph's ability to handle geospatial and chemical data, control experiments, and perform cross-domain simulations and scenario analysis.","How can a comprehensive, interoperable, and dynamic digital twin be implemented to support complex, cross-domain decision-making, such as in urban planning and energy system decarbonization?","The paper proposes a dynamic knowledge graph architecture implemented using Semantic Web technologies (ontologies, RDF, SPARQL). The system, called the World Avatar, uses autonomous computational agents to continuously update the knowledge graph with real-time data and perform simulations. The methodology is demonstrated through several use cases, including the J-Park Simulator for an eco-industrial park, the Cities Knowledge Graph for urban planning, and a project to create a digital twin of the UK for energy system analysis.","The dynamic knowledge graph approach is a viable and powerful method for implementing a Universal Digital Twin. It successfully integrates heterogeneous data from different domains (e.g., weather, shipping, chemistry, urban planning) and scales from atomic to national levels. The use of agents allows the digital twin to remain up-to-date and self-consistent. The ""parallel worlds"" concept provides a robust framework for what-if scenario analysis.","The primary outcome is the World Avatar project itself, a proof-of-concept for a dynamic knowledge graph-based digital twin. The paper also presents a clear architectural framework and a set of design principles for building such systems.",The paper acknowledges that the implementation of a full Universal Digital Twin is still a significant challenge. The suggestion of scenarios and the alignment of goals for complex problems are identified as open research questions. The performance and scalability of the underlying triple stores for industry-scale applications remain a challenge.,"A dynamic knowledge graph built with Semantic Web technologies provides a powerful and suitable foundation for implementing a comprehensive, interoperable digital twin. This approach can support complex, data-driven decision-making for large-scale systems like national infrastructure and energy grids.","The paper highlights the need for research on how to automatically suggest beneficial scenarios for analysis and how to ensure the goals of the digital twin align with societal goals, especially when goals conflict.","Future work includes extending the knowledge-graph-based digital twin of the UK to support the decarbonization of the energy landscape, incorporating more data sources (e.g., biomass, solar, wind), and further developing methods for goal alignment and automated scenario suggestion.","Key implementation insights include: the use of a distributed architecture with SPARQL endpoints for uniform data access; the use of autonomous agents to update the knowledge graph; the separation of the ""base world"" from ""parallel worlds"" for scenario analysis; and the use of ontologies to ensure semantic interoperability across different domains.",https://doi.org/10.17863/cam.32260,10.17863/CAM.32260,"Digital Twin, Dynamic Knowledge Graph, Semantic Web, Ontology, Data Integration, Interoperability, Agents, Scenario Analysis, Smart Cities, Energy Systems, ai, federated, healthcare, knowledge_graph, llm, machine_learning, medicine_access, memory, sdg, semantic, survey, temporal"
alatrash_2024,Transparent Learner Knowledge State Modeling using Personal Knowledge Graphs and Graph Neural Networks,"Rawaa Alatrash, Mohamed Amine Chatti, Qurat Ul Ain, Shoeb Joarder",2024,Yes,High,"This paper is highly relevant as it demonstrates a novel approach combining Personal Knowledge Graphs and Graph Neural Networks for transparent learner modeling, directly aligning with HDM's goals of user-involved, interpretable knowledge representation.","Combines Personal Knowledge Graphs, Graph Convolutional Networks, and transformer sentence encoders to construct transparent learner models that explicitly involve learners in modeling their knowledge state through 'Did Not Understand' concept marking in MOOC platforms",Novel approach integrating PKGs and GNNs for transparent learner knowledge state modeling with explicit learner involvement in knowledge representation,"This paper presents a novel approach that combines Personal Knowledge Graphs, Graph Convolutional Networks, and transformer sentence encoders to construct transparent learner models for educational applications. The research explicitly involves learners in modeling their knowledge state by enabling them to mark concepts as 'Did Not Understand' in MOOC platforms, creating more interpretable and user-controlled knowledge representations. The approach addresses the need for transparency in AI-driven educational systems while maintaining effective predictive capabilities for personalized learning recommendations.",How can personal knowledge graphs and graph neural networks be combined to create transparent learner knowledge state models that actively involve learners in the knowledge representation process?,Integration of Personal Knowledge Graphs with Graph Convolutional Networks; transformer sentence encoder implementation; MOOC platform integration with 'Did Not Understand' concept marking; transparent learner modeling framework development,Demonstrated successful integration of PKGs and GNNs for learner modeling; achieved transparent knowledge state representation with active learner involvement; provided interpretable framework for educational recommendation systems,,Limited evaluation details available from conference abstract; focus on specific MOOC platform may limit generalizability; computational complexity considerations for real-time deployment not characterized,Successfully demonstrates feasibility of transparent learner modeling using PKGs and GNNs with active learner participation in knowledge state representation,,Expand evaluation to diverse educational platforms and contexts; investigate scalability for large-scale MOOC deployments; develop automated knowledge concept extraction methods,Emphasizes transparency and user involvement in AI-driven educational systems; provides practical framework for interpretable learner modeling using PKGs and GNNs for HDM educational applications,https://dl.acm.org/doi/10.1145/3627043.3659545,10.1145/3627043.3659545,"Personal Knowledge Graphs, Graph Neural Networks, Transparent AI, Learner Modeling, Educational Technology"
aldughayfiq_2023,Capturing Semantic Relationships in Electronic Health Records Using Knowledge Graphs: An Implementation Using MIMIC III Dataset and GraphDB,"Bader Aldughayfiq, Farzeen Ashfaq, N. Z. Jhanjhi, Mamoona Humayun",2023,Yes,Super,"This paper is of super relevancy because it provides a detailed, end-to-end implementation of constructing a knowledge graph from a complex, real-world Electronic Health Record (EHR) dataset (MIMIC-III). This directly addresses the HDM project's core challenge of integrating heterogeneous personal health data. The methodology, which covers ontology development, RDF mapping, and graph database implementation (GraphDB), offers a practical and validated blueprint for the HDM's data ingestion and representation layer. The paper's focus on using the KG to enable more efficient and accurate data analysis for patient outcomes is a primary goal of the HDM.","The key insight is that a knowledge graph-based approach provides a more efficient and intuitive way to analyze complex, heterogeneous EHR data compared to traditional relational databases. By creating a semantic layer (ontology) and representing the data as a graph, it becomes possible to easily query complex relationships and visualize connections that are difficult to see in a tabular format, ultimately enabling more comprehensive and holistic data analysis for better clinical decision-making.","The paper demonstrates how to build a knowledge graph from a large EHR dataset (MIMIC-III) to capture complex patient data relationships, enabling faster and more accurate analysis than traditional databases.","This study presents a methodology for constructing a knowledge graph from the MIMIC-III EHR dataset. The authors first develop an OWL ontology using Protégé to define the entities and relationships within the data. They then use Ontotext Refine to map the raw CSV data into RDF triples based on this ontology. The resulting knowledge graph is loaded into GraphDB, where SPARQL is used for querying and analysis. The authors demonstrate through sample queries that this approach can effectively capture semantic relationships, enabling more efficient and accurate analysis of patient data to identify trends and risk factors. They also show that their graph-based approach outperforms a traditional MySQL database in query execution time for similar tasks.",Can a knowledge graph created using the MIMIC III dataset and GraphDB effectively capture semantic relationships within EHRs and enable more efficient and accurate data analysis?,"The methodology involves a four-step process: 1. **Ontology Development**: An OWL ontology was created using Protégé to define the classes (e.g., Patient, Admission, Diagnosis) and properties of the EHR domain. 2. **Data Processing & RDF Mapping**: The MIMIC-III dataset (in CSV format) was processed using Ontotext Refine to map the data to the created ontology, generating RDF triples. 3. **Graph Representation**: The RDF triples were loaded into GraphDB to construct the knowledge graph. 4. **Querying and Analysis**: SPARQL queries were used to retrieve and analyze information from the graph, with use cases focused on identifying patient cohorts and risk factors. The performance was compared to a traditional MySQL database setup.","Knowledge graphs can effectively capture and represent the complex semantic relationships within large, heterogeneous EHR datasets. The graph-based approach allows for more efficient and intuitive data analysis compared to traditional relational databases. SPARQL queries on the GraphDB implementation were significantly faster (e.g., 0.11s vs. 1.33s for a sample query) than equivalent SQL queries on MySQL. The visual nature of the graph also enhances data exploration.","The primary outcome is a practical, implemented framework for converting a complex EHR dataset (MIMIC-III) into a queryable knowledge graph. This includes the developed ontology, the RDF mapping process, and a demonstration of its analytical capabilities through SPARQL queries.",The study acknowledges several limitations: the query performance evaluation was preliminary and used a limited set of sample queries. The ontology's clinical validity was not rigorously evaluated by external experts. The interoperability of the created KG with external standard vocabularies like SNOMED CT and LOINC was identified as a next step and not yet implemented. The study also focused on a single data repository.,"The study concludes that knowledge graphs are a powerful and effective tool for integrating and analyzing complex EHR data. By representing the data semantically, KGs enable more efficient, accurate, and comprehensive analysis, which can provide valuable insights for clinical decision-making and ultimately improve patient outcomes. The framework presented provides a foundation for further research and development in this area.","The paper identifies the need for more comprehensive and tailored ontologies for EHR data, more rigorous clinical validation of such KGs, and better methods for ensuring interoperability with standard medical vocabularies. It also points to the need to explore new research questions and expand the KG to include other data types like patient-generated data and genomics.","Future work plans include: 1. Linking the ontology to standard external vocabularies (SNOMED CT, LOINC) to improve interoperability. 2. Expanding the knowledge graph to include patient-generated data, genetic data, and socioeconomic determinants of health. 3. Applying machine learning algorithms on the knowledge graph to detect new risk factors and predict patient outcomes.","Key implementation insights include: the use of a standard pipeline (Ontology -> RDF Mapping -> Graph DB) for KG construction; the use of specific tools like Protégé for ontology editing, Ontotext Refine for RDF mapping, and GraphDB for storage and querying; the demonstration of significant query performance improvements of a graph database over a relational database for complex relationship-based queries. The paper provides a clear, step-by-step guide that can be followed to implement a similar system.",https://doi.org/10.3390/healthcare11121762,10.3390/healthcare11121762,"ai, digital_twin, healthcare, heterogeneous, integration, knowledge_graph, machine_learning, ontology, personal, privacy, semantic, survey, temporal"
ammar_2021,Using a Personal Health Library–Enabled mHealth Recommender System for Self-Management of Diabetes Among Underserved Populations: Use Case for Knowledge Graphs and Linked Data,"Nariman Ammar, James E Bailey, Robert L Davis, Arash Shaban-Nejad",2021,Yes,Super,"This paper is of super relevancy as it proposes a Personal Health Library (PHL), which is conceptually identical to a Personal Knowledge Graph (PKG), and builds it on the decentralized Solid platform. This directly addresses the HDM project's core architectural principles of user data ownership, privacy, and heterogeneous data integration. The detailed use case for diabetes self-management provides a concrete example of the type of application the HDM aims to enable.","The key insight is the necessity of a patient-centric data model (the PHL/PKG) that gives users true ownership and control over their health data. The paper emphasizes that integrating data from various sources (EHRs, Observations of Daily Living, Social Determinants of Health) into a unified, semantically rich KG is essential for enabling personalized and effective mHealth interventions. The use of Solid for decentralization is a critical architectural choice.","The paper proposes a Personal Health Library (PHL) built on the decentralized Solid platform, using knowledge graphs to integrate diverse health data and deliver personalized recommendations for diabetes self-management.","This paper details the implementation of a mobile health (mHealth) intervention for diabetes self-management, powered by a Personal Health Library (PHL). The PHL, built on the decentralized Solid platform, acts as a secure, patient-controlled repository for integrating heterogeneous health data (EHRs, Observations of Daily Living, Social Determinants of Health). By representing this data as a Personal Knowledge Graph (PKG), the system can deliver tailored recommendations to users via an mHealth app, empowering them to take a more active role in their healthcare. The paper outlines the architecture, identifies key patient requirements, and presents a use case to demonstrate the system's functionality.","How can a Personal Health Library (PHL), incorporating both digital health data and contextual knowledge, be implemented to deliver tailored recommendations for improving self-care behaviors in diabetic adults?","The methodology involves: (1) A thematic assessment of patient requirements from literature. (2) The design of a PHL architecture using the Solid platform for decentralization and privacy. (3) The use of Semantic Web technologies (RDF, ontologies) to create a Personal Knowledge Graph (PKG) for each user. (4) The development of a prototype mHealth recommender system that queries the PKG to provide personalized interventions. (5) The paper also outlines plans for a formative evaluation and a pragmatic clinical trial.","The paper primarily presents a framework and a prototype design. The key ""finding"" is the successful conceptualization and architectural design of a PHL-enabled mHealth system that meets identified patient requirements for data ownership, integration, and privacy. It demonstrates the feasibility of using Solid and KGs to build such a system.","The primary outcome is the proposed PHL framework and the initial prototype design of the mHealth recommender system for diabetes self-management. It provides a comprehensive blueprint for building patient-centric, KG-powered health applications.","The paper itself is a proposal and initial design. The system has not yet undergone the planned formative evaluation or clinical trial. The practical challenges of large-scale deployment, user adoption, and the clinical effectiveness of the recommendations are not yet evaluated.","A Personal Health Library (PHL) built on a decentralized platform like Solid provides a powerful foundation for patient-centered care. By giving patients control over their integrated health data and using knowledge graphs to derive insights, such systems can empower patients and support clinicians in delivering more effective, personalized interventions.","The paper itself is a response to the gap in patient-centric data management. It implicitly points to the need for more research into: (1) large-scale clinical validation of such systems, (2) user-centered design methodologies for PHL interfaces, and (3) scalable methods for dynamic knowledge discovery and integration within personal health contexts.","The authors state that future work will focus on the full implementation of the end-to-end framework, including text summarization and knowledge mapping features. Crucially, they plan to conduct the formative evaluation and the pragmatic clinical trial to assess the usability and effectiveness of the intervention.","Key implementation insights include: (1) The use of the Solid platform for decentralized data storage and access control. (2) The representation of patient data as a Personal Knowledge Graph (PKG) using RDF and ontologies. (3) The separation of data from applications, allowing different apps to securely access the user's PHL. (4) The use of a RESTful API and SPARQL for querying the PKG. (5) The integration of various data sources, including EHRs, Observations of Daily Living (ODLs), and Social Determinants of Health (SDoH).",https://doi.org/10.2196/24738,10.2196/24738,"ai, children, federated, healthcare, heterogeneous, integration, knowledge_graph, machine_learning, ontology, pediatric, personal, privacy, semantic"
amofa_2024,Blockchain-secure patient Digital Twin in healthcare using smart contracts,"Sandro Amofa, Qi Xia, Hu Xia, Isaac Amankona Obiri, Bonsu Adjei-Arthur, Jingcong Yang, Jianbin Gao",2024,Yes,Super,"This paper is of super relevancy as it directly addresses the security, privacy, and data management challenges of personal Digital Twins, which are core to the HDM project. The proposed architecture, which uses blockchain for data integrity and smart contracts for automated, policy-based access control, provides a robust framework for building a trustworthy, patient-centric system. This aligns perfectly with the HDM's focus on secure, heterogeneous data integration and user data ownership.","The key insight is that smart contracts can be used to automate and enforce access control policies for a patient's Digital Twin, creating a secure and programmable layer for managing sensitive health data. This approach, combined with a novel signcryption scheme (mIBSC) optimized for blockchain, provides a practical solution for ensuring data provenance, privacy, and integrity in a distributed healthcare ecosystem.","The paper proposes a blockchain-secured patient Digital Twin that uses smart contracts to automate and control access to the twin, ensuring data privacy and integrity through a novel cryptographic scheme.","This paper presents a framework for a blockchain-secured patient Digital Twin. The system uses smart contracts on the Ethereum blockchain to automate and mediate access to the Digital Twin, which is constructed from various data sources like Electronic Medical Records (EMRs) and Personal Health Records (PHRs). The authors propose a mathematical model for the Digital Twin and a novel Multi-receiver Identity-Based Signcryption (mIBSC) scheme to secure the data with a constant ciphertext size suitable for blockchain storage. The research addresses four key areas: access control, interaction, privacy, and security, and evaluates the proposed system in terms of network latency, smart contract execution costs, and data storage costs.","How can a patient's Digital Twin be secured using blockchain and smart contracts to guarantee access control, privacy, and data provenance, while automating its updates and interactions?","The methodology involves designing a three-layer architecture (Device, Blockchain, Application) and using the Ethereum blockchain with smart contracts to manage the Digital Twin. A novel Multi-receiver Identity-Based Signcryption (mIBSC) scheme is proposed to secure the data. The system's performance is evaluated based on latency, smart contract execution times, and storage costs.","The research demonstrates the feasibility of a blockchain-secured Digital Twin framework. The use of smart contracts provides a robust mechanism for automated, policy-based access control. The proposed mIBSC scheme is shown to be efficient for blockchain applications due to its constant-size ciphertext.","The primary outcome is the proposed framework for a blockchain-secure patient Digital Twin, which includes the system architecture, the smart contract design for access control and data management, and the custom mIBSC cryptographic scheme.","The evaluation is based on a prototype and does not involve a large-scale, real-world deployment with actual patient data. The paper focuses on the technical framework and does not deeply explore the ethical or legal implications of such a system.","The paper concludes that a blockchain-based Digital Twin, secured by smart contracts and appropriate cryptography, is a viable and powerful approach for managing patient health data securely and privately. This automated framework can improve data sharing, facilitate personalized medicine, and give patients more control over their health information.","The paper identifies a gap in the literature regarding robust security and privacy solutions for patient Digital Twins. It also highlights the need for cryptographic schemes that are efficient enough for practical blockchain implementation (e.g., constant ciphertext size).","Future work could involve applying the framework to a wider range of healthcare use cases, further optimizing the performance and scalability of the system, and conducting real-world clinical trials to validate its effectiveness.","Key implementation insights include: (1) The use of smart contracts to codify and automate access control policies. (2) The design of a multi-layered architecture that separates data sources, the blockchain layer, and applications. (3) The application of a custom signcryption scheme (mIBSC) tailored for the constraints of a blockchain environment. (4) The concept of representing the Digital Twin itself as a construct of interacting smart contracts.",https://doi.org/10.1371/journal.pone.0286120,10.1371/journal.pone.0286120,"ai, digital_twin, healthcare, heterogeneous, integration, llm, machine_learning, personal, privacy, semantic, survey"
anderson_2016,Mind the Gap: Two Dissociable Mechanisms of Temporal Processing in the Auditory System,"Lucy A. Anderson, Jennifer F. Linden",2016,Yes,Low,"This paper is of low relevancy. While it deals with ""temporal processing,"" it does so at a neurophysiological level (millisecond-scale auditory perception in mice), which is not applicable to the HDM project's focus on temporal knowledge graphs, heterogeneous data integration, or system architecture.","The key insight is that the brain's ability to process rapid sounds may rely on two distinct channels: one for detecting sound onsets and another for sound offsets. Deficits in temporal hearing, such as difficulty detecting brief gaps in noise, could stem from a specific failure in the offset-detection channel rather than a general 'sluggishness' of the auditory system.","The paper shows that difficulty in hearing brief gaps in noise can be caused by a specific problem in the brain's ability to process the *end* of a sound, not the beginning, suggesting separate brain circuits for 'on' and 'off' sound detection.","This study uses a mouse model with auditory processing deficits to investigate the neural basis of gap-in-noise detection. Through extracellular recordings in the auditory thalamus, the researchers found that these mice have a specific deficit in neural sensitivity to brief gaps, which is linked to reduced neural responses to sound offsets, while responses to sound onsets and other rapidly changing sounds remain normal. They propose a model with two separate channels—one for sound onsets and one for offsets—and show that weakening the offset channel in the model replicates their experimental findings. This suggests that gap-detection deficits can arise from a specific impairment of the sound-offset-sensitive channel, revealing a dissociation in how the brain processes the beginning and end of sounds.","What is the neural mechanism underlying deficits in auditory temporal processing, specifically the detection of brief gaps in noise?","The study used a mouse model of gap-detection deficits (ectopic BXSB/MpJ-Yaa mice). They performed in-vivo extracellular recordings from three subdivisions of the auditory thalamus in anesthetized mice. They presented various auditory stimuli, including gap-in-noise stimuli and click trains, and developed a phenomenological model to test their hypothesis about dissociable onset and offset channels.","The study found that: (1) Ectopic mice have a deficit in thalamic sensitivity to brief gaps in noise, specific to certain auditory pathways. (2) This deficit is not due to a general loss of temporal acuity, as responses to other rapid stimuli are normal. (3) The deficit is specifically linked to reduced neural activity following sound *offsets*, not onsets. (4) In control mice, offset-responsive neurons are exceptionally sensitive to brief gaps, and this sensitivity is lost in ectopic mice.",The primary outcome is the experimental evidence and a supporting computational model demonstrating the existence of two dissociable mechanisms for auditory temporal processing (onset-sensitive and offset-sensitive). It shows that deficits in gap detection can arise specifically from impairment of the offset-sensitive channel.,"The study was conducted on anesthetized mice, which might affect neural responses. The model is phenomenological and not a detailed, physiologically realistic model of the auditory pathway. The link between the cortical ectopias and the thalamic deficit is correlational, and the causal mechanism remains unclear.","The brain has separate mechanisms for processing the start and end of sounds. Deficits in auditory temporal acuity, like the inability to detect brief gaps in noise, can be caused by a specific problem with the brain's 'sound-off' detectors, rather than a general slowdown in auditory processing.","The paper points to the need to understand the precise origin of the offset-response deficit (e.g., brainstem, thalamus, or cortical feedback). The causal relationship between the cortical ectopias and the thalamic deficit is also an open research question.",Future work should focus on further experiments to explore the origin of the offset-response deficit and its relationship to the cortical ectopias.,"This paper is from a fundamental neuroscience domain and has no direct implementation insights for building software, data systems, or knowledge graphs. The concepts are too low-level and biological to be applicable to the HDM project.",https://doi.org/10.1523/jneurosci.1652-15.2016,10.1523/JNEUROSCI.1652-15.2016,"neuroscience, auditory_processing, temporal, thalamus, mouse_model"
anokhin_2024,AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents,"Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, Evgeny Burnaev",2024,Yes,Super,"This paper is of super relevancy because it proposes a novel architecture, AriGraph, for an LLM agent to build a dynamic world model by integrating semantic and episodic memory into a knowledge graph. This directly addresses the HDM project's need for a system that can learn from a user's experiences over time and structure that knowledge for reasoning and planning. The concept of an agent actively constructing its own KG from observations is a core principle for the HDM.","The key insight is that integrating both semantic (factual) and episodic (experiential) memory into a unified, dynamic knowledge graph allows an LLM agent to build a structured world model from scratch. This structured memory significantly outperforms unstructured memory representations (like RAG or summarization) in complex, interactive environments that require long-term reasoning and planning.","An LLM agent can learn a structured world model by building a knowledge graph that combines factual knowledge with personal experiences, enabling it to solve complex tasks more effectively than agents with unstructured memory.","This paper introduces AriGraph, a novel method where an LLM-based agent, named Ariadne, constructs and updates a memory graph that integrates both semantic (factual) and episodic (experiential) memories as it interacts with an environment. This approach is designed to overcome the limitations of unstructured memory in current LLM agents, which hinders complex reasoning and planning. The AriGraph architecture allows the agent to build a structured world model from scratch, which is shown to significantly outperform other memory methods (like full history, summarization, and RAG) and RL baselines in complex text-based games.","Can LLM-based agents learn a useful structured world model from scratch via interaction with an environment, and does this structured knowledge representation improve the retrieval of relevant facts and enable effective exploration?","The authors developed the AriGraph framework, where an LLM agent (Ariadne) extracts semantic triplets from textual observations to build a semantic KG. Episodic memory is captured by linking these triplets to the specific observation (episodic vertex) in which they occurred. The agent uses this memory graph for planning and decision-making in interactive environments (TextWorld, NetHack). The performance was evaluated against various memory baselines (full history, RAG, summarization, etc.) and RL agents.","The AriGraph agent (Ariadne) markedly outperforms other memory methods and strong RL baselines in complex interactive tasks. The structured memory enables the agent to effectively learn from its interactions, build a coherent world model, and solve tasks that are difficult even for human players. The approach also shows competitive performance on static multi-hop Q&A tasks.","The primary outcome is the AriGraph framework itself, a novel memory architecture that demonstrates how to effectively integrate semantic and episodic memory in a knowledge graph for LLM agents. The paper also presents the Ariadne agent as a proof-of-concept for this architecture.","The current approach is focused on text-based environments. The scalability of the graph construction and retrieval for very large, long-term interactions is not fully evaluated. The paper also notes that the quality of the graph construction depends on the capability of the LLM backbone.","The paper concludes that a knowledge graph world model that integrates both semantic and episodic memory is a powerful approach for building more capable LLM agents. This structured memory representation is crucial for enabling effective reasoning, planning, and exploration in complex, partially observable environments.",The paper implicitly points to the need for extending this approach to multi-modal environments (beyond text). It also highlights the need for more sophisticated graph search and retrieval methods for even larger and more complex memory graphs.,"The authors state that future work can focus on incorporating multi-modal observations, procedural memories (skills), and more advanced graph search methods into the AriGraph framework.",Key implementation insights include: (1) The dual structure of the memory graph with semantic vertices/edges and episodic vertices/edges. (2) The process of continuously updating the graph by extracting triplets from new observations and removing outdated knowledge. (3) The two-stage retrieval process (semantic search followed by episodic search) to find relevant information. (4) The use of hyperedges to connect multiple semantic triplets to a single episodic event.,https://arxiv.org/abs/2407.04363,10.48550/arXiv.2407.04363,"ai, knowledge_graph, llm, memory, episodic_memory, semantic_memory, agent, temporal, reasoning"
anzia_2018,Multimodal joint learning for personal knowledge base construction from Twitter-based lifelogs,"Yen An-Zia, Huang Hen-Hsenb, Chen Hsin-Hsia",2018,Yes,Low,"This paper is of low relevancy. While it addresses personal knowledge base construction, its focus on extracting general life events (e.g., dining, visiting places) from Twitter is not aligned with the HDM project's core focus on integrating complex, heterogeneous personal data for deep, longitudinal analysis. The NLP methods for short, informal text are only a small part of the HDM's data integration challenge.","The key insight is that multimodal information (text and images) from social media can be jointly learned to detect and extract personal life events, even when they are expressed implicitly. This is useful for constructing a personal knowledge base from user-generated content.",The paper proposes a system to automatically extract personal life events from Twitter posts (text and images) to build a personal knowledge base for individuals.,"This paper presents a multimodal joint learning approach to extract personal life events from Twitter data (tweets and images). The system aims to identify both explicit and implicit life events, extract their components (subject, predicate, object, time), and map them to a structured personal knowledge base. The authors collected and annotated a dataset from 18 Twitter users and show that their system is effective in extracting these events, which could be used for applications like memory recall.",How can we leverage both visual and textual information from social media (Twitter) to automatically extract personal life events and construct a personal knowledge base for individuals?,"The authors propose a two-stage system. The first stage uses a multimodal joint learning neural network (BiLSTM) to detect if a tweet contains a life event and to recognize explicit and implicit event predicates. The second stage uses a BiLSTM-CRF model to extract the subject, object, and time for each event and performs frame semantic parsing to generate structured quadruples. The model is trained and evaluated on a custom-annotated dataset of tweets from 18 users.",The multimodal joint learning approach is effective for life event extraction from tweets. The model achieves high performance in detecting whether a tweet contains a life event and can recognize both explicit and implicit events. Visual information from images provides complementary cues that improve the recognition of implicit life events.,The primary outcome is a comprehensive system for extracting personal life events from Twitter data and constructing a personal knowledge base. The authors also released the annotated lifelog dataset.,"The dataset is limited to 18 Twitter users and is primarily in Chinese, which may limit generalizability. The system's performance is dependent on the quality of the underlying NLP tools (e.g., POS tagger) and the image captioning API. The mapping from natural language predicates to KB relations can be ambiguous.",The paper concludes that a multimodal joint learning approach is effective for extracting personal life events from social media data. The constructed personal knowledge bases are expected to be useful for applications like memory recall and living assistance.,"The paper highlights the challenge of handling the ambiguity in mapping natural language predicates to knowledge base relations. It also implicitly points to the need for more robust methods for handling noisy, informal user-generated text.","Future work could involve improving the accuracy of the event extraction, expanding the system to other social media platforms and languages, and developing more advanced applications on top of the constructed personal knowledge bases.","The two-stage architecture, separating event detection/predicate recognition from argument extraction, is a practical design pattern. The use of a multimodal approach, combining text and image features, is a key insight for handling social media data. The application of joint learning for related tasks is shown to be effective.",https://doi.org/10.1016/j.ipm.2019.102148,10.1016/j.ipm.2019.102148,"personal_knowledge_base, lifelogging, event_extraction, social_media, twitter, multimodal_learning, nlp"
asprino_2022,Uncovering Values: Detecting Latent Moral Content from Natural Language with Explainable and Non-Trained Methods,"Luigi Asprino, Luana Bulla, Stefano De Giorgis, Aldo Gangemi, Ludovica Marinucci, Misael Mongiovi",2022,Yes,Medium,"This paper is relevant because it explores using knowledge graphs for semantic extraction of nuanced concepts like moral values from text. This is applicable to the HDM project's goal of understanding a user's personal context, values, and sentiments from their data (e.g., journals, notes). While not directly about heterogeneous data integration, the explainable, frame-based approach could inform how the HDM system models and reasons about a user's personal worldview.","The key insight is that explainable, non-trained methods can achieve considerable performance in detecting complex, latent content like moral values, offering a transparent alternative to black-box supervised models.","This paper presents two approaches for detecting moral attitudes from natural language: a frame-based symbolic value detector using knowledge graphs and a zero-shot machine learning model without prior moral value training. The methods are tested on a corpus of tweets annotated according to Moral Foundation Theory, demonstrating that both can achieve considerable performance without the need for prior training.","This paper presents two approaches for detecting moral attitudes from natural language: a frame-based symbolic value detector using knowledge graphs and a zero-shot machine learning model without prior moral value training. The methods are tested on a corpus of tweets annotated according to Moral Foundation Theory, demonstrating that both can achieve considerable performance without the need for prior training.","How can moral attitudes be rapidly extracted from natural language to understand social interaction dynamics and individual cognitive dimensions, using explainable and non-trained methods?","The paper develops and compares two methods: (1) a frame-based symbolic value detector using knowledge graphs (FRED, Framester, ValueNet) and (2) a zero-shot machine learning model (BART-large fine-tuned on NLI and emotion detection). Both are tested on the Moral Foundation Twitter Corpus (MFTC).",Both the zero-shot and frame-based approaches achieve considerable performance (around 45% F1 score) without requiring prior training on moral value detection tasks. The frame-based approach offers the advantage of explainability by tracking the triggers for value detection.,"The development of two novel, non-trained methods for detecting moral content in text, demonstrating the feasibility of using both neural and symbolic approaches for this task.","The evaluation is limited to Twitter data, which has a fragmented syntax. The performance could be improved. The frame-based detector's performance can be affected by the quality of the knowledge graph generated from the input text.","The paper demonstrates the potential for using unsupervised, domain-independent methods to extract nuanced semantic meaning, like moral values, from natural language. It shows that both zero-shot and knowledge-graph-based approaches are viable alternatives to traditional supervised methods.",The paper doesn't explore combining the strengths of both the neural and symbolic approaches in a more integrated way. The evaluation is on a single dataset and could be expanded to other text types.,"Future work could explore the automatic discovery of knowledge patterns and their mappings, potentially drawing from research in analogical reasoning. They also mention the possibility of developing a run-time version of the pattern application mechanism.","The use of Framester and ValueNet provides a practical example of leveraging existing semantic web resources for a specific NLP task. The frame-based approach offers a transparent and explainable alternative to black-box models, which is a valuable consideration for systems like HDM where user trust is important.",https://aclanthology.org/2022.deelio-1.4/,10.18653/v1/2022.deelio-1.4,"Moral Values, Natural Language Processing, Knowledge Graphs, Zero-shot Learning, Semantic Extraction, Explainable AI"
asprino_2023,Knowledge Graph Construction with a Façade: A Unified Method to Access Heterogeneous Data Sources on the Web,"Luigi Asprino, Enrico Daga, Aldo Gangemi, Paul Mulholland",2023,Yes,Super,"This paper is of super relevancy to the HDM project. It directly addresses the core challenge of integrating heterogeneous data sources by proposing Facade-X, a unified meta-model for accessing and transforming diverse data formats (CSV, JSON, XML, etc.) into RDF. This approach, implemented in the SPARQL Anything tool, provides a practical and scalable solution for the HDM's data ingestion pipeline, allowing seamless integration of personal data from various sources into a personal knowledge graph.","The key insight is that it's possible to decouple the structural transformation of data from the semantic mapping by using a unified meta-model (Facade-X) as a 'façade'. This allows KG engineers to interact with heterogeneous data sources using only their existing expertise in RDF and SPARQL, significantly simplifying the KGC process and reducing the complexity of data processing pipelines.","This paper introduces Facade-X, a meta-model for constructing knowledge graphs from heterogeneous data sources on the Web. Implemented in a system called SPARQL Anything, this approach provides a unified method for accessing and transforming various data formats (like CSV, JSON, and XML) into RDF. By acting as a 'façade', the meta-model simplifies the knowledge graph construction process, allowing developers to use SPARQL for both data inspection and transformation, thus streamlining data integration.",How can we create a unified and simplified method for knowledge graph construction that allows KG engineers to access and transform heterogeneous data sources on the Web using their existing expertise in RDF and SPARQL?,"The paper proposes Facade-X, a meta-model based on a set of structural design patterns (containment, ordering, key-values, typing) that can represent data from any format expressible in BNF syntax and relational databases. This meta-model is implemented in SPARQL Anything, a system that uses a virtual SPARQL endpoint to transform non-RDF data into RDF on the fly, making it queryable with standard SPARQL.","The Facade-X meta-model is proven to be generic enough to cover a wide range of structured data formats. The SPARQL Anything implementation demonstrates a significant reduction in the cognitive complexity of mappings compared to state-of-the-art tools like RML and ShExML. Performance evaluations show that the approach is viable, with a triple-filtering strategy offering significant improvements.","The primary outcome is a novel, theoretically sound, and practically implemented approach to knowledge graph construction that simplifies the integration of heterogeneous data. This includes the Facade-X meta-model and the SPARQL Anything tool, which provides a unified interface for querying diverse data formats as RDF.","The performance of the naive implementation can be slow, although the triple-filtering strategy shows significant improvements. The approach relies on the 'façade engineer' to design and map new formats, which could be a bottleneck if a wide variety of new formats need to be supported.","The paper provides a powerful demonstration that a unified meta-model can dramatically simplify the process of knowledge graph construction from heterogeneous sources. By abstracting away the complexities of individual data formats, the 'façade' approach empowers developers to focus on the semantic aspects of data integration, using familiar tools like SPARQL.",The paper identifies the need for further performance optimization and more extensive usability testing with the user community. It also opens up the challenge of extending the Facade-X meta-model to support an even wider range of data formats and more complex data structures.,"Future work includes further performance improvements, the development of more sophisticated query optimization techniques for the virtual SPARQL endpoint, and the extension of the Facade-X meta-model to cover more data formats. The authors also plan to engage further with the user community to improve the usability of the SPARQL Anything tool.","The paper offers a highly practical implementation insight for the HDM project: use SPARQL Anything as a core component of the data ingestion pipeline. This would allow the HDM to easily integrate data from a user's various personal sources (e.g., CSV exports from fitness apps, JSON from social media, etc.) into their personal knowledge graph without needing to write custom parsers for each format.",https://dl.acm.org/doi/10.1145/3555312,10.1145/3555312,"Knowledge Graph Construction, Heterogeneous Data, Data Integration, RDF, SPARQL, Meta-model, Facade-X, Semantic Web",
azevedo_2023,A Polystore Architecture Using Knowledge Graphs to Support Queries on Heterogeneous Data Stores,"Leonardo Guerreiro Azevedo, Renan Francisco Santos Souza, Elton F. de S. Soares, Raphael M. Thiago, Julio Cesar Cardoso Tesolin, Anna C. Oliveira, Marcio Ferreira Moreno",2023,Yes,Super,"This paper is of super relevancy as it proposes a complete polystore architecture, HKPoly, designed to query heterogeneous data stores through a unified Knowledge Graph-based global schema. This directly addresses the HDM project's core challenge of integrating disparate personal data sources. The architecture's use of provenance to create explicit links between data fragments and its focus on reducing query complexity for the end-user provide a strong theoretical and practical foundation for the HDM system design, particularly for the upstream data processing and integration layer.","The key insight is the use of a Knowledge Graph not just as a data model, but as a comprehensive meta-layer that stores the global schema, local schemas, mappings, and provenance data. This allows the system to dynamically generate queries to underlying heterogeneous sources (via something like PostgreSQL FDWs) while presenting a simple, unified view to the user, effectively abstracting away the complexity of data location, format, and linkage.","The paper presents HKPoly, a polystore architecture that uses a Knowledge Graph to create a unified query layer over different types of databases (SQL, NoSQL, etc.). It uses provenance data to automatically link related information across these databases, making it much simpler for users to query complex, fragmented data.","This paper tackles the challenge of querying heterogeneous data residing in different, unlinked data stores. The authors propose a federated database architecture, HKPoly, which provides a single global conceptual schema to users. This schema, represented as a Knowledge Graph, encapsulates data heterogeneity, location, and linkage. The system uses meta-models for the global and local schemas, mappings between them, and captures provenance data from workflows to create explicit links between data. The architecture was implemented as a polystore service and evaluated in an Oil & Gas industry scenario, showing it could reduce query complexity by half with a manageable performance overhead (under 30%).",How to query data that are not explicitly connected residing on heterogeneous data stores?,"The authors propose a polystore architecture (HKPoly) based on the Mediator/Wrapper MDBS pattern. They use a Knowledge Graph (implemented with Hyperknowledge) to store a global conceptual schema, local schemas for each data source, and mappings between them. Data linkage is achieved by capturing workflow provenance using ProvLake. The architecture is implemented as a RESTful service and evaluated by comparing its query complexity (for the user) and query performance (for the database) against a standard PostgreSQL Foreign Data Wrapper (FDW) setup in a simulated Oil & Gas use case.","The proposed architecture allows for writing queries that are more than two times less complex for the user compared to a standard relational multidatabase system (PostgreSQL FDW). The trade-off is a performance overhead in query processing time that does not exceed 30%, which the authors deem acceptable.","The primary outcome is the HKPoly architecture itself, a viable polystore system that successfully uses a Knowledge Graph and provenance to provide a unified, simplified query interface over heterogeneous data stores.","The query processing time, while deemed acceptable, still has an overhead of up to 30% compared to a direct FDW implementation, indicating a need for further optimization. The complexity of setting up the GCS, LCS, and mappings relies on a Knowledge Engineer, which could be a bottleneck.","The paper concludes that the proposed polystore architecture, HKPoly, is a valid and useful approach for querying heterogeneous data stores. By using a Knowledge Graph for schema and provenance management, it significantly reduces the cognitive load on users writing queries, making integrated data access more feasible, with an acceptable performance trade-off.",The paper highlights the need for further research into query optimization for such polystore systems to reduce the performance overhead. Automating the creation of schemas and mappings is another area for future work.,"Future work includes improving query processing performance, further investigation into query optimization techniques, and potentially automating parts of the schema and mapping creation process.","A key insight is the practical use of PostgreSQL Foreign Data Wrappers (FDW) as the ""last mile"" connector to the underlying data stores. This is a pragmatic approach that leverages existing, robust technology. The use of a Knowledge Graph to store all the metadata (schemas, mappings, provenance) in a queryable format is the central implementation pattern that enables the entire system. The idea of using HK's `Context` node to modularize the knowledge is also a useful organizational pattern.",https://doi.org/10.1007/s00778-017-0474-5,10.1007/s00778-017-0474-5,"Polystore, Heterogeneous Data, Knowledge Graph, Data Integration, Federated Query, Provenance, Schema Mapping, Mediator-Wrapper"
bai_2023,Membership Inference Attacks and Defenses in Federated Learning: A Survey,"Li Bai, Haibo Hu, Qingqing Ye, Haoyang Li, Leixia Wang, Jianliang Xu",2023,Yes,Medium,"This paper is of medium relevancy. While not directly focused on heterogeneous data integration or temporal architecture, it provides a comprehensive survey of privacy risks (Membership Inference Attacks) in Federated Learning. As the HDM project will handle sensitive personal data in a distributed manner, understanding these attack vectors and their defenses (like secure aggregation and differential privacy) is crucial for designing a secure and privacy-preserving system architecture.","The key insight is that Federated Learning, despite its privacy-preserving design, is highly vulnerable to Membership Inference Attacks (MIAs). The paper systematically categorizes these attacks (update-based vs. trend-based) and defenses, highlighting that the decentralized nature of FL creates unique attack surfaces (e.g., malicious clients, access to historical models) that differ significantly from centralized learning environments.",This paper provides a comprehensive survey of Membership Inference Attacks (MIAs) and their corresponding defenses within the context of Federated Learning (FL).,"This paper provides a comprehensive survey of Membership Inference Attacks (MIAs) and their corresponding defenses within the context of Federated Learning (FL). It introduces a taxonomy for both attacks and defenses, systematically reviewing existing literature. The authors highlight the unique privacy vulnerabilities in FL, where adversaries can be insiders (server or clients) and can exploit model updates and convergence trends to determine if a specific data record was used in training.","What are the unique membership inference attack surfaces in Federated Learning, and what are the state-of-the-art defense mechanisms to mitigate them?","The authors conduct a systematic literature review, categorizing existing research on MIAs in Federated Learning. They create a taxonomy for attacks (update-based and trend-based) and defenses (partial sharing, secure aggregation, noise perturbation, anomaly detection). The paper analyzes the strengths and weaknesses of each approach and compares them to attacks and defenses in centralized learning settings.","The survey reveals that FL is vulnerable to powerful MIAs from insiders (server or other clients) who can exploit access to model gradients, parameters, and historical versions. It identifies two main attack vectors: update-based attacks that analyze model parameters/gradients at specific points in time, and trend-based attacks that analyze the trajectory of indicators (like loss or prediction confidence) over time. The paper also systematically reviews defenses, noting their respective trade-offs between privacy, utility, and computational cost.",The primary outcome is a comprehensive and structured overview of the state-of-the-art in Membership Inference Attacks and defenses specifically for Federated Learning environments. This provides a clear roadmap for researchers and practitioners to understand the privacy risks and available countermeasures.,"As a survey, the paper synthesizes existing work and does not propose new attack or defense methods. The rapid evolution of the field means new techniques may have emerged since its publication.","Federated Learning is not inherently private. A deep understanding of the specific attack vectors, such as those exploiting model updates and convergence trends, is essential for designing robust, privacy-preserving distributed machine learning systems. Defenses must be carefully chosen based on the specific threat model and the acceptable trade-offs between privacy and model utility.","The paper identifies the need for more research into robust and efficient defense mechanisms that can withstand sophisticated, active attacks from malicious insiders. It also points to the need for better theoretical understanding of the privacy-utility trade-offs for different defense strategies in the FL context.","Future research should focus on developing more advanced and computationally efficient defense mechanisms, particularly against active and colluding adversaries. There is also a need for more comprehensive benchmarks to evaluate the effectiveness of different defense strategies under various FL scenarios.","The paper provides crucial implementation insights for the HDM project by detailing specific defense mechanisms. For example, it discusses the use of Secure Multi-party Computation (SMC) and Homomorphic Encryption (HE) for secure aggregation, and the application of Differential Privacy to add noise to model updates. These are all practical techniques that can be considered when designing the HDM's privacy architecture.",https://doi.org/10.1109/nana53684.2021.00062,10.1109/NaNA53684.2021.00062,"Federated Learning, Privacy, Membership Inference Attack, Security, Survey, Differential Privacy, Secure Aggregation"
balog_2019,Personal Knowledge Graphs: A Research Agenda,"Krisztian Balog, Tom Kenter",2019,Yes,Super,"This is a foundational paper that defines the concept of Personal Knowledge Graphs (PKGs) and outlines a research agenda. It's highly relevant to the HDM project as it directly addresses the core concepts of personal data management, the unique characteristics of PKGs (user-centric, sparse, temporal), and the challenges in their construction and use. This paper provides the ""why"" and ""what"" for the HDM project's ""how"".","The key insight is the distinction between *personal* KGs and *personalized* KGs. A personalized KG is a filtered/customized view of a general KG, whereas a personal KG contains entities that may not exist in any public KG (e.g., ""my mom's dentist""). This highlights the need for a fundamentally different approach to KG construction and maintenance, one that is user-centric and can handle sparse, ephemeral, and private data.","This position paper defines Personal Knowledge Graphs (PKGs) as user-centric knowledge bases containing entities of personal importance, outlines the key challenges in their construction (representation, entity linking, population) and use, and proposes a research agenda for the field.","The authors introduce the concept of a Personal Knowledge Graph (PKG) as a resource of structured information about entities personally related to a user. They differentiate PKGs from general-purpose KGs by three key aspects: (1) they include entities of personal interest, (2) they have a ""spiderweb"" layout with the user at the center, and (3) they are inherently integrated with external data sources. The paper identifies and discusses four main problem areas: Knowledge Representation, Semantic Annotation of Text, Population and Maintenance, and Integration with External Sources, formulating specific research questions for each.","What are the fundamental challenges and research directions for creating, maintaining, and utilizing Personal Knowledge Graphs?","This is a position paper and a research agenda, so the methodology is a conceptual analysis and a synthesis of existing work to define a new research area. It identifies key problems and formulates research questions.","The paper's key ""finding"" is the articulation of the unique properties and challenges of PKGs. It establishes that PKGs require different approaches for knowledge representation (handling sparse and short-lived relations), entity linking (dealing with entities with no digital footprint), population (requiring automatic, user-in-the-loop methods), and integration (continuous, two-way synchronization).","The primary outcome is the definition of a research agenda for Personal Knowledge Graphs, providing a foundational framework for future work in this area. It clearly separates PKGs as a distinct research subfield.","As a position paper, it does not offer concrete solutions or implementations. It raises questions rather than answering them. The authors acknowledge that creating large-scale, open datasets for PKG research is a significant challenge due to privacy concerns.","The paper concludes by emphasizing the need for a coordinated research effort to address the challenges of PKGs, which are seen as a foundational component for truly personal intelligent assistants and other personalized services.","The paper itself is a map of research gaps. Key gaps identified include: how to represent sparse and temporal personal knowledge, how to perform entity linking for ""long-tail"" personal entities, how to automatically populate and maintain PKGs with user-in-the-loop verification, and how to manage continuous, two-way integration with external sources.","The entire paper is a call for future work, structured around the research questions posed for each of the four problem areas.","The paper provides high-level implementation insights. The ""spiderweb"" architecture with the user at the center is a core design principle. The need for handling sparse data and short-lived relations suggests that the underlying data model must be flexible. The requirement for user-in-the-loop verification points to the need for interactive and explainable KG management tools.",https://doi.org/10.1145/3341981.3344241,10.1145/3341981.3344241,"Personal Knowledge Graph, PKG, Research Agenda, Knowledge Representation, Entity Linking, Personal Information Management, Data Integration, Semantic Web"
banking_2022,Incremental Analysis of Legacy Applica4ons Using Knowledge Graphs for Applica4on Moderniza4on,"Saravanan Krishnan, Alex Mathai, Amith Singhee, Atul Kumar, Shivali Agarwal, Keerthi Narayan Raghunath, David Wenk",2022,Yes,Low,"The paper focuses on using knowledge graphs for legacy software modernization, which is not the core focus of the HDM project. However, the approach of using a KG to model and analyze complex systems is tangentially relevant.","The key insight is that a knowledge graph can serve as a powerful abstraction layer for understanding and analyzing large, complex legacy codebases. By representing code artifacts (programs, transactions, tables) and their dependencies as a KG, it becomes possible to perform incremental analysis, identify logical boundaries, and plan modernization efforts without getting lost in the low-level code details.","The paper presents a tool that uses a knowledge graph to analyze large legacy software systems, allowing experts to incrementally define and analyze parts of the system for modernization.","This paper presents a so6ware system analysis tool that allows a subject ma=er expert (SME) or system architect to analyze a large so6ware system incrementally. We analyze the source code and other arGfacts (such as data schema) to create a knowledge graph using a customizable ontology/schema. EnGGes and relaGons in our ontology can be defined for any combinaGon of programming languages and plaVorms. Using this knowledge graph, the analyst can then define logical boundaries around dependent EnGGes (e.g. Programs, TransacGons, Database Tables etc.). Our tool then presents different views showcasing the dependencies from the newly defined boundary to/from the other logical groups of the system. This exercise is repeated interacGvely to 1) IdenGfy the EnGGes and groupings of interest for a modernizaGon task and 2) Understand how a change in one part of the system may affect the other parts. To validate the efficacy of our tool, we provide an iniGal study of our system on two client applicaGons.",How can a large legacy software system be analyzed incrementally to support its modernization?,"The authors developed a software analysis tool that: 1) Uses a staGc analyzer (ADDI) to parse legacy source code. 2) Creates a knowledge graph in a Neo4j database using a custom, language-agnosGc ontology to represent code arGfacts and their relaGonships. 3) Allows a user (SME/architect) to interacGvely define ""increments"" (logical groups of arGfacts) and analyzes the dependencies (inside-out and outside-in edges) between the increment and the rest of the system.","The knowledge graph approach allows for an effective incremental analysis of complex legacy systems. It successfully abstracts away the complexity of the underlying code, enabling SMEs to identify logical boundaries and dependencies for modernization. The tool was validated on two client applications, showing a 20% increase in productivity.","The primary outcome is the tool itself, which provides a practical solution for the incremental analysis of legacy applications using knowledge graphs.",The current tool relies on staGc analysis. The quality of the analysis depends on the completeness of the staGc analyzer and the richness of the ontology defined by SMEs.,The paper concludes that using a knowledge graph for incremental analysis is a beneficial approach for modernizing legacy applications. It provides a structured way to understand complex dependencies and plan modernization efforts effectively.,"The paper suggests that future work should go beyond staGc analysis to include insights from data within the applicaGon (e.g., tables) and from operaGonal logs.","Future work includes understanding the data of the applicaGon (e.g., tables) and extracGng insights from operaGonal logs to enrich the knowledge graph.","A key implementation insight is the use of a language-agnosGc ontology, which makes the approach extensible to different legacy systems. Using a graph database like Neo4j allows for efficient traversal and analysis of the complex dependencies within the software system. The concept of ""increments"" with ""inside-out"" and ""outside-in"" dependency analysis is a practical pa=ern for managing the complexity of modernizaGon projects.",https://doi.org/10.1145/3493700.3493735,10.1145/3493700.3493735,"Knowledge Graph, Software Engineering, Legacy Modernization, Static Analysis, Ontology"
bellomarini_2024a,Privacy-Preserving Synthetically Augmented Knowledge Graphs with Semantic Utility,"Luigi Bellomarini, Costanza Catalano, Andrea Coletta, Michela Iezzi, Pierangela Samarati",2024,Yes,High,"This paper is highly relevant because it directly tackles the privacy concerns inherent in managing and sharing personal knowledge graphs. The HDM project needs robust privacy-preserving mechanisms, and this paper's focus on structural anonymization, protecting against re-identification attacks that exploit derived knowledge (reasoning), and maintaining semantic utility is directly applicable.","The key insight is that traditional graph anonymization techniques are insufficient for Knowledge Graphs because they don't account for new knowledge that can be derived through reasoning. An attacker can use these derived facts to break the anonymity. The paper introduces a privacy model ((k, x)-isomorphism anonymization) that explicitly considers this derived knowledge, ensuring a much stronger privacy guarantee.","This paper proposes a novel framework for sharing Knowledge Graphs while preserving privacy. It introduces a structural anonymization technique that synthetically augments the KG to prevent re-identification attacks, even when an attacker can use reasoning to derive new facts. The framework includes a new privacy measure that accounts for this derived knowledge and a utility metric to ensure the anonymized graph remains useful for downstream tasks.","How can knowledge graphs be shared while protecting sensitive information and maintaining semantic utility, especially when new knowledge can be derived through reasoning?","The authors propose a novel structural anonymization methodology based on (k, x)-isomorphism, which ensures that for any subgraph of size x, there are at least k-1 other structurally indistinguishable subgraphs, even after accounting for derived edges. They introduce two algorithms, KLONE and KGUARD, to achieve this. The approach is evaluated on synthetic and real-world datasets, measuring both privacy (δ-anonymity) and utility (Jaccard similarity of query results).","The proposed methods, KLONE and KGUARD, achieve perfect δ-anonymity (100% of subgraphs are not uniquely identifiable), significantly outperforming existing graph anonymization techniques that don't consider derived knowledge (which can leave up to 40% of subgraphs vulnerable). KGUARD is shown to be more efficient, adding fewer synthetic nodes and edges while preserving higher utility.","The primary outcome is a novel, robust framework for privacy-preserving KG sharing that considers the unique threat posed by logical reasoning. This includes the (k, x)-isomorphism privacy definition and two practical algorithms (KLONE and KGUARD) to implement it.","The proposed algorithms can have exponential worst-case complexity, although they perform well empirically. The paper focuses on structural anonymization and does not deeply explore the trade-offs with other privacy techniques like differential privacy. The utility metric is based on a predefined set of queries, which might not capture all possible uses of the KG.","Existing graph anonymization techniques are not sufficient for Knowledge Graphs. To ensure privacy, one must account for the new facts that can be inferred through reasoning. Structural anonymization through synthetic graph augmentation is a viable approach to achieve strong privacy guarantees while maintaining the utility of the KG for specific business tasks.","The paper identifies the need for more efficient anonymization algorithms, especially for very large graphs. It also suggests that future work could explore more complex diversity requirements beyond just node degrees and labels.","Future work includes investigating the split & merge procedure to speed up computation, and extending the diversity requirements beyond vertex labels and relationships.","The paper provides two concrete algorithms, KLONE and KGUARD, for implementing privacy-preserving KG augmentation. The concept of using a utility metric based on the Jaccard similarity of query results is a practical way to measure the usefulness of the anonymized graph. The idea of bucketing isomorphic subgraphs (in KGUARD) is a key technique for reducing the number of required modifications.",https://arxiv.org/abs/2410.12418,10.48550/arXiv.2410.12418,"Privacy-Preserving, Knowledge Graphs, Synthetic Data, Anonymization, Semantic Utility, Reasoning",
beltran_2023,"ArticleCo-Design, Development, and Evaluation of a Health Monitoring Tool Using Smartwatch Data: A Proof-of-Concept Study","Ruhi Kiran Bajaj, Rebecca Mary Meiring, Fernando Beltran",2023,Yes,Low,"This paper is tangentially related. It focuses on the front-end application and usability aspects of using personal health data (from smartwatches) for clinical monitoring, rather than the back-end challenges of heterogeneous data integration, which is the core focus of the HDM project. However, it provides valuable insights into the user (HCP) requirements for such systems.","The key insight is the critical importance of the co-design process with healthcare professionals (HCPs) to ensure the clinical utility and adoption of health monitoring tools. Simply providing raw data is not enough; tools must be designed to fit into clinical workflows and reduce, not increase, the workload of HCPs.","This study details the co-design, development, and evaluation of a web-based health monitoring tool that uses machine learning to detect anomalies in smartwatch data. The prototype was designed with input from healthcare professionals (HCPs) and evaluated for its usability. The results show that while HCPs see the potential, successful clinical integration requires careful design to meet their specific needs and workflow constraints.",How can a health monitoring tool using smartwatch data be effectively designed and developed to support healthcare professionals in clinical decision-making?,The study employed a three-phase Design Science Research (DSR) methodology: (1) A co-design phase where requirements were gathered from eight HCPs via an online survey. (2) A development phase where a web-based prototype was built using a public smartwatch dataset (PMData) and a clustering-based machine learning algorithm (from the PyCaret library) for anomaly detection. (3) An evaluation phase where the HCPs assessed the prototype's usability using the mHealth App Usability Questionnaire (MAUQ).,"The co-design process successfully identified key HCP requirements, such as the importance of heart rate data and the preference for graphical summaries. The resulting prototype received positive usability scores from over 60% of the HCPs, particularly for its ease of use and interface design. The study also found that HCPs would prefer a workflow where healthcare administrators first review the data to reduce the clinicians' workload.",The primary outcome is a proof-of-concept web application that demonstrates the feasibility of using smartwatch data and machine learning to create a health monitoring tool for HCPs. The study also provides a set of design guidelines and user requirements for future development in this area.,"The study has a small sample size of HCPs, which may not be representative. The machine learning algorithm used was a basic example and was not clinically validated. The study also did not include the perspectives of patients.","The study concludes that integrating smartwatch data into clinical care is feasible and potentially beneficial, but its success is highly dependent on a user-centered, co-design approach that involves HCPs from the beginning. The tool must be designed to be intuitive and to streamline, rather than complicate, the clinical workflow.",The paper highlights the need for larger-scale studies with more diverse participants (both HCPs and patients). There is also a significant gap in the clinical validation of anomaly detection algorithms for smartwatch data and in the development of standardized methods for integrating this data into EMRs.,"Future work includes expanding the prototype to incorporate more health data types (e.g., blood pressure, ECG), testing it with data from various smartwatch models, and conducting a real-world deployment study. Further research is also needed on reimbursement models for using smartwatch data in clinical practice.","A key implementation insight is the value of using a co-design approach to gather user requirements before development. The use of an open-source, low-code machine learning library like PyCaret can accelerate the development of the data analysis component. The suggestion of a tiered review process (with administrators doing initial screening) is a practical workflow consideration for managing HCP workload.",https://doi.org/10.3390/fi15030111,10.3390/fi15030111,"Health Monitoring, Smartwatch, Machine Learning, Anomaly Detection, Co-design, Design Science, Usability Study, Healthcare",
bendiken_2024,KNOW–A Real-World Ontology for Knowledge Capture with Large Language Models,Arto Bendiken,2024,Yes,Medium,"This paper is of medium relevancy. It proposes a new ontology (KNOW) specifically designed for capturing everyday knowledge to be used with LLMs, which aligns with the HDM project's goal of modeling a user's personal world. While the paper focuses on the ontology itself rather than the integration architecture, the principles behind KNOW (pragmatism, focus on human universals, developer experience) are valuable for designing the HDM's own schema. The provision of SDKs for multiple languages is also a good practice to consider.","The key insight is the argument that for neuro-symbolic AI to become practical, a new kind of ontology is needed—one that is less concerned with perfect taxonomic correctness (like Cyc) and more focused on pragmatic, real-world utility and developer experience. The paper posits that LLMs can handle much of the ""commonsense"" knowledge, so the ontology should focus on structuring the most important, universal concepts of human life (spacetime and social relationships) in a way that is easy for both LLMs and software developers to use.","This paper introduces KNOW (Knowledge Navigator Ontology for the World), a new ontology designed to capture everyday knowledge for use in generative AI applications like personal assistants. The ontology focuses on human universals (spacetime and social concepts) and prioritizes pragmatic utility and developer experience over strict taxonomic correctness. The author argues that such an ontology is a crucial component for building practical and interoperable neuro-symbolic AI systems, where LLMs are augmented with explicit knowledge from a KG.","How to design a practical, real-world ontology for capturing everyday knowledge to be used in conjunction with Large Language Models for applications like personal AI assistants?",The paper presents a conceptual design for the KNOW ontology. The methodology is based on identifying human universals as the core concepts to model. The author compares this approach to existing ontologies like Schema.org and Cyc to highlight its unique design principles. The paper also describes the generation of software libraries for 12 programming languages to promote the ontology's adoption.,"The primary outcome is the KNOW ontology itself, presented as a public domain resource with accompanying software libraries. The paper establishes the design principles for this ontology, emphasizing pragmatism, focus on human universals, and developer experience.","The paper presents a first iteration of the ontology, and its scope is currently limited to spacetime and social concepts. It does not provide a detailed evaluation of the ontology's effectiveness in a real-world application. The practical challenges of populating and maintaining a KG based on this ontology are not deeply explored.","The paper concludes that a pragmatic, developer-friendly ontology like KNOW is a necessary step to realize the potential of neuro-symbolic AI. It positions KNOW as a foundational layer for building interoperable AI systems where knowledge can be shared and reused.",The paper identifies the need to extend the ontology's scope beyond the initial set of human universals. It also implicitly points to the challenge of encouraging widespread adoption of a new ontology in a field where ad-hoc schemas are common.,Future work will involve extending the scope of the ontology to cover more aspects of human life.,"The paper provides several valuable implementation insights. First, the focus on ""human universals"" is a good starting point for designing a personal knowledge schema. Second, the emphasis on developer experience, including providing SDKs in multiple languages, is a crucial factor for adoption. Third, the pragmatic approach to taxonomy (a flat class hierarchy) can simplify the modeling process. Finally, the idea of mapping to existing ontologies like Schema.org where possible is a good practice for interoperability.",,,"Ontology, Knowledge Representation, Large Language Models, LLM, Knowledge Graph, Neuro-symbolic AI, Commonsense Knowledge, Semantic Web",,
berkeley_2017,Quality and Relevance Metrics for Selection of Multimodal Pretraining Data,"Roshan Rao, Debadeepta Dey, Sudha Rao, Asli Celikyilmaz, Elnaz Nouri, Bill Dolan",2017,Yes,Low,"This paper is tangentially related. It explores how the quality and relevance of pretraining data affect downstream task performance for visuolinguistic models. While the HDM project deals with heterogeneous data, this paper's focus is on optimizing pretraining for large models, not on the architectural challenges of data integration, schema building, or temporal modeling. The metrics for data quality and relevance could be loosely inspiring but are not directly applicable to the core problems of the HDM project.","The key insight is that data selection for pretraining matters significantly. It's not just about the quantity of data, but also its quality (similarity between image and text) and relevance to downstream tasks. The paper shows that by creating metrics for these aspects, one can intelligently subsample large datasets to improve model performance, which is more efficient than simply using more data.","The paper proposes metrics to measure the quality and relevance of visuolinguistic data and shows that pretraining on a smaller, curated dataset selected using these metrics can outperform pretraining on larger, unfiltered datasets.","This paper investigates the impact of data quality and relevance on the performance of self-supervised pretraining for visuolinguistic models. The authors define two metrics—one for quality (based on image-text similarity using GloVe vectors) and one for relevance (based on TF-IDF similarity to downstream tasks). They evaluate these metrics by pretraining a ViLBERT-style model on various datasets (including ConceptualCaptions and custom-collected ones) and testing on downstream tasks like VQA and VCR. The results show that the proposed metrics correlate well with performance and that a smaller, high-quality, and relevant ""amalgam"" dataset can outperform larger, less-curated ones.","Given a fixed model, task, and data size, how does data quality and data relevance affect performance on downstream tasks? Is it possible to select pretraining data so as to maximize perfomance of a model on downstream tasks?","The authors define two metrics: a TF-IDF-based ""relevance"" score to measure the similarity between pretraining text and downstream task text, and a GloVe-based ""quality"" score to measure the similarity between paired images and text. They use these metrics to score four base datasets. They then pretrain a ViLBERT model on these datasets (downsampled to 2 million examples each) as well as on ""amalgam"" datasets created by selecting the highest-scoring examples. Performance is evaluated on four downstream tasks: VQA, VCR, Grounded Referring Expressions, and Image Retrieval.","Both the quality and relevance metrics showed a strong positive correlation with downstream task performance (Spearman's ρ of 0.893 for quality and 0.577 for relevance). Pretraining on a smaller ""amalgam"" dataset curated for high quality and relevance outperformed pretraining on larger, uncurated datasets. The study also found that data quality (image-text grounding) was a more important factor than data relevance.","The primary outcomes are the two proposed metrics for data quality and relevance, and the empirical demonstration that data curation based on these metrics is an effective strategy for improving the efficiency and performance of pretraining for visuolinguistic models.","The analysis is computationally expensive, making it hard to test many different metrics. The downstream tasks are broad, making it difficult to fully disentangle the effects of quality and relevance. The proposed metrics don't capture all the variance in performance, as shown by the Ngram Image Search dataset which performed better than its scores would suggest.","The paper concludes that simple, inexpensive metrics for data quality and relevance can be used to effectively filter large pretraining datasets, leading to better performance on downstream tasks. This suggests that data curation is a crucial and often overlooked aspect of the pretraining pipeline.","The paper points to the need for faster methods to evaluate data selection strategies. It also suggests that analyzing more niche downstream tasks could help to better disentangle the effects of quality and relevance. Finally, it notes that the proposed metrics are not perfect and that there is room for developing more sophisticated metrics.","The authors plan to apply their metrics to other domains, such as filtering large-scale video and ASR data, where the amount of available data is too large to be used entirely for pretraining.","The use of TF-IDF for relevance and GloVe vector similarity for quality are practical, easy-to-implement techniques for data scoring. The idea of creating a smaller, high-quality ""amalgam"" dataset from multiple sources is a useful strategy for efficient pretraining.",,,"Data Curation, Pretraining, Self-Supervised Learning, Visuolinguistic Models, Data Quality, Data Relevance, Machine Learning"
berlin_2014,Benchmarking Scalability and Elasticity of Distributed Database Systems,"Jorn Kuhlenkamp, Markus Klems, Oliver Ross",2014,Yes,Medium,"This paper is of medium relevancy. It provides a detailed performance, scalability, and elasticity benchmark of distributed database systems (HBase and Cassandra). While not directly about knowledge graphs or heterogeneous data integration in the semantic sense, the HDM project will require a scalable and elastic persistence layer. This paper's methodology for benchmarking these non-functional requirements is highly relevant for evaluating potential database technologies for the HDM system.","The key insight is the clear trade-off between scaling speed and performance stability in distributed databases. The paper demonstrates that while systems can be scaled quickly (e.g., by adding nodes without waiting for data re-distribution), this often comes at the cost of increased performance variability and potential data inconsistency. This highlights the need for careful consideration of scaling strategies based on the specific workload and availability requirements.","This paper reproduces and extends previous performance and scalability benchmarks for the distributed database systems HBase and Cassandra. It verifies that both systems scale nearly linearly but with different performance characteristics (Cassandra being better for reads, HBase for writes). The authors extend the original work by evaluating elasticity, measuring the trade-off between scaling speed and the performance impact on concurrent workloads.",The paper aims to reproduce previous performance and scalability benchmarks of HBase and Cassandra and extend them with an evaluation of elasticity.,"The authors reproduce experiments from prior research [14] using the YCSB benchmark on Amazon EC2 infrastructure. They compare the performance of HBase and Cassandra on various workloads (read-heavy, write-heavy, scan-heavy) and cluster sizes. They then extend these experiments to measure elasticity by observing the performance impact of adding or removing nodes during a running workload, testing different scaling strategies (e.g., varying data streaming throughput).","The reproduced experiments confirm that both Cassandra and HBase scale nearly linearly, and that Cassandra generally has better read performance while HBase has better write performance. The new elasticity experiments quantify the trade-off between scaling speed and performance stability, showing that faster scaling often leads to higher performance variability during the scaling operation.",The primary outcome is a verified and extended set of benchmarks for HBase and Cassandra that not only cover performance and scalability but also elasticity. This provides a more complete picture for decision-makers choosing a distributed database.,"The experiments were conducted on a specific version of the software and on a specific cloud infrastructure (Amazon EC2), which may not perfectly generalize to other environments. The workloads are synthetic (YCSB) and may not fully represent all real-world application behaviors.","The paper concludes that while general performance characteristics of distributed databases can be verified across different environments, absolute performance numbers are highly dependent on the underlying infrastructure. It also highlights that elasticity is a critical but often overlooked aspect of distributed systems, and that there is a clear trade-off between how fast a system can scale and how stable its performance remains during the scaling process.",The paper implicitly points to the need for more standardized and comprehensive benchmarking methodologies that include elasticity as a first-class citizen. It also highlights the challenge of reproducing benchmark results across different infrastructure environments.,Future work could involve developing more sophisticated elasticity benchmarks that cover a wider range of scaling scenarios and failure conditions.,"The paper provides a practical methodology for benchmarking the scalability and elasticity of distributed databases, which is a valuable process for the HDM project to adopt when selecting its own database technology. The detailed analysis of different EC2 instance types and storage configurations provides a good example of the factors to consider when deploying a database in the cloud. The distinction between horizontal and vertical scaling, and the analysis of their respective performance impacts, is also a key implementation consideration.",,,"Benchmarking, Scalability, Elasticity, Distributed Databases, NoSQL, Cassandra, HBase, Performance Evaluation",
bernard_2024,PKG API: A Tool for Personal Knowledge Graph Management,"Nolwenn Bernard, Krisztian Balog, Ivica Kostric, Petra Galuščáková, Martin G. Skjæveland, Weronika Łajewska, Vinay Setty",2024,Yes,Super,"This paper is highly relevant as it presents a concrete implementation of a PKG management system, including a user-friendly API and client. It directly addresses the practical challenges of building and interacting with PKGs, which is a core component of the HDM project. The focus on natural language interaction and a clear vocabulary provides a strong model for the HDM's own user interface and data model.","The key insight is that abstracting the complexity of PKG management behind a user-friendly, natural-language-driven interface is crucial for user adoption. While technologies like Solid provide the decentralized infrastructure, they often lack usability. This paper shows that a combination of a well-defined vocabulary, a service-oriented API, and an LLM-powered natural language layer can bridge this gap.","This paper proposes a complete solution for managing Personal Knowledge Graphs (PKGs), consisting of a user-facing PKG Client and a service-oriented PKG API. It introduces an RDF-based vocabulary to represent user statements, preferences, access rights, and provenance. A key feature is the NL2PKG component, which uses LLMs to translate natural language statements from users into structured data and API calls, making PKG management more accessible to non-expert users.",How to create a user-friendly and practical solution for managing Personal Knowledge Graphs that enables individuals to easily consolidate and control their fragmented personal data?,"The authors developed a complete software solution consisting of: (1) A PKG vocabulary based on RDF reification and existing vocabularies (SKOS, PAV). (2) A PKG API built with Flask, which includes an ""NL2PKG"" module that uses LLMs (Mistral-7b via Ollama) for intent classification and SPO extraction, and an entity linker (REL) to resolve entities. (3) A web-based PKG Client built with React for user interaction.","The work demonstrates the feasibility of creating a user-friendly PKG management system. The use of LLMs for natural language understanding significantly lowers the barrier to entry for end-users. The proposed RDF-based vocabulary is shown to be effective for representing complex user statements, including preferences and provenance.","The primary outcome is the open-source PKG API and Client, a complete, practical tool for personal knowledge graph management. This includes the PKG vocabulary, the NL2PKG component, and the user interface.",The paper acknowledges that the NL2PKG component's performance depends on the underlying LLM and entity linker. The current implementation is a demonstration and has not been evaluated in a large-scale user study. The resolution of entities in the user's private circle is mentioned as a challenge that requires a PKG-specific entity linker.,"The paper concludes that user-centric management tools are essential for the adoption of PKGs. By combining a robust data representation (the PKG vocabulary) with a user-friendly natural language interface, their work represents a major step forward in the practical realization of PKGs.","The paper highlights the general lack of practical, user-friendly PKG implementations. It also points to the need for better entity linking solutions specifically for personal entities that don't exist in public KGs.","Future work could involve improving the NL2PKG component, conducting user studies to evaluate the system's usability and effectiveness, and extending the PKG vocabulary to support more complex types of statements and reasoning.","The architecture of a separate API and Client is a standard and robust design. The use of LLMs via a local framework like Ollama is a good choice for privacy. The modular design of the NL2PKG component, allowing for different LLMs and entity linkers, is a good practice. The use of RDF reification to model statements is a standard Semantic Web pattern that provides flexibility.",https://arxiv.org/abs/2402.07540,10.48550/arXiv.2402.07540,"Personal Knowledge Graph, PKG, API, Knowledge Representation, Natural Language Interface, Semantic Web, User-Friendly, Data Management",
bianchini_2022,A semantics-enabled approach for personalised Data Lake exploration,"Devis Bianchini, Valeria De Antonellis, Massimiliano Garda",2022,Yes,Medium,"This paper is of medium relevancy. It addresses the exploration of heterogeneous data in Data Lakes, which is a related problem to the HDM project's data integration challenge. The PERSEUS approach, with its focus on semantic metadata, multi-dimensional modeling, and personalization, offers valuable patterns. While the context is enterprise BI rather than personal knowledge graphs, the phased approach to building a semantic layer on top of raw data is applicable to the HDM system's design.","The key insight is that a multi-layered, semantics-enabled approach is necessary to make large, heterogeneous Data Lakes usable for a wide range of users. The paper proposes a structured, three-phase process (Semantic Data Lake construction, Exploration Graph definition, and personalized Exploration Context identification) that progressively adds semantic richness and personalization to the raw data, transforming a ""data swamp"" into a navigable and useful information resource.","The paper proposes PERSEUS, a computer-aided approach for personalized data exploration in Data Lakes. It involves three phases: (1) building a semantic metadata catalog on top of the Data Lake, (2) modeling indicators and analysis dimensions into an Exploration Graph using a Multi-Dimensional Ontology, and (3) creating personalized Exploration Contexts based on user profiles and preferences.",How to enable a large number of users with different roles and competencies to extract value and knowledge from heterogeneous data sources stored in a Data Lake?,"The paper proposes the PERSEUS approach, which consists of three phases. First, a semantic metadata catalog is built using a semi-automatic annotation process supported by a tool called DL-DIVER. Second, data analysts use a Multi-Dimensional Ontology (MDO) and the Protégé tool to model indicators and analysis dimensions, creating an Exploration Graph. Third, personalized Exploration Contexts are identified based on user profiles, which are then used to filter and rank indicators for exploration. The approach was validated in a Smart City domain.","The experimental evaluation demonstrates the feasibility of the PERSEUS approach. The DL-DIVER tool effectively supports the semantic annotation process, significantly improving the quality of the metadata catalog compared to using raw attribute names. The preference-based system is shown to be effective in reducing the search space for indicators, and the personalization features improve the retrieval of relevant indicators for different user profiles.","The primary outcome is the PERSEUS methodology, a comprehensive, three-phase approach for personalized data exploration in Data Lakes. This includes the conceptual models (Semantic Data Lake, Exploration Graph, MDO), the supporting tools (DL-DIVER), and a demonstration of its feasibility in a real-world Smart City project.","The approach relies on significant manual effort from domain experts and data analysts, particularly in the initial setup of the semantic metadata catalog and the Exploration Graph. The query performance depends on the underlying Data Lake infrastructure (Hadoop/Spark in this case) and may not be suitable for real-time interactive exploration in all scenarios.","The paper concludes that a semantics-enabled, personalized approach is crucial for unlocking the value of Data Lakes. By progressively enriching the data with semantic metadata and user-specific context, the PERSEUS approach can make complex, heterogeneous data accessible and explorable for a wide range of users.","The paper suggests that future work could focus on improving the automation of the semantic annotation process, developing more sophisticated mapping techniques between the Exploration Graph and the Data Lake sources, and exploring the propagation of preferences across different exploration contexts.","Future research will focus on improving the DL-DIVER tool, enhancing the mapping definition process, and investigating the propagation of preferences across exploration contexts.","The three-phase architecture (semantic catalog, exploration graph, personalized contexts) is a useful pattern for building a semantic layer over raw data. The use of a Multi-Dimensional Ontology (MDO) to guide the modeling of indicators and dimensions is a good practice for ensuring consistency. The concept of using user profiles and preferences to create personalized ""Exploration Contexts"" is a valuable technique for tailoring the user experience.",https://doi.org/10.2785/56118,10.2785/56118,"Semantic Data Lake, Personalised Data Exploration, OLAP, Big Data, Ontology, Heterogeneous Data, Data Integration",
bikakis_2021,Pattern-based design applied to cultural heritage knowledge graphs,"Valentina Anita Carriero, Aldo Gangemi, Maria Letizia Mancinelli, Andrea Giovanni Nuzzolese, Valentina Presutti, Chiara Veninata",2021,Yes,High,"This paper is highly relevant because it provides a detailed, practical case study of building a large, complex knowledge graph (ArCo) using a pattern-based design methodology (eXtreme Design). The HDM project faces similar challenges in modeling a complex domain (personal data) and can directly benefit from the lessons learned in the ArCo project. The focus on Ontology Design Patterns (ODPs), test-driven development, and a modular architectural pattern (root-thematic-foundations) are all best practices that can be adopted for the HDM system's ontology design.","The key insight is that a rigorous, pattern-based, and test-driven methodology is essential for developing high-quality, maintainable, and extensible knowledge graphs, especially in complex domains like Cultural Heritage. The paper demonstrates that the eXtreme Design (XD) methodology, adapted for the specific needs of the domain, provides a structured and effective way to manage the development process, from requirement gathering to validation.","This paper presents ArCo, the knowledge graph of Italian Cultural Heritage, and details the methodology used for its development. It describes how the eXtreme Design (XD) methodology, which is based on Ontology Design Patterns (ODPs) and test-driven development, was adapted and applied to the cultural heritage domain. The paper also introduces a novel architectural pattern for large ontology networks and a new tool for unit-testing KGs.","How to apply a pattern-based and test-driven methodology to the development and validation of a large-scale, domain-specific knowledge graph like the Italian Cultural Heritage KG (ArCo)?","The authors applied the eXtreme Design (XD) methodology, which involves: (1) collecting requirements as user stories and translating them into competency questions (CQs), (2) matching CQs to existing Ontology Design Patterns (ODPs), (3) implementing the ODPs in ontology modules, and (4) performing unit and integration testing based on the CQs. They extended this methodology to handle requirements from a diverse community and developed a new architectural pattern (root-thematic-foundations) for the ontology network.","The successful development of ArCo, a large-scale knowledge graph with ~172.5M triples, demonstrates the effectiveness of the adapted XD methodology. The paper provides a detailed account of the design process, including the selection and specialization of ODPs. It also introduces TESTaLOD, a new tool for supporting unit testing of KGs, and presents a rigorous evaluation of the ArCo ontologies.","The primary outcome is a detailed case study and a set of methodological guidelines for building large, high-quality knowledge graphs. This includes the ArCo KG itself, the adapted XD methodology, the root-thematic-foundations architectural pattern, and the TESTaLOD tool.","The process described still relies heavily on manual effort from domain experts and ontology engineers, particularly for requirement engineering and matching CQs to ODPs. The paper focuses on the cultural heritage domain, and while the methodology is general, some of the specific patterns and challenges may not directly translate to other domains.","The paper concludes that a pattern-based, test-driven methodology like XD is crucial for the successful engineering of complex knowledge graphs. The experience with ArCo shows that such a methodology can be adapted to handle the specific challenges of a domain, leading to a high-quality and maintainable resource.","The paper highlights the need for better tool support for pattern-based ontology design, particularly for matching requirements to ODPs. It also points to the challenge of gathering and managing requirements from a large and diverse community of stakeholders.","Future work includes further refinement of the XD methodology, development of more advanced tools for pattern-based design and testing, and extending the ArCo KG to cover more aspects of cultural heritage.",The paper provides a wealth of practical implementation insights. The root-thematic-foundations architectural pattern is a very useful model for structuring a large ontology network. The process of translating user stories into competency questions and then matching them to ODPs is a clear and effective design workflow. The emphasis on test-driven development and the creation of a dedicated testing tool (TESTaLOD) is a crucial lesson in ensuring the quality and robustness of the KG.,https://doi.org/10.3233/SW-200422,10.3233/SW-200422,"Knowledge Graph, Ontology Design Patterns, ODP, Cultural Heritage, eXtreme Design, Ontology Engineering, Test-Driven Development, Semantic Web",
bloor_2018,Towards a Personal Health Knowledge Graph Framework for Patient Monitoring,"Daniel Bloor, Nnamdi Ugwuoke, David Taylor, Keir Lewis, Luis Mur, Chuan Lu",2018,Yes,High,"This paper is highly relevant as it proposes a concrete framework for constructing Personal Health Knowledge Graphs (PHKGs) for patient monitoring. This directly aligns with the HDM project's goals, particularly in the healthcare domain. The framework's emphasis on integrating heterogeneous data sources (EHRs, sensors, omics), using ontologies for semantic mapping, and employing a reasoning engine for generating alerts and insights provides a strong architectural blueprint for the HDM system.","The key insight is the practical, modular approach to building a PHKG. The framework separates concerns into distinct components: ontology management, data harmonization (for time-series, free text, and multimodal data), KG construction, reasoning, and personalization. This modularity makes the complex problem of building a PHKG more manageable and adaptable to different chronic diseases and data sources.","This paper proposes a framework for constructing Personal Health Knowledge Graphs (PHKGs) to monitor patients with chronic diseases. The framework integrates heterogeneous data from clinical databases, ontologies, and healthcare guidelines to support alerts, interpretation, and querying of patient data. A use case for Chronic Obstructive Pulmonary Disease (COPD) demonstrates the framework's feasibility.",How can a Personal Health Knowledge Graph be constructed from heterogeneous data sources to effectively monitor patients with chronic diseases?,"The paper proposes a 7-component framework: (1) Ontology selection and merging, (2) Data processing and harmonization (including time-series summarization and NLP for free text), (3) General KG construction from ontologies and guidelines, (4) PHKG creation by mapping transformed data to the KG, (5) Reasoning and inference using rule-based and model-based approaches, (6) Personalization and subgraph extraction, and (7) an API for downstream tasks. The authors implemented a prototype for COPD monitoring using the MIMIC-III dataset, MongoDB for data storage, and Neo4j for the graph database.","The authors successfully constructed a PHKG for COPD with 3.5 million nodes and 4 million relationships. The system was able to generate risk-based alerts using personalized thresholds and demonstrate enhanced querying capabilities through ontological inference (e.g., querying for a disease and getting back patients with its subtypes).","The primary outcome is a comprehensive and practical framework for building PHKGs for patient monitoring. The paper also presents a proof-of-concept implementation for COPD, demonstrating the framework's viability.",The paper is a proposal and a preliminary implementation. The system has not been clinically validated or tested with real-time patient data. The machine learning components are mentioned as future work and are not deeply integrated into the current prototype.,"The paper concludes that the proposed framework provides a viable approach for constructing PHKGs for chronic disease monitoring. By integrating diverse data sources and leveraging semantic technologies, the system can provide valuable support for clinicians and patients.","The paper identifies the need for more advanced machine learning and multimodal analysis capabilities, such as using graph neural networks for more effective and accurate inferences.","Future work will focus on improving the machine learning and multimodal analysis capabilities of the system, leveraging advanced algorithms such as graph neural networks.","The paper offers several practical implementation insights. The use of a dual-database approach (MongoDB for raw/metadata, Neo4j for the graph) is a good pattern for managing different types of data. The data harmonization steps, including time-series summarization and using NLP tools like MedCAT for clinical text, are concrete techniques that can be adopted. The use of Cypher query triggers in Neo4j for real-time rule-based inference is another practical tip.",,,"Personal Health Knowledge Graph, PHKG, Patient Monitoring, COPD, Healthcare, Data Integration, Ontology, Knowledge Graph, Neo4j",
sharma_2025,Temporal Reasoning in AI Systems,Abhishek Sharma,2025,Yes,Low,Tangentially related to knowledge management or data systems,,,,,,,,,,,,,,,"ai, healthcare, knowledge_graph, machine_learning, ontology, semantic, temporal"
bogachov_2018,A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction,"Bogdan Bogachov, Yaoyao Fiona Zhao",2018,Yes,Medium,"This paper is of medium relevancy. It proposes a ""Small Language Graph"" (SLG) architecture, which is a system of small, specialized language models (experts) connected in a graph. This is relevant to the HDM project's goal of building a modular and efficient system. While not a knowledge graph in the traditional semantic web sense, the SLG's graph-based, multi-expert approach to handling domain-specific knowledge offers an interesting architectural pattern that could be adapted for managing different facets of a user's personal data.","The key insight is that a system of small, specialized expert models can outperform a single, larger model in domain-specific tasks, both in terms of accuracy (by reducing ""knowledge overshadowing"" and hallucinations) and computational efficiency. By isolating knowledge into distinct, fine-tuned experts, the system can provide more precise and reliable information for specialized domains like engineering.","This paper introduces the Small Language Graph (SLG), a lightweight system of multiple small language models (experts) fine-tuned on specific, concise texts. This graph-based architecture is designed to address the challenges of high computational costs and hallucinations in large language models, particularly in engineering contexts. The system demonstrates superior performance and faster fine-tuning compared to conventional methods.",How to create a lightweight and accurate generative AI system for specific engineering domains that mitigates hallucinations and reduces computational costs?,"The authors propose the Small Language Graph (SLG) system. The methodology involves: (1) Splitting a domain-specific dataset (a Structural Repair Manual) into isolated chunks to avoid data overlap. (2) Fine-tuning small language models (Llama-3.2-1B-Instruct) on each chunk to create ""expert"" nodes. (3) Using another fine-tuned small LLM as an ""orchestrator"" to route user queries to the appropriate expert. (4) Connecting the experts in a graph architecture using LangGraph. The performance is evaluated against fine-tuned standalone LLMs using ROUGE-L, Exact Match, and METEOR metrics.","The SLG system, built on smaller models, outperformed a larger standalone model (Llama-3.1-8B-Instruct) on the Exact Match metric by a factor of three, indicating a significant reduction in hallucinations. The fine-tuning process for the entire SLG system was 1.7 times faster than for the larger model. The system can be run on a single consumer-grade GPU.","The primary outcome is the Small Language Graph (SLG) system, a proof-of-concept demonstrating a novel, lightweight, multi-expert architecture for domain-specific generative AI.","The study is limited to a single engineering domain and dataset. The evaluation of hallucination is based primarily on the Exact Match metric, without extensive human evaluation. The current implementation is not a full chatbot and lacks memory and conversational context. The orchestrator's accuracy in routing queries is around 70% and needs improvement.","The paper concludes that a multi-expert system of small, specialized language models can be more accurate and computationally efficient than a single large model for domain-specific tasks. This approach has the potential to make generative AI more accessible to smaller companies and could pave the way for decentralized AI systems.","The paper identifies the need for more extensive comparisons with other models (like larger Llama models and RAG systems) and more rigorous hallucination checking. Future work should also focus on improving the orchestrator, adding conversational capabilities, and incorporating multimodal data.","Future work includes comparing the SLG with larger models and RAG systems, performing more thorough hallucination checks, improving the orchestrator, and adding features like conversational memory and multimodal data support.","The graph-based architecture of specialized ""expert"" nodes is a valuable pattern for modularizing knowledge and processing. The idea of isolating training data to prevent ""knowledge overshadowing"" is an important consideration for fine-tuning models on diverse personal data. The use of a lightweight orchestrator to route queries is a practical approach for building a multi-component system.",https://doi.org/10.48550/arxiv.2302.04023.,10.48550/arXiv.2302.04023.,"Small Language Model, Small Language Graph, Multi-expert System, Fine-tuning, Generative AI, Engineering, Hallucination",
bontempelli_2017,Lifelong Personal Context Recognition,"Andrea Bontempelli, Marcelo Rodas Britez, Xiaoyue Li, Haonan Zhao, Luca Erculiani, Stefano Teso, Andrea Passerini, Fausto Giunchiglia",2017,Yes,High,"This paper is highly relevant as it directly addresses the core conceptual challenges of building a lifelong personal AI assistant, which is the ultimate vision for the HDM project. It outlines the three key pillars: (1) representing the user's personal situational context, (2) performing lifelong context recognition robust to change, and (3) maintaining human-AI alignment. This provides a strong theoretical and conceptual framework for the HDM system's architecture and long-term research goals.","The key insight is that a truly symbiotic Human-AI system cannot be achieved with KR or ML alone; it requires a synthesis of both. The paper argues that to overcome the ""problem of generality"" and brittleness, an AI must continually align its understanding of the world with its user's subjective perspective. This requires a KR framework to model the user's context and an ML framework that can learn and adapt to this context over a lifetime, all orchestrated through a bidirectional interaction loop.","This paper outlines the key challenges in developing AIs that live in lifelong symbiosis with a human. The authors argue that the central task is for the AI to understand the user's personal situational context at all times. They propose a three-pronged approach: (1) a knowledge representation scheme for personal context, (2) machine learning techniques for lifelong context recognition that are robust to change (e.g., concept drift and knowledge drift), and (3) a machine-human alignment loop to maintain a shared understanding through continual interaction.","How to build an AI that can live in lifelong symbiosis with a human, continuously understanding and adapting to their personal situational context?","The paper presents a conceptual framework and summarizes the authors' research efforts in three areas: (1) Knowledge Representation: They propose modeling the personal situational context as a Knowledge Graph, composed of a ""Life Sequence"" of contexts (defined by location, event, people, objects, etc.). (2) Machine Learning: They discuss the challenges of lifelong Personal Context Recognition (PCR), including handling changes in the user's descriptions (skeptical learning) and changes in the world itself (knowledge drift). (3) Human-AI Alignment: They posit the need for a continual, bidirectional interaction loop to maintain alignment, though this is presented as a major area for future research.",The paper establishes a clear conceptual framework for building a lifelong personal AI. It identifies the key research challenges and provides a roadmap for tackling them. The authors' work on skeptical learning and knowledge drift provides initial solutions for the ML component of the framework.,"The primary outcome is a research agenda and a conceptual framework for Human-AI Symbiosis, centered around the problem of Lifelong Personal Context Recognition.","The paper is a short position paper and does not provide a complete, implemented system. Many of the ideas, particularly around the machine-human alignment loop, are presented as open research questions rather than solved problems.","The paper concludes that building a symbiotic AI requires a deep integration of Knowledge Representation and Machine Learning. Neither field can solve the problem of generality and brittleness in isolation. A lifelong, bidirectional interaction between the human and the AI is essential for maintaining alignment and enabling the AI to adapt to a constantly changing world.","The paper highlights the need for more research on the machine-human alignment loop, including how to structure the interaction to be effective without being burdensome for the user. It also points to the challenge of translating between the outputs of ML models and the symbolic representations in the KR component.","Future research will focus on designing the machine-human alignment loop, including how to manage the cognitive load on the user and how to translate between ML outputs and the KR layer. The authors also emphasize the need for more real-world experiments and interdisciplinary collaboration.","The paper provides a strong conceptual model for the HDM's knowledge representation, based on a ""Life Sequence"" of personal situational contexts represented as KGs. The identification of specific ML challenges like ""knowledge drift"" is a crucial insight for designing the learning components of the HDM. The emphasis on bidirectional interaction highlights the need for an explainable and interactive user interface.",,,"Personal Context, Lifelong Learning, Human-AI Symbiosis, Knowledge Representation, Machine Learning, Explainable AI, Knowledge Drift",
boyd_2025,AMEND 2.0: module identifcation and multi-omic data integration with multiplex-heterogeneous graphs,"Samuel S. Boyd, Chad Slawson, Jefrey A. Thompson",2025,Yes,Medium,"This paper is of medium relevancy. It presents a sophisticated method (AMEND 2.0) for integrating multi-omic data using complex biological networks (multiplex-heterogeneous graphs). While the domain is very specific (bioinformatics), the underlying challenge of integrating heterogeneous data types and the use of advanced graph-based techniques (Random Walk with Restart for Multiplex-Heterogeneous Graphs) are relevant to the HDM project. The architectural patterns for handling multiplex and heterogeneous graphs could inform the design of the HDM's own knowledge graph, especially if it needs to model different types of relationships or entities.","The key insight is that for effective multi-omic data integration, the integration should happen *during* the network diffusion process, not before or after. The proposed RWR-MH (Random Walk with Restart for Multiplex-Heterogeneous) method allows for this by providing fine-grained control over how information flows between different layers (e.g., different interaction types) and components (e.g., different molecule types) of a complex biological network. This enables a more nuanced and powerful analysis than methods that treat the integrated network as a simple, flat graph.","This paper presents AMEND 2.0, an updated method for active module identification that can analyze multiplex and heterogeneous networks integrated with multi-omic data. The method is powered by a novel Random Walk with Restart for Multiplex-Heterogeneous networks (RWR-MH) and includes features for degree bias adjustment and multi-objective module identification. The authors demonstrate its effectiveness on two real-world multi-omic datasets.","How to effectively integrate and analyze multi-omic data using biological networks, accommodating multiple data types and complex network structures in a generalizable framework?","The authors present AMEND 2.0, an iterative active module identification method. Its core is RWR-MH, a network diffusion algorithm for multiplex-heterogeneous graphs. The paper details the mathematical formulation of RWR-MH, including a re-parameterization for more predictable behavior. The method is evaluated on two real-world multi-omic datasets (TCGA-KIRC and an OGT knockout study) and on several node ranking and degree bias adjustment tasks.",AMEND 2.0 is shown to be a versatile and effective tool for multi-omic data integration. The RWR-MH component allows for a more nuanced analysis of complex networks compared to previous methods. The application to real-world datasets successfully identified biologically relevant modules and molecular features involved in kidney cancer and lipid metabolism.,"The primary outcome is the AMEND 2.0 R package, a flexible and powerful tool for active module identification and multi-omic data integration on complex networks. The paper also introduces the RWR-MH algorithm and novel degree bias adjustment methods (IN and BS).","The method is highly specialized for bioinformatics and requires deep domain knowledge to construct the input networks and interpret the results. The complexity of the RWR-MH parameters (seed weights, cross-talk parameters) might be challenging to tune for users without expertise in network analysis.","The paper concludes that as multi-omic datasets become more common, new methods are needed to fully exploit their complexity. AMEND 2.0 provides a versatile and generalizable solution for analyzing multi-omic data in a network context, combining several advanced network analysis techniques into a single framework.","The paper implicitly points to the need for more user-friendly tools for building and analyzing multiplex-heterogeneous networks. It also suggests that the optimal level of ""cross-talk"" between different network layers and components is context-dependent and requires further investigation.","Future work could involve applying AMEND 2.0 to a wider range of multi-omic datasets and disease contexts, as well as developing more automated methods for tuning the RWR-MH parameters.","The paper provides a detailed description of how to model and analyze multiplex-heterogeneous graphs, which is a valuable architectural pattern. The RWR-MH algorithm, with its separate normalization of intra- and inter-layer/component matrices and its explicit cross-talk parameters, is a sophisticated technique for managing information flow in a complex graph. The discussion of degree bias adjustment is also a relevant consideration for any graph-based analysis.",https://doi.org/10.1186/s12859-025-06063-x.,10.1186/s12859-025-06063-x,"Multi-omic, Data Integration, Biological Networks, Active Module Identification, Knowledge Graph, Heterogeneous Graph, Multiplex Graph, Network Diffusion",
brennan_2016,Towards a knowledge driven framework for bridging the gap between software and data engineering,"Monika Solanki, Bojan Božic´, Christian Dirschl, Rob Brennan",2016,Yes,High,"This paper is highly relevant. The HDM project is a data-intensive software system, and this paper's proposal of a suite of ontologies (the ALIGNED suite) to bridge the gap between software and data engineering provides a powerful architectural pattern. The concepts of a Design Intent Ontology (DIO), Software Lifecycle Ontology (SLO), and Data Lifecycle Ontology (DLO) offer a robust, high-level framework for managing the development and maintenance of a system that handles heterogeneous personal data. The emphasis on provenance (using W3C PROV) is also a critical requirement for the HDM.","The key insight is that software engineering and data engineering, while often treated as separate disciplines, can and should be integrated using a common semantic layer. An ontology-based framework can provide this layer, enabling better tool-chain integration, unified governance, and clearer communication across teams and processes.","This paper presents the ALIGNED suite of ontologies, a framework designed to bridge the gap between software and data engineering for large-scale, data-intensive systems. The suite provides semantic models for design intents, software and data lifecycles, and data quality. The authors demonstrate its application in two complex use cases (a legal information system and a historical databank), showing how the ontologies can facilitate tool integration and unified governance.","How can a knowledge-driven framework, based on a suite of ontologies, be used to bridge the gap between software engineering and data engineering processes for data-intensive systems?","The authors developed the ALIGNED suite of ontologies based on a thorough requirements analysis from real-world use cases. The methodology involved: (1) identifying generic and use-case-specific requirements, (2) designing a modular suite of ontologies (reusing existing standards like PROV-O where possible), (3) deploying and validating the ontologies in two large-scale projects (JURION and Seshat), and (4) evaluating the ontologies against established design principles (e.g., Gruber's principles).","The ALIGNED ontology suite provides an effective framework for integrating software and data engineering processes. The use of these ontologies enables better tool-chain integration, unified governance, and the creation of a common semantic understanding across different parts of a data-intensive system. The user-driven evaluation showed a 50% increase in efficiency for certain tasks.","The primary outcome is the ALIGNED suite of ontologies itself, a publicly available and well-documented resource for building data-intensive systems. The paper also provides a set of requirements and design patterns for this domain.","The paper acknowledges that the development of such ontologies still requires significant manual effort from domain experts. The evaluation, while positive, is based on specific use cases and tools within the ALIGNED project.","The paper concludes that a knowledge-driven framework based on ontologies is a powerful approach for aligning software and data engineering. This alignment is crucial for increasing the productivity, agility, and quality of data-intensive systems.",The paper implicitly points to the need for more automated tools for ontology engineering and for mapping between different domain-specific models.,Future work includes the empirical evaluation of the ontologies in a wider range of use cases and the development of more advanced tools that leverage the ALIGNED framework.,"The modular architecture of the ALIGNED suite (provenance, generic, domain-specific layers) is a key implementation pattern. The use of W3C PROV as a foundational layer for all models is a best practice for ensuring data lineage. The paper also provides concrete examples of how to use the ontologies to create audit trails and integrate different tools (like JIRA and Confluence).",https://doi.org/10.1016/j.jss.2018.12.017,10.1016/j.jss.2018.12.017,"Ontology, Software Engineering, Data Engineering, Data Integration, Knowledge-Driven Framework, Semantic Web, Provenance, Data Lifecycle, Software Lifecycle",
briggs_2021,A Review of Privacy-preserving Federated Learning for the Internet-of-Things,"Christopher Briggs, Zhong Fan, Peter Andras",2021,Yes,High,"This paper is highly relevant as it provides a comprehensive survey of privacy-preserving federated learning, a key architectural consideration for the HDM project. Given that the HDM will manage sensitive personal data in a distributed manner, understanding the privacy risks, attack vectors, and defense mechanisms (like differential privacy and secure aggregation) discussed in this paper is crucial for designing a robust and trustworthy system.","The key insight is that federated learning is not inherently private. While it prevents direct data sharing, the model updates themselves can leak sensitive information. Therefore, additional privacy-enhancing technologies are essential. The paper provides a clear overview of the trade-offs between privacy, model utility, and communication efficiency, which is a critical consideration for any practical implementation.","This paper provides a comprehensive review of federated learning as a privacy-preserving machine learning approach for distributed data, with a particular focus on its application in the Internet-of-Things (IoT). It surveys a wide range of methods for improving communication efficiency, handling client heterogeneity, and preserving privacy, identifying the strengths and weaknesses of each.","What are the state-of-the-art privacy-preserving methods for federated learning, and what are the key challenges and future directions for their application in the IoT?","The authors conduct a systematic literature review, surveying a wide variety of papers on federated learning. They categorize the research into three main areas: communication efficiency, client heterogeneity, and privacy-preserving methods. The paper provides a historical context by discussing distributed machine learning and then delves into the specifics of federated learning and its associated privacy challenges.","The paper finds that while federated learning offers a good starting point for privacy, it is vulnerable to various attacks. It identifies a range of privacy-preserving techniques, including anonymization, encryption (homomorphic and SMC), and differential privacy, and discusses their applicability to federated learning. It also highlights the importance of communication-efficient methods (like model compression) for the resource-constrained IoT environment.","The primary outcome is a comprehensive survey that serves as a valuable guide to the field of privacy-preserving federated learning. It provides a clear taxonomy of the challenges and solutions, and outlines a roadmap for future research.","As a survey, the paper synthesizes existing work and does not propose new methods. The field of federated learning is evolving rapidly, so some of the discussed techniques may have been superseded by newer approaches.","The paper concludes that federated learning, when combined with robust privacy-preserving mechanisms, is a powerful paradigm for machine learning on distributed, sensitive data. However, there is no one-size-fits-all solution, and the choice of methods depends on the specific application's requirements regarding privacy, utility, and efficiency.","The paper identifies several research gaps, including the need for better methods for hyperparameter tuning in a federated setting, more research into continual learning on distributed data, and the development of more efficient and robust privacy-preserving techniques.","Future work should focus on developing more advanced and computationally efficient privacy-preserving methods, exploring the combination of federated learning with fog computing, and designing federated learning algorithms specifically for low-power IoT devices.",The paper provides a good overview of practical privacy-preserving techniques that can be implemented in a federated system. This includes the use of Secure Multi-party Computation (SMC) for secure aggregation of model updates and the application of differential privacy (both global and local) to add noise to the updates and protect individual contributions. The discussion of communication-efficient techniques like model quantization and sparsification is also highly relevant for practical deployment.,,,"Federated Learning, Privacy, IoT, Survey, Differential Privacy, Secure Aggregation, Machine Learning",
budhdeo_2021,Scoping review of knowledge graph applications in biomedical and healthcare sciences,"Sanjay Budhdeo, Joe Zhang, Yusuf Abdulle, Paul M Agapow, Douglas GJ McKechnie, Matt Archer, Viraj Shah, Eugenia Forte, Ayush Noori, Marinka Zitnik, Hutan Ashrafian, Nikhil Sharma",2021,Yes,High,"This paper is highly relevant as it provides a comprehensive scoping review of KG applications in biomedicine and healthcare. This is directly applicable to the HDM project, especially its healthcare-related aspects. The focus on data heterogeneity, integration challenges, and the need for validation aligns perfectly with the HDM's research focus.","The key insight is the systematic identification of a gap between the potential of KGs in healthcare and their actual clinical translation. The paper highlights that while there's a lot of research, there's a lack of real-world validation and standardized practices. The delineation of KGs into ""biomedical"" and ""clinical"" clusters is also a significant insight.","This paper conducts a scoping review of 255 articles to characterize the use of knowledge graphs in biomedical and healthcare sciences. It finds that while medical science insights and drug repurposing are common applications, there is a wide variety of use cases. The review highlights the heterogeneity of KGs in terms of size and data sources, with DrugBank being the most common. It also notes that while advanced machine learning techniques are used, simple querying remains a prevalent analysis method. A key finding is the lack of real-world validation for insights derived from these KGs, posing a major challenge for their clinical translation.","What are the use cases, data characteristics, and research attributes of knowledge graphs in the biomedical and healthcare sciences, and to what extent are the findings from these KGs validated in the real world?","The authors conducted a scoping review following the PRISMA-ScR checklist. They performed keyword-based searches on multiple databases (MEDLINE, EMBASE, medRxiv, arXiv, bioRxiv) and screened 255 articles for inclusion. Data was extracted on demographics, graph characteristics, data sources, analysis techniques, and validation methods.","The most common use cases for KGs are medical science insights and drug repurposing. KGs are heterogeneous in size and data sources. DrugBank is the most used data source. A meta-analysis of node classes suggests a split between ""biomedical"" and ""clinical"" graphs. While advanced ML is used, simple querying is the most common analysis technique. A major finding is that most studies rely on ""inside graph"" validation, with very few performing ""outside graph"" validation (e.g., clinical trials).","A comprehensive scoping review that systematically describes the landscape of KG applications in biomedicine and healthcare. It provides a categorization of use cases, an analysis of graph characteristics, and a critical assessment of validation practices in the field.",The review has a cut-off date of November 2021. The search strategy might have missed some relevant papers. The review is limited to academic literature and does not include commercial or patent data. The categorization of use cases was developed by the authors and is exploratory.,"KGs have many possible uses in biomedicine and healthcare, but their full potential is yet to be realised. The two most popular use cases to date are generation of medical science insights and drug repurposing. There is an opportunity to expand work areas across other use cases and across diseases. Heterogeneity in graph size and context specificity suggests further work is needed to understand optimum graph construction. There are many different techniques used in graph analysis - deploying more sophisticated graph machine learning techniques may improve insights gained from KGs. Validation of findings from graphs through external testing will increase the robustness of conclusions drawn from graphs.","The paper identifies several gaps: the need for best practices in KG construction (especially regarding size and connectivity), better integration of -omics and patient data, understanding how to integrate KGs with LLMs, and the need for more open-sourcing of graphs and data. The most significant gap is the lack of ""outside graph"" validation.","Future research should focus on establishing best practices for KG construction and analysis, improving the integration of diverse data types, exploring the synergy between KGs and LLMs, and, most importantly, conducting more real-world validation of KG-derived insights.","The paper highlights the challenges of integrating clinical data (EHRs) due to lack of standardization, noise, and privacy concerns. This is a crucial consideration for the HDM project. The clear distinction between ""biomedical"" and ""clinical"" KGs is a useful conceptual model. The finding that simple querying is a common analysis method suggests that even basic KG functionality can be valuable.",https://doi.org/10.3389/fgene.2020.610798,10.3389/fgene.2020.610798,"Knowledge Graph, Survey, Scoping Review, Healthcare, Biomedicine, Data Integration, Validation, Machine Learning, Ontology",
