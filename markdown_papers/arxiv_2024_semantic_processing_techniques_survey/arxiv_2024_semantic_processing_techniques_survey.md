---
cite_key: maoa_2023
title: A Survey on Semantic Processing Techniques
authors: Rui Maoa, Kai Hec, Xulang Zhangb, Guanyi Chend, Jinjie Nib, Zonglin Yanga,
  Erik Cambriaa
year: 2023
doi: 10.5683/SP2/QPOJSI
date_processed: '2025-07-02'
phase2_processed: true
original_folder: arxiv_2024_semantic_processing_techniques_survey
images_total: 5
images_kept: 5
images_removed: 0
tags:
- Machine Learning
- Natural Language Processing
- Personal Health
---

# A Survey on Semantic Processing Techniques

<span id="page-0-1"></span>Rui Maoa,<sup>∗</sup> , Kai Hec,<sup>∗</sup> , Xulang Zhangb,<sup>∗</sup> , Guanyi Chend,e,f,<sup>∗</sup> , Jinjie Nib,<sup>∗</sup> , Zonglin Yanga,<sup>∗</sup> , Erik Cambriaa,∗∗

*<sup>a</sup>Continental-NTU Corporate Lab, Nanyang Technological University, 50 Nanyang Avenue, 639798, Singapore*

*<sup>b</sup>School of Computer Science and Engineering, Nanyang Technological University, 50 Nanyang Avenue, 639798, Singapore*

*<sup>c</sup>Saw Swee Hock School of Public Health, National University of Singapore, 117549, Singapore*

*<sup>d</sup>Hubei Provincial Key Laboratory of Artificial Intelligence and Smart Learning, Central China Normal University, 382 Xiongchu Avenue, 430079, Wuhan, China*

*<sup>e</sup>National Language Resources Monitoring and Research Center for Network Media, Central China Normal University, 382 Xiongchu Avenue, 430079, Wuhan, China*

*<sup>f</sup>School of Computer Science, Central China Normal University, 382 Xiongchu Avenue, 430079, Wuhan, China*## Abstract

Semantic processing is a fundamental research domain in computational linguistics. In the era of powerful pre-trained language models and large language models, the advancement of research in this domain appears to be decelerating. However, the study of semantics is multi-dimensional in linguistics. The research depth and breadth of computational semantic processing can be largely improved with new technologies. In this survey, we analyzed five semantic processing tasks, e.g., word sense disambiguation, anaphora resolution, named entity recognition, concept extraction, and subjectivity detection. We study relevant theoretical research in these fields, advanced methods, and downstream applications. We connect the surveyed tasks with downstream applications because this may inspire future scholars to fuse these low-level semantic processing tasks with high-level natural language processing tasks. The review of theoretical research may also inspire new tasks and technologies in the semantic processing domain. Finally, we compare the different semantic processing techniques and summarize their technical trends, application trends, and future directions.
*Keywords:*Semantic Processing, Word Sense Disambiguation, Anaphora Resolution, Named Entity Recognition, Concept Extraction, Subjectivity Detection

# Introduction

Semantics is a linguistic term, generally referring to the meaning of language. Unlike syntax which studies the structure of sentences [\(Zhang et al., 2023\)](#page-99-0), the significance of semantics lies in its ability to aid our comprehension of how meaning is conveyed through words, phrases, and sentences, as well as how language is used to express various ideas, thoughts, and emotions. Language is one of the important carriers of meanings. However, the term "meaning" encompasses multiple aspects of language. [Palmer and Frank Robert](#page-92-0) [\(1981\)](#page-92-0) argued that there is a lack of consensus regarding the nature of "meaning", e.g., which components should be considered part of semantics, and how it should be characterized. Thus, the study of "semantics" is also multi-dimensional in academia.

The evolution of semantic research reflects the rich connotation of semantics in linguistics. At the early stage, much attention is given to the study of lexical semantics. The first English dictionary,*Robert Cawdrey's Table Alphabeticall*, dates back to 1604 [\(Noyes, 1943\)](#page-91-0). The construction of dictionaries, e.g., *The Oxford English Dictio-*

<span id="page-0-0"></span>*Published at Information Fusion, Volume 101, 2024, 101988, ISSN 1566-2535. The equal contribution mark is missed in the published version due to the publication policies. Please contact Prof. Erik Cambria for details.*<sup>∗</sup>These authors contributed equally.

<sup>∗∗</sup>This is to indicate the corresponding author.
*Email addresses:*rui.mao@ntu.edu.sg (Rui Mao), kai\_he@nus.edu.sg (Kai He), xulang001@e.ntu.edu.sg (Xulang Zhang),

g.chen@ccnu.edu.cn (Guanyi Chen), jinjie001@e.ntu.edu.sg (Jinjie Ni), zonglin001@e.ntu.edu.sg (Zonglin Yang), cambria@ntu.edu.sg (Erik Cambria)

<span id="page-1-0"></span>![](_page_1_Figure_0.jpeg)
<!-- Image Description: The image is a hierarchical tree diagram classifying semantic theories. The top node is "Semantics," branching into "Lexical semantics," "Structural semantics," and "Cognitive semantics." Each of these branches further subdivides into more specific semantic approaches, for example, lexical semantics includes "Word senses," "Polysemy," and "Word formation." The diagram's purpose is to visually represent the relationship and categorization of different semantic theories within the paper. -->

Figure 1: Semantic research domains in linguistics. Lex. denotes lexical; sem. denotes semantics; und. denotes understanding.
*nary*[\(Simpson and Weiner, 1989\)](#page-94-0) became one of the most significant symbols of lexical semantic research achievements. The research of lexical semantics covers word senses, polysemy, word formation, contrastive lexical semantics, and more. Next, another important research dimension of semantics emerged, termed structural semantics. Structural semantics emphasizes the analysis of sentence structures, including the relationships between words and the ways in which words contribute to the meaning of a sentence. The study of structural semantics includes but is not limited to analyzing the meaning of words by syntax, grammar, and pragmatics. Structural semantics elevates the study of semantics from the word level to the sentence level. The later cognitive semantics further enrich the connotation of semantics. The tenets of cognitive semantics posit that the faculty of language is intricately intertwined with the broader cognitive capacity of human beings [\(Croft and Cruse, 2004\)](#page-82-0). In other words, semantics is a reflection of how humans understand and make sense of the world around them. Under cognitive semantics, researchers extend to frame semantics (semantics is the reflection of encyclopedic knowledge), situation semantics (semantics reflects the relationships between situations) [\(Barwise and Perry, 1981\)](#page-80-0), conceptual semantics (semantics reflects the structural perception of concepts) [\(Jackendo](#page-86-0)ff, [1976\)](#page-86-0), and more. Figure [1](#page-1-0) summarizes partial semantic research domains in linguistics.

The development of automatic semantic processing techniques has largely facilitated semantic research. Many useful tools and knowledge bases[1](#page-1-1) were developed for word sense disambiguation, anaphora resolution, named entity recognition, concept extraction, and subjectivity detection. These tools are the embodiment of many theoretical ideas in semantics. For example, word sense disambiguation is an important task in lexical semantics. Anaphora resolution elucidates the relationship between the anaphor, which is the repetition of a reference, and its antecedent, which is the earlier mention of the entity. Anaphora resolution determines the structural semantics of the anaphor. Named entity recognition categorized named entities in texts by conceptually related classes, e.g., names, and locations. Similarly, concept extraction and subjectivity detection tasks also embody the cognitive properties of semantics.

In addition to improving semantic research, semantic processing techniques can also help other downstream natural language processing (NLP) tasks with more complexity (see Table [1\)](#page-2-0). For example, subjectivity detection can be an upstream task of sentiment analysis, because subjective expressions can be further categorized by positive, negative, and neutral expressions with different opinionated intensities. The semantic processing techniques that have been reviewed possess a range of potential applications, including the ability to generate features that are effective, as well as to be used as a parser in order to obtain desired categories of text. Additionally, these techniques have the potential to improve the explainability of downstream applications.

The emergence of pre-trained language models (PLMs) has greatly enhanced the semantic representation capabilities of deep learning models and the ability to fit downstream tasks [\(Devlin et al., 2019;](#page-83-0) [Liu et al., 2019b;](#page-89-0) [Lewis et al.,](#page-88-0) [2020\)](#page-88-0). Some large language models (LLMs), e.g., GPT-4[2](#page-1-2) and Bard[3](#page-1-3) even realize the functions of multiple complex

<span id="page-1-2"></span><span id="page-1-1"></span><sup>1</sup>A knowledge base normally refers to a collection of organized information that is machine-readable, and supportive for an intelligent system. <sup>2</sup><https://openai.com/product/gpt-4>

<span id="page-1-3"></span><sup>3</sup><https://bard.google.com/>

<span id="page-2-0"></span>

| Downstream tasks                   | WSD     | AR   | NER     | CE      | SD   |
|------------------------------------|---------|------|---------|---------|------|
| Sentiment Computing                | F, P, E | F    |         | F, P, E | P    |
| Information Retrieval              | E       |      |         | F, E    | P    |
| Machine Translation                | F, P, E | F, E |         |         |      |
| Summarization                      |         | F    |         |         |      |
| Textual Entailment                 |         | F    |         |         |      |
| Knowledge Graph Construction       |         |      | P       |         |      |
| Recommendation Systems             |         |      | F, P, E |         |      |
| Dialogue Systems                   |         |      | P, E    | F, P    |      |
| Commonsense Explanation Generation |         |      |         | F, E    |      |
| Hate Speech Detection              |         |      |         |         | F, P |
| Question & Answering Systems       |         |      |         |         | F, P |

Table 1: The surveyed semantic processing tasks and their downstream applications. F denotes that the technique yielded features for a downstream task model; P denotes that the technique was used as a parser; E denotes that the technique improved the explainability for a downstream task. WSD denotes word sense disambiguation. AR denotes anaphora resolution. NER denotes named entity recognition. CE denotes concept extraction. SD denotes subjectivity detection.

NLP tasks by the means of dialogue, such as question answering, translation, and text summarization. Many semantic processing studies have gradually faded out of the field of NLP. Then, in the era of PLMs and LLMs, an intuitive question is what is the motivation for studying semantic processing techniques?

As mentioned before, semantics reflects the multiple aspects of language. Besides understanding word senses, semantics is also the entrance to understanding the mechanism, and perception of language. Language intelligence encompasses more than just achieving a level of accuracy that is equivalent to or surpasses human accuracy for specific tasks. It also entails the capacity to unveil the nature of language and investigate the cognitive processes that underlie language. Much aforementioned semantic research in the context of linguistics has not been explored in computational linguistics to our best knowledge. Thus, we are motivated to propose a survey on semantic processing techniques to encourage future scholars that can expand the depth and breadth of semantic research, leading the public attention from the application value of NLP techniques to the research value of computational linguistics. Nevertheless, we also highlight the fusion of low-level semantic processing techniques and high-level NLP techniques to demonstrate the application value of semantic processing techniques in different domains.

Given the broadness of semantics, our survey scope lies in semantic processing techniques for word sense disambiguation, anaphora resolution, concept extraction, named entity recognition, and subjectivity detection. This is because these low-level semantic processing tasks reflect different aspects of semantics. In addition, there were many research works on these tasks in the field of computational linguistics. We focus on low-level semantic processing tasks, rather than high-level semantic processing tasks, e.g., sentiment analysis and natural language inference, because they provide fundamental building blocks for both high-level semantic processing tasks and higher-level NLP tasks.

Multiple semantic processing techniques were rarely surveyed in the same article. [Salloum et al.](#page-94-1) [\(2020\)](#page-94-1) surveyed several high-level semantic processing tasks, e.g., latent semantic analysis, explicit semantic analysis, and sentiment analysis. Compare to the work of [Salloum et al.](#page-94-1) [\(2020\)](#page-94-1), our survey includes the latest research in low-level semantic processing techniques. Compare to the latest semantic processing surveys focusing on specific tasks [\(Ransing and](#page-93-0) [Gulati, 2022;](#page-93-0) [Poesio et al., 2023;](#page-92-1) [Fu et al., 2020;](#page-84-0) [Wang et al., 2022;](#page-97-0) [Montoyo et al., 2012\)](#page-90-0), we additionally reviewed important theoretical research and downstream task applications in these domains. These contents can help readers better understand the foundation of semantic research in linguistics, as well as potential application scenarios. More importantly, theoretical research shows the big picture of a semantic processing task, which may inspire different research tasks in the computational linguistic community. The collection of multiple semantic processing techniques is helpful for readers to have a comprehensive understanding of a large field, inspiring more fusion research across different domains. Theoretical research of other tasks has the potential to inspire fresh perspectives among researchers who have been concentrating on a specific semantic research task.

The contribution of this survey is threefold:

- We survey recent semantic processing techniques, annotation tools, datasets, and knowledge bases for five low-level semantic processing tasks.
- We highlight important theoretical research, and downstream applications to encourage deeper and wider re-

<span id="page-3-1"></span>![](_page_3_Figure_0.jpeg)
<!-- Image Description: This figure presents a hierarchical tree diagram illustrating the relationships between semantic processing techniques (WSD, Anaphora Resolution, NER, Concept Extraction, Subjectivity Detection) and their associated technical trends and downstream applications. Each semantic processing technique branches into specific technical approaches (e.g., rule-based, statistical-based WSD) which, in turn, connect to various downstream applications (e.g., sentiment computing, machine translation). The diagram visually organizes and clarifies the connections between these areas within natural language processing. -->

Figure 2: The summary of technical trends and downstream applications of surveyed semantic processing tasks. KGC denotes knowledge graph construction. CEG denotes commonsense explanation generation. RE denotes relation extraction.

search in the semantic processing domain upon the currently established task setups.

• We compare different semantic processing techniques, delineate their technical and application trends, and put forth potential avenues for future research in this domain.

In the following sections, we introduce different semantic processing techniques, e.g., word sense disambiguation (Section [2\)](#page-3-0), anaphora resolution (Section [3\)](#page-17-0), named entity recognition (Section [4\)](#page-32-0), concept extraction (Section [5\)](#page-47-0), and subjectivity detection (Section [6\)](#page-60-0). We discuss the interactions between the surveyed tasks and the impacts of deep learning and LLMs on semantic processing in Section [7.](#page-75-0) Finally, we conclude this survey in Section [8.](#page-77-0) Each task is structured by theoretical research, annotation schemes, datasets, knowledge bases, evaluation metrics, methods, downstream applications, and a summary. Figure [2](#page-3-1) demonstrates the taxonomy of methods and downstream applications of each task in this survey.

# <span id="page-3-0"></span>2. Word Sense Disambiguation

The complexity of human language is difficult for machines to understand it. One of the challenges is the ambiguity of word senses. In natural language, a word may have multiple senses, given different contexts. Consider the following example:

<span id="page-4-1"></span>![](_page_4_Figure_0.jpeg)
<!-- Image Description: The image contrasts knowledge-based and supervised Word Sense Disambiguation (WSD) methods. The left side shows a knowledge-based approach illustrated as a graph where nodes represent word senses (gloss), with connections showing relationships. The right side depicts a supervised method using a predictive model that assigns probabilities (0.3, 0.6, 0.1) to different senses based on context. Different shapes represent glosses of target and context words. The image clarifies the contrasting approaches to WSD. -->

Figure 3: Simplified examples of the knowledge-based and supervised WSD.

## (1) He got his shoes wet as he walked along the bank.

According to the Oxford English Dictionary, the major senses of "bank" include (a)*an organization that provides various financial services, for example keeping or lending money*; (b) *the side of a river, canal, etc. and the land near it*. With the context, humans can easily know that "bank" here refers to the sense (b). However, it is challenging for machines to do so because the interpretation made by humans is contingent upon their comprehension of the fact that the probability of getting one's shoes wet is higher when walking alongside a river bank as compared to a financial institution. Machines rarely take the commonsense into account when inferring the meaning of "bank"[4](#page-4-0) , because they don't have human-like cognition and reasoning abilities by nature.

There are two main technical trends in addressing the task of WSD, namely knowledge-based methods and supervised methods. Knowledge-based WSD utilizes the word relations from knowledge graphs, e.g., WordNet and BabelNet [\(Navigli and Ponzetto, 2012\)](#page-91-1) to achieve the disambiguation of word senses. In supervised methods, the WSD task is usually defined as a classification task by word senses. A WSD model is trained with annotated data. Two examples of knowledge-based and supervised WSD are illustrated in Figure [3.](#page-4-1) As shown in the figure, a naive strategy of the knowledge-based WSD is that the sense that shares the most relations with the context words is selected as the best-matched one. For supervised WSD systems, the predictive model predicts the potential senses, given the target word and its context words as input. In recent times, the use of knowledge bases has proven advantageous for several modern supervised systems. As a result, there has been a growing trend in integrating knowledge-based and supervised methods to enhance their performance [\(Wang and Wang, 2020\)](#page-97-1).

WSD has been recognized as a crucial module in numerous NLP tasks that heavily rely on word senses, such as sentiment computing, information retrieval, and machine translation. The application of WSD techniques has been demonstrated to be beneficial for these NLP tasks. While prior surveys [\(Bevilacqua et al., 2021a;](#page-80-1) [Navigli, 2009\)](#page-91-2) have conducted extensive reviews for WSD, the works discussed in them are outdated. Besides, those works do not link WSD with the linguistic theories and diverse downstream tasks.

### *2.1. Theoretical Research*#*2.1.1. Distributional Semantics*The hypothesis from distributional semantics [\(Firth, 1957\)](#page-83-1) argued that word meanings can be inferred from word co-occurrences. Words that appear in similar contexts tend to have similar meanings. Such a hypothesis has been the most significant foundation of developing semantic representations in the computational linguistics community, e.g., vector space representations [\(Turney and Pantel, 2010;](#page-96-0) [Mikolov et al., 2013;](#page-90-1) [Pennington et al., 2014\)](#page-92-2) and PLMs [\(De](#page-83-0)[vlin et al., 2019;](#page-83-0) [Liu et al., 2019b\)](#page-89-0). Based on such a hypothesis, dense semantic vectorial representation research commonly follows a similar training paradigm, e.g., using context words to predict a target word. Currently, Chat-GPT further proves that learning to use words that have appeared before to predict the next possible word can achieve the skills of analogy and reasoning with the help of a very large Transformer [\(Vaswani et al., 2017\)](#page-96-1)-based model.

<span id="page-4-0"></span><sup>4</sup>Current methods likely disambiguate word senses by word co-occurrences. However, word co-occurrences are not commonsense.

#*2.1.2. Selectional Preference*[Wilks](#page-97-2) [\(1973\)](#page-97-2) proposed a concept of selectional preference. It is a procedure for representing the meaning structure of natural language. Compared to the "derivational paradigm" of transformational grammar and generative semantics, [Wilks](#page-97-2) [\(1973\)](#page-97-2) believed that selectional preference is a more efficient procedure in natural language understanding. It focuses on determining preferences between various possible interpretations of a text, rather than identifying a solitary and unequivocally correct interpretation. Selection preference theory allows more flexibility and nuance in understanding word senses and language. Besides, the theory is computation-friendly. [Wilks](#page-97-2) [\(1973\)](#page-97-2) showed how the procedure could be computed and implemented. The work of [Wilks](#page-97-2) [\(1973\)](#page-97-2) supports that there are multiple possible meanings for a word. The meaning can be defined by the sectional preference of contexts.

##*2.1.3. Construction Semantics*

[Goldberg and Suttle](#page-84-1) [\(2010\)](#page-84-1) argued that the meanings of words are frequently derived from larger language units, termed constructions. Constructions consist of a form and a meaning, ranging from single words to full sentences in size. The interpretation of a construction is reliant on both its structure and the situations in which it is employed. [Goldberg and Suttle](#page-84-1) [\(2010\)](#page-84-1) argued that semantic restrictions are better linked with the construction as an entirety rather than with the lexical semantic framework of the verbs. The work of [Goldberg and Suttle](#page-84-1) [\(2010\)](#page-84-1) highlights that the interpretation of meanings of language units can be extended from individual words to constructions. It shows the necessity of defining language units in WSD.

### <span id="page-5-2"></span>*2.1.4. Frame Semantics*[Fillmore et al.](#page-83-2) [\(2006\)](#page-83-2) proposed frame semantics that provides a distinct viewpoint on the meanings of words and the principles behind language construction. Frame semantics emphasizes the significance of the surrounding context and encyclopedic knowledge in comprehending word meanings. [Petruck](#page-92-3) [\(1996\)](#page-92-3) explained that a "frame" refers to a collection of concepts interconnected in a manner that understanding any one concept depends on the understanding of the complete system. In frame semantics, the meaning of "cooking" is beyond its dictionary meaning. It also associates with the concept of "food", "cook", "container", and "heating instrument". Frame semantics motivates later ontology research, e.g., FrameNet [\(Ruppenhofer et al., 2016\)](#page-93-1) and FrameNet-based WSD systems, significantly.

#*2.2. Annotation Schemes*For knowledge-based WSD, the data are normally presented as ontology, such as WordNet, FrameNet, and Babel-Net, where words and concepts are connected by relations. The relations include hyponyms, hypernyms, holonyms, meronyms, attributes, entailment, etc. An explanation (gloss) and a few example sentences are given for each synset. Synsets of the same Part Of Speech (POS) are connected under some relations independently. However, there exist relations when the basic concept of two words is the same but in a different POS (for example, "propose" and "proposal" were characterized as "derivationally related synsets" in WordNet).

For supervised WSD, a particular word in a given sentence is annotated with a sense ID that corresponds to one of the potential senses in a knowledge base, such as WordNet. A sample of annotation is shown in the next section.

##*2.3. Datasets*Our surveyed datasets and their statistics can be viewed in Table [2.](#page-6-0) The biggest manually annotated English corpus currently accessible is SemCor[5](#page-5-0) [\(Miller et al., 1993\)](#page-90-2). It has 200K content terms tagged with their related definitions and around 40K phrases. Although SemCor serves as the principal training corpus for WSD, its limited coverage of the English vocabulary for both words and meanings is its most significant drawback. In essence, SemCor merely includes annotations for 22K distinct lexemes in WordNet, the most extensive and commonly employed computerized English dictionary, which corresponds to less than 15% of all words.

To augment the coverage of words, some studies [\(Vial et al., 2019\)](#page-96-2) incorporated the English Princeton WordNet Gloss Corpus (WNG)[6](#page-5-1) , which contains more than 59K WordNet senses, as a complemented data. The WNG is annotated manually or semi-automatically.

<span id="page-5-0"></span><sup>5</sup><http://web.eecs.umich.edu/~mihalcea/downloads.html>

<span id="page-5-1"></span><sup>6</sup><https://wordnetcode.princeton.edu/glosstag.shtml>

<span id="page-6-0"></span>

| Dataset         | Source                              | # Samples | Reference                   |
|-----------------|-------------------------------------|-----------|-----------------------------|
| SemCor          | WordNet                             | 200,000   | Miller et al. (1993)        |
| MultiSemCor     | WordNet,<br>bilingual Collins       | 51,847    | Pianta et al. (2002)        |
| Line-hard-serve | WSJ, APHB                           | 4,000     | Leacock et al. (1993)       |
| Interest        | HECTOR                              | 2,369     | Bruce and Wiebe (1999)      |
| DSO             | Brown, WSJ                          | 192,800   | Ng and Lee (1996)           |
| OMWE            | Web                                 | 29,165    | Chklovski and Pantel (2004) |
| OMSTI           | UN documents                        | 1,357,922 | Taghipour and Ng (2015)     |
| SensEval-2      | Unknown                             | 2,282     | Edmonds and Cotton (2001)   |
| SensEval-3      | Editorial, news<br>story, & fiction | 1,850     | Snyder and Palmer (2004)    |
| SemEval2007     | Brown, WSJ                          | 455       | Pradhan et al. (2007)       |
| SemEval2013     | SMT workshop                        | 1,644     | Navigli et al. (2013)       |
| SemEval2015     | EMEA, KDEdoc,<br>EUB                | 1,022     | (Moro and Navigli, 2015)    |

Table 2: WSD datasets and statistics. SMT, EMEA, KDEdoc, and EUB denote statistical machine translation, European Medicines Agency documents, KDE manual corpus, and the EU bookshop corpus, respectively.

SemCor and its variations [\(Bentivogli and Pianta, 2005;](#page-80-2) [Bond et al., 2012\)](#page-80-3) lack an acceptable multi-lingual equivalent in the majority of global languages, which limits the scaling capabilities of WSD models beyond English. To address the aforementioned issues, numerous automatic methods for creating multi-lingual sense-annotated data have been developed [\(Pasini and Navigli, 2017;](#page-92-6) [Pasini et al., 2018;](#page-92-7) [Scarlini et al., 2019;](#page-94-2) [Pasini and Navigli, 2020\)](#page-92-8). In an English-Italian parallel corpus known as MultiSemCor [\(Pianta et al., 2002\)](#page-92-4), senses from the English and Italian versions of WordNet are annotated.

The Line-hard-serve corpus [\(Leacock et al., 1993\)](#page-88-1) contains 4K samples of the nominal, adjective, and verbal words with sense tags. The data were sourced from Wall Street Journal (WSJ) corpus and the American Printing House for the Blind (APHB) corpus. The Interest corpus [\(Bruce and Wiebe, 1999\)](#page-81-0) contains 2,369 occurrences of the term*interest*that have been sense-labeled. The data were sourced from the HECTOR word sense corpus [\(Atkins,](#page-79-0) [1992\)](#page-79-0). The Defence Science Organisation (DSO), based in Singapore, created the DSO corpus[7](#page-6-1) [\(Ng and Lee, 1996\)](#page-91-3), which contains 192,800 sense-tagged tokens from 191 words from the Brown and WSJ corpora. The Open Mind Word Expert (OMWE) dataset[8](#page-6-2) [\(Chklovski and Pantel, 2004\)](#page-82-1) is a corpus of sentences with 288 noun occurrences that were jointly annotated by Web users. One Million Sense-Tagged for Word Sense Disambiguation and Induction (OMSTI)[9](#page-6-3) [\(Taghipour and Ng, 2015\)](#page-95-0) is a semi-automatically annotated WSD dataset with WordNet sense inventory. The data were sourced from MultiUN corpus, which is a collection of United Nation documents. The OMSTI includes 687,871 nouns, 412,482 verbs, and 251,362 adjectives and 6,207 adverbs after including selected samples from SemCor and DSO.

The SensEval and SemEval datasets are created from the SensEval/SemEval evaluation campaigns. Now, these datasets have been the most widely used benchmarking datasets in WSD. [Raganato et al.](#page-93-2) [\(2017b\)](#page-93-2) collected these datasets together[10](#page-6-4) and developed a unified evaluation framework for empirical comparison. The statistics of the following datasets are from the collection of [Raganato et al.](#page-93-2) [\(2017b\)](#page-93-2). SensEval-2 [\(Edmonds and Cotton, 2001\)](#page-83-3) used WordNet 1.7 sense inventory, including 2,282 sense annotations for nouns, verbs, adverbs and adjectives. SensEval-3 [\(Snyder and Palmer, 2004\)](#page-95-1) employed WordNet 1.7.1 sense inventory, including 1,850 sense annotations. SemEval-2007 Task 17 [\(Pradhan et al., 2007\)](#page-92-5) employed WordNet 2.1 sense inventory, including 455 nominal and verbal sense annotations. SemEval-2013 Task 12 [\(Navigli et al., 2013\)](#page-91-4) used WordNet 3.0 sense inventory, including 1,644 nominal sense annotations. SemEval-2015 Task 13 [\(Moro and Navigli, 2015\)](#page-90-3) utilized WordNet 3.0 sense inventory, including 1,022 sense annotations. It is worth noting that some of the SemEval tasks are multi-lingual, including SemEval 2013 and 2015, which facilitates multi-lingual WSD.

All of these corpora are annotated using various WordNet sense inventories, with the exception of the Interest corpus (tagged with LDOCE senses) and the Senseval-1 corpus. The Interest corpus and the Senseval-1 corpus were

<span id="page-6-1"></span><sup>7</sup><https://borealisdata.ca/dataset.xhtml?persistentId=doi:10.5683/SP2/QPOJSI>

<span id="page-6-2"></span><sup>8</sup><http://web.eecs.umich.edu/~mihalcea/downloads/OMWE/OMWE1.0.English.tar.gz>

<span id="page-6-3"></span><sup>9</sup><https://www.comp.nus.edu.sg/~nlp/corpora.html>

<span id="page-6-4"></span><sup>10</sup><http://lcl.uniroma1.it/wsdeval/home>

sense-labeled using the HECTOR sense inventories, a lexicon and corpus from a joint Oxford University Press/Digital project [\(Atkins, 1992\)](#page-79-0)

Generally, the data and labels in WSD datasets are organized in the following forms. Then, the task is to identify the sense classes, given contexts, and target words.

```text
context: "You perform well in the exam, I will reward you.",
target word: "perform",
pos: "VB",
sense: "3"
context: "She worked in a renowned university for a long time.",
"target word": "university",
"pos": "NN",
"sense": "2"
```text

#*2.4. Knowledge Bases*| Name          | Knowledge               | # Entities | Structure    |
|---------------|-------------------------|------------|--------------|
| LDOCE 6th ed. | Lexical                 | 230,000    | Unstructured |
| ODE 2022      | Lexical                 | 600,000    | Unstructured |
| CED 12th ed.  | Lexical                 | 722,000    | Unstructured |
| OALD 8th ed.  | Lexical                 | 145,000    | Unstructured |
| WordNet       | Lexical                 | 95,600     | Graph        |
| FrameNet      | Lexical                 | 13,687     | Graph        |
| BabelNet      | Lexical & Multi-lingual | 26,044,643 | Graph        |
| SyntagNet     | Lexical                 | 78,000     | Graph        |

Table 3: Useful knowledge bases for WSD. LDOCE means Longman Dictionary of Contemporary English. ODE means Oxford Dictionary of English. CED means Collins English Dictionary. OALD means Oxford Advanced Learner's Dictionary of Current English. Unstructured or structured means the knowledge base contains unstructured or structured lexical knowledge by concepts.

Machine-Readable Dictionaries (MRDs) have been a useful source for WSD due to their structured knowledge and easy access [\(Navigli, 2009\)](#page-91-2). Dictionaries frequently contain extensive information about the various meanings of a word, as well as illustrative examples of their usage within context. Therefore, dictionaries can serve as valuable knowledge bases for the task of WSD. Additionally, MRDs may provide further information such as synonyms, antonyms, and related words, which can aid in facilitating a more comprehensive comprehension of a word's meaning. Through the analysis of this information, a system may make more precise determinations about which meaning is most fitting in a given context. There are many electronic dictionaries available for machines to refer to, such as the Longman Dictionary of Contemporary English (LDOCE) [\(Mayor, 2009\)](#page-89-1), the Oxford Dictionary of English (ODE) [\(Dictionary, 2010\)](#page-83-4), Collins English Dictionary (CED) [\(Dictionary, 1982\)](#page-83-5), and the Oxford Advanced Learner's Dictionary of Current English (OALD) [\(Hornby and Cowie, 1974\)](#page-85-0).

WordNet [\(Miller et al., 1990\)](#page-90-4) is a sizable, manually curated lexicographic database of English. It is arranged as a network with synsets, or collections of contextual synonyms, as nodes. A synset of synonyms each represents one of a word's senses. Through edges that express lexical-semantic links like meronymies (partof) and hypernymies (is-a), synsets and senses are connected to one another. WordNet additionally offers definitions (glosses) and uses examples for each synset as additional lexical information. The most current English WSD works use the 3.0 version, which was published in 2006 and has 117,659 synsets. Following the initial WordNet for English, many WordNets for other languages have been proposed, including languages such as Chinese [\(Wang and Bond, 2013\)](#page-97-3), Arabic [\(Black et al.,](#page-80-4) [2006\)](#page-80-4), Dutch [\(Postma et al., 2016\)](#page-92-9), etc[11](#page-7-0) .

FrameNet [\(Ruppenhofer et al., 2016\)](#page-93-1) is an English lexical repository that is readable by both humans and machines, established by annotating real-life textual examples that depict the usage of words. It was developed based on the

<span id="page-7-0"></span><sup>11</sup>See <http://globalwordnet.org/resources/wordnets-in-the-world/> for a summary.

theory of frame semantics, containing 1,224 frames (a frame refers to a diagrammatic representation of a scenario encompassing diverse elements such as participants, props, and other conceptual roles), and 13,687 lexical units (lemmas and their PoS) that evoke frames. In FrameNet, the lexical units of a sentence are associated with frame elements. Frame elements are the semantic role of lexical units. For example, given a sentence "I ate an apple this afternoon", "apple" would fill the role of "food" (a frame element).

BabelNet [\(Navigli and Ponzetto, 2012\)](#page-91-1) is a multi-lingual dictionary that covers both lexicographic and encyclopedic entries from 520 languages. These entries were created by semi-automatically mapping numerous sites, including WordNet, Multi-lingual WordNet, and Wikipedia. The topology of BabelNet is that of a semantic network, where the nodes are multi-lingual synsets (collections of synonyms that have been lexicalized in several languages), and the edges represent the semantic connections between them.

SyntagNet [\(Maru et al., 2019\)](#page-89-2) is a manually developed lexical resource that integrates semantically disambiguated lexical combinations, e.g., noun-verb and noun-noun pairs. The development of SyntagNet involved initially extracting lexical combinations from English Wikipedia and the British National Corpus, which were then subjected to a process of manual disambiguation, based on the WordNet. SyntagNet covers five major languages, e.g., English, German, French, Spanish, and Italian.

##*2.5. Evaluation Metrics*In the WSD task, given a sentence of*<sup>n</sup>*words*<sup>T</sup>* <sup>=</sup> {*x*1, ..., *<sup>x</sup>n*}, the model predicts a sense for each word given the dictionary. Normally, the F1 score is adopted, which is a specialization of the F score when α <sup>=</sup> 1:

$$
F = \frac{1}{\alpha \frac{1}{P} + (1 - \alpha) \frac{1}{R}}
$$
\n<sup>(1)</sup>

Where *P*denotes precision and*R*denotes recall:

$$
P = \frac{\text{correct predictions}}{\text{total predictions}}\tag{2}
$$

$$
R = \frac{\text{correct predictions}}{n} \tag{3}
$$

The aforementioned metrics do not accurately represent how well systems can produce a level of confidence for a particular sensory choice. [Resnik and Yarowsky](#page-93-3) [\(1999\)](#page-93-3) developed an evaluation criterion that considers the discrepancies between the accurate and selected senses to weigh misclassification mistakes. Therefore, this error will be penalized less severely than coarser sense distinctions if the chosen sense is a fine-grained distinction of the true sense. There have been evaluation metrics for even more precise measurements, including the Receiver Operation Characteristic (ROC) [\(Cohn, 2003\)](#page-82-2). However, compared with traditional metrics such as precision, recall, and F1, these metrics are not frequently utilized.

###*2.6. Annotation Tools*LX-SenseAnnotator[12](#page-8-0) [\(Neale et al., 2015\)](#page-91-5) provides a user interface for manually annotating word senses. The software has the capability to process lexical data in any language, on the condition that the data is compliant with the format of Princeton WordNet. Human annotators can view the pre-processed text in three different modes, including the source text, sense-annotated text, and raw text, which can be switched between by using a tab widget. The source text mode displays the original text along with all tags, while the sense-annotated text mode displays the same text but with newly added sense tags. This allows the annotator to monitor the output file continually. Annotators can view the sense options in real time when annotating the sense for a word.

LexTag[13](#page-8-1) is another useful tool for WSD. The annotation interface provided is characterized by its user-friendly nature, facilitating users in the annotation of various textual elements such as terms, sentences, and documents. This

<span id="page-8-0"></span><sup>12</sup><http://nlx.di.fc.ul.pt/tools.html>

<span id="page-8-1"></span><sup>13</sup>https://babelscape.com/lextag

annotation process involves attributing meanings drawn from pre-existing knowledge graphs and dictionaries, encompassing reputable sources like WordNet, Wiktionary, and WordAtlas. LexTag has been used to create a recent 10-language parallel dataset ELEXIS-WSD 1.0[14](#page-9-0) .

####*2.7. Methods*##*2.7.1. Knowledge-Based WSD*

Knowledge-based WSD utilizes knowledge bases to disambiguate word senses. Compared with supervised WSD, this class of WSD methods achieves lower performance but better data efficiency. In knowledge-based WSD, there are essentially two research streams.

# A. Semantic Space Matching

One stream of the knowledge-based WSD is to look for overlaps or similarities between the context of a term whose sense needs to be disambiguated and its sense representation, such as the definition of a potential sense and its associated sense that was retrieved from a knowledge base. The predicted sense is considered to be the sense that is the closest.

Lesk [\(Lesk, 1986\)](#page-88-2) is a naive knowledge-based WSD algorithm that looks for terms that are similar to the target word in the context of each sense. The approach aimed to enumerate the intersections among lexicon definitions of the diverse connotations of every target word contained within a given sentence. [Banerjee et al.](#page-79-1) [\(2003\)](#page-79-1) proposed an advanced version of the Lesk, which also includes the definition of related senses, where the standard term frequencyinverse document frequency method is employed for word weighting. Another improved version of Lesk [\(Basile](#page-80-5) [et al., 2014\)](#page-80-5) includes word embedding for better analysis, which improves the accuracy of determining how close the definition and context of the target word are. SREF*KB* [\(Wang and Wang, 2020\)](#page-97-1) is a state-of-the-art (SOTA) WSD system. It is a vector-based technique that disambiguates word senses by using sense embeddings and contextualized word representations. It applied BERT to represent WordNet instances and definitions, as well as the automatically obtained contexts from the Web.

## B. Graph-based Matching

The other stream of the knowledge-based WSD creates a graph using the given context and connections that have been retrieved from knowledge bases. Here, the synsets and the relationships between them are seen as the nodes and edges, respectively. The senses are then disambiguated based on the constructed graphs. A variety of graph-based techniques, such as Latent Dirichlet Allocation (LDA) [\(Blei et al., 2003\)](#page-80-6), PageRank [\(Brin and Page, 1998\)](#page-81-1), Random Walks [\(Agirre et al., 2014\)](#page-78-0), Clique Approximation [\(Moro et al., 2014b\)](#page-90-5), Game Theory [\(Tripodi and Navigli, 2019\)](#page-96-3), etc., are used to disambiguate the meaning of a given word using the created graph.

[Agirre and Soroa](#page-78-1) [\(2009\)](#page-78-1) presented a graph-based unsupervised WSD system that employs random walk over a WordNet semantic network. They employed a customized version of the Page Rank algorithm [\(Haveliwala, 2002\)](#page-85-1). The technique leverages the inherent structural properties of the graph that underlies a specific lexical knowledge base, and shows the capability of the algorithm to identify global optima for WSD, based on the relations among entities. [Agirre et al.](#page-78-0) [\(2014\)](#page-78-0) evaluated this algorithm with new datasets and variations of the algorithm to prove its effectiveness. [Navigli and Lapata](#page-91-6) [\(2007\)](#page-91-6) also introduced a graph-based unsupervised model for WSD, which analyzed the connectivity of graph structures to identify the most pertinent word senses. A graph is constructed to represent all possible interpretations of the word sequence, where nodes represent word senses and edges represent sense dependencies. The model assessed the graph structure to determine the significance of each node, thus finding the most crucial node for each word. Babelfy [\(Moro et al., 2014b\)](#page-90-5) is also a graph-based WSD method that uses random walk to identify relationships between synsets. It used BabelNet [\(Navigli and Ponzetto, 2012\)](#page-91-1) and performed random walks with Restart [\(Tong et al., 2006\)](#page-96-4). In addition, it incorporated the entire document at the time of disambiguation. The candidate disambiguation is upon automatically developed semantic interpretation graph which used a graph structure to represent various possible interpretations of input text. SyntagRank [\(Scozzafava et al., 2020\)](#page-94-3) is a highscoring knowledge-based WSD algorithm. It is an entirely graph-based algorithm that uses the Personalized PageRank algorithm to incorporate WordNet (for English), BabelNet (for non-English) and SyntagNet. SyntagRank is generally considered a stronger method than SREF*KB*. BabelNet enabled SyntagRank to improve its ability to scale across a wide range of languages, whereas SREF*KB*has only been evaluated in English.

<span id="page-9-0"></span><sup>14</sup>https://www.clarin.si/repository/xmlui/handle/11356/1674

#*2.7.2. Supervised WSD*Currently, supervised approaches, especially deep learning-based supervised learning approaches, have become mainstream in the WSD community. Earlier deep learning-based approaches focused on architectures where WSD was defined as token classification over WordNet senses [\(Kågeback and Salomonsson, 2016\)](#page-86-1). Even though they per- ¨ formed well, these structures showed a lot of flaws, particularly when it came to predicting uncommon and invisible senses. To address these issues, numerous works began to supplement the training data by utilizing various lexical knowledge, such as sense definitions [\(Kumar et al., 2019;](#page-87-0) [Blevins and Zettlemoyer, 2020\)](#page-80-7), semantic relations [\(Bevilac](#page-80-8)[qua and Navigli, 2020;](#page-80-8) [Conia and Navigli, 2021\)](#page-82-3), and data generated via novel generative methods [\(Barba et al.,](#page-79-2) [2021b\)](#page-79-2). In this section, we review the representative works in supervised WSD.

## A. Data-Driven Machine Learning Approaches

Data-driven machine learning approaches refer to methodologies and techniques in which the design, training, and optimization of traditional machine learning algorithms, heavily rely on large amounts of data. In these approaches, the model's ability to generalize patterns and make predictions is learned directly from the provided data, rather than being explicitly programmed by humans. In the early days, classic machine learning approaches with handcrafted features were frequently used for WSD.

[Singh et al.](#page-94-4) [\(2014\)](#page-94-4) employed 5-gram and position features, and a decision tree algorithm to represent classification rules in a tree structure where the training dataset is recursively partitioned. Each leaf node indicates the meaning of a word. They developed a dataset, containing 672 Manipuri sentences to test their method. The sentences were sourced from a local newspaper, termed "The Sangai Express". [O'Hara et al.](#page-91-7) [\(2004\)](#page-91-7) proposed a class-based collocation method that integrates diverse linguistic features in a decision tree algorithm. For the collocation, three distinct word relatedness scores are used: the first is based on WordNet hypernym relations; the second is based on cluster-based word similarity classes; and the third is based on dictionary definition analysis. The authors also utilized PoS and word form features. The It Makes Sense (IMS) WSD system [\(Zhong and Ng, 2010\)](#page-99-1) used a Support Vector Machine (SVM) classifier. Different positional and linguistic features were considered, including nearby words, nearby words' PoS tags, and nearby collocations. Later, word embeddings became important features in WSD. [Taghipour and Ng](#page-95-0) [\(2015\)](#page-95-0); [Rothe and Schutze](#page-93-4) [\(2015\)](#page-93-4); [Iacobacci et al.](#page-86-2) [\(2016\)](#page-86-2) used IMS as the base model to examine word embeddings. ¨ [Iacobacci et al.](#page-86-2) [\(2016\)](#page-86-2) offered many approaches where different word embeddings were applied as features to test how many parameters impact the effectiveness of a WSD system. The authors found that word2vec [\(Mikolov et al.,](#page-90-1) [2013\)](#page-90-1) which was trained with OMSTI can yield the strongest results on the three examined all-word WSD tasks.

# B. Data-Driven Neural Approaches

More recently, neural approaches started to be used. Data-driven neural approaches refer to methodologies and techniques that utilize neural networks and supervised learning to learn patterns and representations directly from data.

[Popov](#page-92-10) [\(2017\)](#page-92-10) proposed to use BiLSTM [\(Graves and Schmidhuber, 2005\)](#page-84-2), GloVe word embeddings, and word2vec lemma embeddings. [Yuan et al.](#page-99-2) [\(2016\)](#page-99-2) suggested another LSTM-based word sense disambiguation approach that was trained in a semi-supervised fashion. The semi-supervised learning was achieved by employing label propagation [\(Talukdar and Crammer, 2009\)](#page-95-2) to assign labels to unannotated sentences by assessing their similarity to labeled ones. The best performance on the SensEval-2 dataset can be observed from the model that was semi-supervisiontrained with OMSTI and 1,000 additional unlabeled sentences. Additionally, [Le et al.](#page-87-1) [\(2018\)](#page-87-1) looked more closely at how many elements affect its performance, and several intriguing conclusions were drawn. The initial point to highlight is that achieving strong WSD performance does not necessitate an exceedingly large unannotated dataset. Furthermore, this method provides a more evenly-distributed sense assignment in comparison to prior approaches, as evidenced by its relatively strong performance on infrequent cases. Additionally, it is worth noting that the limited sense coverage of the annotated dataset may serve as an upper limit on overall performance.

With the development of self-attention-based neural architectures and their capacity to extract sophisticated language information [\(Vaswani et al., 2017\)](#page-96-1), the use of transformer-based architectures in fully supervised WSD systems is becoming more and more popular. The WSD task is usually fine-tuned on a pre-trained transformer model, which is a popular strategy. The task-specific inputs are given to the pre-trained model, which is then further trained across a number of epochs with the task-specific objective. Likewise, in recent token classification models for WSD, the contextualized representations are usually generated by a pre-trained model and then fed to either a feedforward network [\(Hadiwinoto et al., 2019\)](#page-84-3) or a stack of Transformer layers [\(Bevilacqua and Navigli, 2019\)](#page-80-9). These methods outperform earlier randomly initialized models [\(Raganato et al., 2017a\)](#page-93-5). [Hadiwinoto et al.](#page-84-3) [\(2019\)](#page-84-3) tested different pooling strategies of BERT, e.g., last layer projection, weighted sum of hidden layers, and Gated Linear Unit [\(Dauphin et al.,](#page-82-4) [2017\)](#page-82-4). The best performance on SensEval-2 is given by the strategy of the weighted sum of hidden layers, accounting for 76.4% F1. [Bevilacqua and Navigli](#page-80-9) [\(2019\)](#page-80-9) proposed a bi-directional Transformer that explicitly attends to past and future information. This model achieved 75.7% F1 on SensEval-2 by training with the combination of SemCor and WordNet's Tagged Glosses[15](#page-11-0). It is worth noting that, the categorical cross-entropy, which is frequently utilized for training, limits the performances. In reality, it has been demonstrated that the binary cross-entropy loss performs better [\(Conia and Navigli, 2021\)](#page-82-3) because it enables the consideration of many annotations for a single instance in the training set as opposed to the use of a single ground-truth sense alone. In the above-mentioned approaches, each sense is assumed to be a unique class, and the classification architecture is limited to the information provided by the training corpus.

##*2.7.3. Knowledge-augmented Supervised WSD*

The edges that connect the senses and synsets are a valuable source of knowledge that augments the annotated data. Traditionally, graph knowledge-based systems, such as those based on Personalized PageRank [\(Scozzafava](#page-94-3) [et al., 2020\)](#page-94-3), have taken advantage of this information. Moreover, utilizing WordNet as a graph has benefited many modern supervised systems. Thus, formally, knowledge-augmented supervised WSD is defined as a methodology that combines traditional supervised machine learning techniques with external knowledge resources to improve the accuracy and performance of word sense disambiguation.

[Wang and Wang](#page-97-1) [\(2020\)](#page-97-1) used WordNet hypernymy and hyponymy relations to devise a try-again mechanism that refines the prediction of the WSD model. The SemCor corpus was utilized to acquire a supervised sense embedding for every annotated sense in their supervised method (SREF*S up*). [Vial et al.](#page-96-2) [\(2019\)](#page-96-2) reduced the number of output classes by mapping each sense to an ancestor in the WordNet taxonomy, then yielding a smaller but robust sense vocabulary. The authors used BERT contextualized embeddings. By training with SemCor and WordNet gloss corpora, the model achieved 79.7% F1 on SensEval-2. Different variations also achieve outstanding performance on diverse WSD datasets.

[Loureiro and Jorge](#page-89-3) [\(2019\)](#page-89-3) created representations for those senses not appearing in SemCor by using the averaged neighbor embeddings in the WordNet. The token-tagger models EWISE [\(Kumar et al., 2019\)](#page-87-0) and EWISER [\(Bevilac](#page-80-8)[qua and Navigli, 2020\)](#page-80-8) both leveraged the WordNet graph structure to train the gloss embedding offline, where EWISER demonstrated how the WordNet entire graph feature can be directly extracted. EWISE used ConvE [\(Dettmers](#page-83-6) [et al., 2018\)](#page-83-6) to obtain graph embeddings. [Conia and Navigli](#page-82-3) [\(2021\)](#page-82-3) provided a new technique to use the same edge information by replacing the adjacency matrix multiplication with a binary cross-entropy loss where other senses connected to the gold sense are also taken into account. The edge information was obtained from WordNet. In general, edge information is increasingly used in supervised WSD, gradually blending with knowledge-based techniques. However, it can only be conveniently utilized by token classification procedures, whereas its incorporation into sequence classification techniques has not yet been researched.

It has also been extensively studied how to use sense definitions as an additional source for supervised WSD apart from the traditional data annotations. It considerably increased the scalability of a model on the senses that are underrepresented in the training corpus. [Huang et al.](#page-85-2) [\(2019a\)](#page-85-2) argued that WSD has traditionally been approached as a binary classification task, whereby a model must accurately decide if the sense of a given word in context aligns with one of its potential meanings in a sense inventory, based on the provided definition. define the WSD task as a sentence-pair classification task, where the WordNet gloss of a target word is concatenated after an input sentence. [Blevins and Zettlemoyer](#page-80-7) [\(2020\)](#page-80-7) used a bi-encoder to project both words in context and WordNet glosses in a common vector space. Disambiguation is then carried out by determining the gloss that is most similar to the target word. Glosses are employed similarly by more advanced techniques like SensEmBERT [\(Scarlini et al., 2020a\)](#page-94-5), ARES [\(Scarlini et al., 2020b\)](#page-94-6), and SREF [\(Wang and Wang, 2020\)](#page-97-1). They used quite different approaches to find new contexts automatically in order to develop the supervised portion of the sense embedding. ARES achieved 78.0% F1 on the SensEval-2 dataset by utilizing collocational relations between senses to get novel example sentences from websites. SensEmBERT leveraged BabelNet and Wikipedia explanations, achieving significant improvements on

<span id="page-11-0"></span><sup>15</sup><https://wordnetcode.princeton.edu/glosstag.shtml>

nominal WSD tasks over 5 major datasets. [Barba et al.](#page-79-3) [\(2021a\)](#page-79-3) proposed to solve WSD as a text extraction problem where, given a word in context and all of its potential glosses, models extract the definition that best matches the term under consideration. The authors demonstrated the advantages of their approach in that it does not require huge output vocabularies and enables models to take into account both the input context and all meanings of the target word simultaneously. By using sparse coding, [Berend](#page-80-10) [\(2020\)](#page-80-10) has demonstrated that it is also possible to make existing sense embeddings sparse. All of these methods handle each word independently of the others when disambiguating multiple words that co-occur in the same context. Thus, a word's explicit meaning is neither taken into account during word disambiguation nor does it have an impact on the disambiguation of surrounding words.

# *2.8. Downstream Applications*#*2.8.1. Sentiment Computing*WSD has been applied in many Sentiment Analysis (SA) works to improve accuracy and explainability. [Farooq](#page-83-7) [et al.](#page-83-7) [\(2015\)](#page-83-7) proposed a WSD framework to enhance the performance of sentiment analysis. To determine the orientation of opinions related to product attributes in a particular field, a lexical dictionary comprising various word senses is developed. The process involves extracting relevant features from product reviews and identifying opinion-bearing texts, followed by the extraction of words used to describe the features and their contexts to form seed words. These seed words, which consist of adjectives, nouns, verbs, and adverbs, are manually annotated with their respective polarities, and their coverage is extended by retrieving their synonyms and antonyms. WSD was utilized to identify the sentiment-orientated senses, such as the positive, negative, or neutral senses of a word in a sentence, because a word may have different sentiment polarities by taking different senses in different contexts.

[Nassirtoussi et al.](#page-91-8) [\(2015\)](#page-91-8) offered a novel approach to forecast intra-day directional movements of the EUR/USD exchange rates based on news headline text mining in an effort to address semantic and sentiment components of text-mining. They evaluated news headlines semantically and emotionally using the lexicons, e.g., WordNet and SentiWordNet [\(Baccianella et al., 2010\)](#page-79-4). SentiWordNet is a publicly accessible lexical resource designed for sentiment analysis that allocates a positivity score, negativity score, and objectivity score to each synset within WordNet. [Nas](#page-91-8)[sirtoussi et al.](#page-91-8) [\(2015\)](#page-91-8) found that both positive and negative emotions may influence the market in the same way. WSD worked as a technique to abstract semantic information in their framework. Thus, it enhances the feature representations and explainability in their downstream task modeling. SentiWordNet has served as a basis for various sentiment analysis models. In the work of [Ohana and Tierney](#page-91-9) [\(2009\)](#page-91-9), the feasibility of using the emotional scores of Senti-WordNet to automatically classify the sentiment of movie reviews was examined. Other applications, e.g., business opinion mining (Saggionα [and Funk, 2010\)](#page-93-6), article emotion classification [\(Devitt and Ahmad, 2007\)](#page-83-8), word-of-mouth sentiment classification [\(Hung and Lin, 2013;](#page-86-3) [Hung and Chen, 2016\)](#page-86-4) also showed that SentiWordNet as a semantic feature enhancement knowledge base can deliver accuracy gains and model insights in sentiment analysis tasks.

#*2.8.2. Information Retrieval*The impacts of using WSD for information retrieval have been examined in many works. [Krovetz and Croft](#page-87-2) [\(1992\)](#page-87-2) disambiguated word senses for terms in queries and documents to examine how ambiguous word senses impact information retrieval performance. The researchers arrived at the conclusion that the advantages of WSD in information retrieval are marginal. This is due to the fact that query words have uneven sense distributions. The impact of collocation from other query terms already plays a role in disambiguation. WSD was used as a parser to study this task. However, the findings from [Gonzalo et al.](#page-84-4) [\(1998\)](#page-84-4) are different. They examined the impact of improper disambiguation using SemCor. By accurately modeling documents and queries together with synsets, they achieved notable gains (synonym sets). Additionally, their study demonstrated that WSD with an error rate of 40%–50% may still enhance IR performance when used with the synset representation, which incorporated synonym information. [Gonzalo et al.](#page-84-5) [\(1999\)](#page-84-5); [Stokoe et al.](#page-95-3) [\(2003\)](#page-95-3) further confirmed the significance of WSD to information retrieval. [Gon](#page-84-5)[zalo et al.](#page-84-5) [\(1999\)](#page-84-5) also found that PoS information has a lower utility for information retrieval. Based on artificially creating word ambiguity, [Sanderson](#page-94-7) [\(1994\)](#page-94-7) employed pseudo words to explore the effects of sense ambiguity on information retrieval. They came to the conclusion that the high accuracy of WSD is a crucial condition to accomplish progress. [Blloshmi et al.](#page-80-11) [\(2021\)](#page-80-11) introduced an innovative approach to multi-lingual query expansion by integrating WSD, which augments the query with sense definitions as supplementary semantic information in multi-lingual neural ranking-based IR. The results demonstrated the advantages of WSD in improving contextualized queries, resulting in a more accurate document-matching process and retrieving more relevant documents.

[Kim et al.](#page-87-3) [\(2004\)](#page-87-3) labeled words with 25 root meanings of nouns rather than utilizing fine-grained sense inventories of WordNet. Their retrieval technique preserved the stem-based index and changed the word weight in a document in accordance with the degree to which it matched the query's sense. They credited their coarse-grained, reliable, and adaptable sense tagging system with the improvement on TREC collections. The detrimental effects of disambiguation mistakes are somewhat mitigated by the addition of senses to the conventional stem-based index.

##*2.8.3. Machine Translation*The challenge of ambiguous word senses poses a significant barrier to the development of an efficient machine translator. As a result, a number of researchers have turned their attention to exploring WSD for machine translation. Some works tried to establish datasets to quantify the WSD capacity of machine translation systems. [Rios Gonzales](#page-93-7) [et al.](#page-93-7) [\(2017\)](#page-93-7) proposed a test set of 6,700 lexical ambiguities for German-French and 7,200 for German-English. They discovered that WSD remains a difficult challenge for neural machine translation, especially for uncommon word senses, even with 70% of lexical ambiguities properly resolved. [Campolungo et al.](#page-81-2) [\(2022\)](#page-81-2) proposed a benchmark dataset that aims at measuring WSD biases in Machine Translation in five language combinations. They also agreed that SOTA systems still exhibited notable constraints when confronted with less common word senses. Incorporating sense labels and lexical chains leads to enhanced performance of Neural Machine Translation (NMT) models, particularly with regard to infrequent word senses. [Raganato et al.](#page-93-8) [\(2019\)](#page-93-8) proposed MUCOW, a multi-lingual contrastive test set automatically created from word-aligned parallel corpora and the comprehensive multi-lingual sense inventory of BabelNet. MUCOW spans 16 language pairs and contains more than 200,000 contrastive sentence pairs. The researchers thoroughly evaluated the effectiveness of the ambiguous lexicons and the resulting test suite by utilizing pre-trained NMT models and analyzing all submissions across nine language pairs from the WMT19 news shared translation task.

Some works analyzed the internal representations to understand the disambiguation process in machine translation systems. [Marvin and Koehn](#page-89-4) [\(2018\)](#page-89-4) examined the extent to which ambiguous word senses could be decoded through the use of word embeddings in relation to deeper layers of the NMT encoder, which were believed to represent words with contextual information. In line with prior research, they discovered that the NMT system frequently mistranslated ambiguous terms. [Tang et al.](#page-96-5) [\(2019\)](#page-96-5) trained a classifier to determine if a translation is accurate given the representation of an ambiguous noun. The fact that encoder hidden states performed much better than word embeddings suggests that encoders are able to appropriately encode important data for disambiguation into hidden states. [Liu et al.](#page-88-3) [\(2018a\)](#page-88-3) discovered that an increase in the number of senses associated with each word results in a decline in the performance of word-level translation. The root of the issue may be the mapping of each word to similar word vectors, regardless of its context. They proposed to integrate techniques from neural WSD systems into an NMT system to address this issue.

###*2.9. Summary*WSD as a computational linguistics task most closely related to lexical semantics research, has won extensive discussions among researchers from different fields. Linguists came up with important hypotheses to guide the modeling of word senses. We have observed that some hypotheses have been well grounded in NLP, e.g., learning and representing word meanings with their contexts and word co-occurrences. However, we also observe some important linguistic arguments were rarely studied in the computational linguistic domain, e.g., defining the scope of linguistic units for WSD and integrating relevant concepts (frames) for word sense representations. The development of WSD datasets has greatly ignited the research enthusiasm of scholars in WSD. However, we also observed that the computational research on WSD is also limited by these well-defined datasets because WSD datasets generally follow a very similar labeling paradigm. Relevant linguistic studies have shown broader possibilities in WSD. Finally, we find that many of WSD modeling techniques do not link well with downstream applications. The research of WSD methods has intersections with downstream applications, whereas they cannot well cover the needs of downstream tasks. This also shows that the research opportunities in WSD can be largely extended besides word sense classification.

####*2.9.1. Technical Trends*Table [4](#page-14-0) shows the technical trends of WSD methods. As seen in the table, earlier approaches likely used knowledge-based and supervised approaches. WordNet and BabelNet are useful knowledge bases that were frequently

<span id="page-14-0"></span>

| Task      | Reference                      | Tech  | Feature and KB.         | Framework                                     | Dataset                | Score  | Metric |
|-----------|--------------------------------|-------|-------------------------|-----------------------------------------------|------------------------|--------|--------|
|           | Lesk (1986)                    | Prob. | Statistics, OALD        | Count def. overlaps                           | -                      | -      | -      |
|           | Banerjee et al. (2003)         | ML    | Emb., WN                | Score function                                | SensEval-2             | 34.60% | F1     |
|           | Navigli and Lapata (2007)      | Graph | Sense graph, WN         | Connectivity measures                         | SemCor                 | 31.80% | F1     |
|           | Basile et al. (2014)           | Prob. | Emb., BN                | DSM                                           | SE2013-EN              | 71.50% | F1     |
| Knwl      | Wang and Wang (2020)KB         | DL    | BERT, WN                | Vector represent.                             | SensEval-2             | 72.70% | F1     |
|           | Agirre and Soroa (2009)        | Graph | WN                      | PageRank                                      | SensEval-2             | 58.60% | Recall |
|           | Moro et al. (2014b)            | Graph | Sem. graph, BN          | PageRank                                      | SE2013-EN              | 69.20% | F1     |
|           | Scozzafava et al. (2020)       | Graph | WN, SN                  | PageRank                                      | SensEval-2             | 71.60% | F1     |
|           | Singh et al. (2014)            | ML    | 5-gram, position        | Decision Tree                                 | Manipuri               | 71.75% | Acc    |
|           | O'Hara et al. (2004)           | ML    | Relatedness scores      | Decision Tree                                 | SensEval-3             | 65.90% | F1     |
|           | Zhong and Ng (2010)            | ML    | Position, PoS           | SVM                                           | SensEval-2             | 68.20% | F1     |
|           | Iacobacci et al. (2016)        | ML    | Emb., position, PoS     | SVM                                           | SensEval-2             | 68.30% | F1     |
| Sup.      | Popov (2017)                   | DL    | Emb.                    | BiLSTM                                        | SensEval-2             | 70.11% | Acc    |
|           | Yuan et al. (2016)             | DL    | Emb., label propag.     | LSTM                                          | SensEval-2             | 74.40% | F1     |
|           | Le et al. (2018)               | DL    | Emb.                    | LSTM                                          | SensEval-2             | 72.00% | F1     |
|           | Hadiwinoto et al. (2019)       | DL    | BERT                    | Transformer                                   | SensEval-2             | 76.40% | F1     |
|           | Bevilacqua and Navigli (2019)  | DL    | Emb.                    | BiTransformer                                 | SensEval-2             | 75.70% | F1     |
|           | Wang and Wang (2020)S up       | DL    | BERT, WN                | Vector represent.                             | SensEval-2             | 78.60% | F1     |
|           | Vial et al. (2019)             | DL    | BERT, WN                | Transformer                                   | SensEval-2             | 79.70% | F1     |
|           | Loureiro and Jorge (2019)      | DL    | BERT, WN                | Transformer                                   | SensEval-2             | 76.30% | F1     |
|           | Kumar et al. (2019)            | DL    | Graph emb.,<br>emb., WN | BiLSTM, Att.<br>ConvE                         | SensEval-2             | 73.80% | F1     |
|           | Bevilacqua and Navigli (2020)  | DL    | BERT, WN                | Trans., Struct. logit                         | 5 datasets             | 80.80% | F1     |
|           | Conia and Navigli (2021)       | DL    | BERT, WN                | Transformer                                   | SensEval-2             | 78.40% | F1     |
|           | Huang et al. (2019a)           | DL    | BERT, WN                | Transformer, sentence-<br>pair classification | SensEval-2             | 77.70% | F1     |
| Knwl<br>+ | Blevins and Zettlemoyer (2020) | DL    | BERT, WN                | Trasformer, Score Func.                       | SensEval-2             | 79.40% | F1     |
| Sup.      | Scarlini et al. (2020a)        | DL    | BERT, BN, Wiki          | Transformer, Context<br>Retrieval             | 5 datasets<br>Nouns of | 80.40% | F1     |
|           | Scarlini et al. (2020b)        | DL    | BERT, WN, SN            | Transformer, Context<br>Retrieval             | SensEval-2             | 78.00% | F1     |
|           | Barba et al. (2021a)           | DL    | BERT, WN                | Transformer, Extractive<br>Sense Learning     | SensEval-2             | 81.70% | F1     |
|           | Berend (2020)                  | DL    | BERT, WN                | Transformer, sparse<br>coding, PMI            | SensEval-2             | 79.60% | F1     |
|           |                                |       |                         |                                               |                        |        |        |

Table 4: A summary of representative WSD techniques. Knwl denotes knowledge-based methods. Sup. denotes supervised methods. KB denotes knowledge bases. WN denotes WordNet. BN denotes BabelNet. DSM denotes Distributional Semantics Models. Prob. denotes probability. SE2013-EN denotes the SemEval2013 English WSD task. PMI denotes Pointwise Mutual Information.

| Reference                   | Downstream Task       | Feature | Parser | Explainability |
|-----------------------------|-----------------------|---------|--------|----------------|
| Farooq et al. (2015)        | Sentiment Computing   | ✓       |        |                |
| Nassirtoussi et al. (2015)  | Sentiment Computing   | ✓       |        | ✓              |
| Ohana and Tierney (2009)    | Sentiment Computing   | ✓       | ✓      |                |
| Saggionα and Funk (2010)    | Sentiment Computing   | ✓       | ✓      | ✓              |
| Devitt and Ahmad (2007)     | Sentiment Computing   | ✓       |        | ✓              |
| Hung and Lin (2013)         | Sentiment Computing   | ✓       | ✓      |                |
| Hung and Chen (2016)        | Sentiment Computing   | ✓       | ✓      | ✓              |
| Krovetz and Croft (1992)    | Information Retrieval |         | ✓      |                |
| Gonzalo et al. (1998)       | Information Retrieval |         |        | ✓              |
| Gonzalo et al. (1999)       | Information Retrieval |         |        | ✓              |
| Sanderson (1994)            | Information Retrieval |         |        | ✓              |
| Stokoe et al. (2003)        | Information Retrieval |         |        | ✓              |
| Kim et al. (2004)           | Information Retrieval | ✓       | ✓      |                |
| Blloshmi et al. (2021)      | Information Retrieval | ✓       |        | ✓              |
| Rios Gonzales et al. (2017) | Machine Translation   | ✓       |        |                |
| Raganato et al. (2019)      | Machine Translation   | ✓       |        |                |
| Marvin and Koehn (2018)     | Machine Translation   |         | ✓      | ✓              |
| Tang et al. (2019)          | Machine Translation   | ✓       |        | ✓              |
| Liu et al. (2018a)          | Machine Translation   | ✓       |        |                |

Table 5: A summary of the representative applications of WSD in downstream tasks. ✓denotes the role of WSD in a downstream task.

used by knowledge-based methods. Word embeddings, pre-trained language models, and linguistic features, e.g., PoS tags and semantic relatedness were frequently used by supervised methods. For old pure knowledge-based methods, the PageRank framework was likely used, because many knowledge bases are represented as graphs. PageRank is an algorithm used in graph computation to measure the importance of nodes in a graph. Classical machine learning techniques, e.g., Decision Tree, SVM, LSTM, and Transformers were commonly used by supervised WSD methods. Supervised learning algorithms demonstrate superior performance in comparison to knowledge-based approaches. Nevertheless, it is not always reasonable to assume the availability of substantial training datasets for different areas, languages, and activities. [Ng](#page-91-10) [\(1997\)](#page-91-10) predicted that a corpus of around 3.2 million sense-tagged words would be necessary in order to produce a high-accuracy, wide-coverage disambiguation system. The creation of such a training corpus requires an estimated 27 person-years of labor. The accuracy of supervised systems might be greatly improved above the SOTA methods with such a resource. However, the success of this hypothesis is at the cost of huge resource consumption.

We observe more hybrid approaches that leverage knowledge bases in a supervised learning fashion in recent years. This is because researchers have observed the limitations of typical supervised WSD in processing rare or unseen cases. Knowledge bases provide additional information to support the learning of unseen cases. Knowledge bases provide additional knowledge for the languages whose annotated data are scarce. In this case, multi-lingual knowledge bases can enhance the representations of word senses in a new domain. As a result, we can observe the accuracy of the hybrid approaches surpasses the pure knowledge-based or supervised approaches.

Most existing WSD datasets define the task as a word sense classification task. Then, the following methodology research upon the datasets focused on improving the accuracy of mapping the sense of a word to its dictionary sense class. However, should the research on WSD be limited to word sense classification? We have observed that many knowledge-based systems used existing knowledge bases to conduct word sense classification tasks. They have realized the importance of developing an effective knowledge base for WSD. However, it is rare to see that WSD research tries to improve the construction of knowledge bases according to the effectiveness of word sense classification. On the other hand, the meaning of WSD is much larger than detecting the definition of words in a dictionary. Mapping a word to a sense in a dictionary is just an aspect of WSD. Previous works rarely studied what is an appropriate linguistic unit for WSD; what concepts are associated with a word sense in a context. These are very interesting research topics from linguistic and cognitive aspects. However, these topics were not well studied in the computational WSD community.

#*2.9.2. Application Trends*The WSD task was commonly defined as a word sense classification task. However, we observe that classifying words by sense classes is not the only need for downstream NLP tasks.

There are three main tasks that are strongly related to WSD, e.g., sentiment computing, information retrieval, and

machine translation in our survey. One of the roles of WSD on the three tasks is to deliver or enhance features to gain improvements on the three tasks. On the other hand, we also observe many downstream works used WSD techniques as a parser to obtain words with different levels of word sense ambiguity or used WSD to gain insights into their model behaviors to improve the explainability of a study. In these cases, defining WSD as a sense classification task may be sub-optimal for downstream applications.

WSD has a huge potential in NLP research. For example, disambiguating word senses in a large corpus can lead to a deeper understanding of language usage patterns and the semantic relationships between words. WSD is also a significant component in semantic explainable AI, because it helps researchers better understand the decisionmaking process of a model on the semantic level. Researchers can develop a more transparent and trustworthy model by explaining word senses in contexts. As a feature generator, a WSD may be more effective if it can generate contextualized word meanings in natural language, rather than predict a sense class that maps to a predefined gloss in a dictionary. However, research in these fields is rare in the WSD community.

Finally, according to [Navigli](#page-91-2) [\(2009\)](#page-91-2), the lack of end-to-end applications that utilize WSD can be attributed to the insufficient accuracy of current WSD systems. This suggests that in the future, more precise WSD systems may be developed, which could potentially enable the use of more semantics-dependent applications.

##*2.9.3. Future Works*As argued before, the task of WSD can be broader than the current word sense classification task setup from either the theoretical research side or the downstream application side. Besides, the improvements in WSD accuracy can also attract more downstream applications. Thus, we come up with the following future work suggestions.

Extending the form of WSD. WSD can have different learning forms, besides word sense classification, e.g., paraphrasing an ambiguous word into a less ambiguous one [\(Mao et al., 2018,](#page-89-5) [2022b\)](#page-89-6), generating contextualized word senses in natural language. Such an extension may have significance in downstream applications. From the perspective of linguistic and cognitive research, studying how to define a language unit to better disambiguate word senses, or studying how to link a word to its associated concepts in a context can also improve the significance of WSD in the era of LLM-based NLP. Future works may study how to define the task of WSD to better support the research in different disciplines.

Rethinking existing knowledge bases by WSD. Most of the existing knowledge bases were developed according to human-defined ontologies and word senses. These knowledge bases have been considered as an important resource for many knowledge-based systems. Although the knowledge bases have been used on different tasks, few works analyzed the weakness of the ontologies. Future WSD-related research may try to improve the knowledge bases by rethinking the sense definition, concept node connections, and coverage, rather than simply developing models to enhance the learning ability on a specific task.

Multi-lingual WSD. Most of the semantic representations are learned from monolingual corpora. As a result, the semantic representations are different between different languages. However, the disambiguation of meanings is not characterized by languages [\(Boroditsky, 2011\)](#page-80-12). It will significantly improve multi-lingual semantic research if WSD research can break down language barriers from a cognitive perspective. As argued by frame semantics [\(Fillmore et al.,](#page-83-2) [2006\)](#page-83-2), the meaning of a word is beyond its dictionary definitions. It also associates with the concepts, interconnected with the word. Representing word senses by concepts may achieve a more robust multi-lingual WSD.

Learning WSD as a pre-training task. Recent years witness great success of PLMs in various domains. The existing PLMs followed the same hypothesis that the sense of a word can be learned from its associated context. However, there has not been a PLM that explicitly disambiguates word senses to enhance the learning of semantic representations. Naively learning the semantic representation of a target word by its associated context words cannot learn the conceptual association of the target word. For example, many words can associate with the word "apple". How can we know an apple as fruit is red or green, sweet, tree-growing, nutritious, etc? As an electronic device, Apple is associated with an operating system, a circuit board, a brand, etc. Disambiguating word senses before pre-training may build such connections between concepts.

Fusing WSD with other tasks. As [Bevilacqua et al.](#page-80-13) [\(2021b\)](#page-80-13) argued, WSD can also be integrated with an entity linking task [\(Moro et al., 2014b\)](#page-90-5), where the model predicts associated entities to help WSD systems explore the related glosses and relations. Related fusion works also include fusing WSD for Sentiment Analysis [\(Farooq et al.,](#page-83-7) [2015\)](#page-83-7), Information Retrieval [\(Blloshmi et al., 2021\)](#page-80-11) and Machine Translation [\(Campolungo et al., 2022\)](#page-81-2). The future study of WSD can be grounded on an end task so that the end task can more effectively benefit from the fusion of a WSD model.

### <span id="page-17-0"></span>3. Anaphora Resolution

In computational linguistics, Ruslan Mitkov defined anaphora as a*phenomena of pointing back a previously mentioned item in the text*[\(Mitkov, 2022\)](#page-90-6). The pointing back phrase is called an*anaphor*while the previously mentioned item is called an*antecedent*.

The concept of anaphora should not be confused with co-reference. On the one hand, either anaphora or cataphora (e.g., the phenomena of pointing ahead to a subsequently mentioned item) could be a kind of co-reference. On the other hand, an anaphor and its antecedent are not always co-referential. By definition, the difference between anaphora and co-reference is that anaphora does not require *identify-of-reference*while co-reference requires. In other words, anaphora may describe a relation between expressions that do not have the same referent. For example, in sentence [\(2\),](#page-17-1) the anaphor "one" has the same sense as its antecedent "a dog", but they do not refer to the same dog.

<span id="page-17-1"></span>(2) Jack has a dog and Mary also has one.

Building on this, in relation to anaphora, both anaphor and its antecedent are not necessarily referring expressions. For instance, an anaphor can be a verb (henceforth, verb anaphora). In the following example from [Mitkov](#page-90-7) [\(2014\)](#page-90-7),

(3) When Manchester United swooped to lure Ron Atkinson away from the Albion, it was inevitable that his midfield prodigy would follow, and in 1981 he did.

the anaphor "did" is a verb, having an antecedent "follow". Another example is the*bound anaphora*where the antecedent is a quantified expression [\(Reinhart, 1983\)](#page-93-9):

(4) Each manager exploits the secretary who works for "him".

The anaphor "him" refers to the quantified expression "each manager". Since antecedents in both above two examples are not referring expressions, neither of them is a co-reference.

Given the definition of anaphora, the task of anaphora resolution is to identify the antecedent of an anaphor. In this survey, we decided to merely focus on anaphora resolution (rather than co-reference resolution) because, on the one hand, most semantic processing tasks only require identifying antecedents. On the other hand, we are not only interested in referring to noun phrases but also other phrases that an anaphor can refer to (e.g., verb phrases and quantified expressions; see the discussion above).

It is worth noting that there have been reviews in the past 20 years about AR/CR from either computer scientists [\(Sukthanker et al., 2020;](#page-95-4) [Liu et al., 2023c\)](#page-89-7) or linguists [\(Mitkov, 2022;](#page-90-6) [Poesio et al., 2023\)](#page-92-1). In this survey, our objective is to establish a connection between AR techniques across theoretical research and practical applications.

#*3.1. Theoretical Research*

## <span id="page-17-2"></span>*3.1.1. Constraints*

When human beings resolute co-reference, there are semantic and syntactic constraints. As for the semantic constraints, agreements such as gender and number agreements are the strongest type [\(Garnham, 2001\)](#page-84-6). However, most recently, agreement mismatch problems (especially for gender agreements) have been becoming more frequent since more people have started to use plural pronouns to avoid gender bias.

As for syntactic constraints, according to the binding theory [\(Buring, 2005\)](#page-81-3), in the sentence (a) of the following ¨ example, "John" cannot co-refer with "him" while in the sentence (b) "John" can.

### (5) a. John likes him.

b. John likes him in the mirror.

# <span id="page-18-7"></span>*3.1.2. Centering Theory*Centering Theory [\(Joshi and Kuhn, 1979;](#page-86-5) [Grosz et al., 1983,](#page-84-7) [1995\)](#page-84-8) was introduced as a model of*local coherence*[16](#page-18-0) based on the idea of *center of attention*. The theory assumes that, during the production or comprehension of a discourse, the discourse participant's attention is often centered on a set of entities (a subset of all entities in the discourse) and such an *attentional state*evolves dynamically. It models transitions of the attentional state and defines three types of transitions: CONTINUE, RETAIN, and SHIFT. For each utterance, the transition is decided by its backward-looking center (defined as the most salient entity in the previous utterance that is also realized in the current utterance and denoted as*Cb*) as well as forward-looking center (defined as the most salient entity in the current utterance and denoted as *Cf*). Consider the following discourse adopted from [Kehler](#page-87-4) [\(1997\)](#page-87-4):

- <span id="page-18-2"></span><span id="page-18-1"></span>(6) a. Terry really gets angry sometimes.
  - b. Yesterday was a beautiful day and he was excited about trying out his new sailboat. [*C<sup>b</sup>*= Terry,*C<sup>f</sup>* = Terry]
  - c. He wanted Tony to join him on a sailing expedition, and left him a message on his answering machine. [*C<sup>b</sup>*= Terry,*C<sup>f</sup>* = Terry]
  - d. Tony called him at 6AM the next morning. [*C<sup>b</sup>*= Terry,*C<sup>f</sup>* = Tony]
  - e. Tony was furious with him for being woken up so early. [*C<sup>b</sup>*= Tony,*C<sup>f</sup>*= Tony]

<span id="page-18-4"></span><span id="page-18-3"></span>where we annotate each utterance with its backward-looking and forward-looking centers. The transition from utterance [\(6-a\)](#page-18-1) to [\(6-b\)](#page-18-2) is a CONTINUE as both backward-looking and forward-looking centers are unchanged. The next one is a RETAIN transition since although the most salient entity changes (i.e.,*Cf*), the forward-looking center stays the same, whereas the transition from utterance [\(6-d\)](#page-18-3) to [\(6-e\)](#page-18-4) is a SHIFT transition because of the change of backwardlooking transition. Intuitively, a discourse with more CONTINUE transitions is more coherent than the one with more SHIFT transitions.

Though Centering Theory is not a theory of Anaphora Resolution, Anaphora Resolution can directly benefit from modeling transitions, which provides certain information about the preference for the referents of pronouns (e.g., in a coherent segment, centers co-refer; see [Joshi et al.](#page-86-6) [\(2006\)](#page-86-6) for more discussion about the relation between Centering Theory and Anaphora Resolution).

# <span id="page-18-6"></span>*3.1.3. Discourse Salience*A prominent strand of work in psycholinguistics investigates how human beings use anaphora. A referent is more likely to be realized as a pronoun if it is salient in a given discourse [\(Givon, 1983\)](#page-84-9) (aka. ´*discourse salience*). Discourse salience is thought to be influenced by various factors, including givenness [\(Chafe, 1976;](#page-81-4) [Gundel et al.,](#page-84-10) [1993\)](#page-84-10), grammatical role [\(Brennan, 1995;](#page-81-5) [Stevenson et al., 1994\)](#page-95-5), recency [\(Givon, 1983;](#page-84-9) [Arnold, 1998\)](#page-79-5), syntactic ´ parallelism [\(Chambers and Smyth, 1998;](#page-81-6) [Arnold, 1998\)](#page-79-5), and many other factors. Similar to Centering Theory, most research on discourse salience is about the production of anaphora [\(McCoy and Strube, 1999;](#page-90-8) [Orita et al., 2014,](#page-91-11) [2015;](#page-91-12) [Chen et al., 2018a\)](#page-81-7), but it also provides insights about an antecedent's relative likelihood for a given anaphor in a given discourse. In this sense, it is plausible to use the aforementioned factors as features to rank candidate antecedents of an anaphor [\(Lappin and Leass, 1994a;](#page-87-5) [Bos, 2003\)](#page-81-8).

# <span id="page-18-5"></span>*3.1.4. Coolness*

[Huang](#page-85-3) [\(1984\)](#page-85-3) classified human languages into cool languages and hot languages. If a language is "cooler" than another language, then understanding a sentence in that language relies more on context (see [Chen](#page-81-9) [\(2022\)](#page-81-9); [Chen and](#page-81-10) [van Deemter](#page-81-10) [\(2022\)](#page-81-10); [Chen et al.](#page-81-11) [\(2023\)](#page-81-11) for computational investigations of the theory of Coolness). The evidence that [Huang](#page-85-3) [\(1984\)](#page-85-3) identified is about the differences between the use of anaphora. Specifically, cool languages (e.g., Mandarin) make liberal use of zero pronouns. Take the following conversation as an example:

(7) a. <sup>你</sup>今天看见比尔了吗?(Did you see Bill today?) b. \*pro\*看见\*pro\*了。(\*I\* saw \*him\*.)

<span id="page-18-0"></span><sup>16</sup>Instead of focusing on the whole discourse, centering theory focuses only on the *discourse segment*.

where a \*pro\* represents a zero pronoun[17](#page-19-0) (ZP). The first ZP refers to one of the speakers while the second ZP refers to Bill. ZPs of this kind are called Anaphoric ZPs (AZPs). In addition to Mandarin, a number of other languages (i.e., cool languages) also allow ZPs, including examples like Japanese, Arabic, and Korean. The current theory suggests that the anaphora resolution of cool languages should also take AZPs into consideration, namely AZP resolution [\(Chen](#page-81-12) [and Ng, 2013\)](#page-81-12).

## <span id="page-19-1"></span>*3.2. Annotation Schemes*In this subsection, we introduce two commonly used annotation schemes for anaphora resolution: MUC and MATE. There are also other schemes, for example, the Lancaster scheme [\(Fligelstone, 1992\)](#page-83-9) and the DRAMA scheme [\(Passonneau, 1997\)](#page-92-11).

###*3.2.1. MUC*MUC [\(Hirschman et al., 1997;](#page-85-4) [Hirschman and Chinchor, 1998\)](#page-85-5) is one of the very first schemes, which is used for annotating the MUC [\(Chinchor and Sundheim, 1995\)](#page-82-5) and the ACE [\(Doddington et al., 2004\)](#page-83-10) corpora and is still widely used these years. It is primary goal is to annotate co-reference chains in discourse, in which MUC defines and proposes to annotate the IDENTITY (IDENT) relation. Relations as such are symmetrical (i.e., if A IDENT B, then B IDENT A) and transitive (i.e., if A IDENT B and B IDENT C, then A IDENT C). Annotation is done using SGML, for example:

<span id="page-19-2"></span>(8) ⟨COREF ID="100"⟩Lawson Mardon Group Ltd.⟨/COREF⟩said ⟨COREF ID="101" TYPE="IDENT" REF="100"⟩it⟨/COREF⟩ ...

The annotation above construct a link between the pronoun "it" and the noun phrase "Lawson Mardon Group Ltd.".

MUC proposes to annotate co-reference chains following a paradigm analogous to anaphora resolution. Annotators are first asked to annotate markable phrases (e.g., nouns, noun phrases, and pronouns) and partition the phrases into sets of co-referring elements. This helps the annotation task achieve good inter-annotator agreement (i.e., larger than 95%).

Nevertheless, it has been pointed out by [Deemter and Kibble](#page-83-11) [\(2000\)](#page-83-11) that MUC has certain flaws: MUC does not guarantee that the annotated relations are all co-referential. It includes either relation that does not follow the principle of identity-of-reference or bound anaphora. Therefore, the resulting corpus would often be a mixture of co-reference and anaphora.

####*3.2.2. MATE*Instead of annotating a single device INDENT, MATE [\(Poesio et al., 1999a;](#page-92-12) [Poesio, 2004\)](#page-92-13) was proposed to do so-called "anaphoric annotation" which is explicitly based on the discourse model assumption [\(Heim, 1982;](#page-85-6) [Gundel](#page-84-10) [et al., 1993;](#page-84-10) [Webber, 2016;](#page-97-4) [Kamp and Reyle, 2013\)](#page-86-7). The scheme was first proposed to annotate anaphora in dialogues but was then extended to relations in discourse (see [Pradhan et al.](#page-92-14) [\(2012\)](#page-92-14) for more details). Such a good extensibility is a result of the fact that MATE is a*meta-scheme*: It consists of a core scheme and multiple extensions. The core scheme can be used to conduct the same annotation task as MUC and can be extended with respect to different tasks. The annotation normally uses XML, but many of its extensions use other their own formats.

#### *3.2.3. Zero Pronoun, Bridging Reference, and Deictic Reference*

In addition to the "co-referential" relation discussed above, many are also interested in "hard" cases, each kind of which is often annotated as following an extension of MATE. These include the following three: (1) zero pronoun: [Pradhan et al.](#page-92-14) [\(2012\)](#page-92-14) annotated (both anaphoric and non-anaphoric) ZPs in Chinese and Arabic (see Section [3.1.4\)](#page-18-5); (2) bridging reference: bridging anaphora is a kind of indirect referent, where the antecedent of an anaphor is not explicitly mentioned but "associated" information is mentioned [\(Clark, 1975\)](#page-82-6). Identifying such a relation needs commonsense inference. Consider the following example from [Clark](#page-82-6) [\(1975\)](#page-82-6):

(9) I looked into the room. The ceiling was very high.

<span id="page-19-0"></span><sup>17</sup>In linguistics, a zero pronoun is a pronoun that is implied but not explicitly expressed in a sentence.

<span id="page-20-1"></span>

| Reference                    |
|------------------------------|
| Chinchor and Sundheim (1995) |
| Doddington et al. (2004)     |
| Poesio (2000)                |
| Hovy et al. (2006)           |
| Levesque et al. (2012)       |
| Rahman and Ng (2012)         |
| Webster et al. (2018)        |
| Hasler et al. (2006)         |
| Cybulska and Vossen (2014)   |
| Poesio and Artstein (2008b)  |
|                              |

Table 6: Anaphora Resolution datasets and statistics.

<span id="page-20-0"></span>"the room" is an antecedent of "the ceiling" because the room has a ceiling; (3) deictic reference: deixis [\(Webber,](#page-97-6) [1988\)](#page-97-6) is a phrase that refers to the "speaker's position" (e.g., time, place, and situation), which is always abstracted. For example, in

#### (10) I went to school yesterday.

the first person pronoun "I" and the word "yesterday" are deictic references, which refer to the speaker and the day before the date when [\(10\)](#page-20-0) was uttered, respectively. Schemes like ARRAU [\(Poesio and Artstein, 2008a\)](#page-92-17) extended MATE and is able to annotate bridging and deictic references.

### <span id="page-20-3"></span>*3.3. Datasets*

As we discussed when we introduced annotation schemes in Section [3.2,](#page-19-1) there is no clear cut between co-reference and anaphora in computational linguistics research. We hereby review either mainstream corpora utilized in Anaphora Resolution or co-reference resolution, while being mindful of the scope of each of them. The datasets and their statistics are summarized in Table [6.](#page-20-1)

The 6th version of MUC (MUC-6, [Chinchor and Sundheim, 1995\)](#page-82-5) is the first corpus that enables the co-reference resolution, where the task of co-reference resolution and the MUC annotation scheme was first defined. Its texts are inherited from the prevision MUCs and are English news. An example of MUC-6 is shown in Example [\(8\).](#page-19-2) [Chinchor](#page-82-8) [\(1998\)](#page-82-8) updated MUC-6 in 2001 and construct the MUC-7/MET-2 corpus. MUC-7 was designed to be multi-lingual (NB: data in Chinese and Japanese are included in MET-2, which has been considered as a part of MUC-7) and to be more carefully annotated than MUC-6 by providing annotators with a clearer task definition and finer annotation guidelines.

ACE is a multi-lingual (i.e., English, Chinese, and Arabic) multi-domain co-reference resolution corpus [\(Dod](#page-83-10)[dington et al., 2004\)](#page-83-10). In terms of co-reference resolution, it was built with the same purpose as MUC[18](#page-20-2) and they same problems pointed by [Deemter and Kibble](#page-83-11) [\(2000\)](#page-83-11) (see Section [3.2](#page-19-1) for more discussion). In addition to MUC and AEC, there are works following the MUC scheme, while targeting domains other than news, which include GENIA [\(Kim](#page-87-6) [et al., 2003\)](#page-87-6), GUM [\(Zeldes, 2017\)](#page-99-3), and PRECO [\(Chen et al., 2018b\)](#page-82-9).

The GNOME corpus was first proposed to investigate the effect of salience on language production (see Section [3.1.3](#page-18-6) and [Poesio](#page-92-15) [\(2000\)](#page-92-15); [Pearson et al.](#page-92-18) [\(2001\)](#page-92-18)) and then be used to develop and evaluate anaphora resolution algorithms [\(Poesio, 2003;](#page-92-19) [Poesio and Alexandrov-Kabadjov, 2004\)](#page-92-20) targeting especially the bridging reference resolution, in the course of which the MATE scheme was introduced (see Section [3.2\)](#page-19-1). GNOME is an English multi-domain corpus. The initial GNOME corpus [\(Poesio et al., 1999b\)](#page-92-21) consists of data from the museum domain (building on the SOLE project [\(Hitzeman et al., 1998\)](#page-85-8)) and patient information leaflets (building on the ICONOCLAST project), which is then expended to include tutorial dialogues [\(Poesio, 2000\)](#page-92-15). GNOME followed the MATE scheme. Each noun phrase is marked by an ⟨*ne*⟩ and its anaphoric relations (marked by) are annotated separately, for example:

<span id="page-20-2"></span><sup>18</sup>Though, in terms of entity recognition, they don't have the same purpose.

```text
⟨ne ID="ne07" ... ⟩
Scottish-born, Canadian-based jeweller, Alison Bailey-Smith⟨/ne⟩
...
⟨ne ID="ne08"⟩ ⟨ne ID="ne09"⟩Her⟨/ne⟩ materials⟨/ne⟩
⟨ante current="ne09"⟩
⟨anchor ID="ne07" rel="ident" ... ⟩
⟨/ante⟩
```text

OntoNotes [\(Hovy et al., 2006\)](#page-85-7) is a multi-lingual (i.e., English, Chinese, and Arabic) multi-domain dataset. It is one of the most commonly used anaphora/co-reference resolution and was used in the CoNLL 2012 shared task [\(Pradhan](#page-92-14) [et al., 2012\)](#page-92-14). It was annotated following an adapted version of the MATE (named M/O scheme by [Poesio et al.](#page-92-1) [\(2023\)](#page-92-1)). Though it has been widely used in co-reference resolution tasks, many of its relations are not co-reference. For example, bound anaphora frequently appear (see the start of this section for more discussion). Additionally, OntoNotes annotates ZPs in its Chinese and Arabic portions (see Section [3.1.4\)](#page-18-5). There are other corpora following M/O, but targeting different domains, including the biomedical (e.g., CRAFT [\(Cohen et al., 2017\)](#page-82-10)), Wikipedia (e.g., GAP [\(Webster et al., 2018\)](#page-97-5) and WikiCoref [\(Ghaddar and Langlais, 2016\)](#page-84-12)), and literary text (e.g., LitBank [\(Bamman](#page-79-6) [et al., 2020\)](#page-79-6)); and different anaphorical phenomena, including bridging anaphora (e.g., ISNOTE [\(Hou et al., 2018\)](#page-85-9)), style variation (e.g., WikiCoref [\(Ghaddar and Langlais, 2016\)](#page-84-12)), and ambiguity (e.g., GAP [\(Webster et al., 2018\)](#page-97-5)).

ARRAU is an English multi-domain (i.e., dialogue, narrative, and news) anaphora resolution dataset, annotated following the MATE scheme [\(Poesio and Artstein, 2008b;](#page-92-16) [Uryupina et al., 2020\)](#page-96-6). However, different from other corpora that also follow MATE, ARRAU extended MATE to annotate anaphoric ambiguity explicitly (recall that MATE is a meta-scheme). [Poesio and Artstein](#page-92-16) [\(2008b\)](#page-92-16) introduced the *Quasi-identity* relation, which is used for the situation when co-refer is possible but not certain by annotators and allowed each anaphor to have two distinct interpretations. In the example sample below, the footnote "1,2" of the anaphor "it" means ambiguity exists and it can either refer to 'engine E2' or "the boxcar at Elmira".

(u1) M: can we .. kindly hook up ... uh ... [engine E2]<sup>1</sup> to [the boxcar at Elmira]<sup>2</sup> (u2) M: +and+ send [it]1,<sup>2</sup> to Corning as soon as possible please

The Winograd Scheme Challenge (WSC, [Levesque et al., 2012\)](#page-88-4) focuses on the "hard" cases of CR, which often require lexical and commonsense knowledge. It can be traced back to Terry Winograd's minimal pair [\(Winograd,](#page-97-7) [1972\)](#page-97-7):

<span id="page-21-1"></span><span id="page-21-0"></span>(11) a. The city council refused the demonstrators a permit because they feared violence.

b. The city council refused the demonstrators a permit because they advocated violence.

The antecedent of "they" changes from "the city council" to "the demonstrators" from [\(11-a\)](#page-21-0) to [\(11-b\).](#page-21-1) [Levesque](#page-88-4) [et al.](#page-88-4) [\(2012\)](#page-88-4) introduced the WSC benchmark consisting of hundreds of such minimal pairs. Since then, many largerscale WSC-like corpora have been constructed. This includes the DPR corpus [\(Rahman and Ng, 2012\)](#page-93-10), the PDP corpus [\(Davis et al., 2017\)](#page-82-11), and the Winogrande corpus [\(Sakaguchi et al., 2021\)](#page-94-8). Following a similar paradigm, GAP [\(Webster et al., 2018\)](#page-97-5), Winogender [\(Rudinger et al., 2018\)](#page-93-11) and Winobias [\(Zhao et al., 2018\)](#page-99-4) were proposed for "hard" cases that link to gender bias.

NP4E [\(Hasler et al., 2006\)](#page-84-11) and ECB+ [\(Cybulska and Vossen, 2014\)](#page-82-7) are corpora for investigating cross-document co-reference. They annotated both entities and events co-reference and both within and cross-document co-reference. These corpora were built by starting from a set of clusters of documents, the documents of each of which describe the same fundamental events.

The corpora mentioned above are all in English, some of which have Chinese and Arabic portions. There are anaphora/co-reference resolution corpora that focus on languages other than them. These include ANCOR (in French, [Muzerelle et al., 2013\)](#page-90-9), ANCORA (in Catalan and Spanish [Taule et al., 2008\)](#page-96-7), COREA (in Dutch [Hendrickx](#page-85-10) ´ [et al., 2008\)](#page-85-10), NAIST (in Japanese [Iida et al., 2007b\)](#page-86-8), PCC (in Polish [Ogrodniczuk et al., 2013\)](#page-91-13), PCEDT (in Czech [Nedoluzhko et al., 2014\)](#page-91-14), and TUBA-DZ (in German [Telljohann et al., 2004\)](#page-96-8).

| Name       | Knowledge | #Entities   | Structure    |
|------------|-----------|-------------|--------------|
| WordNet    | Lexical   | 155,327     | Graph        |
| COW        | Lexical   | 157,112     | Graph        |
| ODW        | Lexical   | 92,295      | Graph        |
| AWN        | Lexical   | ≈10,000     | Graph        |
| Wikipedia  | World     | 13,489,694  | Unstructured |
| Wikidata   | World     | 100,905,254 | Graph        |
| DBpedia    | World     | ≈4,580,000  | Graph        |
| Freebase   | World     | ≈2.4 B      | Graph        |
| YAGO       | World     | 4,595,906   | Graph        |
| WikiNet    | World     | 3,347,712   | Graph        |
| OMCS       | World     | 62,730      | Graph        |
| Medical-KG | World     | 22,234      | Graph        |

Table 7: Useful knowledge bases for anaphora resolution.

# <span id="page-22-1"></span>*3.4. Knowledge Bases*Both lexical and world knowledge are useful for anaphor interpretation. See the following examples from [Martin](#page-89-8) [\(2015\)](#page-89-8):

- (12) a. There was a lot of Tour de France riders staying at our hotel. Several of the athletes even ate in the hotel restaurant.
  - b. She was staying at the Ritz, but even that hotel didn't offer dog walking service.

We need the lexical knowledge that indicates "riders" are "athletes" while need the world knowledge of the fact that "Ritz" is a "hotel".

WordNet provides lexical knowledge of English [\(Miller, 1998\)](#page-90-10), including lexical entries (e.g., meaning, part-ofspeech, etc.) and relations (e.g., synonyms, hyponyms, and meronyms, etc.) among them.

Wikipedia has been an important world knowledge source for many anaphora/co-reference resolution systems. These knowledge bases consist of documents from Wikipedia as well as related meta-data. Typical examples include bases from those directly dumped from raw Wikipedia documents[19](#page-22-0) to better-structured ones, such as Wikidata [\(Vrandeciˇ](#page-96-9) c´ [and Krotzsch, 2014\)](#page-96-9), DBpedia [\(Auer et al., 2007\)](#page-79-7), and Freebase [\(Bollacker et al., 2008\)](#page-80-14). ¨

Knowledge Graphs have become popular in anaphora/co-reference resolution tasks because bases that build on raw Wikipedia are needed to be further processed (e.g., entity and relation extraction) before use. Popular knowledge graphs include those that build on Wikipedia (e.g., YAGO [\(Suchanek et al., 2008\)](#page-95-6) and WikiNet [\(Nastase et al.,](#page-91-15) [2010\)](#page-91-15)), that are about Commonsense (e.g., OMCS [\(Singh, 2002\)](#page-94-9)), and that are about expert knowledge (e.g., Medical-KG [\(Uzuner et al., 2012\)](#page-96-10)).

Search Engines, e.g., Bing and Google were also used by a few works (e.g., [\(Emami et al., 2018\)](#page-83-12)) to "hunt" knowledge for the target entities in order to resolve hard anaphora like those in WSC (see Section [3.3\)](#page-20-3), in addition to the above knowledge bases in the strict sense.

##*3.5. Evaluation Metrics*

Vanilla Precision, Recall and F1. A plausible way to assess anaphora resolution systems is by viewing both mention detection and mention linking tasks as simple classification tasks and measuring the performance using vanilla precision, recall, and F1 scores. A good evaluation metric needs to be both interpretable and discriminative. However, unfortunately, these measures cannot meet any of these criteria [\(Moosavi and Strube, 2016\)](#page-90-11), especially for the mention linking task as they overlook the structure of these relations (most of which are chain-structured).

MUC and Beyond. Along with MUC-6 (see Section [3.3\)](#page-20-3), [Vilain et al.](#page-96-11) [\(1995\)](#page-96-11) proposed the MUC score. It computes the recall and precision of anaphora/co-reference resolution outputs by considering co-reference chains in a document as a graph. [Vilain et al.](#page-96-11) [\(1995\)](#page-96-11) first defined two sets: a set of key entities K, in which there are gold standard reference chains (NB: a chain is sometimes named as a class or a cluster), and a set of response entities R, in which there are

<span id="page-22-0"></span><sup>19</sup><https://dumps.wikimedia.org/>

system generated chained. MUC score computes the recall based on the number of missing links in R compared to K, formally:

$$
\text{Recall} = \frac{\sum_{k_i \in \mathcal{K}} (|k_i| - |p(k_i, \mathcal{R})|)}{\sum_{k_i \in \mathcal{K}} (|k_i| - 1)} \tag{4}
$$

where |*k<sup>i</sup>*| is the number of mentions in the chain*k<sup>i</sup>*and*p*(*k<sup>i</sup>*,R) is the set of partitions that is constructed by intersecting*k<sup>i</sup>*with R. The computation of MUC precision is done by switching K and R. However, it has been pointed out that MUC has certain flaws: on the one hand, since MUC is merely building on mismatches of links between the two sets, it is not discriminative enough [\(Bagga and Baldwin, 1998;](#page-79-8) [Luo, 2005\)](#page-89-9). For example, it does not tell the difference between an extra link between two singletons or two prominent entities. On the other hand, [Luo](#page-89-9) [\(2005\)](#page-89-9); [Kubler](#page-87-7) ¨ [and Zhekova](#page-87-7) [\(2011\)](#page-87-7) argued that MUC prefers singletons. For instance, if we merge all mentions in OntoNotes into singletons, the resulting MUC will be higher than that of the SOTA [\(Moosavi and Strube, 2016\)](#page-90-11).

Many metrics beyond MUC have been proposed by measuring recall and precision using mentions instead of links. [Bagga and Baldwin](#page-79-8) [\(1998\)](#page-79-8) proposed*B*3 , which considers the fractions of the correctly identified mentions in R:

$$
\text{Recall} = \frac{\sum_{k_i \in \mathcal{K}} \sum_{r_j \in \mathcal{R}} \frac{|k_i \cap r_j|^2}{|k_i|}}{\sum_{k_i \in \mathcal{K}} |k_i|} \tag{5}
$$

The precision is also computed by switching K and R. As pointed by [Luo](#page-89-9) [\(2005\)](#page-89-9) and [Luo and Pradhan](#page-89-10) [\(2016\)](#page-89-10),*B* 3 still cannot fully properly handle singletons and, additionally, repeated mentions. To solve this, [Luo](#page-89-9) [\(2005\)](#page-89-9) proposed CEAF to incorporate measures of similarities between entities:

$$
\text{Recall} = \frac{\sum_{k_i \in \mathcal{K}^*} \phi(k_i, g(k_i))}{\sum_{k_i \in \mathcal{K}} \phi(k_i, k_i)} \tag{6}
$$

where K<sup>∗</sup> is the set of key entities that have the optimal mapping with R, which is found by the Kuhn-Munkres algorithm, and ϕ(·) is a similarity measure. Nevertheless, CEAF has two shortcomings: it overlooks all unaligned response entities [\(Denis and Baldridge, 2009\)](#page-83-13) and weights entities equally [\(Stoyanov et al., 2009\)](#page-95-7).

In addition to above mentioned based metrics, to handle singletons, [Recasens and Hovy](#page-93-12) [\(2011\)](#page-93-12) proposed BLANC to also consider non-coreference/non-anaphoric links. It measures the fiction of both correctly identified co-reference links and non-coreference entities, and averages them to obtain the final score.

[Moosavi and Strube](#page-90-11) [\(2016\)](#page-90-11) conducted controlled experiments and proved that all the aforementioned computations of precision and recall are neither interpretable nor reliable as they suffer from the so-called *mention identification e*ff*ect*. They proposed the LEA metric, which was claimed to be able to solve the above issues from two perspectives: (1) it considers both links and mentions; (2) it weights entities with respect to their importance.

### *3.6. Annotation Tools*

Text Editors. In the early years, anaphora/co-reference were annotated using text editors or manipulation tools. For example, MUC-6 and ACE were annotated using plain text editors while GNOME was annotated using the XML manipulation tool developed by the University of Edinburgh[20](#page-23-0) .

Co-reference Annotation Tools. Later, linguists and computer scientists developed software that enables multi-layer annotation. The software that is designed for annotating co-reference or allows the annotations of relations between phrases can be used for anaphora/co-reference annotation tasks. For example, ARRAU and PCC used MMAX2, which is a free, extensible, general-purpose, and desktop-based annotation tool. It allows users to annotate relations using fields in a form, and the form is customizable. The NP4E project used PALinkA and ECB+ used CAT [\(Bartalesi Lenzi](#page-80-15) [et al., 2012\)](#page-80-15). Both of them were designed for the event and reference annotation. More recently, co-reference annotation tools that provide better visualization, allow drag-and-drop annotation, and offer post-annotation analysis have been built. Typical examples include CorefAnnotator [\(Reiter, 2018\)](#page-93-13), which is open-sourced and desktop-based, SCAR [\(Oberle, 2018\)](#page-91-16), which is open-sourced and web-based, and LightTag, which is not fully free but provides good online teamwork services.

<span id="page-23-0"></span><sup>20</sup>http://www.ltg.ed.ac.uk/software/

Annotation Tools with Advanced Functionalities. Some annotation tools provide extra services that help to make sure the annotation procedure is fast and reliable. We classify these services into three categories: (1) External Knowledge: BRAT [\(Stenetorp et al., 2012\)](#page-95-8) and INCEpTION [\(Klie et al., 2018\)](#page-87-8) integrate external knowledge bases, e.g., Freebase and Wikidata (see Section [3.4\)](#page-22-1). Once an annotator identifies an entity, these tools would search the linked base and return related entry; (2) Pre-trained Models: Tools such as TagEditor, Togtag, INCEpTION, and MyMiner [\(Salgado et al., 2012\)](#page-94-10) can call embedded pre-trained entity recognition models so that they can suggest positions of possible name entities during annotation, in which MyMiner was designed specifically for the medical domain (see [Neves and](#page-91-17) Seva [\(2021\)](#page-91-17) for an overview of annotation tools for medical NLP). Additionally, beyond name ˇ entities, TagEditor and INCEpTION can also suggest potential reference chains based on their integrated pre-trained co-reference resolvers, enabling active learning for anaphora/co-reference resolution; (3) Cross-document Annotation: using CROMER [\(Girardi et al., 2014\)](#page-84-13) and CoRefi [\(Bornstein et al., 2020\)](#page-80-16), annotators can tag, link, or update entities across multiple documents. This is done by allowing annotators to cluster documents based on topics and annotate documents in a cluster together.

# <span id="page-24-0"></span>*3.7. Methods*#*3.7.1. Rule-based Methods*## A. Linguistically-inspired Approaches

Like many other tasks in NLP, early works on anaphora resolution built on rules that are rooted cognitively and linguistically. Here, the term "early" represents the age when systematic evaluations of anaphora resolution, e.g., MUC, had not been introduced. The very first algorithm is the naive algorithm proposed by [Hobbs](#page-85-11) [\(1978\)](#page-85-11). It first does a breadth-first search from the parse tree of the sentence to search for identifying mentions and links mentions based on constraints introduced in Section [3.1.1.](#page-17-2)

Later on, a series of anaphora resolution systems were proposed together with computational investigations of the effect of salience (see Section [3.1.3\)](#page-18-6). Based on a set of factors that proved to influence salience, [Sidner](#page-94-11) [\(1979\)](#page-94-11) introduced rules that are used to compute the expected focus of discourse and rules that are used to interpret anaphora. As a matter of fact, this work was built on the "centering view" rooted from [Grosz](#page-84-14) [\(1977\)](#page-84-14), which suggests that, during anaphora resolution, the searching of antecedents should be restricted to the set of centered entities. It could be seen as a prototype of the idea of "center of salience" of the centering theory (see Section [3.1.2\)](#page-18-7), but the rules proposed by [Sidner](#page-94-11) [\(1979\)](#page-94-11) are extremely complex.

Starting from [Sidner](#page-94-11) [\(1979\)](#page-94-11), [Carter](#page-81-13) [\(1987\)](#page-81-13) focused on the rules about salience and developed a system coined Shallow Processing Anaphor Resolver (SPAR). SPAR maintains linguistically-inspired rules as domain knowledge and does commonsense inference over them. As pointed out by [Carter](#page-81-13) [\(1987\)](#page-81-13), since maintaining domain knowledge and reasoning rules is expensive, SPAR made them as simple as possible. That is why it was called "shallow processing". Carter assessed SPAR on a set of 322 test samples and found that SPAR could successfully resolve 93% pronominal anaphors and 87% non-pronominal anaphora. [Hobbs et al.](#page-85-12) [\(1988\)](#page-85-12) formalized commonsense inference in anaphora resolution as abduction and introduced TACITUS. To do abduction, in TACITUS, knowledge (i.e., rules) is maintained in formal logic (first-order predicate logic in this case). Focusing on salience, [Lappin and Leass](#page-87-9) [\(1994b\)](#page-87-9) proposed the Resolution of Anaphora Procedure (RAP) algorithm. After selecting a set of candidate antecedents based on semantic and syntactic constraints, RAP contains a rule-based procedure for assigning values to several salience parameters, which are then used for resolute anaphors. An assessment on 360 hand-crafted texts containing pronouns showed RAP defeated the naive algorithm by 2%.

Also starting from [Sidner](#page-94-11) [\(1979\)](#page-94-11), there were subsequent works that extended the idea of "focus" on the basis of the introduction of the concept of "centering". [Brennan et al.](#page-81-14) [\(1987\)](#page-81-14) introduced the BFP algorithm for anaphora resolution, which roughly has three stages: (1) construct a set of candidate antecedents with accordance to the rules of the semantic constraint; (2) filter and classify the candidates based on which action a candidate belongs to in centering theory (see Section [3.1.2\)](#page-18-7); and (3) select the best candidate in according to a pre-defined preference over the actions. One limitation of the BFP algorithm is that its final choice is merely based on a linear preference order. To optimize this selection process, [Beaver](#page-80-17) [\(2004\)](#page-80-17) marries BFP with the optimality theory. Another limitation is that, by only considering the center theory, BFP overlooked a key pattern of how human resolute pronouns, namely, incremental resolution [\(Kehler, 1997\)](#page-87-4). In response to this problem, [Tetreault](#page-96-12) [\(2001\)](#page-96-12) proposed the Left-to-Right Centering (LRC) algorithm, which is an incremental resolution algorithm that adheres to centering constraints. An evaluation on the New York Time corpus [\(Ge et al., 1998\)](#page-84-15) suggests that LRC outperformed both BFP and the naive algorithm.

### B. Knowledge-poor Approaches

After the introduction of the MUC-6 shared task, anaphora resolution systems are able to be evaluated on a large scale. However, the trade-off is that the anaphora resolution systems can no longer access inputs that are annotated with gold-standard semantic and syntactic knowledge. Building on this setting, "knowledge-poor" approaches were proposed and most systems of this kind prefer rules that have high precision but do not rely on knowledge. The most influential work is CogNIAC [\(Baldwin, 1997\)](#page-79-9), which is a heuristic precision-first anaphora resolver that relies on rules that are almost always true. For example, CogNIAC contains a rule saying*if there is just one possible antecedent in entire the prior discourse, then that entity is the antecedent*. Its rules were selected based on the precision tested on a set of test sentences. It is worth noting that rules in CogNIAC are still used in many SOTA practical anaphora resolution systems (e.g., the Stanford Deterministic Coreference Resolver [\(Lee et al., 2013\)](#page-88-5)).

## C. Approaches with Approximate Knowledge

As pointed out by [Poesio et al.](#page-92-1) [\(2023\)](#page-92-1), this encourages two major changes in anaphora resolution: one this that instead of relying on perfect knowledge and doing reasoning on it, anaphora resolution systems started to syntactic parsers and approximate knowledge like WordNet. The other is that the focus of anaphora resolution models moved from being aware of only pronouns to all kinds of nominal phrases (that function as referring).

[Kameyama](#page-86-9) [\(1997\)](#page-86-9) proposed to resolve anaphors that are proper names, descriptions, and pronouns. It relies on syntactic and semantic constraints, but the related information came from a syntactic parser and morphological filter based on person, number, and gender features. Later on, approaches that marry rules with WordNet were introduced [\(Harabagiu and Maiorano, 1999;](#page-84-16) [Liang and Wu, 2003\)](#page-88-6). They made use of heuristic rules (as in CogNIAC), some of which consider lexical information from WordNet.

The most famous rule-based anaphora resolution system is the one proposed by [Haghighi and Klein](#page-84-17) [\(2009\)](#page-84-17), which is still frequently used as a strong baseline in today's research on anaphora resolution. In addition to aforesaid syntactic and semantic constraints, [Haghighi and Klein](#page-84-17) [\(2009\)](#page-84-17) makes full use of the parse trees. For example, it contains rules that rely on the distance between mentions, which is obtained from computing the shortest path between two mentions in the parse tree. It also uses Wikipedia as a resource for acquiring semantic knowledge of each entity.

One limitation of heuristic-based systems is that lower precision features often overwhelm higher precision features. In response to this, more recently rule-based systems [\(Raghunathan et al., 2010;](#page-93-14) [Lee et al., 2013\)](#page-88-5) categorized rules into sieves and made decisions with an ordered set of rules. These works are often called multi-sieve approaches.

### <span id="page-25-0"></span>*3.7.2. Statistical-based Methods*The introduction of large-scale benchmarks also encourages the trend of using machine learning techniques in anaphora resolution. Basically, these learning-based models treat anaphora resolution as a series of classification problems. We categorize them on the basis of how they define the classification task.

#### A. Mention-pair Models

Mention-pair models train a classifier to determine whether two mentions co-refer or not. It was first introduced by [Aone and Bennett](#page-79-10) and then perfected by [Soon et al.](#page-95-9) [\(2001\)](#page-95-9). To build a mention-pair model, there are five steps:

- 1. Identifying Mentions: As a practical anaphora resolution model, the first step of this framework is to identify mentions. [Soon et al.](#page-95-9) [\(2001\)](#page-95-9) break down the mention identification into two stages: they first used three statistical sequence taggers (which is a Hidden Markov Model [\(Church, 1989\)](#page-82-12)) to do part-of-speech tagging, noun phrase identification, and name entity recognition, respectively. The outputs of them are noun phrases as well as name entities. Then, they designed rules to recognize nested noun phrases based on the identified noun phrases. For each discourse, the resulting set of mentions is the union of noun phrases, name entities, and nested noun phrases. In later works, this module was replaced by more advanced sequence taggers, e.g., conditional random field. See [\(Lata et al., 2022\)](#page-87-10) for a survey.
- 2. Feature Engineering: Akin to many statistical models, feature engineering is always needed. [Soon et al.](#page-95-9) [\(2001\)](#page-95-9) made use of not only syntactic and semantic features as usual but also lexical features with the help of WordNet. In addition to [Soon et al.](#page-95-9) [\(2001\)](#page-95-9), many works used knowledge bases for feature engineering (e.g., [Vieira and](#page-96-13) [Poesio](#page-96-13) [\(2000\)](#page-96-13); [Ponzetto and Strube](#page-92-22) [\(2006\)](#page-92-22)). In 2008, [Bengtson and Roth](#page-80-18) [\(2008\)](#page-80-18) found that a simple model with good feature engineering can defect the SOTA model at that moment.
- 3. Generating Training Examples: They used a heuristic-based method to generate training pairs (i.e., a pair of positive and negative examples). More specifically, a positive instance consists of an anaphor*A*<sup>1</sup> and its

closest preceding antecedent *A*<sup>2</sup> while a negative instance consists of the same anaphor *A*<sup>1</sup> and the mention that intervenes *A*<sup>1</sup> and *A*2. There has been a number of modifications to this strategy. For example, [Ng and](#page-91-18) [Cardie](#page-91-18) [\(2002b\)](#page-91-18) forced that *A*<sup>1</sup> can only be a non-pronominal once *A*<sup>2</sup> is also a non-pronominal. [Harabagiu](#page-84-18) [et al.](#page-84-18) [\(2001\)](#page-84-18); [Ng and Cardie](#page-91-19) [\(2002a\)](#page-91-19); [Strube et al.](#page-95-10) [\(2002\)](#page-95-10); [Yang et al.](#page-98-0) [\(2003\)](#page-98-0) further enhanced this process by applying rule-based or learning-based filters.

- 4. Building a Classifier: In this step, statistical machine learning techniques have been used. These include decision trees [\(Soon et al., 2001;](#page-95-9) [McCarthy and Lehnert, 1995\)](#page-90-12), random forests [\(Lee et al., 2017a\)](#page-88-7), Max Entropy classifier [\(Berger et al., 1996;](#page-80-19) [Ge et al., 1998\)](#page-84-15), and memory-based learning [\(Daelemans et al., 2004\)](#page-82-13).
- 5. Generating Co-reference Chains: The last step is to partition these anaphora into co-reference chains. Normally, clustering techniques are used in this step. These include closest-first clustering [\(Soon et al., 2001\)](#page-95-9), bestfirst clustering [\(Ng and Cardie, 2002b\)](#page-91-18), correlational clustering [\(McCallum and Wellner, 2004\)](#page-89-11), and graph partitioning algorithms [\(McCallum and Wellner, 2003;](#page-89-12) [Nicolae and Nicolae, 2006\)](#page-91-20).

#### B. Entity-Mention Models

As a matter of fact, the task mention-pair anaphora resolution is counter-intuitive from the perspective of linguists and cognitive scientists. Additionally, [Poesio et al.](#page-92-1) [\(2023\)](#page-92-1) pointed out that mention-pair models also overlook features of entities [\(Ng, 2010\)](#page-91-21). In response to this, entity-mention models were proposed. They directly link mentions to entities by clustering. Specifically, [Cardie and Wagsta](#page-81-15)ff [\(1999\)](#page-81-15) trained a model to classify whether a mention belongs to a partially constructed cluster. However, according to the evaluation by [Luo](#page-89-9) [\(2005\)](#page-89-9), the performance of the models of this kind is not comparable to mention-pair models.

## C. Mention-Ranking Models

Another problem of mention-pair models is that they only do binary classification without comparing different potential antecedents. To remedy this, [Denis and Baldridge](#page-83-14) [\(2008\)](#page-83-14) proposed an entity-ranking model, replacing the binary classification loss with a ranking loss. [Rahman and Ng](#page-93-15) [\(2011\)](#page-93-15) combined entity-ranking strategy with the entity-mention model, yielding SOTA performance at that moment.

# *3.7.3. Neural Anaphora Resolution*## A. Conventional Deep Learning Models

[Wiseman et al.](#page-97-8) [\(2015\)](#page-97-8) was the first to use deep neural networks in anaphora resolution. It is a non-linear mentionranking model. Instead of conjunction features (as in statistical models), the model of [Wiseman et al.](#page-97-8) uses a neural network to learn feature representations as an extension to the mention-ranking model. They defined two feature vectors, each of which is obtained from pre-training the model on any of the sub-tasks of anaphora resolution, namely, mention identification and mention linking. The final decision is made through a non-linear classification, based on these features. Both [Wiseman et al.](#page-97-9) [\(2016\)](#page-97-9) and [Clark and Manning](#page-82-14) [\(2016b\)](#page-82-14) augmented the work of [Wiseman](#page-97-8) [et al.](#page-97-8) [\(2015\)](#page-97-8) by inducing global features, but they followed different schemes. [Wiseman et al.](#page-97-9) [\(2016\)](#page-97-9) ran a recurrent neural network (RNN) to encode the representation of each sequence of mentions corresponding to an entity (i.e., a cluster) in the history. Whereas, [Clark and Manning](#page-82-14) [\(2016b\)](#page-82-14) first used a feed-forward neural network to encode each mention-pair of an entity and computed the entity representation by pooling over all mention-pairs. Later on, [Clark](#page-82-15) [and Manning](#page-82-15) [\(2016a\)](#page-82-15) extended their previous work [\(Clark and Manning, 2015\)](#page-82-16), which built up co-reference chains with agglomerative clustering. Each mention starts in its own cluster and then pairs of clusters are merged using imitation learning (a type of reinforcement learning technique) by assuming merging clusters are actions. [Clark and](#page-82-15) [Manning](#page-82-15) [\(2016a\)](#page-82-15) replaced imitation learning with deep reinforcement learning. [Liu et al.](#page-89-13) [\(2023b\)](#page-89-13) proposed a multitask learning framework for mention detection and mention linking tasks, because they found that the learning of mention detection task can enhance the learning of dependent information of input tokens, which is complimentary for mention linking detection. Such an approach achieved comparable performance to [Kocijan et al.](#page-87-11) [\(2019\)](#page-87-11) with only 0.05% WIKICREM training samples.

# B. End-to-End Models

A significant benefit of employing deep learning models lies in their capacity to operate without the requirement of handcrafted features, thus enabling the creation of end-to-end (End2End) systems. [Lee et al.](#page-88-8) [\(2017b\)](#page-88-8) proposed the first End2End anaphora resolution system. It needs no human-craft feature or parser and, more importantly, it learns to process mention identification and linking tasks jointly. To this end, the fundamental idea is to first view all spans in the previous discourse as candidate antecedents and do mention ranking (NB: it was called span ranking in [Lee et al.](#page-88-8) [\(2017b\)](#page-88-8) as the spans it sent for rank are not always mentions). The inputs pass through an RNN and each span is represented by the concatenation of the RNN hidden states of the first token and the last token as well as the weighted sum of all tokens in the span using the attention mechanism [\(Bahdanau et al., 2015\)](#page-79-11). The final decision of each pair is made using a feed-forward neural network. One limitation of this method is that since it searches over all possible spans, the search space would be extremely large. To remedy this, candidate spans are pruned by limiting the maximum span width, the number of spans per word, the maximum number of antecedents, and the length of input documents. This End2End model was tested on the OntoNotes dataset and outperformed all previous works.

Akin to mention-pair anaphora resolution systems, End2End anaphora resolution is problematic because it ranks every span-anaphor pair separately. In response to this problem, [Lee et al.](#page-88-9) [\(2018\)](#page-88-9) introduced a higher-order coarse-tofine inference strategy for End2End anaphora resolution models (henceforth, C2F-AR), which, in short, does cluster ranking. It infers in an iterative manner. The antecedent distributions are used to update the span representations before doing inference, enabling later decisions conditioned on previous decisions. C2F-AR uses a coarse factor that can further prune candidate span during this higher-order inference,

More recent works focused on either improving span representations or selecting candidate spans. For example, [Luo and Glass](#page-89-14) [\(2018\)](#page-89-14) used a two-layer bi-directional RNN and combined the representations of adjacent sentences in order to improve span representation with cross-sentence dependency information. [Zhang et al.](#page-99-5) [\(2018\)](#page-99-5) proposed to enrich the span representations by training a mention identification model jointly assigning each candidate span an antecedent score. For each pair of spans, [Kirstain et al.](#page-87-12) [\(2021\)](#page-87-12) replaced span representations with a combination of lightweight bilinear functions between pairs of endpoint token representations. [Wu et al.](#page-98-1) [\(2020b\)](#page-98-1) formalized the End2End anaphora resolution as a question-answering task. A query is produced for each entity and predicts the positions of all spans in the co-reference chain.

## C. Knowledge-based Models

Analog to classical rule-based and statistical-based approaches, works on neural anaphora resolution models also seek to integrate knowledge. In terms of the use of open knowledge bases, [Aralikatte et al.](#page-79-12) [\(2019\)](#page-79-12) used world knowledge to compute rewards for reinforcement learning-based anaphora resolution models. More specifically, they submitted the predictions to an OpenIE system and compared the predicted anaphora with the knowledge to compute the reward. [Zhang et al.](#page-99-6) [\(2019\)](#page-99-6) extracted knowledge triples related to each entity from knowledge graphs and used them to enrich span representations using a knowledge attention module.

It has been pointed out that pre-trained language models are knowledge bases [\(Petroni et al., 2019\)](#page-92-23). Many recent anaphora resolution models have incorporated pre-trained language models, including BERT [\(Devlin et al., 2019\)](#page-83-0), SpanBERT [\(Joshi et al., 2020\)](#page-86-10), and CorefBERT [\(Ye et al., 2020\)](#page-98-2).

There has been a line of work focusing on addressing mention linking in WSC-like corpora (see Section [3.3\)](#page-20-3). As aforementioned, resolving these "hard" cases needs reasoning with world knowledge. Works of this line incorporate either external knowledge bases [\(Emami et al., 2018\)](#page-83-12) or pre-trained language models [\(Kocijan et al., 2019;](#page-87-11) [Attree,](#page-79-13) [2019\)](#page-79-13).

###*3.7.4. Anaphoric Zero Pronoun Resolution*As mentioned in Section [3.1.2,](#page-18-7) "cool" languages (e.g., Chinese, Japanese, Korean, and Arabic) contain anaphoric zero pronouns (AZPs), and many works have focused on resolving AZPs. As with other anaphora resolution tasks, early works on AZP resolution (AZPR) used rule-based approaches and statistical approaches. Theoretically, these works are built on the fact that speakers process zero pronouns (ZPs) in the same way as pronouns [\(Yang et al.,](#page-98-3) [1999\)](#page-98-3). Early on, most of the works are for Japanese because of the NAIST corpus [\(Iida et al., 2007b\)](#page-86-8), in which AZPs are annotated. [Kameyama](#page-86-11) [\(1985\)](#page-86-11); [Okumura and Tamura](#page-91-22) [\(1996\)](#page-91-22) used center theory-based approaches for AZPR in Japanese. Statistical-based approaches were proposed with a focus on exploring useful features, including syntactic pattern features [\(Iida et al., 2007a\)](#page-86-12), heuristic rules [\(Isozaki and Hirao, 2003\)](#page-86-13), and features that had been considered in anaphora resolution systems [\(Nakaiwa et al., 1995;](#page-90-13) [Nakaiwa and Shirai, 1996;](#page-90-14) [Seki et al., 2001,](#page-94-12) [2002;](#page-94-13) [Sasano et al.,](#page-94-14) [2008;](#page-94-14) [Sasano and Kurohashi, 2011\)](#page-94-15). Meanwhile, there were also a number of Korean AZPR systems building on the Korean portion of Penn Treebank [\(Byron et al., 2006;](#page-81-16) [Han, 2006\)](#page-84-19).

Later on, the development of systems for Chinese [\(Zhao and Ng, 2007;](#page-99-7) [Kong and Zhou, 2010;](#page-87-13) [Chen and Ng, 2013,](#page-81-12) [2014,](#page-81-17) [2015\)](#page-81-18) and Arabic AZPs became active after the introduction of OntoNotes [\(Aloraini and Poesio, 2020\)](#page-79-14).

From [Chen and Ng](#page-81-19) [\(2016\)](#page-81-19), AZPR systems also went into the age of deep learning. Most of the works were for Chinese AZPR, including approaches that use deep feedforward neural networks [\(Chen and Ng, 2016\)](#page-81-19), RNNs [\(Yin](#page-98-4) [et al., 2017a,](#page-98-4) [2019\)](#page-98-5), attention network [\(Yin et al., 2018a\)](#page-98-6), memory network [\(Yin et al., 2017b\)](#page-98-7), deep reinforcement learning [\(Yin et al., 2018b\)](#page-98-8) and BERT [\(Song et al., 2020\)](#page-95-11).

The training of AZPR systems shares the problem of lacking annotated training data. For example, the AZPR largest corpus, i.e., the Chinese portion of OntoNotes, contains only 12,111 AZPs. To incorporate more data into training, there have been three paradigms: (1) Joint modeling: [Chen et al.](#page-82-17) [\(2021\)](#page-82-17) and [Aloraini et al.](#page-79-15) [\(2022\)](#page-79-15) proposed to train a model that resolves either AZPs and non-zero pronouns jointly; (2) Multi-linguality: [Iida and Poesio](#page-86-14) [\(2011\)](#page-86-14) and [Aloraini and Poesio](#page-79-14) [\(2020\)](#page-79-14) trained multi-lingual AZPR systems which were trained on AZPR data in multiple languages; (3) Data augmentation: [Liu et al.](#page-89-15) [\(2017a\)](#page-89-15) made use of large-scale reading comprehension dataset in Chinese to generate pseudo training data for Chinese AZPR. [Aloraini and Poesio](#page-79-16) [\(2021\)](#page-79-16) augmented Arabic AZPR data by a number of augmentation strategies, e.g., back translation, masking candidate mentions, etc.

####*3.8. Downstream Applications*#*3.8.1. Machine Translation*[Stojanovski and Fraser](#page-95-12) [\(2018\)](#page-95-12) provided the following example to illustrate how oracle anaphora singles can help machine translation systems.

- (13) a. Let me summarize the novel for you.
  - b. It presents a problem.
  - c. er!@#\$XPRONOUN It presents a problem.
  - d. Er prasentiert ein Problem.

Given the context (a) and the course sentence (b), based on the oracle anaphora information, [Stojanovski and Fraser](#page-95-12) [\(2018\)](#page-95-12) pre-pend the input sentence of machine translation with pronoun translation as shown in (c) and ask the system to translation with a target (d) in German. In this case, the pronoun "it" which refers to "novel" (in German "Roman") is translated to "er" (the German masculine pronoun agreeing with "Roman"). Without this information, they argued that machine translation will be hard to produce "er". The experiment on a number of Neural machine translation models suggested that would improve the BLEU scores by 4-5 points. This argumentation was strengthened by the experiments conducted by [Saunders et al.](#page-94-16) [\(2020\)](#page-94-16), who concluded that NMT does not translate gender co-reference. Despite these theoretical studies, many works [\(Le Nagard and Koehn, 2010;](#page-88-10) [Hardmeier and Federico, 2010;](#page-84-20) [Guil](#page-84-21)[lou, 2012\)](#page-84-21) focused on improving machine translation with anaphora resolution outputs. The solution is often using anaphora resolution outcomes to obtain features of each pronoun (including, gender, number, and animacy) in order to enhance the pronoun translation performance. Beyond these works, [Miculicich and Popescu-Belis](#page-90-15) [\(2017\)](#page-90-15) proposed to use clustering scores which are used for generating co-reference chains in anaphora resolution (see Section [3.7.2\)](#page-25-0) as features for re-ranking machine translation results.

There has been a long tradition of studying the impact of AZPs on machine translation systems, especially when translating from a pro-drop language to a non-pro-drop language. For example, the Japanese-English machine translation in the 1990s had already been deployed an AZPR systems [\(Nakaiwa and Ikehara, 1992\)](#page-90-16). Later systems followed a slightly different strategy. Instead of doing a full anaphora resolution, these systems only detect AZPs in the source language and directly translate them into the target language without further resolute them [\(Tan et al., 2019;](#page-96-14) [Wang](#page-97-10) [et al., 2019b\)](#page-97-10).

##*3.8.2. Summarization*There are two major uses of anaphora resolution in text summarization [\(Steinberger et al., 2007\)](#page-95-13). One is to help with finding the important terms while the other is to help with evaluating the coherence of the summarization. Many works have demonstrated that incorporating the information of co-reference chains contributes to both the faithfulness and the coverage of summarization systems [\(Bergler et al., 2003;](#page-80-20) [Witte and Bergler, 2003;](#page-98-9) [Sonawane and Kulkarni,](#page-95-14) [2016;](#page-95-14) [Liu et al., 2021b\)](#page-89-16). Nevertheless, it is also worth noting that there are also some studies that showed that anaphora resolution had negative effects [\(Orasan, 2007;](#page-91-23) [Mitkov et al., 2007\)](#page-90-17). One possible explanation is that the effect highly depends on the task the summarization system is addressing and the performance of the anaphora resolution systems (NB: these studies have been 15 years old).

#*3.8.3. Textual Entailment*For textual entailment, to understand the impact of anaphora resolution, [Mirkin et al.](#page-90-18) [\(2010\)](#page-90-18) manually analyzed 120 samples in the RTE-5 development set [\(Bentivogli et al., 2009\)](#page-80-21). They found that for 44% samples anaphora relations are mandatory for inference and for 28% sample anaphora optionally support the inference. Based on this fact, many systems that got involved in the RTE challenge made use of anaphora resolution. Nevertheless, since anaphora resolution systems at that moment were not strong enough, errors they made would propagate to downstream textual entailment systems [\(Adams et al., 2007;](#page-78-2) [Agichtein et al., 2008\)](#page-78-3). As a consequence, the contribution of anaphora resolution was negative or not significant [\(Bar-Haim et al., 2008;](#page-79-17) [Chambers et al., 2007\)](#page-81-20).

#*3.8.4. Sentiment Computing*For sentiment computing, [Sukthanker et al.](#page-95-4) [\(2020\)](#page-95-4) listed two situations when anaphora resolution can help. One is when doing sentiment analysis on online reviews, a characteristic of them is that online reviews often focus on a particular entity and, therefore, the mentions often in less elaborated forms (e.g., pronouns). Resolution of these mentions can chain them into a global entity and, hence, improve the sentiment analysis performance. The other is that anaphora resolution can also be used in fine-grained aspect-based sentiment analysis. Anaphora resolution plays a pivotal role in this task by facilitating the clustering of entities into distinct aspects. This, in turn, aids in the extraction of sentiments and opinions associated with each aspect.

The contribution of anaphora resolution in sentiment computing tasks can be summarized as follows: it enables discourse-level sentiment analysis by linking mentions from different sentences. Many efforts have been carried out to demonstrate such an ability for anaphora resolution. [Nicolov et al.](#page-91-24) [\(2008\)](#page-91-24) conducted systematic experiments to understand the impacts of anaphora resolution on sentiment analysis. Specifically, they tried to incorporate anaphora information into a number of sentiment analysis models and assessed them on varieties of datasets. They concluded that, on average, anaphora resolution can boost sentiment analysis performance by 10%. Based on this finding, sentiment analysis systems that are assembled with anaphora resolution have been proposed [\(Jakob and Gurevych,](#page-86-15) [2010;](#page-86-15) [Ding and Liu, 2010;](#page-83-15) [Le et al., 2016\)](#page-87-14).

##*3.9. Summary*Anaphora resolution has been explored extensively by theoretical linguists, psycholinguists as well as computational linguistics. It is the manifest of structural semantics because the meaning of an anaphor elucidates the syntactic relationship between the anaphor and its antecedent. Early anaphora resolution models were inspired by theories and findings in linguistics, such as the theory of syntactic and semantic constraints from theoretical linguistics and the findings about factors that influence the choice of referential form from psycholinguists. Later on, by marrying these theories with computational models, linguists also gained insights regarding the comprehension and production of anaphora from anaphora resolution systems. For instance, we could understand better how each salience factor contributes to the use of anaphora through the importance analysis of a computational model that considers the factor. Most recently, though most computational works focus on building End2End anaphora resolution systems based on deep learning techniques, linguistic theories about anaphora are still proven to play vital roles [\(Chai and Strube, 2022\)](#page-81-21). Dataset is core for either practical or theoretical anaphora resolution research. Though many annotation schemes and datasets have been introduced, we found that they share two limitations: one is that due to the fact that anaphora is a complex concept, annotations of anaphora resolution datasets are always imperfect [\(Deemter and Kibble, 2000\)](#page-83-11). The other is the lack of wide-coverage datasets that covers all kinds of anaphora. Finally, we found that anaphora resolution is useful in many downstream tasks, including major tasks of both natural language understanding and natural language generation. It is always utilized as a producer of additional features for downstream tasks. Different from other tasks in this survey, we rarely see how anaphora resolution techniques help boost the explainability of downstream models, apart from the work of [Saunders et al.](#page-94-16) [\(2020\)](#page-94-16). We also have not observed that anaphora resolution techniques are used for constructing datasets for downstream tasks.

#*3.9.1. Technical Trends*As seen in Table [8,](#page-30-0) there are two clear technical trends. One is that the research interest in the realm of anaphora resolution has shifted from machine learning-based or rule-based anaphora resolution to neural approaches, especially the End2End neural anaphora resolution, which does mention identification and linking simultaneously. Another one

<span id="page-30-0"></span>

| Task        | Reference                                                          | Feature                                      | Framework                                      | Dataset                                                            | Score                      | Metric            |
|-------------|--------------------------------------------------------------------|----------------------------------------------|------------------------------------------------|--------------------------------------------------------------------|----------------------------|-------------------|
|             | Lappin and Leass (1994b)<br>Brennan et al. (1987)<br>Carter (1987) | Semantic constraints<br>Salience<br>Salience | Centering theory<br>Logic rules<br>Logic rules | self-collected dataset<br>self-collected dataset<br>New York Times | 93.00%<br>85.00%<br>59.40% | Acc<br>Acc<br>Acc |
| Rule-based  | Tetreault (2001)<br>Baldwin (1997)                                 | Semantic constraints<br>Syntactic, Semantic, | Centering theory<br>Logic rules                | self-collected dataset<br>New York Times                           | 80.40%<br>77.90%           | Acc<br>Acc        |
|             | Liang and Wu (2003)                                                | Discourse<br>WordNet                         | Logic rules                                    | Brown Corpus                                                       | 77.00%                     | Acc               |
|             | Haghighi and Klein (2009)                                          | Syntactic, Semantic                          | Logic rules                                    | ACE                                                                | 79.60%                     | MUC-F             |
|             | Soon et al. (2001)                                                 | Syntactic, Semantic,<br>WordNet              | Mention-pair                                   | MUC-6                                                              | 62.60%                     | MUC-F             |
|             | Cardie and Wagstaff (1999)                                         | Lexical, Syntactic,<br>Semantic              | Entity-Mention                                 | MUC-6                                                              | 64.90%                     | MUC-F             |
| Stat.-based | Denis and Baldridge (2008)                                         | Linguistic & Positional                      | Mention-ranking                                | ACE                                                                | 67.00%                     | CEAF-F            |
|             | Rahman and Ng (2011)                                               | Lexical, Syntactic,<br>Semantic              | Mention-ranking                                | ACE                                                                | 60.80%                     | CEAF-F            |
|             | Wiseman et al. (2015)                                              | Syntactic, Semantic                          | Mention-rank., DNN                             | OntoNotes                                                          | 82.86%                     | Acc               |
|             | Wiseman et al. (2016)                                              | Syntactic, Semantic,<br>Global Feature       | Mention-rank., RNN                             | OntoNotes                                                          | 64.21%                     | CoNLL-F           |
|             | Clark and Manning (2016a)                                          | Syntactic, Semantic                          | DRL                                            | OntoNotes                                                          | 65.73%                     | CoNLL-F           |
|             | Clark and Manning (2016b)                                          | Syntactic, Semantic,<br>Global Feature       | Mention-ranking, DNN                           | OntoNotes                                                          | 65.52%                     | CoNLL-F           |
| DL-based    | Lee et al. (2017b)                                                 | Word & Cha. Emb.                             | End2End, LSTM, DNN                             | OntoNotes                                                          | 68.80%                     | CoNLL-F           |
|             | Lee et al. (2018)                                                  | ELMo                                         | End2End, LSTM, DNN                             | OntoNotes                                                          | 73.00%                     | CoNLL-F           |
|             | Zhang et al. (2018)                                                | Glove & Cha. Emb.                            | BiLSTM, Joint Learning                         | OntoNotes                                                          | 69.20%                     | CoNLL-F           |
|             | Joshi et al. (2019)                                                | BERT                                         | Lee et al. (2018)                              | OntoNotes                                                          | 76.90%                     | CoNLL-F           |
|             | Joshi et al. (2020)                                                | SpanBERT                                     | Lee et al. (2018)                              | OntoNotes                                                          | 79.60%                     | CoNLL-F           |
|             | Wu et al. (2020b)                                                  | SpanBERT                                     | QA                                             | OntoNotes                                                          | 83.10%                     | CoNLL-F           |
|             | Kocijan et al. (2019)                                              | WikiCREM<br>BERT                             | DNN                                            | DPR                                                                | 84.80%                     | Acc               |
|             | Liu et al. (2023b)                                                 | BERT                                         | Transformer, MTL                               | DPR                                                                | 84.58%                     | Acc               |
|             | Okumura and Tamura (1996)                                          | Salience                                     | Center Theory                                  | self-collected dataset                                             | 78.30%                     | Acc               |
|             | Sasano et al. (2008)                                               | Salience                                     | Probalistic                                    | self-collected dataset                                             | 39.10%                     | F1                |
|             | Chen and Ng (2016)                                                 | Syntactic, Lexical                           | DNN                                            | OntoNotes                                                          | 52.20%                     | F1                |
| AZPR        | Yin et al. (2017a)                                                 | Word2Vec, Global                             | RNN                                            | OntoNotes                                                          | 53.60%                     | F1                |
|             | Yin et al. (2018b)                                                 | Word Embedding                               | DRL                                            | OntoNotes                                                          | 57.20%                     | F1                |
|             | Song et al. (2020)                                                 | BERT                                         | DNN, MTL                                       | OntoNotes                                                          | 58.49%                     | F1                |
|             |                                                                    |                                              |                                                |                                                                    |                            |                   |

Table 8: A summary of representative anaphora resolution techniques. Note that [Rahman](#page-93-15) and Ng [\(2011\)](#page-93-15) reported that the performance of Denis and [Baldridge](#page-83-14) [\(2008\)](#page-83-14) was 57.7% CEAF-F and that CoNLL-F is the average of MUC, B3, and CEAF scores. Stat. denotes statistics. DL denotes deep learning. AZPR denotes Anaphoric Zero Pronoun Resolution. Cha. Emb. denotes character embedding. ACE denotes automatic content extraction. MTL denotes multi-task learning. DRL denotes deep reinforcement learning is that, as previously elucidated in Section [3.7,](#page-24-0) there exist distinct shortcomings associated with each of the task formulations such as mention pair, entity mention, and mention ranking. Consequently, a recent tendency is to employ higher-order inferences [\(Lee et al., 2018\)](#page-88-9) to directly rank clusters or entities, which allows for the incorporation of benefits from all the formulations. To sum up, the SOTA anaphora resolution models are often*End2End cluster ranking models*.

Most recent advances tended to further improve this paradigm from two angles, namely reducing the search space as an End2End anaphora resolution searches across all possible spans in its inputs for antecedents [\(Wu et al.,](#page-98-1) [2020b\)](#page-98-1); and equipping anaphora resolution systems with knowledge (which, recently, often large-scale pre-trained language models) to boost their ability of reasoning [\(Joshi et al., 2019,](#page-86-16) [2020\)](#page-86-10). Furthermore, recent investigations on anaphora resolution have also led to advancements in various deep learning paradigms. Deep reinforcement learning and multi-task learning were employed for obviating the need for language-orientated hyperparameter tuning [\(Clark](#page-82-15) [and Manning, 2016a\)](#page-82-15), investigating the enduring impact of pronoun-candidate antecedent pairs [Yin et al.](#page-98-8) [\(2018b\)](#page-98-8), and enhancing the dependency learning of mention pairs [\(Liu et al., 2023b\)](#page-89-13).

Meanwhile, there were also certain efforts that concentrated on resolving "hard" cases and multi-linguality in anaphora resolution. As for the former one, people were aware of the models' capacity to resolve ambiguous pronouns and biases (especially, gender bias) learned by anaphora resolution models [\(Levesque et al., 2012;](#page-88-4) [Rudinger](#page-93-11) [et al., 2018\)](#page-93-11). The SOTA models of this line of work are often assembled with knowledge bases [\(Emami et al., 2018\)](#page-83-12) or pre-trained language models [\(Kocijan et al., 2019\)](#page-87-11). As for the latter one, multi-lingual anaphora resolution systems were developed in order to either, theoretically, unify the theory of reference for different languages [\(Nedoluzhko et al.,](#page-91-25) [2022\)](#page-91-25), or, practically, enrich the datasets for low-resource anaphora resolution languages or tasks (e.g., AZPR; [Alo](#page-79-14)[raini and Poesio](#page-79-14) [\(2020\)](#page-79-14)).

In addition to these two trends for developing practical anaphora resolution systems, there is also a long tradition of studying how human beings understand and use anaphors with the algorithms introduced in this section from the age of rule-based methods [\(Sidner, 1979;](#page-94-11) [Carter, 1987\)](#page-81-13) to the most recent deep learning based methods [\(Chai and](#page-81-21) [Strube, 2022;](#page-81-21) [Same et al., 2022\)](#page-94-17).

| Reference                                                   | Downstream Task                                 | Feature | Explain. |
|-------------------------------------------------------------|-------------------------------------------------|---------|----------|
| Le Nagard and Koehn (2010)<br>Hardmeier and Federico (2010) | Machine Translation<br>Machine Translation      | ✓<br>✓  |          |
| Miculicich and Popescu-Belis (2017)                         | Machine Translation                             | ✓       |          |
| Saunders et al. (2020)<br>Steinberger et al. (2007)         | Machine Translation<br>Summarization Evaluation | ✓<br>✓  | ✓        |
| Bergler et al. (2003)                                       | Summarization                                   | ✓<br>✓  |          |
| Liu et al. (2021b)<br>Agichtein et al. (2008)               | Summarization<br>Textual Entailment             | ✓       |          |
| Jakob and Gurevych (2010)<br>Ding and Liu (2010)            | Sentiment Computing<br>Sentiment Computing      | ✓<br>✓  |          |
|                                                             |                                                 |         |          |

## <span id="page-31-0"></span>*3.9.2. Application Trends*Table 9: A summary of the representative applications of anaphora resolution in downstream tasks. ✓denotes the role of anaphora resolution in a downstream task.

Many demonstrations were carried out approximately 15 years ago to validate the necessity of anaphora resolution for both language generation and understanding downstream tasks [\(Steinberger et al., 2007;](#page-95-13) [Mirkin et al., 2010;](#page-90-18) [Nicolov et al., 2008;](#page-91-24) [Li et al., 2021;](#page-88-11) [He et al., 2022a\)](#page-85-13). Nevertheless, practically, at that moment, anaphora resolution often had negative effects [\(Bar-Haim et al., 2008;](#page-79-17) [Chambers et al., 2007;](#page-81-20) [Orasan, 2007;](#page-91-23) [Mitkov et al., 2007\)](#page-90-17). This is mainly because anaphora resolution systems were not powerful enough and errors they made may propagate to their downstream tasks.

Recently, with significant advancements in the capabilities of anaphora resolution systems, more and more anaphora resolution systems have been used for providing anaphora information for downstream tasks (see Table [9\)](#page-31-0). In short, anaphora resolution helps its downstream applications mainly in two ways. It links noun phrases in different sentences. As a consequence, these applications have better performance in comprehending discourse-level information. On the other hand, linking noun phrases helps downstream applications to do higher-level reasoning, e.g., extracting global entities [\(Sukthanker et al., 2020\)](#page-95-4) and recovering the ellipses [\(Aralikatte et al., 2021\)](#page-79-18).

Most downstream task models utilize anaphora resolution as an additional feature to improve task performance. However, we did not see how anaphora resolution techniques help to explain how and why anaphora is used in a certain context.

##*3.9.3. Future Works*Developing robust annotation schemes. Current annotation schemes for anaphora practically work fine but are theoretically problematic as there is no unified rule of what is remarkable, and no clear cut between co-reference and anaphora (though there is a clear boundary between them in linguistic theory). Annotation schemes so far are imperfect to improve the practicality so that large anaphora/co-reference resolution datasets (that can be used for training and assessing data-driven anaphora resolution systems) could be constructed. In exchange, the resulting corpora are imperfect in terms of both quality (i.e., some annotated relations might not be anaphoras) and coverage (i.e., some kinds of anaphora are not covered). On a different note, anaphora resolution, which can also be seen as a pragmatics task, disagreement on how an anaphora is interpreted happens across different readers [\(Uma et al., 2022\)](#page-96-15). Nonetheless, many datasets resolve disagreements through majority voting, while only a few works explicitly annotated ambiguities, which are the causes of the disagreements (e.g., [Poesio and Artstein](#page-92-16) [\(2008b\)](#page-92-16)). In aggregate, it is plausible to design a scheme (probably by extending MATE) that not only handles disagreements but also balances quality, practicality, and coverage. Furthermore, it is important to empirically investigate how the errors and limitations inherent in the annotation scheme can impact the performance of anaphora resolution systems.

Anaphora resolution evaluation. Analogue to the disagreements in the anaphora annotation, one can expect that, for a single mismatch between an output and a reference answer, it might be an error for some readers but not an error for the rest. For different mismatches, they might have different severity. The impact of severity of errors has been studied for the production of reference (see [van Miltenburg et al.](#page-90-19) [\(2020\)](#page-90-19); e.g., saying "a woman is a man" is more serious than saying "a red coat is pink"), but it has never been explored in the realm of anaphora resolution. This said, roughly computing the overlaps between model outputs and reference outputs might be problematic. On the one hand, due to discrepancies and varying degrees of errors in anaphora resolution, human evaluation [\(Martschat](#page-89-17) [and Strube, 2014\)](#page-89-17) is necessary to improve the analysis and evaluation of anaphora resolution models, as well as to establish benchmarks for developing more accurate evaluation metrics. On the other hand, when designing new evaluation metrics, disagreements, and error severity should be considered by data-driven methods.

Model development. Regarding future advancements in anaphora resolution models, a significant area of focus should be on computational studies of anaphora resolution tasks that are firmly grounded in theory but have yet to be extensively explored. Examples of such tasks include but are not restricted to (1) bridging, deictic, and plural references, which are crucial aspects of referential language, yet their computational treatment has been limited, possibly due to a shortage of relevant annotated datasets; and (2) disagreement resolution, which involves learning from discrepancies in human interpretations of anaphoric expressions to better capture the pragmatic nuances of such references, and should be incorporated into future models [\(Uma et al., 2021\)](#page-96-16); and (3) cross-document anaphora resolution, which is critical for downstream applications such as knowledge graph construction and cross-document information extraction, yet has received insufficient attention in terms of data, methods, and evaluation metrics, particularly in relation to event resolution.

### <span id="page-32-0"></span>4. Named Entity Recognition

Name Entity Recognition (NER) is a critical component of Information Extraction, which involves identifying entity mentions in text, defining their boundaries, and assigning them entity types. The most commonly recognized entity types by NER systems are Location, Person, and Organization, and tokens referring to these entities are classified as entity mentions. In the following example:

(14) Steve Jobs is the founder of Apple.

an NER system would recognize the entities that "Steve Jobs" is Person; "Apple" is Organization. NER systems use pre-defined entity types, which may vary across different implementations. For example, Stanford's widely used NER software [\(Finkel et al., 2005\)](#page-83-16) provides three versions that recognize three classes (Location, Person, Organization), four classes (Location, Person, Organization, Misc), and seven classes (Location, Person, Organization, Money, Percent, Date, Time), respectively. NER is a critical component in the field of NLP [\(Jinjie et al., 2022;](#page-86-17) [Gao et al., 2022;](#page-84-22) [He et al., 2021\)](#page-85-14) and is often combined with other tasks, such as Relation Extraction (RE), to serve as a foundation for various NLP applications. Besides, NER is also used in various data mining tasks to recognize keywords, topics, and attributes [\(He et al., 2019a;](#page-85-15) [Li et al., 2021,](#page-88-11) [2019\)](#page-88-12).

NER can be traced back to the third Message Understanding Conference (MUC-3) [\(Chinchor et al., 1993\)](#page-82-18). The task for MUC-3 was designed to extract relevant information from the text and convert it into a structured format based on a predefined template, e.g., incident, the targets, perpetrators, date, location, and effects. Early NER systems that participated in MUC-3 primarily relied on rule-based approaches, which involved the manual creation of rules to identify named entities based on their linguistic and contextual features. However, with the dominance of deep learning in the NLP community, most NER tasks are now performed using neural networks. One of the first neural networks for NER was proposed by [Collobert and Weston](#page-82-19) [\(2008\)](#page-82-19), which used a single convolutional neural network with manually constructed feature vectors. Later, this approach was replaced with high-dimensional continuous vectors, which were learned from large amounts of unlabeled data in an unsupervised manner [\(Collobert et al., 2011\)](#page-82-20). With stronger models, now, the research in NER has been largely extended to nested NER [\(Su et al., 2022\)](#page-95-15), few-shot NER [\(Huang et al., 2022a\)](#page-85-16), joint entity and relation extraction (JERE) [\(Zhong and Chen, 2021;](#page-99-8) [Mao et al., 2022a\)](#page-89-18).

Compared to standard NER whose entity relationship is absent, entities in nested NER have a hierarchical or nested structure, where one entity is embedded within another entity. For example, given

#### (15) The Ontario Supreme Court said ...

"Ontario" is a state entity that is embedded under the government entity of "Ontario Supreme Court" [\(Ringland et al.,](#page-93-16) [2019a\)](#page-93-16). Given the very expensive annotation costs, few-shot NER is also a very important research trend. It learns NER with a limited amount of labeled data. JERE tasks are established based on the needs of downstream applications. In many cases, people not only need to know what an entity is but also need to know the relationship between entities. Thus, JERE needs to identify named entities in text as well as extract the relationships that exist between them. In the following example

(16) Greg Christie has been one of the greatest engineers at Apple.

For standard NER, "Greg Christie" should be identified as Person; "Apple" should be identified as Company. However, for JERE, besides the above entity recognition, an additional relationship label, "work at" should also be predicted. Compared to identifying entities that are hierarchically structured within each other in nested NER tasks, the outcomes of JERE deliver another relationship dimension to connect entities. Both tasks are helpful in developing a comprehensive knowledge graph.

Due to the wide range of applications of NER, there have been several surveys conducted on this typical NLP task [\(Li et al., 2020a;](#page-88-13) [Yadav and Bethard, 2018\)](#page-98-10). One recent study [\(Song et al., 2021\)](#page-95-16) focused specifically on NER in the biomedical field, also known as Bio-NER. In this domain, the presence of meaningless characters in biomedical data presents a significant challenge, particularly with regards to inconsistent word distribution. Similarly, [Liu et al.](#page-88-14) [\(2022a\)](#page-88-14) summarized and discussed the challenges specific to Chinese NER, rather than the more general English NER tasks. Meanwhile, [Nasar et al.](#page-91-26) [\(2021\)](#page-91-26) explored both NER and RE tasks, as they are closely linked and are typically composed of pipeline tasks. The aforementioned surveys focus on the technical perspective of NER, based on deep learning technology, while this section broadens the horizon of NER from theoretical foundations to applications.

#*4.1. Theoretical Research*

## <span id="page-33-0"></span>*4.1.1. Prototype Theory*

[Rosch](#page-93-17) [\(1973\)](#page-93-17) argued that our classification system, which includes the classification of named entities, is based on a central or prototype example. A prototype is a typical example of a category that represents the most common features or characteristics associated with the category. For example, the prototype of "bird" must associate the features, such as wings, feathers, and the ability to fly. Birds such as ostriches or penguins, which do not perfectly possess these characteristics, may be viewed as less typical examples. [Rosch and Mervis](#page-93-18) [\(1975\)](#page-93-18) discovered that individuals can identify typical category examples faster and with greater precision than atypical examples. Thus, learning from prototypes can help to quickly grasp the important features of a named entity with a few examples.

# <span id="page-34-0"></span>*4.1.2. Graded Membership*

[Rosch et al.](#page-93-19) [\(1976\)](#page-93-19) argued that the classification of categories is frequently determined not by strict boundaries, but by various degrees of membership. We can use this theory for NER because the NER task also categorizes entities by predefined classes. The idea of Graded Membership implies how humans perceive and categorize the world around us. Some categories, e.g., "vegetable", may be viewed as less distinct and vaguer. The theory suggests that the borders between categories may not be well-defined in some cases, leading to ambiguities when attempting to classify certain items, such as tomatoes or mushrooms. The ambiguity can be further compounded by cultural or regional differences in how categories are defined or classified.

# <span id="page-34-2"></span>*4.1.3. Conceptual Blending*

According to [Fauconnier and Turner](#page-83-17) [\(2008\)](#page-83-17), the act of blending different elements and their corresponding relationships is an unconscious process that is believed to be ubiquitous in everyday thought and language. This process involves the combination of various mental spaces or cognitive domains that are drawn from different scenarios and experiences. These scenarios may be derived from personal experiences, cultural practices, or societal norms, among others. Concept blending allows us to create a new concept by combining existing ones in novel ways. For example "SpaceX" may be mapped to mental spaces related to "aerospace" and "technology"; "Tesla" may be mapped to mental spaces related to "car" and "clean energy". Conceptual blending provides an explanation for the recognition and comprehension of newly named entities by mapping them onto existing mental spaces or concepts.

## <span id="page-34-1"></span>*4.1.4. Grammatical Category*From the aspect of computational linguistics, the core issue of NER is how to define a named entity. [Marrero et al.](#page-89-19) [\(2013\)](#page-89-19) group the criteria of a named entity as grammatical category, rigid designation, unique identification, and the domain of applications. However, many of the entity definitions in the NER domain are imperfect. From the view of grammatical category, a named entity is traditionally defined as a proper noun or a common name for a proper noun. Previous work has described NER as the recognition of proper nouns in general. However, as pointed out by [Borrega](#page-80-22) [et al.](#page-80-22) [\(2007\)](#page-80-22), the classic grammatical approach to proper noun analysis is insufficient to deal with the challenges posed by NER applications. For instance, in a toy question-answering task such as

### (17) Do crocodiles live in the sea or on land?

"crocodiles", "sea", and "land" are not proper nouns, while they are commonly recognized as the essential entities for a proper understanding of the question. Consequently, a proper noun is no longer considered a criterion for identifying named entities in current NER research.

####*4.1.5. Rigid Designation*The rigid designation is a concept in the philosophy of language which suggests that certain names or labels are inherently linked to the things they represent, e.g., "Barack Obama" rigidly designates the person who is the 44th President of the US, and it cannot be used to refer to any other person or entity. NER can be viewed as a form of rigid designation as it assigns labels to entities based on their intrinsic identity [\(Kripke, 1972\)](#page-87-15), rather than on their usage in the text. However, [LaPorte](#page-87-16) [\(2006\)](#page-87-16) noted that not all expressions that appear to designate rigidly can be analyzed as directly referring to an object in every possible world. This highlights the difficulty of defining entities with complex concepts in real-world applications. As a result, annotators likely make subjective judgments when labeling complex entities, which may be affected by entity descriptions and annotators' understanding.

####*4.1.6. Unique Identification*From the view of unique identification, the MUC conferences require that NER tasks annotate the "unique identification" of entities for all expressions [\(Grishman and Sundheim, 1996\)](#page-84-23). However, determining what is unique depends on contextual elements, and can be a subjective process. While this "unique identification" is typically considered to be the reference being referred to, the definition itself poses a challenge in terms of defining what is truly unique.

<span id="page-35-0"></span>

| Tokens: | West | African | Crocodile | are | semiaquatic | reptiles | that | live | in | Africa |
|---------|------|---------|-----------|-----|-------------|----------|------|------|----|--------|
| IO      | I    | I       | I         | O   | I           | I        | O    | O    | O  | I      |
| BIO     | B    | I       | I         | O   | B           | I        | O    | O    | O  | B      |
| BIOES   | B    | I       | E         | O   | B           | E        | O    | O    | O  | S      |

Table 10: The three common annotation schemes for NER.

####*4.1.7. Domain of Applications*The definition of named entities was frequently grounded in the domain of applications. Entity definitions can be different between different NER tasks. For instance, in drug-drug interaction tasks [\(Deng et al., 2020\)](#page-83-18), diseases may not be considered entities, whereas they are entities in adverse drug events [\(Demner-Fushman et al., 2019\)](#page-83-19). Inconsistent entity definitions create challenges for machine learning. Because inconsistent entity definitions mean that for the same semantic unit, the machine has to summarize different entity representations to distinguish their labels under different tasks. This is also not conducive to training an all-around NER classifier on different application domains.

####*4.2. Annotation Schemes*

NER is typically approached as a sequence labeling task, where each token in a sentence is assigned a label. Three common annotation schemes are shown in Table [10.](#page-35-0) The IO scheme is a classification task that distinguishes between two classes, namely "Inner" and "Other", to determine whether a token belongs to an entity or not. On the other hand, the BIO scheme employs three labels, namely "Beginning", "Inner", and "Other", to identify tokens that represent the start of an entity, tokens that belong to an entity, and tokens that do not belong to any entity. The BIOES scheme expands on the BIO scheme by incorporating two additional labels, namely "Single" and "End", to more precisely define the boundaries of entities.

By employing the IO scheme, the binary classification of tokens is simplified, as each token is labeled as either belonging to an entity or not. This straightforward labeling system makes it easier to identify entities in a text, but it fails to specify the position of the entities within the text. In contrast, the BIO scheme provides more precise annotations by identifying the beginning and continuation of an entity in the text. This labeling system allows for more accurate recognition of entities in a text and better classification of individual tokens. The BIOES scheme further extends the BIO scheme by providing more precise boundaries for entities, thereby allowing for better recognition of entity boundaries in a text. The "Single" label is used to denote an entity that consists of a single token, whereas the "End" label is used to indicate the final token of an entity. By incorporating these additional labels, the BIOES scheme provides a more nuanced approach to entity recognition and annotation.

#### <span id="page-35-1"></span>*4.3. Datasets*

| Dataset      | Source       | # Sample              | Reference                    |
|--------------|--------------|-----------------------|------------------------------|
| MUC-6        | Newswire     | 318 articles          | Grishman and Sundheim (1996) |
| ACE-05       | Social media | 12,548 sentences      | Walker et al. (2006)         |
| TACRED       | Newswire     | 106,264 instances     | Zhang et al. (2017b)         |
| CoNLL-2003   | Reuters21    | 1,499 articles        | Sang and De Meulder (2003)   |
| I2B2         | ECI Corpus22 | 1,600 patient records | Stubbs and Uzuner (2015)     |
| ADE          | MEDLINE 23   | 2,972 document        | Gurulingappa et al. (2012)   |
| DDI          | DrugBank24   | 1,025 document        | Herrero-Zazo et al. (2013)   |
| WNUT-17      | Social media | 2,295 documents       | Derczynski et al. (2017)     |
| OntoNote 5.0 | Social media | -                     | Weischedel et al. (2013)     |
| CPR          | MEDLINE      | -                     | Krallinger et al. (2017)     |
| MultiNERD    | Wikipedia    | 10 languages          | Tedeschi and Navigli (2022)  |
| HIPE-2020    | Newspapers   | 17,553 mentions       | Ehrmann et al. (2022)        |
| NNE          | Newswire     | 49,208 sentences      | Ringland et al. (2019a)      |
| GENIA        | MEDLINE      | 18,546 sentences      | Kim et al. (2003)            |

Table 11: NER datasets and statistics.

The surveyed popular NER datasets and their statistics can be viewed in Table [11.](#page-35-1) The first NER-focused dataset was published in the 6th MUC Conference [\(Grishman and Sundheim, 1996\)](#page-84-23). This task consists of three sub-tasks, including entity names, temporal expressions, and number expressions. The defined entities include organizations, persons, and locations; The defined time expressions include dates and times; The defined quantities include monetary values and percentages. More details can be seen in the office website[25](#page-36-0). The example of this dataset is shown as follows.

```text
text: "Taga Co.",
type: "ORGANIZATION".
```text

The MUC conference was replaced by Automatic Content Extraction (ACE) after 1997. ACE05 [\(Walker et al.,](#page-96-17) [2006\)](#page-96-17) is another popular NER dataset published at ACE Conference. ACE05 is a multi-lingual dataset, which contains English, Arabic, and Chinese data. The corpus consists of data of various types annotated for entities, relations, and events. Its data source includes broadcast conversation, broadcast news, newsgroups, telephone conversations, and weblogs. More details can be seen on the office website[26](#page-36-1). The example of this dataset is shown as follows.

```text
entity id: "NN ENG 20030630 085848.18-E1",
type: "GPE",
subtype: "State-or-Province",
class: "SPC",
start: "82",
end: "91",
name: "california".
```text

After MUC, the Text Analysis Conference (TAC) published the Knowledge Base Population challenge. In this challenge, the Stanford NLP Group developed TAC Relation Extraction Dataset (TACRED) [\(Zhang et al., 2017b\)](#page-99-9), which contains 106,264 instances with annotated entities, relations and some other NLP tasks. More details can be seen on the office website[27](#page-36-2). The example of this dataset is shown as follows.

```text
id: "e7798fb926b9403cfcd2",
docid: "APW ENG 20101103.0539",
relation: "per:title",
token: "['At', 'the', 'same', 'time', ',', 'Chief', ...]",
subj start: "8",
subj end: "9",
obj start: "12",
obj end: "12",
subj type: "PERSON",
obj type: "TITLE",
stanford pos: "['IN', 'DT', 'JJ', 'NN', ',', 'NNP', 'NNP', ...]",
stanford ner: "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...]"
stanford head: "[4, 4, 4, 12, 12, 10, 10, 10, 10, 12, ...]",
stanford deprel: "['case', 'det', 'amod', 'nmod', 'punct', ....]".
```text

CoNLL-2003 [\(Sang and De Meulder, 2003\)](#page-94-18) is another widely used NER dataset. This task concerned languageindependent named entity recognition, which concentrates on four kinds of named entities: locations, persons, organizations, and names of miscellaneous entities that do not belong to the previous three kinds. The related data files are available in English and German. More details can be seen on the office website[28](#page-36-3). The example of this dataset is shown as follows.

<span id="page-36-0"></span><sup>25</sup>https://cs.nyu.edu/grishman ˜ /NEtask20.book 2.html

<span id="page-36-1"></span><sup>26</sup>https://catalog.ldc.upenn.edu/LDC2006T06

<span id="page-36-2"></span><sup>27</sup>https://nlp.stanford.edu/projects/tacred/#intro

<span id="page-36-3"></span><sup>28</sup>https://www.clips.uantwerpen.be/conll2003/ner/

```text
text: "['U.N.', 'official', 'Ekeus', 'heads', ...], ",
pos: "['NNP', 'NN', 'NNP', 'VBZ', ...], ",
syntactic chunk: "['I-NP', 'I-NP', 'I-NP', 'I-VP', ...], ",
named entity tag: "['I-ORG', 'O', 'I-PER', 'O', ...]".
```text

Besides the above famous datasets, MultiNERD [\(Tedeschi and Navigli, 2022\)](#page-96-18), HIPE-2020 [\(Ehrmann et al., 2022\)](#page-83-21), and NNE [\(Ringland et al., 2019b\)](#page-93-20) are also popular NER datasets in general domain. NER tasks have garnered considerable attention in numerous specialized domains. Informatics for Integrating Biology and the Bedside (I2B2) [\(Stubbs](#page-95-17) [and Uzuner, 2015\)](#page-95-17) is a national biomedical computing project sponsored by the National Institutes of Health (NIH) from 2004 to 2014. I2B2 actively advocates mining medical value from clinical data and has organized a series of evaluation tasks and workshops for unstructured medical record data, and these evaluation tasks and open datasets have gained wide influence in the medical NLP community. I2B2 is maintained in the Department of Biomedical Information at Harvard Medical School and continues to conduct assessment tasks and workshops, and the project has been renamed National NLP Clinical Challenges (N2C2). More details can be seen on the office website[29](#page-37-0) . Besides, there also exist many other biomedical datasets for specific medical NER tasks, including Adverse Drug Events (ADE) [Gurulingappa et al.](#page-84-24) [\(2012\)](#page-84-24); [Alvaro et al.](#page-79-19) [\(2017\)](#page-79-19), Drug-Drug Interaction [Herrero-Zazo et al.](#page-85-17) [\(2013\)](#page-85-17), and Chemical Protein Reaction (CPR) [Krallinger et al.](#page-87-17) [\(2017\)](#page-87-17), and GENIA [\(Shibuya and Hovy, 2020\)](#page-94-19).

<span id="page-37-1"></span>*4.4. Knowledge Bases*| Name      | Knowledge    | # Entities      | structure    |
|-----------|--------------|-----------------|--------------|
| Wikipedia | World        | 13,489,694      | unstructured |
| Wikidata  | World        | 100,905,254     | graph        |
| DrugBank  | Medical      | over 500,000    | structured   |
| UMLS      | Medical      | 16,857,345      | structured   |
| BioModels | Medical      | unclear         | structured   |
| SNOMED CT | Medical      | over 350,000    | structured   |
| ICD-10    | Medical      | unclear         | structured   |
| MIMIC-III | Medical      | unclear         | structured   |
| MeSH      | Medical      | over 28,000     | structured   |
| GeoNames  | Geographical | over 25,000,000 | structured   |
| EDGAR     | Financial    | unclear         | structured   |
| EduKG     | Educational  | 5,452           | structured   |

Table 12: Useful knowledge bases for NER.

Table [12](#page-37-1) illustrates useful knowledge bases for NER. The biggest ones are Wikidata[30](#page-37-2) and Wikipedia[31](#page-37-3), which are multi-lingual free online encyclopedias maintained by worldwide volunteers.

There are also knowledge bases in a specific field. SNOMED CT (Systematized Nomenclature of Medicine - Clinical Terms) [\(Donnelly et al., 2006\)](#page-83-22) is a systematically organized collection of medical terms that provides a standardized representation of clinical information, which is often used in NER tasks involving clinical data. MeSH (Medical Subject Headings) [\(Lipscomb, 2000\)](#page-88-15) is another controlled vocabulary, developed by the U.S. National Library of Medicine. It is used for indexing and organizing biomedical literature. Other medical knowledge bases include UMLS (Unified Medical Language System) [\(Wheeler et al., 2007;](#page-97-12) [Bodenreider, 2004\)](#page-80-23), ICD-10 [\(Hirsch et al., 2016\)](#page-85-18), MIMIC-III [\(Johnson et al., 2016\)](#page-86-18), DrugBank [\(Wishart et al., 2018\)](#page-98-11), and bioinformatics knowledge base BioModels [\(Li](#page-88-16) [et al., 2010\)](#page-88-16). GeoNames [\(Ahlers, 2013\)](#page-78-4) is a comprehensive geographic knowledge repository that encompasses over 25 million geographical names and comprises over 11 million distinctive features, including cities, countries, and landmarks. EDGAR (Electronic Data Gathering, Analysis, and Retrieval) [\(Branahl, 1998\)](#page-81-22) is a database maintained by the U.S. Securities and Exchange Commission (SEC), containing financial filings and reports from publicly traded companies. EduKG [\(Hu et al., 2016\)](#page-85-19) is an educational knowledge base.

<span id="page-37-0"></span><sup>29</sup><https://www.i2b2.org/>

<span id="page-37-2"></span><sup>30</sup><https://www.wikidata.org/>

<span id="page-37-3"></span><sup>31</sup><https://en.wikipedia.org/>

#*4.5. Evaluation Metrics*

In the process of named entity recognition task evaluation, the main evaluation metrics are also Precision, Recall, and F-value.

## <span id="page-38-5"></span>*4.6. Annotation Tools*One AI[32](#page-38-0) is an online platform that offers NLP-as-a-service. The utilization of APIs enables developers to effectively analyze, manipulate, and transform natural language inputs within their programming code without requiring any specialized knowledge of NLP. One AI facilitates the interpretation of both the meaning and information conveyed in textual data, and can produce structured data in context via language processing.

GATE Teamware[33](#page-38-1) [\(Bontcheva et al., 2013\)](#page-80-24) is an integrated annotation tool for comprehensive language processing tasks, especially for Information Extraction systems. The University of Sheffield developed GATE Teamware that enables collaborative semantic annotation projects through a shared annotation environment. The software comprises several beneficial attributes such as the ability to load document collections, create project templates that can be used multiple times, initiate projects based on templates, assign project roles to individual users, monitor progress and obtain various project statistics in real-time, report project status, annotator activity, and statistics, and apply automatic annotations or post-annotation processing via GATE-based processing routines.

MAE[34](#page-38-2) [\(Rim, 2016\)](#page-93-21) (Multi-document Annotation Environment) is a general-purpose and lightweight natural language annotation tool. The tool enables users to specify and create their customized annotation tasks, annotate any text spans of their choice, utilize non-consuming tags, effortlessly establish links between annotations, and produce annotations in stand-off XML format. It also provides a simple adjudication process with a visualization feature that displays the extent tags, link tags, and non-consuming tags of any XML standoff annotated documents.

UIMA[35](#page-38-3) [\(Ferrucci and Lally, 2004\)](#page-83-23) (Unstructured Information Management Applications) is a framework that falls under the purview of the Apache Software Foundation. It serves as a comprehensive platform for managing language processing projects and is licensed under Apache's open-source license. With its versatile capabilities, UIMA can effectively handle a diverse array of language processing tasks and extract various types of information. The UIMA's Regular Expression Annotator is capable of identifying entities such as email addresses, phone numbers, URLs, zip codes, or any other entities based on the utilization of regular expressions and concepts. The tool can generate an annotation for each detected entity or update an existing annotation with relevant feature values.

Brat[36](#page-38-4) (Browser-based Rapid Annotation Tool) is a free data labeling tool that offers a seamless browser-based interface for annotating text. It streamlines numerous annotation tasks related to natural language processing. With a thriving support community, Brat is a well-known and widely used tool in NER. It also offers the option of integrating with external resources, such as Wikipedia. Moreover, Brat enables organizations to establish servers that allow multiple users to collaborate on annotation tasks. However, implementing this feature does necessitate some technical proficiency and server management skills.

###*4.7. Methods*#*4.7.1. Nested NER*## A. Multi-label Method

Due to the fact that nested named entities can have multiple labels for a single token, traditional sequence labeling methods are not directly applicable to the recognition of nested named entities. To address this issue, researchers have attempted to convert the multi-label problem into a single-label problem or adjust the decoder to assign multiple labels to the same entity.

[Katiyar and Cardie](#page-86-19) [\(2018\)](#page-86-19) proposed a method to address nested named entity recognition by modifying the label representation in the training set. Instead of using one-hot encoding, they used a uniform distribution over the specified classes as the label. During inference, a hard threshold is set and any class with probability above this threshold is

<span id="page-38-0"></span><sup>32</sup><https://docs.oneai.com/docs>

<span id="page-38-1"></span><sup>33</sup><https://gate.ac.uk/teamware/>

<span id="page-38-2"></span><sup>34</sup><https://keighrim.github.io/mae-annotation/>

<span id="page-38-3"></span><sup>35</sup><https://uima.apache.org/sandbox.html>

<span id="page-38-4"></span><sup>36</sup><https://brat.nlplab.org/>

predicted for the token. However, this approach has two limitations: it is difficult to determine the objective for model learning; the method is sensitive to the manually chosen threshold value.

[Strakova et al.](#page-95-18) [\(2019\)](#page-95-18) changed nested NER from multi-label to single-label tasks by modifying the annotation ´ schema. They combined any two categories that may co-occur to produce a new label (e.g., combine B-Location with B-Organization to construct a new label*B Loc Org*). One benefit of this approach is that the final classification task is still a single category because all possible classification targets had been covered in the schema. Nonetheless, this method brought about a proliferation of label categories in an exponential manner, leading to sparsely annotated labels that proved difficult to learn, particularly in the context of entities nested across multiple layers.

In order to address the issue of label sparsity, [Shibuya and Hovy](#page-94-19) [\(2020\)](#page-94-19) proposed a hierarchical approach. If the classification of nested entities cannot be resolved in a single pass, the classification is continued iteratively until either the maximum number of iterations is reached or no new entities can be generated. Nevertheless, this approach is susceptible to error propagation, whereby an erroneous classification in a preceding iteration could impact subsequent iterations.

### B. Generation-based Method

[Li et al.](#page-88-17) [\(2020c\)](#page-88-17) proposed a unified framework to accomplish flat and nested NER tasks by formulating NER as a machine reading comprehension (MRC) task [\(Liu et al., 2023a\)](#page-88-18). In this approach, the extraction of each entity type corresponds to specific questions. For instance, when the model is given the question "which location is mentioned in the sentence?" along with the original sentences, it generates an answer such as "Washington". This approach is similar to Prompt Tuning [\(Liu et al., 2021a\)](#page-89-20), which avoids the labor-intensive process of constructing manual questions. However, in this method, the generated tokens must be mapped to pre-defined named entity types.

[Yan et al.](#page-98-12) [\(2021a\)](#page-98-12) proposed a novel pointer generation network. Given an input sentence, the model generates the entity indexes in this sentence that belong to entities. In such a way, flat, nested, and discontinuous entities can be recognized in a unified framework. [Skylaki et al.](#page-94-20) [\(2020\)](#page-94-20); [Fei et al.](#page-83-24) [\(2021\)](#page-83-24); [Yang and Tu](#page-98-13) [\(2022\)](#page-98-13); [Su et al.](#page-95-15) [\(2022\)](#page-95-15) are also following the idea of generating indexes of a sentence to recognize nested entities.

## C. Hypergraph-based Method

A hypergraph is a generalized variant of a normal graph, which is characterized by an edge that can connect an arbitrary number of vertices [\(Feng et al., 2019\)](#page-83-25). It is widely used in the NLP community for the tasks of syntactic parsing, semantic parsing, and machine translation because it can accurately describe the relationship between objects with multiple associations. A set of objects with only binary relations can be described by a normal graph. However, when the objects are often related to each other in a more complex one-to-many or many-to-many, e.g., nested named entities, hypergraphs become a more appropriate data structure. A typical example of nested NER with a hypergraph solution is shown in Figure [4.](#page-39-0)

<span id="page-39-0"></span>![](_page_39_Figure_8.jpeg)
<!-- Image Description: The image shows two diagrams illustrating nested label results in named entity recognition. (a) depicts a sequence labeling result with nested entities, where "that her fellow pilot Erik Satie" contains overlapping entities. (b) presents the corresponding hypergraph structure, representing the nested relationships more explicitly using directed edges, resolving the overlapping entity issue present in the sequence labeling. The diagrams use boxes representing words and labels (PER_S, PER_B, PER_I, PER_E, O). -->

(b) Corresponding hypergraph structure

Figure 4: A typical example for nested NER with hypergraph solution

[Finkel and Manning](#page-83-26) [\(2009\)](#page-83-26) firstly introduced hypergraphs into nested NER tasks, named Mention Hypergraph. In their model, Mention Hypergraph utilized nodes and directed hyper-edges to jointly represent named entities and their combinations. To compute the training loss, the proportion of accurate structures was calculated and divided by a normalized term. This term was obtained using a dynamic programming algorithm that aggregated feasible nested subgraphs for NER. However, the normalized terms obtained from this algorithm included fractions of pseudo-structures, which led to errors.

To deal with the problem of pseudo-structures, [Muis and Lu](#page-90-20) [\(2017\)](#page-90-20) proposed a gap-based marker model to identify nested entity structures by combining mention separators with features. In this method, the authors manually designed 8 types of mention separators for various scenarios. Based on the mention separators' states for any two consecutive tokens, they defined accurate and novel graph structures. However, since this approach only utilized local information to construct the graph structures, it may not be unambiguous for long-nested named entities. For instance, when presented with the nested entity "a West African Crocodile", which includes two separate entities, "West African" and "a West African Crocodile", their approach may also recognize "a West African" as a named entity.

This ambiguous problem was solved by [Wang and Lu](#page-97-13) [\(2018\)](#page-97-13), which proposes a segmental hypergraphs method. The method used an unambiguous ambiguity-free compact hypergraph representation to encode all possible combinations of nested named entities. Upon Mention Hypergraph [Finkel and Manning](#page-83-26) [\(2009\)](#page-83-26), segmental hypergraphs employed an inside-outside message-passing algorithm that can summarize the features of child nodes to the parent node and achieve efficient interference.

Besides the above work, [Wan et al.](#page-96-19) [\(2021\)](#page-96-19) introduced the concept of regional hypernodes and a combination method of graph convolutional network (GCN) and BiLSTM to generate hypernodes for each region. [Yan and Song](#page-98-14) [\(2022\)](#page-98-14) employed start token candidates and generated corresponding queries with related contexts, then used a querybased sequence labeling module to form a local hypergraph for each candidate.

# *4.7.2. Few-shot NER*# A. Metric Learning

Metric Learning is a common technology in various few-shot tasks. Prototypical Networks [\(Snell et al., 2017\)](#page-94-21) is a milestone in few-shot metric learning. Prototypical Networks compute the centroid of each category based on the support set. They determine the distance between the samples in the query set and the prototype center, followed by updating the model by optimizing this distance. Upon completion of the training phase, the embedding of each sample will be situated in closer proximity to the centroid of the corresponding category. Such an idea was largely inspired by Prototype Theory (see Section [4.1.1\)](#page-33-0).

[Fritzler et al.](#page-83-27) [\(2019\)](#page-83-27) adopted the prototypical network into few-shot NER tasks. They argued that words in a sentence are interdependent and, therefore, the labeling of adjacent words should be taken into account. To address this issue, they substituted the conventional token input of Prototypical Networks with complete sentences. However, this method ignores the problem of the Outside (O) class in NER tasks, which actually represent different semantic meanings. This problem would significantly affect the model's performance under few-shot settings.

To avoid the above issues, [Yang and Katiyar](#page-98-15) [\(2020\)](#page-98-15) followed the nearest neighbor inference [\(Wiseman and Stratos,](#page-97-14) [2019\)](#page-97-14) to assign labels to tokens. In contrast to Prototypical Networks, which learn a prototype for each entity class, this study characterized each token by its labeled instances in the support set alongside its context. The approach determined the nearest labeled token in the support set, followed by assigning labels to the tokens in the query set that require prediction.

[Das et al.](#page-82-21) [\(2022\)](#page-82-21) proposed CONTaiNER, which optimized the inter-token distribution distance. CONTaiNER employed generalized objectives to different token categories based on their Gaussian-distributed feature vectors. Such a method has the potential to mitigate overfitting problems that arise from the training domains.

## B. Prompt Tuning

Recently, prompt tuning has shown great potential on few-shot tasks by reformulating other tasks as mask language tasks [\(He et al., 2023;](#page-85-20) [Mao et al., 2022c;](#page-89-21) [Schick and Schutze, 2021\)](#page-94-22). Prompt tuning-based methods need ¨ construct prompts to obtain masked word predictions and then map predicted works into pre-defined labels, as shown in Figure [5.](#page-41-0)

[Cui et al.](#page-82-22) [\(2021\)](#page-82-22) proposed a template-based method for NER, which first applied the prompt tuning to NER tasks. However, their method had to enumerate all possible spans of sentences combined with all entity types to predict labels, which suffered serious redundancy when entity types or sentence lengths increased.

Manually defined prompts were labor-intensive and made the algorithm sensitive to these prompts. To avoid the manual prompt constructions, [Ma et al.](#page-89-22) [\(2022a\)](#page-89-22) tried to explore a prompt-free method for few-shot NER. The present

<span id="page-41-0"></span>![](_page_41_Figure_0.jpeg)
<!-- Image Description: The figure illustrates a masked language model's application in entity recognition. An input sentence is processed. "Person" is mapped to "People" via a label words mapping. The model then predicts the masked entity "[MASK]" in the prompt "Erik Satie is [MASK] entity," based on the input sentence context, effectively identifying Erik Satie as a person. The diagram uses boxes to represent labels and text, arrows show processing flow, and a shaded rectangle represents the core model. -->

Figure 5: A typical prompt tuning example for NER tasks.

study introduced an entity-oriented language model that decodes input tokens into their corresponding label words if they belong to entities. In cases where the tokens are not entities, the entity-oriented language model decodes the original tokens. Nevertheless, this approach encounters difficulties in labeling word engineering. While this study proposed an automated label selection technique, the associated experiments revealed some degree of instability.

COPNER [\(Huang et al., 2022b\)](#page-86-20) introduced class-specific words to construct prompt tuning. By comparing each token with manually selected class-specific words, this method needed neither manual prompts nor label words engineering. The selected class-specific words (a representative word corresponding to a class) were directly concatenated with original sentences as prompts. However, the manual selection of class-specific words is subjective, and a single word may not entirely capture the semantics of an entity category.

###*4.7.3. Joint NER and Relation Extraction*#### A. Parameter Sharing-based Multi-tasks Learning

Considering that NER is usually combined with relation extraction tasks applied in various downstream tasks, jointly recognizing named entities and classifying relations is a hot topic in related fields. Multi-task learning is the most common solution in joint NER and relation extraction. [Miwa and Bansal](#page-90-21) [\(2016\)](#page-90-21) firstly employed a shared Bi-LSTM encoder to obtain token representations, and then fed encoded representations into NER and relation extraction classifiers, respectively. [Sun et al.](#page-95-19) [\(2020\)](#page-95-19) utilized a GCN as a shared encoder to enable joint inference of both entity and relation types. The core idea of the above study is that multi-task models can enhance the interactions between the learning of NER and relation extraction, and further alleviate the error propagation by sharing common parameters [\(He](#page-85-14) [et al., 2021\)](#page-85-14). However, this work cannot ensure that the sharing of information is useful and proper. NER and relation extraction might need different features to result in precise predictions.

To deal with such a problem, [Yan et al.](#page-98-16) [\(2021b\)](#page-98-16) proposed an information filtering mechanism to provide valid features for NER and relation extraction. Their method used an entity and relation gate to divide cell neurons into different parts and established a two-way interaction between NER and relation extraction. In the final employed network, each neuron contained a shared partition and two task-specific partitions.

#### B. Table Filling

While multi-task learning can improve the interdependence between NER and relation extraction, the relation extraction process still requires the pairing of all entities from the NER tasks to classify relations, making it impossible to completely eliminate error propagation. To solve the problem, [Miwa and Sasaki](#page-90-22) [\(2014\)](#page-90-22) proposed a table-filling strategy to achieve joint NER and relation extraction by labeling input tokens in a table. The method utilized token lists of sentences to form rows and columns. Then, they extracted entities using the diagonal elements and classified relations with a lower/upper triangular matrix of the table. This basic table-filling strategy can be seen in Figure [6.](#page-42-0) Nonetheless, this approach involved the explicit integration of entity-relation label interdependence, which necessitated the use of intricate features and search heuristics.

[Gupta et al.](#page-84-25) [\(2016\)](#page-84-25) incorporated neural networks with a table-filling strategy via a unified multi-task recurrent neural network. This method detected both entity pairs and the related relations with an entity-relation table, which alleviated the need for search heuristics and explicit entity-relation label dependencies. [Zhang et al.](#page-99-10) [\(2017a\)](#page-99-10) further integrated global optimization and syntax information into the table-filling strategy to combine NER and relation

<span id="page-42-0"></span>

| Sentence  | The | United | States | president | Biden | will | visit |    |
|-----------|-----|--------|--------|-----------|-------|------|-------|----|
| The       | --  | --     | --     | --        | --    | --   | --    | -- |
| United    |     | LOC_B  | --     | --        | CP    | --   | --    | -- |
| States    |     |        | LOC_E  | --        | CP    | --   | --    | -- |
| president |     |        |        | --        | --    | --   | --    | -- |
| Biden     |     |        |        |           | PER_S | --   | --    | -- |
| will      |     |        |        |           |       | --   | --    | -- |
| visit     |     |        |        |           |       |      | --    | -- |
|           |     |        |        |           |       |      |       |    |

Figure 6: The illustration of the table-filling strategy.

extraction tasks. [Ren et al.](#page-93-22) [\(2021\)](#page-93-22) argued that the above table-filling-based studies only focus on utilizing local features without the global associations between relations and pairs. [Ren et al.](#page-93-22) [\(2021\)](#page-93-22) first produced a table feature for every relation, followed by extracting two types of global associations from the generated table features. Finally, the table feature for each relation was integrated with the global associations. Such a process is performed iteratively to enhance the final features for joint learning of NER and relation extraction tasks.

#### C. Tagging Scheme

The table-filling approach can mitigate issues related to error propagation. However, these techniques require the pairing of all sentence elements to assign labels, resulting in significant redundancy. To address the redundancy and avoid error propagation, [Zheng et al.](#page-99-11) [\(2017\)](#page-99-11) proposed a novel tagging scheme that converted joint NER and relation extraction into a united task. The idea was similar to the solution for nested entities [\(Strakova et al., 2019\)](#page-95-18), which ´ combined NER labels with relation extraction labels by modifying the annotation schema. For example, given the sentence "The United States president Biden will visit ...", by allocating the customized labels "Country-President B 1", "Country-President E 1" for tokens "United", "States", and "Country-President E 2" for token "Biden", the proposed method can directly obtain the triplet (United State, Country-President, Biden).

[Yu et al.](#page-98-17) [\(2020\)](#page-98-17); [Wei et al.](#page-97-15) [\(2019\)](#page-97-15) proposed two similar methods. In contrast to conventional joint approaches for NER and relation extraction, which involve recognizing entities followed by relation classification, the two methods first identified all head entities. Next, for each identified head entity, they simultaneously predicted corresponding tail-entities and relations, achieving cascade frameworks combined with a customized tagging scheme. The typical joint NER and relation extraction tasks learn to model the conditional probability:

<span id="page-42-1"></span>
$$
P(h,r,t) = P(s)P(t | h)P(r | h,t),
$$
\n<sup>(7)</sup>

where*h*represent head entity;*r*represent relation;*h*represent tail entity. The above methods combined the last two parts in Eq. [7,](#page-42-1) yielding

$$
P(h,r,t) = P(s)P(t,r \mid s). \tag{8}
$$

#*4.8. Downstream Applications*##*4.8.1. Knowledge Graph Construction*Knowledge graphs are structured semantic knowledge bases for rapidly describing concepts and their interrelationships in the physical world, aggregating large amounts of knowledge by reducing the data granularity from the document level to the instance level [\(Yao et al., 2022\)](#page-98-18). Thus, knowledge graphs enable rapid response and reasoning about knowledge. At present, the application of knowledge graphs has become prevalent in industrial domains, such as Google search. Generally, the construction of Knowledge Graphs consists of three main parts: information extraction, information fusion, and information processing. The task of information extraction involves the identification of nodes through NER and the establishment of edges via relation extraction. The task of information fusion is utilized for normalizing nodes and edges. The normalized nodes and edges need to go through a quality assessment with the task of information processing to be added to knowledge graphs.

[He et al.](#page-85-14) [\(2021\)](#page-85-14) proposed a multi-task learning-based method for the construction of genealogical knowledge graphs. At first, [He et al.](#page-85-21) [\(2019b\)](#page-85-21) collected unstructured online obituary data. Then, they extracted named entities as nodes and classified family relationships for these recognized people as edges to construct genealogical knowledge graphs. Similarly, [Jiang et al.](#page-86-21) [\(2020\)](#page-86-21) utilized NER and relation extraction for obtaining the nodes and edge in biomedical knowledge graphs. They proposed a customized tagging schema to convert the construction of biomedical knowledge graphs into a sequence labeling task with multiple inputs and multiple outputs. [Li et al.](#page-88-19) [\(2020b\)](#page-88-19) proposed a systematic approach for constructing a medical knowledge graph, which involves extracting entities such as diseases and symptoms, as well as related relationships, from electronic medical records. [Silvestri et al.](#page-94-23) [\(2022\)](#page-94-23), [Peng](#page-92-24) [et al.](#page-92-24) [\(2019\)](#page-92-24), and [Shafqat et al.](#page-94-24) [\(2022\)](#page-94-24) aimed to collect and utilize medical knowledge for NER. Further, constructing knowledge graphs requires the task of Entity Linking [\(Tedeschi et al., 2021a\)](#page-96-20) to normalize entities with different names. Entity Linking and NER are typically performed as pipeline tasks to yield more nodes and edges for the constructed graphs. Additionally, Entity Linking can be seen as a downstream task for NER, as it further refines the identified entities by linking them to a specific reference entity in a knowledge graph.

###*4.8.2. Recommendation Systems*Recommendation systems can be classified into two primary categories based on their solutions, namely contentbased recommenders and collaborative filtering-based recommenders [\(Batmaz et al., 2019\)](#page-80-25). For both of these groups, gathering data on users and products is a crucial step in the entire process. In this regard, NER modules play a pivotal role. For example, [Kim et al.](#page-87-18) [\(2012\)](#page-87-18) introduced the 5W1H model, which utilizes NER to extract contextual information, specifically Who, Why, Where, What, When, and How, to generate contextual recommendations.

[Zhou et al.](#page-99-12) [\(2020\)](#page-99-12) argued that recommendation systems currently in use suffer from a deficiency of contextual information in conversational data, as well as a semantic gap between natural language expressions and the preferences of individual users for specific items. To overcome these challenges, word- and entity-oriented knowledge graphs were incorporated to enhance the data representations. Mutual Information Maximization was adopted to align the wordlevel and entity-level semantic spaces. The aligned semantic representations were used to develop a knowledge graphenhanced recommender component to make accurate recommendations, and a knowledge graph-enhanced dialog component that can generate informative keywords or entities in the response text. A NER module is a crucial component in creating such a knowledge graph-enhanced system [\(Wu et al., 2023\)](#page-98-19).

[Iovine et al.](#page-86-22) [\(2020\)](#page-86-22) proposed a domain-independent, configurable recommendation system framework, named ConveRSE (Conversational Recommender System framEwork). ConveRSE utilized various interaction mechanisms, including natural language, buttons, and a combination of the two. The framework comprised a dialog manager, an intent recognizer, a sentiment analyzer, an entity recognizer, and a set of recommendation services. The entity recognizer component specifically focused on identifying relevant entities that were mentioned in the user's input, and linking them to an appropriate concept in the knowledge base. The ConveRSE framework's success is heavily reliant on the performance of the NER component, as it plays a crucial role in enhancing the system's overall performance.

[Wang et al.](#page-97-16) [\(2019a\)](#page-97-16) proposed RippleNet, an end-to-end framework that incorporates the knowledge graph into a recommender system. RippleNet overcame the limitations of previous embedding-based and path-based approaches to knowledge graph-aware recommendation by incorporating the knowledge graph as a form of supplementary information. RippleNet included both inward aggregation and outward propagation models. The inward aggregation version aggregated and incorporated neighborhood information when computing the representation of a given entity. By extending the neighborhood to multiple hops away, it was possible to model high-order proximity, thereby capturing users' long-distance interests. On the other hand, the outward propagation model propagated users' potential preferences and explored their hierarchical interests in knowledge graph entities.

[Upadhyay et al.](#page-96-21) [\(2021\)](#page-96-21) proposed an explainable job recommendation system by matching users with the most pertinent jobs, based on their profiles. The system also provided a human-readable explanation for each recommendation. The NER module was customized to extract pertinent details from both job postings and user profiles. These details were utilized to create comprehensible explanations for each recommendation. By identifying and categorizing entities, the NER module enhanced the accuracy and understandability of the textual explanations, providing a clear representation of the reasoning behind the recommendation system.

####*4.8.3. Dialogue Systems*Commonly, dialogue systems can be categorized into three main types, namely task-oriented, question-answering, and open-domain [\(Ni et al., 2022\)](#page-91-27). NER plays a role in enhancing the natural language understanding of the three types of dialogue systems, organizing original user messages into semantic slots, and classifying data domain and user intention [\(Li et al., 2017b\)](#page-88-20). [Abro et al.](#page-78-5) [\(2022\)](#page-78-5) proposed an argumentative dialogue system with NER and other natural language understanding tasks. The approach can enhance comprehension of user intent by comprehending injected entities and relationships. For the question-answering [Dimitrakis et al.](#page-83-28) [\(2020\)](#page-83-28) and open-domain dialogue systems, NER also plays a crucial role in the part of intent recognition and knowledge retrieval. For example, [Zhang et al.](#page-99-13) [\(2021b\)](#page-99-13) developed a sequence of sub-goals with external knowledge to improve generation performance. External knowledge refers to a range of named entities and relationships that are associated with a conceptual entity. Leveraging external knowledge allows the dialogue system to deliver a more cohesive small talk from the open domain.

####*4.9. Summary*NER is a very important semantic processing technique for information retrieval. It is the manifest of cognitive semantics, because named entities are not simply categorized by their semantics. The classified named entities also reflect their inherent attributes in people's cognition. According to Prototype Theory (see Section [4.1.1\)](#page-33-0), the inherent attributes of named entities can be represented by prototypes. It is gratifying to observe that a theory has had a significant influence on research related to few-shot NER. On the other hand, the ambiguity of named entity classification argued by Graded Membership (see Section [4.1.2\)](#page-34-0) and Grammatical Category (see Section [4.1.4\)](#page-34-1) was rarely analyzed from computational linguistic aspects. We also do not see explainable NER studies that explain why an entity is classified into a particular category from the perspective of conceptual blending (see Section [4.1.3\)](#page-34-2). The NER research on these aspects is helpful for achieving human-like intelligence in categorizing named entities.

The availability of numerous named entity recognition (NER) datasets, both in general and medical domains, has significantly enhanced computational research in this area. This may be attributed to the great application value of NER, as well as a wide range of data annotation tools. Encyclopedias knowledge and domain-specific knowledge also provide external information to help NER models better understand the context and commonsense. Now, NER has developed many practical task setups to the need of technical applications, e.g., nested NER, few-shot NER, joint NER and relation extraction, and downstream tasks, e.g., knowledge graph construction, recommendation systems, and dialogue systems.

#*4.9.1. Technical Trends*

Due to the extensive research conducted on typical NER methods over the years, researchers are shifting their focus towards NER techniques that are more applicable to practical scenarios, for example, nested NER, few-shot NER, and joint NER and relation extraction. Recent technological trends for the aforementioned NER tasks are summarized in Table [13.](#page-45-0)

Overall, nested NER can be addressed by multi-label, generation-based, and hypergraph-based methods. Among them, multi-label methods are straightforward and easy to implement. However, there are several limitations in the surveyed multi-label methods. For example, thresholds for multi-label selection are hard to decide empirically [\(Kati](#page-86-19)[yar and Cardie, 2018\)](#page-86-19); multiple labels are suffering sparsity [\(Strakova et al., 2019\)](#page-95-18) or error propagation [\(Shibuya and](#page-94-19) ´ [Hovy, 2020\)](#page-94-19), which can lower model performance. Generation-based methods are flexible. By reformulating NER tasks as question-answering, they can generate any results which satisfied the pre-defined requirements [\(Shibuya and](#page-94-19) [Hovy, 2020;](#page-94-19) [Li et al., 2020c\)](#page-88-17). These methods are used for handling Flat NER [Skylaki et al.](#page-94-20) [\(2020\)](#page-94-20), nested NER [\(Yan](#page-98-12) [et al., 2021a\)](#page-98-12), and discontinuous NER [Fei et al.](#page-83-24) [\(2021\)](#page-83-24). However, a generation-based method is hard to control what is generated, even if some studies [\(Skylaki et al., 2020;](#page-94-20) [Fei et al., 2021;](#page-83-24) [Yang and Tu, 2022;](#page-98-13) [Su et al., 2022\)](#page-95-15) have attempted to restrict the outputs of generation-based methods to a specific set of indexes (pointer network). The core point of the hypergraph-based method is about how to establish a hypergraph data structure to better represent interaction among all tokens in a sentence. These methods are good at modeling the interactions among all tokens in a sentence. It is important to note that the majority of hypergraph-based methods exhibit a task-specific nature, indicating a limited scope of applicability. These methods may not be universally applicable, and their effectiveness may be constrained by the specific task they are designed for.

Few-shot NER is usually achieved by metric learning and prompt tuning. Metric learning has demonstrated its effectiveness in various few-shot tasks [\(Kaya and Bilge, 2019;](#page-87-19) [Fritzler et al., 2019\)](#page-83-27). For few-shot NER tasks, some works predict the final labels by comparing token-to-token distance [\(Yang and Katiyar, 2020;](#page-98-15) [Das et al., 2022\)](#page-82-21) or token-to-prototype distance [\(Huang et al., 2022b\)](#page-86-20). These methods have to decide different distance calculation functions according to different task [\(Kulis et al., 2013\)](#page-87-20) and suffer instability introduced by insufficient data. By exploiting

| Tas<br>k                                     | R<br>efer<br>enc<br>e                                                                                                                                                                                                                                                                    | T<br>ech                                                                                | F<br>eatu<br>re a<br>nd<br>KB                                                                                                                                                                                                                                                          | F<br>ram<br>ewo<br>rk                                                                                                                                                                                                                    | D<br>atas<br>et                                                                                                              | S<br>cor<br>e                                                                                                       | M                               |
|----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|---------------------------------|
|                                              | Shi<br>Stra<br>Kat<br>buy<br>iya<br>kov<br>r a<br>a a<br>a e<br>´<br>nd<br>nd<br>t a<br>Car<br>Hov<br>l. (2<br>die<br>019<br>y (2<br>(20<br>)<br>020<br>18)<br>)                                                                                                                         | D<br>D<br>D<br>L<br>L<br>L                                                              | E<br>E<br>E<br>mb<br>mb<br>mb                                                                                                                                                                                                                                                          | L<br>L<br>B<br>STM<br>STM<br>i-L<br>STM<br>-CR<br>-CR<br>F<br>F                                                                                                                                                                          | A<br>A<br>A<br>CE<br>CE<br>CE<br>-05<br>-05<br>-05                                                                           | 8<br>8<br>7<br>4.3<br>4.3<br>0.2<br>%<br>%<br>%                                                                     | F<br>F<br>F                     |
| Nes<br>ted<br>NE<br>R                        | Wa<br>Mu<br>Fin<br>Li e<br>Su<br>Yan<br>Yan<br>et a<br>ng<br>kel<br>t a<br>is a<br>g a<br>et a<br>l. (2<br>and<br>l. (2<br>nd<br>nd<br>and<br>l. (2<br>Tu<br>Lu<br>020<br>Lu<br>022<br>Ma<br>021<br>(20<br>(20<br>(20<br>c)<br>)<br>nni<br>a)<br>22)<br>17)<br>18)<br>ng<br>(20<br>09)   | D<br>D<br>G<br>G<br>G<br>D<br>D<br>rap<br>rap<br>rap<br>L<br>L<br>L<br>L<br>h<br>h<br>h | B<br>B<br>E<br>E<br>E<br>B<br>B<br>ER<br>ER<br>mb<br>mb<br>mb<br>ER<br>ER<br>., S<br>., M<br>., C<br>T<br>T<br>T<br>T, W<br>egm<br>ons<br>ulti<br>ikip<br>titu<br>ent<br>gra<br>edi<br>enc<br>al H<br>ph<br>a<br>Rep<br>y P<br>ype<br>arsi<br>rese<br>rgra<br>ng<br>nta<br>phs<br>tion | P<br>P<br>H<br>H<br>H<br>P<br>U<br>oin<br>oin<br>oin<br>ype<br>ype<br>ype<br>nifi<br>ter<br>ter<br>ter<br>ed F<br>rgra<br>rgra<br>rgra<br>Net<br>Net<br>Net<br>ph<br>ph<br>ph<br>ram<br>wor<br>wor<br>wor<br>ewo<br>ks<br>ks<br>ks<br>rk | C<br>A<br>G<br>G<br>G<br>A<br>A<br>ON<br>CE<br>EN<br>EN<br>EN<br>CE<br>CE<br>-05<br>-05<br>-05<br>IA<br>IA<br>IA<br>LL<br>04 | 8<br>8<br>7<br>7<br>7<br>8<br>8<br>5.0<br>5.1<br>0.8<br>2.0<br>4.7<br>6.9<br>8.6<br>%<br>%<br>%<br>%<br>%<br>%<br>% | F<br>F<br>F<br>F<br>F<br>F<br>F |
| (5 s<br>Few<br>hot<br>-sh<br>)<br>ot N<br>ER | Hu<br>Das<br>Frit<br>Cui<br>Yan<br>ang<br>zler<br>g a<br>et a<br>et a<br>nd<br>et a<br>l. (2<br>l. (2<br>et a<br>Kat<br>l. (2<br>l. (2<br>021<br>022<br>iya<br>022<br>)<br>019<br>)<br>r (2<br>a)<br>)<br>∗<br>020<br>)                                                                  | D<br>D<br>D<br>D<br>DL<br>L<br>L<br>L<br>L                                              | B<br>B<br>B<br>B<br>P<br>roto<br>ER<br>ER<br>ER<br>ER<br>T<br>T<br>T<br>T<br>typ<br>ical<br>net<br>wor<br>k                                                                                                                                                                            | P<br>P<br>C<br>N<br>R<br>rom<br>rom<br>ont<br>NN<br>ear<br>rast<br>est<br>+<br>pt T<br>pt T<br>CR<br>Nei<br>ive<br>uni<br>uni<br>F<br>Lea<br>ghb<br>ng<br>ng<br>or<br>rnin<br>g                                                          | I2<br>I2<br>I2<br>I2<br>O<br>nto<br>B2<br>B2<br>B2<br>B2<br>not<br>es                                                        | 4<br>3<br>3<br>2<br>-<br>6.7<br>2.1<br>3.7<br>1.8%<br>%<br>%<br>%                                                   | F<br>F<br>F<br>F<br>F           |
| Join<br>and<br>t N<br>RE<br>ER               | Zhe<br>Zha<br>Miw<br>Yu<br>Gup<br>Yan<br>Sun<br>et a<br>ng<br>ng<br>ta e<br>et a<br>a a<br>et a<br>l. (2<br>et a<br>et a<br>t a<br>nd<br>l. (2<br>l. (2<br>l. (2<br>l. (2<br>l. (2<br>Ban<br>020<br>020<br>021<br>016<br>017<br>017<br>)<br>sal<br>b)<br>)<br>)<br>(20<br>)<br>a)<br>16) | D<br>D<br>D<br>M<br>D<br>G<br>D<br>rap<br>L<br>L<br>L<br>L<br>L<br>L<br>h               | E<br>E<br>E<br>E<br>B<br>E<br>E<br>mb<br>mb<br>mb<br>mb<br>ER<br>mb<br>mb<br>.,<br>.,<br>.,<br>.,<br>T<br>.,                                                                                                                                                                           | T<br>T<br>T<br>T<br>P<br>B<br>B<br>agg<br>agg<br>abl<br>abl<br>arti<br>ipa<br>i-L<br>e fi<br>e fi<br>tion<br>rtite<br>STM<br>ing<br>ing<br>llin<br>llin<br>sch<br>sch<br>Filt<br>Gra<br>g<br>g<br>em<br>em<br>er<br>ph<br>e<br>e         | N<br>N<br>A<br>C<br>A<br>A<br>A<br>CE<br>oNL<br>CE<br>CE<br>CE<br>YT<br>YT<br>-05<br>-05<br>-05<br>-05<br>L04                | 5<br>4<br>5<br>7<br>6<br>5<br>5<br>9.0<br>9.5<br>7.5<br>2.1<br>6.8<br>9.1<br>5.6<br>%<br>%<br>%<br>%<br>%<br>%<br>% | F<br>F<br>F<br>F<br>F<br>F<br>F |
| Tas<br>k-d<br>rive<br>n N<br>ER              | Pen<br>Hir<br>Sha<br>sch<br>g e<br>fqa<br>t a<br>t e<br>et a<br>t a<br>l. (2<br>l. (2<br>l. (2<br>019<br>016<br>022<br>)<br>∗<br>)<br>∗<br>)<br>∗                                                                                                                                        | DL<br>DL<br>DL                                                                          | B<br>E<br>E<br>ER<br>mb<br>mb<br>., U<br>., IC<br>T, M<br>ML<br>D-1<br>IM<br>S<br>IC-<br>0<br>III                                                                                                                                                                                      | F<br>B<br>B<br>ine<br>i-L<br>i-L<br>STM<br>STM<br>Tun<br>ing                                                                                                                                                                             | n<br>n<br>n<br>o p<br>o p<br>o p<br>ubl<br>ubl<br>ubl<br>ic<br>ic<br>ic                                                      | -<br>-<br>-                                                                                                         | F<br>F<br>F                     |
|                                              |                                                                                                                                                                                                                                                                                          |                                                                                         |                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                          |                                                                                                                              |                                                                                                                     |                                 |

<span id="page-45-0"></span>Table 13: A summary of representative NER techniques. The study with ∗ means it cannot be compared with other studies since it did not report 5-shot results. the full potential of language models, prompt tuning is proposed and demonstrated as a promising technology for few-shot tasks [\(Liu et al., 2021a;](#page-89-20) [He et al., 2023;](#page-85-20) [Liu et al., 2022b\)](#page-89-23). Prompt tuning reformulate NER as a mask language model task to reduce the gap between NER and employed pre-training LMs. The backward is that prompt tuning needs extra template construction and label word mappings and some studies have tried to deal with such problems [\(Huang et al., 2022b\)](#page-86-20).

For Joint NER and RE tasks, we summarize related studies into three groups, including parameter sharing-based multi-task learning, table-filling strategy, and customized tagging scheme. Parameter sharing is the basic idea in multitask learning, which can be used to enhance the interaction between NER and RE [\(Li et al., 2017a;](#page-88-21) [Bekoulis et al.,](#page-80-26) [2018\)](#page-80-26). This method can provide some relief from error propagation, but it cannot completely eliminate the issue. Also, this method has to pair every two entities for relation extraction, which introduces unnecessary redundancy. Table filling-based joint NER and relation extraction can completely eliminate error propagation by converting NER and relation extraction into a whole sequence-tagging task [\(Gupta et al., 2016;](#page-84-25) [Ren et al., 2021;](#page-93-22) [Ma et al., 2022b\)](#page-89-24). However, these methods have to label every two token pairs in an input sentence in an enumerable fashion. If relation extraction is defined as an unidirectional task, the half of calculations are wasted. Following the idea of the table filling strategy, tagging scheme-based methods also model the NER and relation extraction as an integrated task. The fundamental concept of the tagging scheme is to merge the labels assigned for NER with those assigned for relation extraction into a unified label [\(Zheng et al., 2017;](#page-99-11) [Strakova et al., 2019\)](#page-95-18). Such a method has the potential to circumvent ´ issues related to both error propagation and redundancy; however, it may also lead to a sparsity of labels.

## <span id="page-46-0"></span>*4.9.2. Application Trends*| Reference                       | Downstream Task              | Feature | Parser | Explain. |
|---------------------------------|------------------------------|---------|--------|----------|
| Yao et al. (2022)               | Knowledge Graph Construction |         | ✓      |          |
| He et al. (2021)                | Knowledge Graph Construction |         | ✓      |          |
| Jiang et al. (2020)             | Knowledge Graph Construction |         | ✓      |          |
| Li et al. (2020b)               | Knowledge Graph Construction |         | ✓      |          |
| Kim et al. (2012)               | Recommendation Systems       |         | ✓      | ✓        |
| Adomavicius and Tuzhilin (2011) | Recommendation Systems       | ✓       |        |          |
| Zhou et al. (2020)              | Recommendation Systems       | ✓       |        |          |
| Iovine et al. (2020)            | Recommendation Systems       | ✓       |        |          |
| Wang et al. (2019a)             | Recommendation Systems       |         | ✓      |          |
| Li et al. (2017b)               | Dialogue Systems             |         | ✓      |          |
| Abro et al. (2022)              | Dialogue Systems             |         | ✓      |          |
| Dimitrakis et al. (2020)        | Dialogue Systems             |         | ✓      |          |
| Zhang et al. (2021b)            | Dialogue System              |         | ✓      | ✓        |

Table 14: A summary of the representative applications of NER in downstream tasks. ✓denotes the role of NER in a downstream task.

We have discussed three main downstream applications of NER, including knowledge graph construction, dialogue systems, and recommendation systems. Table [14](#page-46-0) illustrate related studies. Usually, NER is the basic module for providing recognized entities for further utilization. In this case, a NER model works as a parser to mine knowledge from unstructured text. The recognized entities and relations can be used as nodes and edges for knowledge graph construction. The entities can also serve as intent recognition methods in recommendation systems, and slot-filling methods in dialogue systems. For example, [Wu et al.](#page-98-20) [\(2020a\)](#page-98-20) proposed a pre-trained task-oriented dialogue BERT, which significantly boosts the performance of a dialogue system by improving the intent detection sub-task. [Wang](#page-97-17) [et al.](#page-97-17) [\(2020\)](#page-97-17) proposed a method for recognizing related spans and value normalization with slot attention to improve the dialogue system. Besides, we also observe that using the identified named entities as features can also improve the performance of recommendation systems, because NER can help identify important entities that could be useful for making recommendations.

The most common problem is error propagation between NER and other components in a downstream system. [Kim et al.](#page-87-21) [\(2018\)](#page-87-21) employed a two-step neural dialog state tracker to alleviate the impact of the original error. With the development of PLMs and LLMs, many downstream tasks are organized as end-to-end processing tasks to achieve higher accuracy and mitigate error propagation issues. However, we can still observe that NER can improve the explainability in recommendation and dialogue systems [\(Kim et al., 2012;](#page-87-18) [Zhang et al., 2021b\)](#page-99-13), which is also an important aspect of AI research. There is still a considerable untapped potential for integrating NER with other downstream tasks, e.g., explaining how concepts blend each other between different entities; what the inherent attribute of a group of entities the selected prototypes represent; how robust an identified named entity is.

###*4.9.3. Future Works*Open-domain NER. Compared with typical single-domain NER, open-domain NER has more categories. Besides, the entity classes are hardly defined in advance. For such reason, open-domain NER is more capable of handling rapidly expanding data, and mining more potential knowledge which is hidden in massive unstructured text data [\(Ho](#page-85-22)[henecker et al., 2020;](#page-85-22) [Kolluru et al., 2020\)](#page-87-22). Open-domain NER is significant because it discovers and connects world knowledge via automatic text mining. Many manually developed lexical resources, e.g., WordNet can only cover limited concepts. When the concepts come to multi-word expressions, manually mining, structuring and updating those concepts can result in the exponential growth of human efforts. Open-domain NER is helpful for mitigating human efforts and delivering a knowledge base that connects entities from different domains.

Multi-lingual NER. In light of the fact that a significant number of languages in existence lack sufficient annotated data, knowledge transfer from high-resource languages to low-resource languages can serve as a viable solution to compensate for the paucity of data [\(Rahimi et al., 2019;](#page-93-23) [Tedeschi et al., 2021b\)](#page-96-22). Developing robust multi-lingual NER systems that can perform across multiple languages will achieve more comprehensive knowledge graphs, linking entities from different languages. It is valuable because it may lead to a united concept representation system covering different languages. On the other hand, the task of developing multi-lingual NER systems is fraught with difficulties, primarily due to the inherent dissimilarities in entity types and language structures across different languages. As a result, aligning entities and transferring knowledge learned from one language to another can present significant challenges for multi-lingual NER systems.

Unified framework for NER. In the real-world scenario, there exist flat-named entities, nested entities, and discontinuous entities. Most NER-related studies only focus on the combination of flat with nested entities or flat discontinuous entities. Both of them cannot recognize all kinds of entities. Developing a unified framework to simultaneously handle such a problem becomes an urgent need for NER [\(Fei et al., 2021\)](#page-83-24). Hierarchical concept representation knowledge bases may provide a preliminary ontology that can be used for organizing entities and their relationships. However, most of the ontology systems were manually developed by experts. This manually constructed knowledge may be invalid in specific application scenarios. A potential avenue for future research in NER is the development of a unified and robust framework for organizing entities. Such a framework could facilitate the creation of comprehensive knowledge graphs that capture the relationships between entities and can better support downstream tasks.

Continual-learning for NER. Humans exhibit a remarkable aptitude for transferring acquired knowledge from one task to another and retain their ability to perform the former task even after learning the latter. This ability is called continuous learning or life-long learning, which a regarded as an important characteristic of an intelligent system. Also, such ability can help us continue to use already deployed models when a new class of entity to be identified appears, rather than developed a new model from scratch [\(De Lange et al., 2021\)](#page-82-23). There are some exploratory studies started to pay attention to such a problem. However, a satisfactory solution has not been found yet and existing methods still suffer the severe Catastrophic Forgetting [\(Monaikul et al., 2021;](#page-90-23) [Xia et al., 2022;](#page-98-21) [Vijay and Priyanshu,](#page-96-23) [2022\)](#page-96-23). Continual learning is a critical skill for NER because NER is corpus-dependent. It is very important to update entity collections and the associated label sets, when a new corpus arrives [\(He et al., 2022b\)](#page-85-23). In this case, detecting new entities and new labels with a former trained NER model represents a challenging yet highly promising research avenue.

#### <span id="page-47-0"></span>5. Concept Extraction

Concept extraction is a process to extract concepts of interest from the text. To our best knowledge, the task of computational concept extraction was first proposed by [Montgomery](#page-90-24) [\(1982\)](#page-90-24), which analyzed the next 5 years of evolutionary progress in contemporary military message routing systems, with a focus on their transition towards becoming more advanced and knowledge-based systems. They argue that taxonomic hierarchies could be constructed to allow property inheritance of concepts, and therefore to perform rudimentary inference and analogic reasoning based on the taxonomies. [Montgomery](#page-90-24) [\(1982\)](#page-90-24) also highlighted two important sub-tasks of concept extraction for the next-generation knowledge-based systems from the perspective of 1982, namely lexicon development and conceptual structure construction.

Recent research on concept extraction has been conducted in various fields of AI research, including natural language processing (NLP) and data mining [\(Miner et al., 2012\)](#page-90-25). Keyphrase generation [\(Alami Merrouni et al., 2020\)](#page-79-20) is one of the most common concept extraction tasks. It is a summarization task focusing on extracting keyphrases from a full passage to help readers quickly understand the passage, where keyphrases can be understood as the important concepts within a passage. Methods for keyphrase extraction can be both extractive (copying from existing words) and abstractive (not copying but summarizing and abstracting from existing texts). The process of generating keyphrases facilitates the creation of a lexicon that corresponds to a specific set of concepts. Another stream of concept extraction aims at the development of ontological knowledge bases to represent, e.g., commonsense knowledge [\(Havasi and](#page-85-24) [Speer, 2007\)](#page-85-24), hypernym and synonym knowledge [\(Snow et al., 2006\)](#page-95-20), sentic knowledge [\(Cambria et al., 2022a\)](#page-81-23). These tasks tried to extract concepts to fit into pre-defined knowledge structures. Then, the structured knowledge can be directly used in downstream tasks.

Current concept extraction research is also grounded on related application scenarios, such as clinical concept extraction [\(Fu et al., 2020\)](#page-84-0), course concept extraction [\(Pan et al., 2017a\)](#page-92-25), and patent concept extraction [\(Liu et al.,](#page-89-25) [2020\)](#page-89-25). Clinical concept extraction is to transform massive unstructured electronic health records data into structured data; Course concept extraction is to extract important phrases in course captions to help to understand. Among them, clinical concept extraction is very similar to the information extraction task in NLP which aims at extracting most of the details in the unstructured text. Course and patent concept extraction are more similar to summarization tasks in NLP that target extracting important phrases.

The main difference between concept extraction and NER tasks is that the extracted concepts or keyphrases are not identified by pre-defined entity classes. In contrast, they reflect the general idea of their contexts or target domain whose concepts are being discussed, while the goal of NER is to extract important factual information from the text. However, there are overlaps between NER and concept extraction when some concepts of interest, e.g., proper nouns can be also defined as named entities. Many domain-specific concept extraction tasks, e.g., clinical concept extraction, course concept extraction, and patent concept extraction can also be categorized as NER tasks because they aim at extracting concepts that are related to specific events. These events are also factual information. We review them in this section because they define themselves as concept extraction tasks in their original works. It also has become a trend of domain-specific concept extraction.

Another related field is relation extraction, which is a sub-field of information extraction. Relation extraction extracts information from raw text and represents it in the form of a semantic relation between entities [\(Kartik Detroja,](#page-86-23) [2023\)](#page-86-23). The main difference is that, relation extraction targets at extracting relations between entities, while concept extraction targets at extracting noun entities. In knowledge graph development, relation extraction can help to connect nodes of concepts with purposeful relationships.

Concept extraction has also accelerated and contributed to multiple downstream applications, such as sentiment computing [\(Cambria et al., 2022a\)](#page-81-23), information retrieval [\(Xiong et al., 2017\)](#page-98-22), commonsense explanation generation [\(Fang and Zhang, 2022\)](#page-83-29). These applications mostly leverage explicitly extracted concepts.

Previous survey on concept extraction on focuses on clinical concept extraction [\(Fu et al., 2020\)](#page-84-0), which is a particular application field of concept extraction. In this section, we provide a more comprehensive review on concept extraction.

#*5.1. Theoretical Research*##*5.1.1. Exemplar Theory*[Medin and Scha](#page-90-26)ffer [\(1978\)](#page-90-26) argued that concepts are represented by a collection of particular exemplars or individual instances that are linked to the category. When we categorize an instance, we compare it with multiple specific exemplars of the category. This is different from Prototype Theory where a new instance is categorized by comparing the instance to the abstract prototype of the category (see Section [4.1.1\)](#page-33-0). [Medin and Scha](#page-90-26)ffer [\(1978\)](#page-90-26) formed the task of concept categorization as a classification task, and conducted experiments with 32 participants. The experiments showed that the classification judgments made by participants were impacted by various factors. These factors included the extent of resemblance between the probe item and exemplars previously acquired, the number of prior exemplars that shared resemblances with the probe item, and the similarity present both within and between the categories of the previously learned exemplars. For concept extraction and categorization, Exemplar Theory may suggest that models may take categorized instances into account when categorizing a new instance.

#*5.1.2. Semantic Primitives*[Wierzbicka](#page-97-18) [\(1972\)](#page-97-18) believed that it is possible to describe every human language by using a limited number of universal semantic primitives. These primitives are representative of fundamental concepts that form the basis of human communication and thinking. [Wierzbicka](#page-97-18) [\(1972\)](#page-97-18) established 64 universal semantic primes, which consist of basic words or ideas that cannot be defined in relation to more elementary concepts. However, these primes can be utilized to describe all other concepts present within a language. Semantic Primitives suggest that concepts should be organized as multiple layers from the concrete to abstract ones. Decision-making that runs on concrete concepts can be completed through the upper-level abstract concepts that contain those concrete concepts. Thus, it is critical to represent the hierarchical and linking relationships between concepts. There are other theories mentioned before, e.g., Frame Semantics (see Section [2.1.4\)](#page-5-2), that may guide concept structure development. Frame Semantics highlights the connection of related concepts, while Semantic Primitives suggest the hierarchical relationships between concepts and the distinction between primitive concepts and others.

##*5.1.3. Conceptual Spaces*

[Gardenfors](#page-84-26) [\(2004\)](#page-84-26) defined concept as the "theoretical entities that can be used to explain and predict various empirical phenomena concerning concept formation". The author believed that concept representations are multidimensional, where each dimension is indicative of a different characteristic or property associated with the concept. For example, one could represent the concept of a car within a conceptual space that includes dimensions such as size, speed, color, and shape. This is very similar to current vectorial representations of words or entities in NLP, while the dimensionality of Conceptual Spaces is explainable by concept properties. [Gardenfors](#page-84-26) [\(2004\)](#page-84-26) also placed significant emphasis on the role of context in understanding and representing concepts. This is due to the fact that different contexts may emphasize different features or dimensions of concepts. Then, the connections between concepts are determined by the relationships between their property similarity in the conceptual space. For example, "dog" and "cat" are similar in the animal concept space, because their properties are similar; "mammals" can be separated from "reptiles" by a property difference boundary, although both are in the animal space. This may encourage concept extraction tasks to extract both concept entities and properties associated in contexts. This is because properties define how concepts are connected from the view of [Gardenfors](#page-84-26) [\(2004\)](#page-84-26).

### <span id="page-49-0"></span>*5.2. Annotation Schemes*From the goal of the keyphrase annotation aspect, there are in general two types of annotation schemes for keyphrase extraction-liked concept extraction. The first is to precisely select existing keyphrases from input text, but not to create semantically-equivalent phrases. The second is to both select existing keyphrases and create "absent keyphrases" that are necessary but do not exist in the input text [\(Hulth, 2003\)](#page-86-24).

From the format of assigned annotations aspect, there are in general two annotation schemes as well. The first scheme is to directly give the keyphrases existing in the source text. The second scheme treats the keyphrase extraction task as a sequence labeling task, and assigns a label to each of the tokens in source text [\(Hulth, 2003\)](#page-86-24). The assigned labels in the current dataset follow a BIO scheme defined in table [10.](#page-35-0) Specifically, three labels are used: B (Beginning), I (Inner), and O (Other).

####*5.3. Datasets*The surveyed popular concept extraction datasets and their statistics can be viewed in Table [15.](#page-50-0) Overall the main thread of dataset development is (1) larger scale of datasets; (2) attending to both extractive keyphrases and abstractive keyphrases; (3) more fine-grained annotations for tags; (4) more application domains. [Hulth](#page-86-24) [\(2003\)](#page-86-24) proposed one of the first keyphrase extraction datasets, termed Inspec. Their dataset is based on the scientific papers under*Computers and Control*, and *Information Technology* disciplines in the Inspec database. The keywords used in the scientific papers are selected as the keyphrases. Abstracts are used as the keyphrase extraction context. Keywords in scientific papers are used as keyphrases. Each abstract has two sets of keywords: a set of controlled terms, i.e., terms restricted to the Inspec thesaurus; and a set of uncontrolled terms that can be any suitable terms that may or may not be present in the abstracts. They collected 1000 abstracts as a train set, 500 as a validation set, and 500 as a test set.

<span id="page-50-0"></span>

| Dataset     | Task | Source                    | # Samples   | Reference              |
|-------------|------|---------------------------|-------------|------------------------|
| Inspec      | KE   | Inspec database           | 2,000       | Hulth (2003)           |
| NUS         | KE   | Google SOAP API           | 211         | Nguyen and Kan (2007)  |
| Krapivin    | KE   | ACM Digital Library       | 2,304       | Krapivin et al. (2009) |
| SemEval2010 | KE   | ACM Digital Library       | 244         | Kim et al. (2010)      |
| Twitter     | KE   | Twitter                   | 1,000       | Zhang et al. (2016)    |
|             |      | ACM Digital Library,      |             |                        |
| KP-20K      | KE   | ScienceDirect,            | 567,830     | Meng et al. (2017)     |
|             |      | and Web of Science        |             |                        |
| CCF         | KE   | China Computer Federation | 13,449      | Wang et al. (2018b)    |
| MLDBMD      | KE   | Academic Conferences      | 128.1k      | Li et al. (2018)       |
| TempEval    | ClCE | Mayo Clinic               | 600         | Bethard et al. (2016)  |
| i2b2-2010   | ClCE | Clinical Records          | 826         | Uzuner et al. (2011)   |
| n2c2-2018   | ClCE | Clinical Records          | 505         | Henry et al. (2020)    |
| MIMIC       | ClCE | MIMIC-III Database        | 1,610       | Gehrmann et al. (2018) |
| MOOCs       | CoCE | Coursera and XuetangX     | 4375 videos | Pan et al. (2017b)     |
| EMRCM       | CoCE | Chinese Textbooks         | 3,730 pages | Huang et al. (2019b)   |
| USPTO       | PCE  | USPTO Database            | 94,000      | Liu et al. (2020)      |

Table 15: Concept extraction datasets and statistics. KE denotes Keyphrase Extraction. ClCE denotes Clinical Concept Extraction. CoCE denotes Course Concept Extraction. PCE denotes Patent Concept Extraction.

```text
abstract: "[ 'A', 'scalable', 'model', 'of', 'cerebellar', 'adaptive', 'timing', 'and', 'sequencing', ':', ...]"
doc bio tags: "[ 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', ...]"
extractive keyphrases: "[ 'scalable model', 'cerebellar adaptive timing', ... ]"
abstractive keyphrase: "[ 'cerebellar sequencing', ...]"
```text

[Nguyen and Kan](#page-91-28) [\(2007\)](#page-91-28) proposed the NUS dataset with the motivation that keyphrase extraction requires multiple judgments and cannot rely merely on the single set of author-provided keyphrases. They first used Google Search API to retrieve scientific publications, and then recruited student volunteers to participate in manual keyphrase assignments. They finally collect 211 documents, each with two sets of keyphrases: one is given by the original authors of the paper, and the other is given by student volunteers. The data format of NUS is the same as Inspec [\(Hulth, 2003\)](#page-86-24).

[Krapivin et al.](#page-87-23) [\(2009\)](#page-87-23) proposed the Krapivin dataset, consisting of around 2,000 scientific papers as well as their keywords assigned by the original authors. The scientific papers were published by ACM in the period from 2003 to 2005, and were written in English. One of the novelties of this dataset is that the text data in the scientific papers were collected with three distinct categories: title, abstract, and main body. They finally collect 460 test data and 1.84k validation data. The data format is similar to Inspec[\(Hulth, 2003\)](#page-86-24) but has a title and body in addition to the abstract.

SemEval-2010 Task 5 [\(Kim et al., 2010\)](#page-87-24) is on automatic keyphrase extraction from scientific articles. Input for this task is a document from either of the four domains: distributed systems, information search, and retrieval, distributed artificial intelligence, and social and behavioral sciences. Outputs are manually annotated keyphrases for the document. This dataset contains 144 documents as a train set, and 100 documents as a test set. It also selects 40 documents from the train set to compose a trial set. For each set, documents are evenly distributed from the four topics. The annotation follows the first scheme in Section [5.2.](#page-49-0) The data format is the same as Inspec [\(Hulth, 2003\)](#page-86-24).

[Zhang et al.](#page-99-14) [\(2016\)](#page-99-14) constructed a keyphrase extract dataset from Twitter using an automatic text mining method. Their core assumption is that hashtags in a tweet can be used as keyphrases for the tweet. To construct the dataset, they first collected 41 million tweets, and then filtered them which contain non-Latin tokens. URL links, and reply tweets were removed. Thus, the remaining text only contains tweets and a hashtag. They finally kept 110K tweets. To evaluate the quality of the collected tweets, they sampled 1000 tweets and chose three volunteers to score them. As a result, 90.2% tweets are suitable, and 66.1% are perfectly suitable. The annotation follows the first scheme in Section [5.2.](#page-49-0)

```text
tweet: "Hard to believe it but these are REAL state alternatives to taking Obamacare
funds from the gov't (via @Upworthy)"
keyphrase: "obamacare"
```text

[Pan et al.](#page-92-26) [\(2017b\)](#page-92-26) proposed a keyphrase extraction dataset, where data were sourced from online course captions.

Labels are existing phrases in the captions. The courses are computer science and economics courses, selected from two famous MOOC platforms — Coursera and XuetangX. Labels were first filtered from captions using automatic methods and then annotated by two human annotators. A candidate concept was only labeled as a course concept if the two annotators were in agreement. As a result, they collected captions from 4375 videos, and 16720 labeled concepts.

course caption: "You might learn how to write a bubble sort and learn why a bubble sort is not as good as a heapsort." keyphrase: "[ 'bubble sort', 'heapsort' ]"

KP-20K [\(Meng et al., 2017\)](#page-90-27) is a testing dataset, where the input texts are titles and abstracts of computer science research papers collected from ACM Digital Library. The labeled keyphrases are the keyphrases shown in the research papers. The annotation follows the second scheme in Section [5.2,](#page-49-0) since the keyphrases given by authors were not necessarily existing keyphrases in the papers. KP-20K has the same data format as Inspec.

[Huang et al.](#page-85-26) [\(2019b\)](#page-85-26) were motivated to automatically construct an educational concept map. The educational concept map shows concepts that will be learned in courses, as well as the temporal relation between the concepts (e.g., to learn concept A, it is a prerequisite to learn concept B; Concept A and concept B can help with the understanding of each other). To construct the dataset written in Chinese, they first used OCR to obtain the text from textbooks, then manually labeled key concepts for each textbook (as "key concept" or "not key concept") and finally manually annotated the relationships among the labeled key concepts (as "*w<sup>i</sup>*is*wj*'s prerequisite", "*w<sup>i</sup>*and*w<sup>j</sup>*has collaboration relationship", or "no relationship"). As a result, they collected 3730 pages in textbooks, 1092 key concepts, 818 prerequisite relations, and 916 collaboration relations. However, in their GitHub repo, only keyphrases and relations between keyphrases can be found, while the text cannot be found.

```text
keyword: "[ 'average', 'weighted average', ... ]"
relation: "[ 'average : weighted average', ... ]"
```text

There are concept extraction datasets focused on a specific domain, e.g., clinical concepts (TempEval [\(Bethard](#page-80-27) [et al., 2016\)](#page-80-27), i2b2-2010 [\(Uzuner et al., 2011\)](#page-96-24), n2c2-2018 [\(Henry et al., 2020\)](#page-85-25), and MIMIC [\(Gehrmann et al., 2018\)](#page-84-27)), course concepts (MOOCs [\(Pan et al., 2017b\)](#page-92-26), and EMRCM [\(Huang et al., 2019b\)](#page-85-26)), and patent concepts (USPTO [\(Liu](#page-89-25) [et al., 2020\)](#page-89-25)). They also followed keyphrase extraction setups, whereas the targets are to extract concepts of interest.

####*5.4. Knowledge Bases*Besides classical lexicon resources such as WordNet, encyclopedias (including Baidu Encyclopedias and Wikipedia) can also be used to provide external knowledge for concepts [\(Pan et al., 2017b\)](#page-92-26). Methods for extracting concepts based on embedding techniques may encounter issues with low frequency, where some of the concepts have infrequent occurrences. [Pan et al.](#page-92-26) [\(2017b\)](#page-92-26) utilize word embeddings [\(Mikolov et al., 2013\)](#page-90-1), which is trained on encyclopedias, to obtain the semantic embedding for each concept. Inspec database is a scientific and technical database storing scientific papers. The papers of this database have been used to construct a keyphrase extraction dataset.

| Name                            | Knowledge      | #Entities              | Structure                    |
|---------------------------------|----------------|------------------------|------------------------------|
| WordNet                         | Lexical        | 155,327                | Tree                         |
| Baidu Encyclopedia<br>Wikipedia | World<br>World | 6,223,649<br>9,834,664 | Unstructured<br>Unstructured |
| Inspec                          | Science        | 20,000,000             | Unstructured                 |

Table 16: Useful knowledge bases for concept extraction.

####*5.5. Evaluation Metrics*The field of concept extraction also uses Precision, Recall, and F1-score as evaluation metrics. Some keyphrase extraction research considered the task as an information retrieval task. Then, the information retrieval metric, e.g., mean average precision (MAP) was also used for keyphrase extraction as the main measure. It is calculated by taking the average of the average precision scores for each query in a dataset.

$$
MAP = \frac{1}{n} \sum_{i=1}^{n} Avg\_Precision_i,
$$
\n(9)

where n is the total number of queries.*Avg Precision<sup>i</sup>*denotes the averaged precision of query*i*. In the context of keyphrase extraction, the MAP score is determined by comparing the generated list of keyphrases with a predefined gold standard set, and evaluating the average precision of the top *n*keyphrases, where*n*corresponds to the total number of keyphrases in the gold standard set. Each generated keyphrase is considered as a query; The gold standard set serves as the relevant document.

####*5.6. Annotation Tools*Since the annotation schemes of concept extraction are similar to that of NER. The aforementioned NER annotation tools can also be used for annotating concept extraction data. Numerous studies have investigated the utilization of pre-existing keywords in scientific publications [\(Hulth, 2003;](#page-86-24) [Nguyen and Kan, 2007;](#page-91-28) [Krapivin et al., 2009;](#page-87-23) [Kim](#page-87-24) [et al., 2010;](#page-87-24) [Chen et al., 2018c\)](#page-82-24) or hashtags in tweets [\(Zhang et al., 2016\)](#page-99-14), whereby such in-context information can serve as labels without requiring additional annotation efforts, provided that the labels align with the research objectives.

####*5.7. Methods*####*5.7.1. Keyphrase Extraction*

The task of keyphrase extraction is to obtain keyphrases from a document to represent and summarize the document with the keyphrases. There are generally two trends of methods, namely extractive keyphrase extraction and generative keyphrase extraction.

Extractive methods appear first but have a systematic disadvantage in that they can only extract existing phrases in the documents. For example, [Meng et al.](#page-90-27) [\(2017\)](#page-90-27) argued that in addition to present keyphrases, there are also absent keyphrases, which can better summarize a document but do not explicitly present in the document. Generative methods, however, can generate every possible word. Therefore generative methods can alleviate the disadvantage of extractive methods, but might be more difficult because it requires a model to accurately catch the "semantic meaning" of a document to precisely generate a keyphrase.

#### A. Extractive Keyphrase Extraction

[Zhang et al.](#page-99-14) [\(2016\)](#page-99-14) focused on the task of keyphrase generation on Twitter data, and framed this task as a sequence labeling task. They proposed a joint-layer RNN model. For each input token, the joint-layer RNN model outputs two indicators (ˆ*y*<sup>1</sup> and ˆ*y*2), where ˆ*y*<sup>1</sup> has two values *True*and*False*, indicating whether the current word is a keyword. ˆ*y*<sup>2</sup> has 5 values *S ingle*, *Begin*, *Middle*, *End*and*Not* indicating the current word is a single keyword, the beginning of a keyphrase, the middle of a keyphrase, the ending of a keyphrase, or not a part of a keyphrase, respectively. Their experiments show that the joint-layer RNN model outperforms both the vanilla RNN model and the LSTM model. However, when ˆ*y*<sup>1</sup> and ˆ*y*<sup>2</sup> have contradictions, it might be hard to find an optimal strategy to determine which indicator to refer to. In addition, joint-layer RNN can only extract an existing sequence as a keyphrase, but cannot abstractively obtain a (non-existing but better) keyphrase.

[Wang et al.](#page-97-19) [\(2018b\)](#page-97-19) hypothesized that the performance of keyphrase extraction could be improved in the unlabeled or insufficiently labeled target domain by transferring knowledge from a resource-rich domain. They accordingly proposed a topic-based adversarial neural network (called TANN) that can learn transferable knowledge across domains efficiently by performing adversarial training. The experiment section shows that TANN largely outperforms jointlayer RNN [\(Zhang et al., 2016\)](#page-99-14).

[Li et al.](#page-88-22) [\(2018\)](#page-88-22) proposed an unsupervised method for concept mining, which was motivated by the fact that supervised methods might be hard to generalize to unseen domains. They assumed that the quality of an extracted concept can be measured by its occurrence contexts and proposed a pipeline method for concept mining. The method first populates many raw concepts extracted from text, and then evaluates the concepts by comparing the embedding of concepts against the current local context.

[Al-Zaidy et al.](#page-79-21) [\(2019\)](#page-79-21) identified two limitations of previous supervised approaches: 1) They classify the labels of each candidate phrase independently without considering potential dependencies between candidate phrases. 2) They do not incorporate hidden semantics in the input text. Correspondingly, [Al-Zaidy et al.](#page-79-21) [\(2019\)](#page-79-21) addressed keyphrase extraction as a sequence labeling task, and proposed a model named Bi-LSTM-CRF that unite both the advantages of LSTM (capturing semantics) and CRF (Conditional Random Field, capturing dependencies,(Laff[erty et al., 2001\)](#page-87-25)). Their results show that Bi-LSTM-CRF outperforms CopyRNN [\(Meng et al., 2017\)](#page-90-27) by a large margin.

[Fang et al.](#page-83-30) [\(2021\)](#page-83-30) hypothesized that previous extractive methods ignore structured information in the raw textual data (title, topic, and clue words), which might lead to worse performance. They accordingly proposed a model named GACEN that can utilize the title, topic, and clue words as additional supervision to provide guidance. GACEN also utilized CRF to model dependencies in the output. The experiment section shows that GACEN outperforms Joint-layer-RNN [\(Zhang et al., 2016\)](#page-99-14) and CopyRNN [\(Meng et al., 2017\)](#page-90-27).

#### B. Generative Keyphrase Extraction

[Meng et al.](#page-90-27) [\(2017\)](#page-90-27) were motivated that classic keyphrase generation methods can only extract the keyphrases that appear in the source text. Those methods are unable to reveal and leverage the full semantics for keyphrase ranking. Consequently, they proposed an RNN-based generative model incorporating a copying mechanism [\(Gu](#page-84-28) [et al., 2016\)](#page-84-28) (named with CopyRNN), which can generate absent keyphrases. Their method uses an encoder-decoder architecture to catch the semantics of the input text.

Previous methods such as [Meng et al.](#page-90-27) [\(2017\)](#page-90-27) suffered from both coverage (not all keyphrases are extracted) and repetition (similar keyphrases are extracted) problems. For the coverage issue, [Chen et al.](#page-82-24) [\(2018c\)](#page-82-24) integrated a coverage mechanism [\(Tu et al., 2016\)](#page-96-25) into their approach, which enhances the attention distributions of multiple keyphrases in order to cover a wider range of information within the source document and effectively summarize it into keyphrases. For the repetition issue, they constructed a target side review context set that contains contextual information of generated phrases.

[Ye and Wang](#page-98-23) [\(2018\)](#page-98-23) believed that although sequence-to-sequence (seq2seq) models have achieved good performance, model training still relies on large amounts of labeled data. Correspondingly, they leveraged unsupervised learning methods such as TF-IDF and self-learning algorithms to create keyphrase labels for large amounts of unlabeled data. Then, they train their model with a mixture of self-labeled and labeled data together for training. They also used multi-task learning to train their model. Experiments show that their performance outperforms previous works.

[Chen et al.](#page-82-25) [\(2019\)](#page-82-25) argued that prior research on keyphrase generation has treated the document title and main body in the same manner, overlooking the significant role that the title plays in shaping the overall document. They accordingly proposed a Title-Guided Network (TG-Net) where the title is additionally employed as a query-like input to particularly assign attention to the title. The performance of TG-Net outperforms CopyRNN [\(Meng et al., 2017\)](#page-90-27). Their ablation study also shows the importance of additional attention to the title.

#### *5.7.2. Structured Concept Extraction*

Compared with keyphrase extraction-liked concept extraction, structured concept extraction aimed to develop an ontology where concepts are connected with each other by certain relationships. Here, we introduce three knowledge bases resulting from concept extraction: WordNet, ConceptNet, and SenticNet. Out of them, WordNet focuses more on a word-level ontology, ConceptNet focuses more on a concept-level ontology (e.g., also including phrases for concepts), and SenticNet is a concept-level ontology focusing on contributing to sentiment analysis tasks.

WordNet is a manually developed knowledge base, where words and concepts are hierarchically organized. [Snow](#page-95-20) [et al.](#page-95-20) [\(2006\)](#page-95-20) proposed a taxonomy induction method to expand WordNet 2.1 concepts by automatic noun hyponym acquisition, achieving 10,000 novel synsets with 84% precision. Compared to previous methods that relied on individual classifiers to uncover new relationships based on pre-designed or automatically extracted textual patterns, the proposed approach considers input from multiple classifiers to enhance the overall structure of the taxonomy and prioritizes the optimization of the entire taxonomy structure with a probabilistic architecture. [Snow et al.](#page-95-20) [\(2006\)](#page-95-20) also proposed an (m,n)-cousin classification-based model to learn coordinate terms, which allows it to integrate heterogeneous evidence from different classifiers and choose the correct word sense to which to attach a new hypernym. The evaluation of the inferred taxonomies produced by the algorithm was conducted by directly comparing them with the WordNet 2.1 taxonomy. This was achieved by testing each taxonomy using a set of human judgments of noun pairs sampled from newswire text, to determine the hypernym and non-hypernym relationships.

ConceptNet [\(Havasi and Speer, 2007\)](#page-85-24) grew out of Open Mind Common Sense project that aimed at commonsense acquisition. Contributors delivered knowledge by fulfilling blanks within a sentence, For example, given "[ ] can be used to [ ]", the concepts, e.g., "ink" and "print" and the associated relationship "UsedFor" can be obtained. ConceptNet aimed to obtain and structure concepts automatically from natural language. It obtained concepts (the nodes) in the form of noun phrases, verb phrases, adjective phrases, prepositional phrases, or complete verb phrases [\(Havasi](#page-85-24) [and Speer, 2007\)](#page-85-24). The edges of ConcepNet are predicates that represent the relationships between two concept nodes, such as "IsA", "PartOf", UsedFor, and more. [Havasi and Speer](#page-85-24) [\(2007\)](#page-85-24) defined 21 basic relation types. In the latest ConceptNet 5.5 [\(Speer et al., 2017\)](#page-95-21), the relations are increased to 36. Concepts and predicates were obtained via pattern matching. Each collected sentence is compared with pre-defined regular expressions, e.g., "NP is used for VP"(UsedFor), "NP is a kind of NP"(IsA), "NP can VP" (CapableOf). NP (noun phrases) and VP (verb phrases) are concepts, while "UsedFor", "IsA", and "CapableOf" are predicates. In the case of a complex sentence that contains several clauses, the patterns are employed to extract a simpler sentence from it, which can then be subjected to the concept and predicate extraction process. To evaluate ConceptNet, its assertions were compared with those in similar lexical resources to determine their alignment.

SenticNet is a commonsense knowledge base that is used for affective computing. The concepts were extracted by a graph-based semantic parsing method [\(Cambria et al., 2014\)](#page-81-24) and assigned with sentiment polarity labels. Sentences are divided into chunks, e.g., "go walk", first. Then, verb-noun chunks are normalized by stemming, and included in the concept set. The PoS-based bigram algorithm is used to extract object concepts. To capture event concepts, the approach explores matches between object concepts and normalized verb-noun chunks. Finally, single-word concepts, e.g., "house" that have appeared in the clause as multi-word concepts "beautiful house" are deemed redundant and are therefore excluded. In the following version of SenticNet [\(Cambria et al., 2016\)](#page-81-25), the authors proposed an automatic method to discover primitives from the SenticNet concepts, based on hierarchical clustering and dimensionality reduction. Thus, the "animal" concept can be identified as the primitive of "cat", "dog", or "pet". Later, [Cambria](#page-81-26) [et al.](#page-81-26) [\(2022b\)](#page-81-26) proposed a pipeline method for concept extraction, which is used for expanding SenticNet with multiword expressions. They first deconstructed text using sentence chunking, semantic parser, and PoS tagging. Then, verb and noun chunks are extracted and normalized as concepts. The proposed method offers novel contributions in utilizing morphology for syntactic normalization and employing primitives for semantic normalization. The method was evaluated on a sentiment analysis task, achieving explainable and primitive- and concept-level sentiment analysis via algebra operations. The latest version of SenticNet [\(Cambria et al., 2022a\)](#page-81-23) offers the function that sentiment predictions can be effectively conducted on the primitive level, mitigating symbol grounding problems.

An important task of concept extraction is to abstract concept representations from entities. Unlike SenticNet which obtains abstract concepts (primitives) by selecting the most typical entities from a group of extracted similar entities [\(Cambria et al., 2016\)](#page-81-25), [Ge et al.](#page-84-29) [\(2022\)](#page-84-29) proposed a conceptualization method that can directly abstract concepts from input text. The task is realized in the metaphor identification and interpretation domain. The authors aimed to generate concept mappings from metaphorical word pairs to explain the metaphoricity of the word pairs. For example, given "*blind*alley", "street is adult" can be automatically generated. This work is the realization of conceptual metaphor theory (Lakoff [and Johnson, 1980\)](#page-87-26) (see Section [6.1.3\)](#page-61-0) that the generated concept mapping explains the mapping of source (e.g., adult) and target (e.g., street) concepts of a metaphor. The conceptualization (e.g., from "alley" to "street") was achieved by selecting the most appropriate hypernym on the chain from the leaf node of "alley" to the root node "entity" in WordNet. The most appropriate hypernym is defined as the node that can cover the major senses of the leaf, meantime, keeping it as concrete as possible[37](#page-54-0). The conceptualization and concept mapping method was evaluated on a metaphor identification task, yielding better performance and explainability on the task. Subsequently, within MetaPro Online [\(Mao et al., 2023b\)](#page-89-26), the conceptualization algorithm is synergistically integrated with sequential metaphor identification and interpretation techniques, culminating in the attainment of endto-end concept mapping generation from full sentences.

#*5.7.3. Domain-specific Concept Extraction*A. Clinical Concept Extraction

<span id="page-54-0"></span><sup>37</sup>Intuitively, "entity" can cover all possible senses of the "alley" in WordNet, while it is not the ideal concept representation of "alley", because it is too abstract. Thus, the authors aimed at a concrete concept representation that can cover the majority senses of a word.

The task of clinical concept extraction is to extract structural information from unstructured clinical narratives [\(Fu](#page-84-0) [et al., 2020\)](#page-84-0). [Li and Huang](#page-88-23) [\(2016\)](#page-88-23) constructed a dataset for a seminal task called "UTA-DLNLP at SemEval-2016 Task 12" for clinical concept extraction. A system developed for this task should task raw clinical notes or pathology reports as input, and identify event expressions consisting of the "the spans of the expression in the raw text", "contextual modality", "degree", "polarity", and "type". As a baseline for this task, they propose a convolutional neural network to learn hidden feature representations for predictions, taking text and part-of-speech tags as input.

[Liu et al.](#page-89-27) [\(2017b\)](#page-89-27) adopted BiLSTM to recognize the entity in clinical text. They found that BiLSTM outperforms the CRF baselines. [Gehrmann et al.](#page-84-27) [\(2018\)](#page-84-27) compared CNN with classic rule-based methods, bag of words, n-grams, and embedding-based logistic regression. They found that CNN is a valid alternative to rule-based and classic NLP methods, and should be further investigated. [Yang et al.](#page-98-24) [\(2020\)](#page-98-24) comprehensively explored 4 widely used transformerbased architectures, including BERT [\(Devlin et al., 2019\)](#page-83-0), RoBERTa [\(Liu et al., 2019b\)](#page-89-0), ALBERT [\(Lan et al., 2020\)](#page-87-27), and ELECTRA [\(Clark et al., 2020\)](#page-82-26). They compared the 4 models to long short-term memory conditional random fields (LSTM-CRFs) [\(Huang et al., 2015\)](#page-86-25) baselines and found that transformer-based models are effective for clinical concept extraction tasks. [Lange et al.](#page-87-28) [\(2020\)](#page-87-28) proposed a joint model for both clinical concept extraction and deidentification tasks. De-identification is important since in some clinical concept extraction scenarios, the privacy of patients should be protected. They hypothesized that jointly modeling the two tasks can be beneficial, and proposed two end-to-end models. One is a multitask model where the tasks share the input representation across tasks; the other is a stacked model, which used the privacy token predictions to mask the corresponding embeddings in the input layer and only use the masked embeddings for concept extraction. They found that the performance of the concept extraction model can be improved by training and evaluating it on anonymized data, thereby confirming their initial hypothesis.

## B. Course Concept Extraction

In tasks involving the extraction of course concepts, the concepts are typically defined as the knowledge concepts that are taught in the course videos, as well as the related topics that aid in the students' comprehension of the course videos [\(Pan et al., 2017b\)](#page-92-26). Identifying course concepts at a fine level is very important, as students with different backgrounds need different concepts to quickly understand the main content of a course [\(Pan et al., 2017b\)](#page-92-26).

[Pan et al.](#page-92-26) [\(2017b\)](#page-92-26) contributed the first attempt to systematically investigate the problem of course concept extraction in MOOCs. in the past, course concepts were presented by instructors at a general level, with only a few concepts being covered in an entire course video. However, they emphasized the significance of identifying course concepts at a granular level, i.e., automatically identifying all course concepts from each video clip, to facilitate easier comprehension. They identified a challenge for the task that the course concept appears at a low frequency mainly because the different courses have different concepts. They accordingly proposed to utilize word embedding to catch the semantic relations between words and incorporate online encyclopedias to learn the latent representations for candidate course concepts. They also proposed a graph-based propagation algorithm to rank the candidates based on learned representations.

[Wang et al.](#page-97-20) [\(2018a\)](#page-97-20) argued that external knowledge must be involved to solve the concept extraction problem and proposed to utilize both the structured and unstructured data in Wikipedia to provide external knowledge to concept extraction. Their results show that their method outperforms prior works [\(Pan et al., 2017b\)](#page-92-26).

# C. Patent Concept Extraction

[Liu et al.](#page-89-25) [\(2020\)](#page-89-25) developed a framework to extract technical concepts from patents. Patent documents have different structures than other documents. For instance, they have "title", "abstract", and "claim", which exhibit a multilevel of information. Motivated by this, the authors proposed a framework named UMTPE, which can effectively leverage multi-level information to extract concepts.

#*5.8. Downstream Applications*##*5.8.1. Sentiment Computing*SenticNet 7 [\(Cambria et al., 2022a\)](#page-81-23) is a neuro-symbolic sentiment analysis system, based on SenticNet knowledge base. It assumes that concepts that share the same primitive would have similar sentiments. One can use algebra operations to achieve sentiment analysis with the symbolic and structural knowledge base. Incorporating a symbolic knowledge base and a transparent algorithm provides SenticNet's reasoning process with the benefit of interpretability and accuracy.

[Li et al.](#page-88-24) [\(2023\)](#page-88-24) proposed a neuro-symbolic system for conversational emotion recognition. ConceptNet was used as a knowledge base to acquire commonsense knowledge out of context. For example, if a person mentions that he will "chop all onions we have and cry", another conversation participant expresses "disgust" emotion. This is because "onion IsA lacrimator" is a commonsense in ConceptNet. Such a commonsense cannot be obtained from the dictionary meanings of "onion" and the context, while ConceptNet commonsense knowledge provides the evidence and explainability to infer such an emotional status from the context. The authors used an utterance dependency parser and a neural network to learn symbolic knowledge to enhance the explainability and accuracy of their method.

By using the concept mapping method from the work of [Ge et al.](#page-84-29) [\(2022\)](#page-84-29), [Han et al.](#page-84-30) [\(2022\)](#page-84-30) used concept mappings to support depression detection and explanation. The hypothesis is that depression patients may have similar cognition patterns that are reflected in their metaphorical expressions. Thus, they used concept mappings as additional features besides tweets. The concept mappings were generated from tweets that contained metaphors. They also proposed an explainable encoder that can identify significant concept mappings that contribute to depression detection. The concept mappings also improve the accuracy of depression detection, besides explaining the common concept mapping patterns.

###*5.8.2. Information Retrieval*[Xiong et al.](#page-98-22) [\(2017\)](#page-98-22) manually analyzed the potential problems of a literature search website SemanticScholar.org, and found that the issue of "Concept Not Understood" represents one of the most significant challenges. The reason is that previous methods measure similarity based on text, but not on their semantic embeddings. As a result, they proposed an embedding-based similarity matching method, which extracts the concepts in both query and documents and measures the similarity between these concepts to obtain the similarity between a query and a document. [Liu](#page-88-25) [et al.](#page-88-25) [\(2018b\)](#page-88-25) used extracted knowledge concepts as one of the inputs to obtain a unified semantic representation for educational excises. The representation is further used to retrieve similar excises based on similarity with other representations.

####*5.8.3. Dialogue Systems*[Young et al.](#page-98-25) [\(2018\)](#page-98-25) integrated commonsense knowledge from ConceptNet in their dialogue system. They believed that in human dialogues, individuals responding to each other is not dependent on the most recent utterance only, but also on recollecting pertinent information related to the concepts addressed within the dialogue, e.g., commonsense. Thus, in retrieval-based dialogue generation, the model considers both the message content and relevant commonsense knowledge to effectively choose a suitable response.

[Huang et al.](#page-85-27) [\(2020\)](#page-85-27) proposed a new dialogue coherence evaluation matric, termed Graph-enhanced Representations for Automatic Dialogue Evaluation (GRADE). [Liu et al.](#page-88-26) [\(2016\)](#page-88-26) argued that traditional BLEU-liked statisticbased metrics are biased in response coherence. Thus, [Huang et al.](#page-85-27) [\(2020\)](#page-85-27) were motivated to propose a metric that measures the coherence by the topics of utterances. They believe that a cohesive exchange of dialogues is characterized by a seamless transition between topics. Thus, they used a ConceptNet-based method to construct topic-level dialogue graphs. The topic-level dialogue graphs were constructed by connecting the concepts that are extracted from utterances. The edge was weighted and undirected, which was derived from the shortest path between two nodes in the ConceptNet. Such an evaluation metric can better represent the coherence of topics between utterances because it measures the relatedness of concepts from different utterances.

####*5.8.4. Commonsense Explanation Generation*[Fang and Zhang](#page-83-29) [\(2022\)](#page-83-29) grounded concept extraction in the context of commonsense explanation generation. Commonsense explanation generation aims to generate an explanation in natural language to explain the reason why a statement is anti-commonsense. For example, given "he took a nap in the sink", the model aimed to generate "a sink is too small and dirty to take a nap in". The concepts, "small" and "dirty" (bridge concepts), are obtained via a prompt-tuning method. The authors developed a masked word prediction template to query the bridge concepts that are most likely to appear in the "mask" position. Then, they use a generator to generate the explanation with the concatenation of the original statement and the discrete bridge concepts. This method improves the explainability in explaining why a statement is anti-commonsense.

#*5.9. Summary*A concept is an abstract idea that is reflected in the mind. Concept extraction is the foundation of detecting the main idea of a context and developing conceptual knowledge bases. Related theoretical research showed that concepts may be abstracted from multiple specific exemplars [\(Medin and Scha](#page-90-26)ffer, [1978\)](#page-90-26) or prototypes [\(Rosch, 1973\)](#page-93-17). There are limited primitives that construct human cognition and reasoning, which are the foundation of complex concepts [\(Wierzbicka, 1972\)](#page-97-18). According to [Gardenfors](#page-84-26) [\(2004\)](#page-84-26), conceptual space is multi-dimensional. The similarity between concepts can be measured by the similarity between concept properties. These theoretical research works frame the tasks of concept extraction from the perspectives of lexicon development and conceptual structure construction. On the other hand, current computational concept extraction methods divide this task into three categories, namely keyphrase extraction, structured concept extraction, and domain-specific concept extraction. We found that the existing computational approaches inadequately address the tasks that have been put forth by the academic community's theoretical research. Although current concept extraction methods are limited, this task has greatly improved the explainability of downstream tasks such as sentiment computing, information extraction, and counter-commonsense recognition.

##*5.9.1. Technical Trends*Within the domain of keyphrase extraction, generative keyphrase extraction takes advantage of generating "absent keyphrases", compared to extractive keyphrase extraction. Both tasks followed the general development of the NLP fields. They likely considered the task as a sequence labeling task (extractive keyphrase extraction) or a generation task (generative keyphrase extraction), and used typical NLP frameworks, e.g., sequence labeling and sequence to sequence frameworks. However, it is unclear if these general NLP frameworks have really learned how to summarize the main idea of context or just have learned by label distributions. There were no task-specific mechanisms proposed to explicitly learn the keyphrase extraction task on the concept level, with an explainable decision-making process. On the other hand, keyphrase extraction-based concept extraction is helpful for obtaining concept lexicons. However, compared to structured concept extraction, keyphrase extraction cannot learn the relationships between concepts. The theoretical research of Conceptual Spaces from [Gardenfors](#page-84-26) [\(2004\)](#page-84-26) suggested that the similarity between concepts can be measured by their properties. It suggests that keyphrase extraction-based concept extraction should consider extracting properties together with keyphrases. Thus, the later works can use keyphrases and the associated properties to structure concepts by similarities.

In contrast, structured concept extraction research likely utilized statistical learning and syntactic parsing methods. This is because the aim of structured concept extraction is to develop a large knowledge base or detect structured relationships between concepts. Labeled data are insufficient in these areas. Thus, unsupervised methods are preferred. However, the concept knowledge base development is task-specific. As a result, the concepts in different knowledge bases share different relationships. For example, ConceptNet aimed to parse concepts sharing 36 commonsense relationships; Stanford WordNet [\(Snow et al., 2006\)](#page-95-20) was expended in synonyms and hypernyms relationships; SenticNet grouped concepts and extract primitives for sentiment computing; [Ge et al.](#page-84-29) [\(2022\)](#page-84-29) abstracted concepts for concept mappings. Then, the evaluation of different concept extraction methods is different. Most of the evaluation was implemented on different downstream tasks. It shows

Domain-specific concept extraction is very similar to NER tasks. They used graph, machine learning methods, and external knowledge, e.g., encyclopedias and Wikipedia to discover concepts in a domain, e.g., clinical, course, or patent concepts. Similar to keyphrase-based concept extraction, these domain-specific concept extraction methods did not try to structure concepts after extraction. This is important because it distinguishes concept extraction from current NER tasks in specific domains.

###*5.9.2. Application Trends*Concept extraction methods and their product, e.g., knowledge bases have been widely used in downstream tasks, e.g., sentiment computing, information retrieval, dialogue systems, and commonsense explanation generation. Compared to other low-level semantic processing techniques, the roles of concept extraction are more diverse in downstream applications. For all the surveyed downstream tasks, the products of concept extraction can be used as additional features to improve model performance on downstream tasks. On the other hand, concept extraction techniques can be used as a parser to obtain knowledge from unstructured text. The structured concepts with certain relationships

| word2vec<br>word2vec<br>word2vec<br>word2vec<br>word2vec<br>word2vec<br>word2vec<br>word2vec<br>word2vec<br>Knwl. eng.<br>Pipeline<br>DL<br>DL<br>DL<br>DL<br>DL<br>DL<br>DL<br>DL<br>SL<br>Havasi and Speer (2007)<br>Al-Zaidy et al. (2019)<br>Ye and Wang (2018)<br>Wang et al. (2018b)<br>Zhang et al. (2016)<br>Chen et al. (2018c)<br>Meng et al. (2017)<br>Snow et al. (2006)<br>Chen et al. (2019)<br>Fang et al. (2021)<br>Li et al. (2018)<br>Generative KE<br>Extractive KE |           |                                                                                 |                                                                                                    |                                                          |                                                | Metric                       |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|---------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|----------------------------------------------------------|------------------------------------------------|------------------------------|
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |           |                                                                                 | BiLSTM, adversarial loss<br>Similarity matching<br>Joint-layer RNN<br>Attention; CRF<br>BiLSTM-CRF | MLDBMD<br>KP-20K<br>KP-20K<br>Twitter<br>CCF             | 86.40%<br>29.60%<br>97.00%<br>35.63%<br>45.69% | MAP<br>F1<br>F1<br>F1<br>F1  |
|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |           |                                                                                 | Seq2seq, semi-supervised<br>Seq2seq, additional<br>Title input<br>Seq2seq<br>RNN                   | Krapivin<br>KP-20K<br>KP-20K<br>KP-20K                   | 32.80%<br>31.80%<br>30.80%<br>37.20%           | F1@5<br>F1@5<br>F1@5<br>F1@5 |
| sem. pars.,<br>PoS tag.<br>SL<br>Ge et al. (2022)<br>SenticNet<br>Structured CE                                                                                                                                                                                                                                                                                                                                                                                                        | chunking, | feature vectors, WN<br>syntactic patterns<br>textual patterns<br>statistics, WN | Syntactic parsing<br>Pattern matching<br>Elbow algorithm<br>Probabilistic                          | -<br>-<br>-<br>-                                         | -<br>-<br>-<br>-                               | -<br>-<br>-<br>-             |
| word2vec<br>word2vec<br>word2vec<br>DL<br>DL<br>DL<br>DL<br>DL<br>Gehrmann et al. (2018)<br>Li and Huang (2016)<br>Lange et al. (2020)<br>Yang et al. (2020)<br>Liu et al. (2017b)<br>Clinical CE                                                                                                                                                                                                                                                                                      |           | word2vec, character2vec<br>token mention, pos<br>tag, word shape                | Multitask-biLSTM<br>Transformer<br>BiLSTM<br>CNN<br>CNN                                            | n2c2-2018<br>i2b2-2010<br>i2b2-2010<br>TempEval<br>MIMIC | 78.80%<br>85.78%<br>76.00%<br>88.36%<br>88.90% | F1<br>F1<br>F1<br>F1<br>F1   |
| Graph<br>Graph<br>Wang et al. (2018a)<br>Pan et al. (2017b)<br>Course CE                                                                                                                                                                                                                                                                                                                                                                                                               |           | word2vec; Encyclopedia<br>word2vec, Wikipedia                                   | Graph-based propagation<br>Graph-based propagation                                                 | MOOCs<br>MOOCs                                           | 41.60%<br>47.50%                               | MAP<br>MAP                   |
| ML<br>Liu et al. (2020)<br>Patent CE                                                                                                                                                                                                                                                                                                                                                                                                                                                   |           | word2vec, DBpedia<br>self pretrained                                            | Clustering                                                                                         | USPTO                                                    | 43.37%                                         | F1                           |

Table 17: A summary of representative concept extraction techniques. KE denotes keyphrase extraction. CE denotes concept extraction. SL denotes statistical learning. Knwl. eng. denotes knowledge engineering. SenticNet denotes the works of [Cambria](#page-81-24) et al. [\(2014,](#page-81-24) [2016,](#page-81-25) [2022a,](#page-81-23)[b\)](#page-81-26). We do not show the evaluation results for structured concept extraction methods, because they all used very task-specific evaluation methods and datasets, where the results are not comparable.



| Reference              | Downstream Task                    | Feature | Parser | Explain. |
|------------------------|------------------------------------|---------|--------|----------|
| Cambria et al. (2022a) | Sentiment Computing                | ✓       | ✓      | ✓        |
| Li et al. (2023)       | Sentiment Computing                | ✓       |        | ✓        |
| Han et al. (2022)      | Sentiment Computing                | ✓       | ✓      | ✓        |
| Xiong et al. (2017)    | Information Retrieval              | ✓       |        | ✓        |
| Liu et al. (2018b)     | Information Retrieval              | ✓       |        | ✓        |
| Young et al. (2018)    | Dialogue Systems                   | ✓       |        |          |
| Huang et al. (2020)    | Dialogue Systems                   | ✓       | ✓      |          |
| Fang and Zhang (2022)  | Commonsense Explanation Generation | ✓       |        | ✓        |

Table 18: A summary of the representative applications of concept extraction in downstream tasks.

can also improve the explainability of a downstream task model, e.g., explaining anti-commonsense [\(Fang and Zhang,](#page-83-29) [2022\)](#page-83-29) and concept mapping patterns of depressive patients [\(Han et al., 2022\)](#page-84-30).

In the era of PLM and LLM, it seems many complex tasks can be achieved from end-to-end with deep neural networks. However, black box-liked neural networks prevent humans from understanding their decision-making mechanisms. This may be contrary to the original intention of human beings to build AI, e.g., giving machines the ability to think like humans. Neuro-symbolic AI which combines the knowledge of symbolic representations with neural networks, seems to be able to compensate for the lack of model interpretability of pure neural networks because symbolic representations in natural language, e.g., words and concepts are human-readable. We can explain a prediction by viewing what symbolic knowledge is activated. Meantime, symbolic knowledge can represent commonsense knowledge, which is difficult for neural networks to learn from corpora. As the fundamental technique of knowledge base development, concept extraction has a huge potential in downstream applications.

####*5.9.3. Future Works*Open domain concept extraction. Prior research on concept extraction has primarily concentrated on extracting concepts within a particular domain, while other concept extraction efforts aimed at developing knowledge bases have focused on extracting concepts with predefined relations. These approaches severely limit the application scope of knowledge bases. It would be more practical to extract concepts and relations in an open domain, where both the concepts and relations are not focused on specific types. This requires an ontology study to guide the concept extraction, e.g., what can be defined as concepts and relations. It is a more challenging task than the joint NER and relation extraction task, because relationships and concepts are self-aware within a learning model, rather than pre-defined by humans.

Multi-modal concept extraction. "Concept" is also very relevant to human visual recognition. It is argued that for humans, the ability of visual classification is obtained from concept learning, which learned the generalized concept description from sample observations such that a given observation can be identified as a learned concept [\(Seel, 2011;](#page-94-25) [Xiong et al., 2021\)](#page-98-26). On the other hand, the abstractness of concepts is strongly related to imagery [\(Paivio, 1965\)](#page-91-29), because abstract concepts are those that are not applicable to tangible, perceptible objects that can be observed through touch, sight, hearing, or other sensory experiences [\(Lohr, 2022\)](#page-89-28). Thus, learning the relationships between concepts ¨ and imagery can help concept extraction research hierarchically organized concepts, e.g., primitives, concepts, and entities. However, till now, there is a lack of research papers working on multi-modal concept extraction to our best knowledge. It could be also interesting to investigate possible synergies in concept extraction between different modalities.

Concept extraction evaluation. Current concept extraction methods were evaluated on an application task, e.g., sentiment analysis to SenticNet or testing specific relationships, e.g., hypernym and hyponym relationship to ConceptNet and WordNet extension. The issue with such an evaluation method is that it can only reflect the effectiveness of a developed knowledge base or concept extraction method on a specific domain. Since different knowledge bases have different application targets, it's hard to evaluate and compare them with unified criteria. It would be valuable to propose a framework for knowledge base evaluation that is independent of specific tasks. It would be helpful to understand the quality of included concepts, relationships, and their representations.

More concept extraction applications. Despite the attention some scholars have given to neuro-symbolic AI, the body of related works remains relatively scant in comparison to end-to-end neural network models. One possible explanation for this disparity is that, at present, there is greater emphasis placed on the accuracy of the model rather than the transparency of its decision-making process. Thus, there is a need for more concept extraction applications, which can aid in enhancing the explainability of neural network-based models. It offers insights for the development of knowledge bases, prompting researchers to reassess how they extract and organize concepts in order to more effectively support subsequent applications.

# <span id="page-60-0"></span>6. Subjectivity Detection

Conventionally, subjectivity detection is defined as a task to determine whether a text is subjective or not, where a subjective text expresses personal feelings, evaluations, and speculations [\(Wiebe, 1994\)](#page-97-21), whereas an objective one merely delivers factual information. Generally, subjectivity can manifest in different forms, e.g., opinions, allegations, desires, beliefs, and suspicions [\(Liu et al., 2010\)](#page-88-27) to express private states. It is not an easy task to identify the use of subjective language, as a subjective sentence does not always contain an opinion [\(Liu et al., 2010\)](#page-88-27). Therefore, it is important for the subjectivity detection task to find reliable clues. Aside from opinion-bearing words, syntax also provides essential clues in reporting private states, because grammaticalization involves the recruitment of items to mark the speaker's point of view [\(Traugott, 2010\)](#page-96-26).

Early works often equated the presence of subjectivity to the presence of subjectivity-bearing words in a sentence (Riloff [and Wiebe, 2003;](#page-93-24) [Kim and Hovy, 2005;](#page-87-29) [He et al., 2022c;](#page-85-28) [Bao et al., 2021\)](#page-79-22). However, subjectivity is context- and domain-dependent. Some words are only subjective in certain contexts or domains. Therefore, many researchers incorporated syntactic dependencies [\(Wilson et al., 2004;](#page-97-22) [Xuan et al., 2012\)](#page-98-27), interactions between neighboring sentences [\(Wiebe, 1994;](#page-97-21) [Pang and Lee, 2004\)](#page-92-27) or in discourse [\(Biyani et al., 2014\)](#page-80-28) to extract different levels of contextual information. An alternative to this subjective-lexicon-based approach is the word-frequency-based approach [\(Rustamov et al., 2013;](#page-93-25) [Kamil et al., 2018\)](#page-86-26), which is completely domain-independent by learning from document-level information. However, this approach has difficulties capturing syntactic dependencies. By now, subjectivity detection research has been divided into several distinct tasks, each with its unique objectives. One such task is individual subjectivity detection, which focuses on detecting subjectivity at the sentence level. In contrast, context-dependent subjectivity detection aims to incorporate discourse information and a broader context in detecting subjectivity. Cross-lingual subjectivity detection, on the other hand, strives to identify subjectivity in various languages. Moreover, multi-modal subjectivity detection is concerned with identifying subjective expressions in different modalities such as audio and video. Finally, the bias detection task is centered on identifying biased statements in ostensibly impartial articles.

Subjectivity detection is commonly considered as a sub-task of sentiment analysis since it serves as a filtering step for polarity detection [Liu et al.](#page-88-27) [\(2010\)](#page-88-27). It can also be helpful for downstream tasks that require a distinction between opinionated and non-opinionated sentences, such as opinion and information retrieval [\(Zhang et al., 2007;](#page-99-15) [Wiebe and](#page-97-23) [Rilo](#page-97-23)ff, [2011\)](#page-97-23), analyses in financial and political domains [\(Wang et al., 2021;](#page-97-24) [Tang et al., 2014;](#page-96-27) [Al Hamoud et al.,](#page-79-23) [2022\)](#page-79-23), question answering systems [\(Yu and Hatzivassiloglou, 2003;](#page-98-28) [Li et al., 2008a\)](#page-88-28), etc.

##*6.1. Theoretical Research*Given the broadness of subjective expressions, e.g., expressing personal feelings, evaluations, and speculations, the related theoretical research in this domain is also rich.

###*6.1.1. Subjective Elements*Early linguistic works studied subjective language extensively in third-person narrative text. [Banfield](#page-79-24) [\(2014\)](#page-79-24) defined the*SELF*of a sentence as the speaker in conversation or the narrating character in third-person fictional text. She identified a variety of morphological, lexical, and syntactic elements, termed subjective elements, that always express the private states, i.e., emotions and opinions, of the sentence's*SELF*. However, many linguistic elements are subjective only in certain conditions. Therefore, following [Banfield](#page-79-24) [\(2014\)](#page-79-24), [Wiebe](#page-97-25) [\(1990\)](#page-97-25) further defined a category termed potential subjective elements, which expanded the subjective elements with some linguistic elements that can, but not always, report the private state of a character. [Wiebe et al.](#page-97-26) [\(2000\)](#page-97-26) applied these findings to identify the subjective language in the non-fictional text, suggesting that potential subjective elements are also valid subjectivity clues for texts other than third-person narrative fiction.

## *6.1.2. Speech Acts*

Speech acts have a strong connection with subjective expressions because speech acts perform actions, such as making a promise, giving an order, or expressing a belief. [Austin](#page-79-25) [\(1975\)](#page-79-25) argued that language is not just a tool for describing the world but also a means of accomplishing things in the world. Through speech acts, individuals can influence the world around them and the actions of others. In this sense, many seemingly objective expressions with speech acts can become subjective. For example, if someone says,

### (18) I promise to do it.

The utterance is not just conveying information but also performing the act of making a promise. A more subjective case is

#### (19) I believe that it will rain tomorrow.

When individuals express belief in such a manner, they are essentially asserting their mental disposition or perspective towards a specific statement. This entails making a claim about their inner state or outlook toward a proposition. [Austin](#page-79-25) [\(1975\)](#page-79-25) argues that a considerable number of utterances possess illocutionary force, which signifies that their purpose is not merely to communicate information but also to accomplish something beyond that. Thus, subjective expressions may be more than we think in our everyday language.

#### <span id="page-61-0"></span>*6.1.3. Conceptual Metaphor*Lakoff [and Johnson](#page-87-26) [\(1980\)](#page-87-26) argued that metaphors are not solely a linguistic phenomenon, but also mirror human cognition via concept mappings. When an individual uses a metaphorical expression, they employ a source concept to represent a target concept in a particular context, thereby conveying their cognitive attitude toward the target concept. This process, known as concept mappings, facilitates such representation. In instances such as the statement

<span id="page-61-1"></span>(20) Our love is a journey.

The individual utilizes the concept of a "journey" as the source to represent the target concept of "love", expressing their subjective feeling that their love is characterized by both ups (joy) and downs (sadness). "Our love is a journey" cannot be an objective statement, because the two concepts are from different domains, i.e., literally, love is not a journal. Thus, there is a semantic contrast between the literal and contextual meanings of a metaphor [\(Mao et al.,](#page-89-29) [2019\)](#page-89-29). The semantic disparities inherent in metaphors suggest that relying on the literal meanings of a statement alone is insufficient in substantiating its subjectivity. Even though the statement of Example [\(20\)](#page-61-1) does not use any obvious opinionated words, e.g., "happy" and "sad", it also expresses a personal feeling. Thus, the pragmatics of statements must also be taken into account in subjective detection.

####*6.2. Annotation Schemes*For general subjectivity detection, it is sufficient for a dataset to annotate a sentence, snippet, or document as subjective (positive/negative) or objective (neutral). Nevertheless, [Wiebe et al.](#page-97-27) [\(2005\)](#page-97-27) proposed the MPQA scheme, which annotates text at the word and phrase levels. The MPQA scheme is suitable for fine-grained subjectivity detection that aims to identify the source, target, and properties of each expression of the private state.

[Wilson](#page-97-28) [\(2008\)](#page-97-28) proposed the AMIDA Scheme for annotating subjectivity in speech. This scheme marks word spans that are in the following three main categories: subjective utterances, objective polar utterances, and subjective questions. A subjective utterance is a word span that expresses a private state. An objective polar utterance delivers positive or negative factual information without expressing a private state. A subjective question is a question in which the speaker is eliciting the private state of someone else. Each category is divided into finer classes that indicate the polarity and certainty of an utterance.

#*6.3. Datasets*A summary of all the introduced datasets can be found in Table [19.](#page-62-0) Generally, subjectivity detection data are organized in the following forms. A text is typically labeled as either subjective or objective, with the former category often further classified as positive, negative, or neutral. The following examples are from SemEval-2013 Task 2B: Sentiment Analysis on Twitter [\(Nakov et al., 2013\)](#page-91-30).

<span id="page-62-0"></span>

| Dataset       | Task      | Source                                 | # Samples | Reference                      |
|---------------|-----------|----------------------------------------|-----------|--------------------------------|
| MPQA          | ISD, CDSD | English news articles                  | 9,700     | Wiebe et al. (2005)            |
| MPQA Gold     | ISD, CLSD | Spanish sentences                      | 504       | Mihalcea et al. (2007)         |
| Multi-MPQA    | ISD, CLSD | Machine-translated MPQA                | 9,700     | Banea et al. (2010)            |
| Movie         | ISD, CDSD | Rotten Tomatoes, IMDB                  | 10,000    | Pang and Lee (2004)            |
| WebDoc        | CDSD      | English web documents                  | 1,076     | Chesley et al. (2006)          |
| TREC          | CDSD      | WSJ                                    | 2,000     | Yu and Hatzivassiloglou (2003) |
| Debate        | CDSD      | Political and ideological<br>dataset   | 53,453    | Al Hamoud et al. (2022)        |
| Twitter1      | ISD       | English tweets                         | 200,000   | Barbosa and Feng (2010)        |
| Twitter2      | ISD       | English tweets                         | 498       | Serrano-Guerrero et al. (2015) |
| Forum         | CDSD      | online forums                          | 700       | Biyani et al. (2014)           |
| SemEval 2013  | ISD       | English tweets                         | 12,002    | Nakov et al. (2013)            |
| NET           | ISD       | English nuclear energy<br>tweets       | 2,308     | Khatua et al. (2020)           |
| MLT           | ISD, CLSD | Multi-lingual nuclear<br>energy tweets | 7,700     | Satapathy et al. (2017)        |
| TASS          | ISD, CLSD | Spanish tweets                         | 10,000    | Villena et al. (2015)          |
| Email         | CDSD      | BC3 corpus                             | 1,800     | Murray and Carenini (2011)     |
| AMIDA         | MMSD      | AMI Meeting Corpus                     | 13        | Wilson (2008)                  |
| ICT-MMMO      | MMSD      | Youtube review videos                  | 370       | Wollmer et al. (2013)<br>¨     |
| MOUD          | MMSD      | Youtube review videos                  | 498       | Morency et al. (2011)          |
| Conservapedia | BD        | Conservapedia statements               | 1,000     | Hube and Fetahu (2018)         |
| WNC           | BD        | Wikipedia sentence pairs               | 180,000   | Pryzant et al. (2020)          |
|               |           |                                        |           |                                |

Table 19: Subjectivity detection datasets and statistics. ISD denotes individual subjectivity detection. CDSD denotes context-dependent subjectivity detection. CLSD denotes cross-lingual subjectivity detection. MMSD denotes multi-modal subjectivity detection. BD denotes bias detection.

```text
id1: "264215390773727232"
id2: "276151090"
text: "Alex Poythress had 11 points and 7 rebounds in his debut with Kentucky during an exhibition game on Thursday. He
played 28 minutes."
label: objective
id1: "263732569508552704"
id2: "369152026"
text: "Kick-off your weekend with service! EV!'s Get on the Bus trip to the Boys & Girls Club is Friday from 3-6! Hope
to see you there :)"
label: "positive"
id1: "213342054351257601"
id2: "189656827"
text: "Desperation Day (February 13th) the most well known day in all mens life."
label: negative
id1: "263803288074477568"
id2: "396953010"
text: "It seem like Austin Rivers is tryin to had to get a bucket. I feel em tho my 1st game in the league I was trying
hard too"
label: neutral
```text

For fine-grained subjective annotation, the labels are annotated at the span level. The following examples are from SemEval-2013 Task 2A: Sentiment Analysis on Twitter [\(Nakov et al., 2013\)](#page-91-30).

```text
id1: "255732290246815744"
id2: "315400337"
text: "Billy Cundiff may be leaving Washington. Hopefully he won't miss the door on the way out."
start id: "7"
end id: "7"
```text

```text
label: "positive"
id1: "255732290246815744"
id2: "315400337"
text: "Billy Cundiff may be leaving Washington. Hopefully he won't miss the door on the way out."
start id: "9"
end id: "10"
label: "positive"
```text

MultiParty Question Answering (MPQA) [\(Wiebe et al., 2005\)](#page-97-27) is derived from 535 English news articles from a wide variety of news sources, manually annotated for subjectivity. The corpus contains 9,700 sentences, 55% of which are labeled as subjective and 45% as objective. The MPQA Gold [\(Mihalcea et al., 2007\)](#page-90-28) contains 504 Spanish sentences manually annotated for subjectivity, where 273 sentences are subjective and 231 are objective. The Multi-MPQA [\(Banea et al., 2010\)](#page-79-26) contains parallel corpora to the MPQA dataset in five languages other than English, namely, Arabic, French, German, Romanian, and Spanish.

The Movie Review dataset (Movie) [\(Pang and Lee, 2004\)](#page-92-27) contains 5,000 movie review snippets collected from Rotten Tomatoes[38](#page-63-0), considered as subjective. Furthermore, 5,000 sentences are collected from plot summaries from the Internet Movie Database (IMDB)[39](#page-63-1), considered as objective. All reviews and plot summaries are sourced from movies released post-2001, preventing overlap with the polarity benchmark dataset [\(Pang and Lee, 2004\)](#page-92-27). A data sample, either sentence or snippet, is at least 10 words long. The Debate dataset [\(Al Hamoud et al., 2022\)](#page-79-23) is derived from the political and ideological dataset [\(Somasundaran and Wiebe, 2010\)](#page-95-22), containing 53,453 sentences from political and ideological posts and comments. The instances are automatically labeled for subjectivity by using lexicon-based and syntactic-pattern-based classifiers (Riloff [and Wiebe, 2003\)](#page-93-24).

Numerous microtext corpora exist that can serve as benchmark datasets for subjectivity detection. [Barbosa and](#page-79-27) [Feng](#page-79-27) [\(2010\)](#page-79-27) presented a dataset containing 200,000 English tweets, where roughly 100,000 are subjective and the rest are objective. [Serrano-Guerrero et al.](#page-94-26) [\(2015\)](#page-94-26) manually annotated 498 English tweets as positive, negative, or neural. SemEval 2013 [\(Nakov et al., 2013\)](#page-91-30) is a collection of 12,002 English tweets labeled as objective, positive, negative, or neutral. Nuclear Energy Tweets (NET) [\(Khatua et al., 2020\)](#page-87-30) contains 2,308 English tweets about nuclear energy, manually annotated for subjectivity. The Multilingual Tweets (MLT) dataset [\(Satapathy et al., 2017\)](#page-94-27) is a collection of 12,719 tweets about nuclear energy in English, French, Spanish German, Malay, and Indonesian, 7,700 out of which are manually labeled for subjectivity. The Taller de Analisis de Sentimientos en la SEPLN (TASS) corpus [Villena](#page-96-28) [et al.](#page-96-28) [\(2015\)](#page-96-28) contains 10,000 tweets in Spanish, collected from posts by 150 public figures in fields of sports, politics, and communication during the period from 2011 to 2012. Each tweet is labeled as positive, neutral, negative, or without opinion.

The Web Document dataset [\(Chesley et al., 2006\)](#page-82-27) contains 1,076 English web documents, sourced from traditional news websites and blog posts on diverse topics. Each document is manually annotated as objective, positive, or negative. The Text REtrieval Conference (TREC) dataset [\(Yu and Hatzivassiloglou, 2003\)](#page-98-28) is a collection of 8,000 WSJ articles evenly distributed in the categories of editorial, letter to editor, business, and news. The articles and sentences from the former two categories are mapped as opinions (subjective), while the ones from the latter two are facts (objective). The Forum dataset [\(Biyani et al., 2014\)](#page-80-28) contains 700 threads from online forums Trip Advisor–New York[40](#page-63-2) and Ubuntu Forums[41](#page-63-3), manually annotated for subjectivity. Email [\(Murray and Carenini, 2011\)](#page-90-29) contains 1,800 sentences derived from BC3 corpus [\(Ulrich et al., 2008\)](#page-96-29), 172 out of which are labeled as subjective.

For multi-modal subjectivity detection, the AMIDA dataset [\(Wilson, 2008\)](#page-97-28) consists of 19,071 dialogue act segments from 20 conversations from the AMI Meeting Corpus [\(McCowan et al., 2005\)](#page-90-31), manually annotated with the AMIDA scheme. 42% of the dialogue act segments are tagged with at least one subjective annotation. The Institute for Creative Technologies Multi-Modal Movie Opinion (ICT-MMMO) dataset [\(Wollmer et al., 2013\)](#page-98-29) contains 370 ¨ Youtube review videos labeled as strongly negative, weakly negative, neutral, weakly positive, and strongly positive.

<span id="page-63-0"></span><sup>38</sup><https://www.rottentomatoes.com>

<span id="page-63-1"></span><sup>39</sup><https://www.imdb.com>

<span id="page-63-2"></span><sup>40</sup>[http://www.tripadvisor.com/ShowForum-g60763-i5-New\\_York\\_City\\_New\\_York.html](http://www.tripadvisor.com/ShowForum-g60763-i5- New_York_City_New_York.html)

<span id="page-63-3"></span><sup>41</sup><http://ubuntuforums.org>

Multimodal Opinion Utterances Dataset (MOUD) [\(Morency et al., 2011\)](#page-90-30) is a collection of 80 Youtube review videos annotated as positive, negative, and neutral.

For the bias detection task, which aims to identify subjective bias in Wikipedia, the following datasets are widely used. Conservapedia [\(Hube and Fetahu, 2018\)](#page-86-27) is a collection of 1,000 single-sentence statements from Conservapedia[42](#page-64-0), manually annotated as biased or unbiased. Wiki Neutrality Corpus (WNC) [Pryzant et al.](#page-92-28) [\(2020\)](#page-92-28) contains 180,000 aligned Wikipedia sentence pairs. Each pair consists of a sentence before and after bias neutralization by English Wikipedia editors.

##*6.4. Knowledge Bases*Lexicons of subjectivity clues and patterns are commonly used for subjectivity detection, as summarized in Table [20.](#page-64-1) The General Inquirer [\(Stone et al., 1966\)](#page-95-23) is a lexicon consisting of 10,000 words sorted into 180 categories for content analysis. The Subjectivity Clues lexicon (Riloff [and Wiebe, 2003\)](#page-93-24) is a list of words that are subjective in most cases (strongly subjective) and words that may have subjective use in certain contexts (weakly subjective). MPQA Subjectivity Lexicon [\(Wilson et al., 2005\)](#page-97-29) expanded the Subjectivity Clues using additional dictionaries and lexicons, containing over 8,000 subjectivity clues.

<span id="page-64-1"></span>Knowledge bases that provide sentiment information are also widely used for subjectivity detection. WordNet-Affect [\(Strapparava and Valitutti, 2004\)](#page-95-24) is a set of synsets derived from WordNet that effectively represents affective concepts. SentiWordNet, as introduced in the previous section, is based on WordNet. Each word in SentiWordNet is given three scores indicating its positivity, negativity, and objectivity. SenticNet [\(Cambria et al., 2022a\)](#page-81-23) is a conceptlevel knowledge base that includes semantic, sentic, and polarity associations.

| Name                      | Knowledge                                  | # Entities | Structure |
|---------------------------|--------------------------------------------|------------|-----------|
| The General Inquirer      | Sentiment labels                           | 4,000      | List      |
| MPQA Subjectivity Lexicon | Subjectivity clues                         | 8,000      | List      |
| SentiWordNet              | Structured lexical<br>knowledge by concept | 100,000    | Graph     |
| WordNet-Affect            | lexical knowledge                          | 4,787      | Graph     |
| SenticNet                 | Sentiment scores                           | 200,000    | Graph     |

Table 20: Useful knowledge bases for subjectivity detection.

###*6.5. Evaluation Metrics*The performance of subjectivity detection is commonly evaluated via accuracy and F-measure.

####*6.6. Annotation Tools*The aforementioned NER annotation tools (see Section [4.6\)](#page-38-5) can be used for subjectivity detection because these tools can annotate labels for spans (fine-grained subjectivity detection) and sentences (coarse-grained subjectivity detection).

####*6.7. Methods*####*6.7.1. Individual Subjectivity Detection*In individual subjectivity detection, the subjectivity of a sentence is evaluated in isolation and irrespective of any contextual factors. The primary methods used for addressing this task include lexicon-based, word frequency, and deep learning approaches.

#### A. Lexicon-based

Drawing on the premise that sentences that contain commonly-subjective expressions are more likely to be subjective, lexicon-based methods utilize a manually-constructed lexicon of subjective words, clues, or patterns to determine the subjectivity of a given sentence.

Riloff [and Wiebe](#page-93-24) [\(2003\)](#page-93-24) introduced an unsupervised rule-based classifier that leverages the identification of subjective clues and patterns to detect subjective sentences, while also employing bootstrapping to recognize objective

<span id="page-64-0"></span><sup>42</sup><http://www.conservapedia.com>

sentences based on the absence of such indicators. The clues were manually collected and annotated. The patterns were generated by the AutoSlog-TS algorithm [\(Rilo](#page-93-26)ff, [1996\)](#page-93-26), based on pre-defined syntactic templates. [Wiebe and](#page-97-30) [Rilo](#page-97-30)ff [\(2005\)](#page-97-30) further improved this bootstrapping system by using the labeled sentence produced by the rule-based method as initial training data for a Na¨ıve Bayes classifier. The major weakness of these methods is the unreliable assumption that the absence of subjective clues and patterns indicates objectivity, resulting in false-positive errors.

[Kim and Hovy](#page-87-29) [\(2005\)](#page-87-29) first compiled lists of words that convey opinions and those that do not, which were manually annotated with corresponding classes and levels of strength. They expanded the lists with a common English word list by measuring the WordNet distance between a common word and the compiled seed lists. They further identified additional opinion words and non-opinion words from editorial and non-editorial WSJ documents by computing their relative frequencies. By detecting the subjectivity of a given sentence based on the presence of a single strong valence word, their method achieved 65% accuracy on MPQA.

[Benamara et al.](#page-80-29) [\(2011\)](#page-80-29) argued that sentence-level subjectivity detection cannot fully leverage context, because a sentence may contain several opinion clauses, and opinion expressions may be discursively related. As such, they proposed a segment-level annotation based on the Segmented Discourse Representation Theory [\(Asher and Lascarides,](#page-79-28) [2003\)](#page-79-28), where segments are labeled as explicitly subjective, implicitly subjective, subjective non-evaluative, and objective. This fine-grained annotation can better enhance polarity detection, as segments in the latter two categories do not covey positive, negative, or opinion. However, the limitation of this method is that the four label classes are unbalanced in the corpus. Additionally, implicitly subjective segments are often nuanced and hard to identify. Thus, it would be challenging to design an appropriate classifier. The paper circumvented this problem by reframing the task as two parallel binary classification tasks and obtained 82.31% accuracy with a manually compiled French lexicon and SVMs as classifiers.

Merely detecting the existence of subjective keywords is often an insufficient indication of a sentence's subjectivity. Other works attempted to enrich the feature set by incorporating more sentence-level information. Relying on expert knowledge of parse tree, [Xuan et al.](#page-98-27) [\(2012\)](#page-98-27) manually constructed a set of syntax-based patterns from unigrams and bigrams to extract features. A MaxEnt model was employed as the classifier, obtaining 92.1% accuracy on the Movie dataset. [Remus](#page-93-27) [\(2011\)](#page-93-27) hypothesized that the readability of a sentence was related to its subjectivity. Hence, readability formulae such as Devereux Readability Index [\(Smith, 1961\)](#page-94-28) and Easy Listening [\(Fang, 1966\)](#page-83-31) were incorporated as features in addition to the MPQA Subjectivity Lexicon, obtaining 84.5% F-measure on Moive.

Compared to standard text, microtext such as tweets contains informal and irregular expressions, making it more difficult for machines to process. Many works proposed subjectivity detection systems that specifically targeted Twitter text. Given the word constraint imposed by Twitter, a tweet is generally regarded as a sentence. [Barbosa and](#page-79-27) [Feng](#page-79-27) [\(2010\)](#page-79-27) believed that using subjectivity detection as an upstream task would improve the performance of polarity detection on Twitter text. Aside from conventional features such as subjective clues and PoS tags, they leveraged Tweet-specific syntax features, e.g., links and upper case. An SVM classifier was employed, which achieved 81.9% accuracy on the Twitter dataset, and improved the accuracy of polarity detection by 5.6%. Following their footsteps, [Sixto et al.](#page-94-29) [\(2016\)](#page-94-29) incorporated more Tweet-specific features that leveraged the structure of Twitter, e.g., the relationship between tweets, users, hashtags, and links. Using the stacking classifier proposed by [Cotelo et al.](#page-82-28) [\(2015\)](#page-82-28), their method obtained 89.8% on TASS. To reduce human effort, [Keshavarz and Saniee Abadeh](#page-87-31) [\(2018\)](#page-87-31) created a Twitter subjectivity lexicon automatically through a meta-heuristic approach, i.e., a genetic algorithm, which produced separate lists of subjective and objective words. A Bayse network was employed to classify a given tweet based on its subjective and objective word counts, achieving 60.9% on SemEval 2013. Alternatively, [Khatua et al.](#page-87-30) [\(2020\)](#page-87-30) leveraged the concept-level knowledge base SenticNet as their lexicon, which is able to provide implicit meaning associated with commonsense concepts. Their method obtained 80.7% accuracy on the NET dataset.

The methods introduced above have a common limitation, i.e., the lexicons are lists of keywords, instead of word meanings. Some subjective clues in fact have both subjective and objective word senses, which are not distinguishable in keyword lexicons, leading to false-positive errors. This problem can be mitigated by incorporating a Subjectivity WSD (SWSD) system to build a sense-aware lexicon. [Akkaya et al.](#page-78-7) [\(2009\)](#page-78-7) trained a supervised targeted SWSD system using SVM. The training data was compiled using words that are both in the MPQA Subjectivity Lexicon and the sense-tagged SENSEVAL corpora (Kilgarriff [and Palmer, 2000;](#page-87-32) [Preiss and Yarowsky, 2001;](#page-92-29) [Litkowski, 2004\)](#page-88-29). Alternatively, [Ortega et al.](#page-91-31) [\(2013\)](#page-91-31) applied an unsupervised, clustering-based SWSD system [\(Anaya-Sanchez et al.,](#page-79-29) ´ [2006\)](#page-79-29) on SentiWordNet to label each subjective word with fine-grained sense. Both SWSD systems were applied to a rule-based classifier similar to the one proposed by Riloff [and Wiebe](#page-93-24) [\(2003\)](#page-93-24). The supervised one improved accuracy by 1.3% on MPQA, while the unsupervised one improved F-measure by 6.5% on Movie. A prominent limitation of lexicon-based methods is that they require external resources such as sentiment lexicon and knowledge base.

# B. Word Frequency

Word-frequency-based methods detect subjectivity by modeling word presence or occurrence within a corpus. Therefore, compared to lexicon-based methods, they are language-independent and require neither manual annotation nor linguistic knowledge. They are also less computationally expensive due to the reduction of feature sets.

[Rustamov et al.](#page-93-25) [\(2013\)](#page-93-25); [Kamil et al.](#page-86-26) [\(2018\)](#page-86-26) proposed a language-independent feature extraction algorithm with a novel statistical measure of word occurrence called Pruned ICF (Inverse-Class Frequency), which is proven to be more effective than the standard IDF (Inverse-Document Frequency). Additionally, they applied two widely-used methods for pattern recognition to detect subjectivity, namely Fuzzy Control System (FCS) [\(Helmi and AlModarresi, 2009\)](#page-85-29) and Adaptive Nero-Fuzzy Inference System (ANFIS) [\(Fuller, 1995\)](#page-84-31), achieving the accuracy of 91.3% and 91.66% on ´ the Movie dataset, respectively. The latter obtained better performance due to the addition of a neural network layer. Inspired by empirical evidence that hybrid systems improve the performance of NLP classifiers, [Rustamov](#page-93-28) [\(2018\)](#page-93-28) further integrated FCS, ANFIS, and HMM into a sequential hybrid system, where input sentences that are wrongly labeled by the prior classifier are passed onto the subsequent one. Using the same feature extraction method as the previous paper, this system increased the accuracy to 92.24% on Movie.

[Wang and Manning](#page-97-31) [\(2013\)](#page-97-31) proposed a novel dropout algorithm to optimize the feature learning process. Conventional dropout training in neural network [\(Hinton et al., 2012\)](#page-85-30) prevents feature co-adaptation by randomly sampling neurons and input features and setting them to zeros, which leads to slow training. The authors suggested fast dropout training as a more efficient alternative, using a Gaussian approximation to draw samples. They applied this dropout method to Na¨ıve Bayes Support Vector Machine (NBSVM) [\(Wang and Manning, 2012\)](#page-97-32), which extracts features based on word presence. Their method not only achieved the accuracy of 93.6% on Movie and 86.3% on MPQA, but also greatly decreased the training time. Experiments also showed that fast dropout training could be applied to other loss functions and neural networks.

Latent Dirichlet Allocation (LDA) [Blei et al.](#page-80-6) [\(2003\)](#page-80-6) is a weakly-supervised generative model that assumes every document is a distribution of latent topics, which is determined by word frequencies. [He](#page-85-31) [\(2010\)](#page-85-31); [Maas et al.](#page-89-30) [\(2011\)](#page-89-30) suggested that subjectivity detection can be solved by LDA, based on the intuition that subjective sentences likely contain opinionated words. Hence, the paper modified conventional LDA so that the latent topics are word-level sentiment labels. An additional layer is inserted between word and document levels to model sentence-level subjectivity labels. Sentiment lexicons are incorporated to establish an informed prior distribution for word-level sentiment labels, achieving 71.2% accuracy on MPQA. On the other hand, [Lin et al.](#page-88-30) [\(2011\)](#page-88-30) argued that LDA likely discovers topics based on semantic similarities, instead of sentiment. Therefore, they modified LDA, so that it directly models word probabilities conditioned on topic distributions to capture semantic information. To explicitly extract sentiment information, they incorporated supervised sentiment analysis as an auxiliary task. Their method achieved 88.58% on the Movie dataset.

The drawback of the word frequency approach is that the order of the words is not considered. Thus, syntactic information cannot be effectively learned using this approach.

## C. Deep Learning

The acquisition of precise sentence representations is crucial for subjectivity detection, and as such, numerous studies have examined neural sentence modeling as a language-independent alternative to parse trees. [Kalchbrenner](#page-86-28) [et al.](#page-86-28) [\(2014\)](#page-86-28) presented a Dynamic CNN (DCNN) that is able to capture short- and long-range relations. The core component of DCNN is dynamic pooling, which outputs the sub-sequence of*k*maximum values in the input sequence, where*k*can be dynamically chosen. Hence, DCNN produces a hierarchical feature graph that contains syntactic, semantic, and structural patterns of the input sentence. However, their sentence representations do not retain any intermediate information, e.g., word-level and phrase-level features. To address this, [Zhao et al.](#page-99-16) [\(2015\)](#page-99-16) described a self-adaptive hierarchical sentence model named AdaSent. Inspired by gated recursive CNN [\(Cho et al., 2014\)](#page-82-29), AdaSent forms a pyramid-shape directed acyclic graph, where the bottom level is word representations and the top level is sentence representations. In this process, the gating network receives information from each level and selects the most appropriate representations for the given task. Their method obtained an accuracy of 95.5% on Movie and 93.3% MPQA.

With similar motivation for higher-order dependencies, [Chaturvedi et al.](#page-81-27) [\(2018\)](#page-81-27) proposed a Bayesian Networkbased Extreme Learning Machine (BNELM) framework for subjectivity detection. Single-layer feedforward neural networks, known as Extreme Learning Machines (ELMs), excel at inductive learning. However, the excessive number of hidden neurons in ELMs often leads to overfitting and slow performance. To address these weaknesses, Bayesian networks were introduced to model connections among the hidden neurons of ELM, as they can prune redundant and irrelevant hidden neurons and capture high-dimensional features. Furthermore, ELM cannot handle non-linear data such as sequences of sentences. Thus, an RNN layer was used to extract temporal features. Upon it, a fuzzy classifier was applied to achieve stability in case of noisy data, producing the output labels. Additionally, a deep CNN was employed prior to BNELM to provide low-dimensional features. The framework achieved the accuracy of 75% on MPQA Gold and 89% on TASS, outperforming previous ELM-centric models, namely, standard ELM and Sparse Bayesian ELM [\(Soria-Olivas et al., 2011\)](#page-95-25).

Likewise, [Satapathy et al.](#page-94-27) [\(2017\)](#page-94-27) employed CNN and RNN to extract spatial and temporal information respectively. To make the model more robust, they incorporated reinforcement learning, namely Point-wise Probability Reinforcement (PPR) [\(Frenay and Verleysen, 2015\)](#page-83-32), to regularize the learning process of CNN and reduce the influ- ´ ence of outliers. Specifically, convolutional layers in the CNN component were added iteratively, where the weight of each neuron was fine-tuned by the reinforced maximum likelihood of PPR. Their method did not perform very well on MPQA, obtaining 50% F-measure. However, it achieved a good performance of 76% F-measure on the multi-lingual Twitter dataset MLT.

In the same vein, PLMs can also provide beneficial universal representations learned from a plethora of unlabeled text. For instance, [Al Hamoud et al.](#page-79-23) [\(2022\)](#page-79-23) fed GloVe embeddings to different types of RNN variants, among which LSTM with attention mechanism achieved the best accuracy of 89.53% on MPQA and 83.83% on their proposed political and ideological dataset, whereas Bi-LSTM with attention achieved the best accuracy of 92.8% on Movie. [Kim](#page-87-33) [\(2014\)](#page-87-33) fine-tuned pre-trained Word2Vec with a simple CNN, obtaining accuracy of 93.4% on Movie and 89.5% on MPQA.

Furthermore, many works observed that it is complementary to combine PLM and MTL for more effective learning of text representations [\(Liu et al., 2019a;](#page-89-31) [Sun et al., 2019;](#page-95-26) [Mao and Li, 2021\)](#page-89-32). Motivated by this, [Huo and Iwaihara](#page-86-29) [\(2020\)](#page-86-29) fine-tuned BERT using MTL, where the BERT layers are shared among subjectivity detection and three other text classification tasks. Similarly, [Satapathy et al.](#page-94-30) [\(2022\)](#page-94-30) proposed an MTL framework for subjectivity and polarity detection. The framework leverages BERT as embedding, which is fed into two separate self-attention Bi-LSTM layers. A neural Tensor Network (NTN) [\(Socher et al., 2013\)](#page-95-27) was used as the information-sharing layer. Both methods employed a simple softmax classifier for each task. The former achieved 95.23% accuracy on Movie, while the latter obtained 95.1%. However, a shared limitation is that, despite their overall good performance, some of the tasks did not exceed single-task learning baselines. This is likely because both methods adopted hard parameter sharing MTL [\(Crawshaw, 2020\)](#page-82-30), which emphasizes more on generalization rather than optimization.

[Sagnika et al.](#page-94-31) [\(2021\)](#page-94-31) presented an attention-based CNN-LSTM model for subjectivity detection, which served as a pre-processing step for sentiment analysis. The combination of CNN and LSTM enabled the model to capture both spatial and temporal information. Additionally, it utilized word embeddings enhanced by sentiment-related information [\(Sagnika et al., 2020\)](#page-93-29). Initially, the training of the model was carried out with the Movie dataset, after which it was utilized to analyze the sentiment of the IMDb dataset. The objective sentences were eliminated from the dataset to form a modified set of reviews. Various models were tested as sentiment classifiers. The subjectivity detection model not only obtained 97.1% accuracy on the Movie dataset, but also consistently improved the performance of sentiment analysis.

###*6.7.2. Context-Dependent Subjectivity Detection*The method of individual detection categorizes each sentence without considering its context. However, subjectivity detection and sentiment classification are contextual problems since lexical items can affect each other in a discourse setting [\(Aue and Gamon, 2005;](#page-79-30) [Polanyi and Zaenen, 2006\)](#page-92-30). [Pang and Lee](#page-92-27) [\(2004\)](#page-92-27) was the first to leverage inter-sentence context information to filter out objective sentences, in order to better serve document-level polarity detection. Based on the hypothesis that adjacent text spans might have the same subjectivity label [\(Wiebe, 1994\)](#page-97-21), suggested an algorithm known as the "minimum cuts algorithm" that aims to optimize the subjectivity status score for every sentence separately, while also punishing the assignment of different labels to two closely related sentences. These two sub-objectives are independent of each other, making the model more flexible for the addition of features.

Context-dependent methods can be divided into two categories, namely, the feature engineering approach and the statistical approach.

#### A. Feature Engineering

A common way to incorporate document-level information is by designing relevant features. [Das and Bandy](#page-82-31)[opadhyay](#page-82-31) [\(2009\)](#page-82-31) proposed a domain-independent rule-based algorithm, named theme detection. The model utilized document-level features, e.g., positional aspects (document title, first paragraph, last two sentences), the positions of subjectivity clues, and the distance between any two thematic words. As with many techniques at the sentence level, this approach also integrated syntactic characteristics and resources such as SentiWordNet and MPQA Subjectivity Lexicon. It achieved precision and recall of 76.08% and 83.33% on MPQA.

To automatically select an appropriate feature set, [Das and Bandyopadhyay](#page-82-32) [\(2010\)](#page-82-32) employed the genetic algorithm (GA) [\(Holland, 1992;](#page-85-32) [Sastry et al., 2005\)](#page-94-32), which is a probabilistic search method, to find the optimal range of values of every feature. To capture context information, positional aspects, word distribution, and document theme [\(Wiebe et al.,](#page-97-26) [2000\)](#page-97-26) were incorporated as discourse-level features, aside from the commonly-used lexical and syntactic features. The GA then identified the globally optimal feature set by natural selection and computed the corresponding accuracy of the classifier through the fitness function. An advantage of the proposed method over other statistical classifiers is that the entire input sentence is encoded by GA and used as features, instead of using*n*-gram. Their method obtained the F-measure of 93.02% on MPQA and 95.69% on Movie.

[Biyani et al.](#page-80-28) [\(2014\)](#page-80-28) noticed a gap in subjectivity detection targeting online forums. Moreover, they argued that lexical features are highly dimensional, leading to the risks of overfitting and slow training. Thus, they presented a Forum dataset, and designed a set of non-lexical thread-specific features. Specifically, they leveraged thread structure and dialog acts and utilized lexicons and tools such as MPQA Subjectivity Lexicon and SentiStrength [\(Thelwall et al.,](#page-96-30) [2012\)](#page-96-30) to extract sentiment features. With the addition of conventional lexical features, the logistic regression classifier obtained 77.01% accuracy on Forum.

#### B. Statistical Approach

To minimize human effort in designing features, a statistical approach automatically learns features from a given corpus using statistical models. [Yu and Hatzivassiloglou](#page-98-28) [\(2003\)](#page-98-28) simply implemented a Na¨ıve Bayes classifier for document-level subjectivity detection, which achieved the F-measure of 97% on the TREC dataset proposed by them.

Motivated by the observation that language models are adept at representing knowledge of the text they were trained on, [Karimi and Shakery](#page-86-30) [\(2017\)](#page-86-30) proposed a language-model-based document-level subjectivity detection method. During training, a subjective reference language model and an objective were built using labeled documents. During inference, a language model was constructed for each input document, which was compared with the reference language models using KL-divergence (Laff[erty and Zhai, 2001\)](#page-87-34), producing two similarity scores. The difference between these two scores was regarded as the subjectivity score of the document. The final output of the model was a sorted list of input documents, based on their subjectivity scores. To achieve language non-specificity, the paper also proposed a semi-supervised method where the reference language models were built on a lexicon divided into subjective and objective parts, based on polarity scores. The supervised method obtained 94.63% MAP on the Movie dataset, whereas the unsupervised obtained 53.61% MAP.

Word embeddings can only provide limited syntactic and semantic information [\(Belinkov et al., 2017\)](#page-80-30). Therefore, to better initialize their model, [Chaturvedi et al.](#page-81-28) [\(2016a\)](#page-81-28) employed a Gaussian Bayesian Network (GBN) [Friedman](#page-83-33) [et al.](#page-83-33) [\(1998\)](#page-83-33) layer to capture long-range features among successive sentences, which were used to pre-train the CNN classifier. The GBN layer converted the sentence sequence from the MPQA dataset into a time series of word frequency, captured second-order word dependencies with a time lag of 2, and generated a subset of sentences that contained the most significant words from the MPQA Subjectivity Lexicon. The model adopted a CNN sentence model with convolution kernels of increasing size, which combined the local word dependencies within the kernel size to model long-range syntactic relations. It was pre-trained with the sub-set of sentences produced by GBN before being trained on the full dataset, obtaining the accuracy of 93.2% on MPQA and 96.4% on Movie.

#### *6.7.3. Cross-Lingual Subjectivity Detection*#### A. Language-Independent Approach

For feature-engineering-based subjectivity detection, lexical resources and tools are often not readily available for non-English languages. A common approach to circumvent this problem is to use non-language-specific features that are based on the presence or occurrence statistics of a corpus, e.g., word frequency [\(Rustamov et al., 2013;](#page-93-25) [Wang and Manning, 2013;](#page-97-31) [Kamil et al., 2018;](#page-86-26) [Blei et al., 2003;](#page-80-6) [Lin et al., 2011;](#page-88-30) [Belinkov et al., 2017\)](#page-80-30) and language modeling [\(Karimi and Shakery, 2017\)](#page-86-30). [Mogadala and Varma](#page-90-32) [\(2012\)](#page-90-32) further introduced language-independent feature weighing, leveraging unigram and bigram frequencies, and unigram word length. Entropy-based category coverage difference [\(Largeron et al., 2011\)](#page-87-35) was employed as the feature selection method.

#### B. Translation Approach

Another solution is the translation approach, where lexical resources for the target language are automatically generated by translating the resources and tools available for English, usually with the help of statistical machine translation (SMT) [\(Kim and Hovy, 2006;](#page-87-36) [Mihalcea et al., 2007;](#page-90-28) [Banea et al., 2008;](#page-79-31) [Wan, 2009;](#page-97-33) [Banea et al., 2011;](#page-79-32) [Amini et al., 2019\)](#page-79-33). [Banea et al.](#page-79-26) [\(2010\)](#page-79-26) conducted a study on English and five other highly lexicalized languages, proving that a multi-lingual feature space constructed through SMT improved the accuracy of subjectivity detection on all languages involved. However, the sentence translation process can lead to the loss of essential lexical information such as inflection and formality, which often served as an indicator of subjectivity [\(Banea et al., 2008\)](#page-79-31).

[Chaturvedi et al.](#page-81-29) [\(2016b\)](#page-81-29) mitigated this information loss during translation by using a neural network to transfer resources from English to Spanish. They first translated the MPQA Subjectivity Lexicon into Spanish using an SMT system [\(Lopez, 2008\)](#page-89-33). A MaxEnt-based PoS tagger [\(Toutanvoa and Manning, 2000\)](#page-96-31) and a multi-lingual WSD system [\(Moro et al., 2014a\)](#page-90-33) were incorporated in the preprocessing stage to minimize the loss of lexical information during translation. Their proposed model, named Lyapunov Deep Neural Network (LDNN), extracted spatial features from the input Spanish sentence and its translated English form using CNN, which were then combined with an RNN to capture the bilingual temporal features. To mitigate the vanishing gradient problem with RNN, a Lyapunov function was used as the error function of RNN for stable convergence. Utilizing the high-level features produced by Lyapunov-guided RNN, a multiple kernel learning [\(Subrahmanya and Shin, 2009;](#page-95-28) [Zhang et al., 2010\)](#page-99-17) classifier yielded the prediction. Their model obtained 84.0% F-measure on MPQA Gold, and 88.4% accuracy on TASS.

####*6.7.4. Multi-Modal Subjectivity Detection*While most studies on detecting subjectivity have concentrated on text-based data, the identification of subjective expressions in other modalities, such as audio and video, presents an important area for research. For instance, [Mur](#page-90-34)[ray and Carenini](#page-90-34) [\(2009,](#page-90-34) [2011\)](#page-90-29) proposed an automatic pattern extraction method for subjective expression in spoken conversation, which is able to extract Varying Instantiation N-Grams (VIN) from labeled and unlabeled data. Unlike convention*n*-gram, a VIN is a trigram where each unit can be either a word or a PoS label, which is a more robust alternative to syntactic parsers for fragmented and disfluent text, such as meeting transcripts. Combined with a large raw feature set, a MaxEnt classifier scored the F-measure of 52% on the AMIDA dataset.

The method above, however, did not leverage any information from other modalities. [Raaijmakers et al.](#page-93-30) [\(2008\)](#page-93-30) explored the effectiveness of lexical and acoustic features in speech subjectivity detection. Specifically, they investigated word, character, prosody, and phoneme *n*-grams. Following [Wrede and Shriberg](#page-98-30) [\(2003\)](#page-98-30); [Banse and Scherer](#page-79-34) [\(1996\)](#page-79-34), the prosodic features were extracted based on pitch, energy, and the distribution of energy in the long-term averaged spectrum. The word-, character-, and phoneme-level features were extracted from manual speech transcripts. A separate BoosTexter classifier [\(Schapire and Singer, 2000\)](#page-94-33) was employed for each feature set, whose predictions were combined using a simple linear interpolation strategy [\(Raaijmakers, 2007\)](#page-92-31) to obtain the final output. The combination of the four types of feature sets achieved 75.4% accuracy and 67.1% F-measure on AMIDA. Furthermore, experiments showed that word- and character-level features contributed the most to higher results, whereas prosodic features yielded marginal improvements.

#### *6.7.5. Bias Detection*Bias detection refers to the task of identifying biased statements from supposedly impartial articles. Specifically, in Wikipedia, the Neutral Point of View (NPOV) is a core principle that ensures neutrality for controversial topics. Thus, the goal of this task is to detect sentences that violate NPOV policy on a Wikipedia page. Bias detection is closely related to subjectivity detection. Its development mirrors the technical trends of the latter. However, it is considered to be more complex, because the linguistic cues of biased language are often nuanced, and depend heavily on the context.

For lexicon-based approaches, [Recasens et al.](#page-93-31) [\(2013\)](#page-93-31) manually compiled a biased word lexicon and feature set that covered framing bias (use of subjective words or phrases that links to a particular point of view), and epistemological bias (linguistic cues that modify the credibility of a statement). However, their method focused only on detecting a single bias-inducing word in a known biased statement. Furthering their work, [Hube and Fetahu](#page-86-27) [\(2018\)](#page-86-27) constructed a more comprehensive biased word lexicon for sentence-level bias detection. To minimize human efforts, they leveraged Word2Vec to expand a seed word list by measuring the distance between word vectors. Aside from the lexicon, other syntactic and semantic features were incorporated, e.g., tri-gram, PoS tags, Linguistic Inquiry Word Count (LIWC) [\(Pennebaker et al., 2001\)](#page-92-32), framing bias features, and epistemological bias features. By using a Random Forest classifier, their method obtained 74% precision on their proposed Conservapedia dataset.

[Aleksandrova et al.](#page-79-35) [\(2019\)](#page-79-35) proposed a semi-automatic method to construct a multi-lingual bias detection corpus, consisting of Bulgarian, French, and English sentences from Wikipedia. Their method was applicable for building a corpus from a Wikipedia archive in any language, as it does not rely on language-specific features. Additionally, they provided the performance of three baseline models, namely BoW, fastText [\(Joulin et al., 2017\)](#page-86-31), and logistic regression [\(Hosmer and Lemeshow, 2000\)](#page-85-33), among which BoW achieved the best overall average F-measure of 59.57% across the three languages.

For neural network approaches, [Hube and Fetahu](#page-86-32) [\(2019\)](#page-86-32) employed RNN to capture the inter-dependency of words and their context. To address the weakness of RNN in modeling long-range information, a hierarchical attention mechanism [\(Yang et al., 2016\)](#page-98-31) was adopted, which applied word-level attention on each sentence to compute sentence representations, upon which sentence-level attention was applied to learn biased cues from different samples. Following previous feature-based works, they concatenated GloVe embedding, PoS tags, and LIWC features as word representations.

PLMs were also widely used in bias detection. [Pryzant et al.](#page-92-28) [\(2020\)](#page-92-28) extended the work of [Recasens et al.](#page-93-31) [\(2013\)](#page-93-31) by using a pre-trained BERT-based detector to identify bias-inducing words and neutralizing them via an LSTMbased editor. A join embedding mechanism was employed to allow the detector control over the editor. They also introduced the WNC dataset for detecting and editing biased language, on which their model obtained 93.52% BLEU and 45.80% accuracy for the produced edits. However, a limitation is that they primarily targeted single-biased words. To mitigate this, [Pant et al.](#page-92-33) [\(2020\)](#page-92-33) enabled multi-word detection by identifying bias at the sentence level. They employed the weighted-average ensemble method on several BERT-based models to detect biased language, which obtained 71.61% accuracy and 70.40% F-measure on WNC.

#*6.8. Downstream Applications*##*6.8.1. Sentiment Computing*The presence of objective texts can dilute the task of sentiment computing. Therefore, the machine can better classify the remaining non-objective opinions by using subjectivity detection as an upstream task[\(Satapathy et al.,](#page-94-27) [2017;](#page-94-27) [Das and Sagnika, 2020\)](#page-82-33). For document-level sentiment analysis specifically, [Bonzanini et al.](#page-80-31) [\(2012\)](#page-80-31) showed that subjectivity detection reduced the amount of data to 60% while still producing the same polarity classification results as full-text classification. The analysis reveals that a considerable portion of real-world textual data is objective in nature, and this may cause an imbalance in sentiment analysis and opinion-mining tasks without subjectivity detection.

[Pang and Lee](#page-92-27) [\(2004\)](#page-92-27); [Das and Sagnika](#page-82-33) [\(2020\)](#page-82-33) applied subjectivity detection to filter out objective sentences in reviews prior to classifying their polarity. Similarly, [Kamal](#page-86-33) [\(2013\)](#page-86-33) first extracted subjective sentences from customer reviews and then employed a rule-based system to mine feature-opinion pairs from the subjective sentences. [Barbosa](#page-79-27) [and Feng](#page-79-27) [\(2010\)](#page-79-27); [Soong et al.](#page-95-29) [\(2019\)](#page-95-29) used subjectivity detection in sentiment analysis for Twitter microtext. These works proved that removing objective content from the dataset indeed makes the learning of sentiment more effective.

###*6.8.2. Information Retrieval*Subjectivity detection can serve as a subsystem in an information retrieval system to determine whether a document is subjective or objective [\(Soong et al., 2019\)](#page-95-29), because information retrieval systems normally aim to retrieve either opinionated or factual topic-relevant text from web sources, e.g., tweets, blog posts, reviews webpages, etc. [\(Paltoglou and Giachanou, 2014\)](#page-92-34).

For opinion retrieval, it helps to select candidate opinionated documents. For instance, [Zhang et al.](#page-99-15) [\(2007\)](#page-99-15) first employed an SVM classifier that used unigram and bigram features to identify subjective documents. Then, they separated relevant documents from irrelevant ones. For factual information retrieval, on the other hand, subjectivity detection helps to filter out opinionated text such as allegations and speculations to prevent false hits. [Wiebe and Rilo](#page-97-23)ff [\(2011\)](#page-97-23) implemented a Na¨ıve Bayes subjectivity classifier and a domain-relevant indicator for selective subjective sentence filtering. If a sentence was classified as subjective, it would be discarded unless it was also labeled as relevant by the indicator.

#*6.8.3. Hate Speech Detection*Hate speech detection is a task that identifies abusive speech targeting a person or a group based on stereotypical group characteristics, e.g., ethnicity, religion, or gender, on social media [\(Warner and Hirschberg, 2012\)](#page-97-34). Since hate speech is often marked by its content, tone, and target [\(Cohen-Almagor, 2011\)](#page-82-34), its detection is similar to that of polarity. Additionally, subjectivity clues tend to be surrounding the polarizing and arguing topics, which aligns well with hate speech detection. As such, subjectivity detection can be used as a filtering subsystem in hate speech detection.

For instance, [Gitari et al.](#page-84-32) [\(2015\)](#page-84-32) employed a rule-based subjectivity classifier that leveraged lexicons including MPQA Subjectivity Lexicon and SentiWordNet to identify subjective sentences. From the extracted sentences, they built a hate speech lexicon using bootstrapping and WordNet. Experiments showed that the addition of subjectivity detection significantly improved the performance of the hate speech classifier.

##*6.8.4. Question Answering System*QA systems generally encounter two types of questions - the ones that expect truth as answers, and the ones that expect opinions. Therefore, it is crucial for a QA system to distinguish opinions from facts, and provide the appropriate type depending on the question [\(Yu and Hatzivassiloglou, 2003\)](#page-98-28).

To achieve this goal, a QA system should operate in two stages. First, it must determine whether a question calls for a subjective or objective answer, which is its subjectivity orientation [\(Li et al., 2008a](#page-88-28)[,b;](#page-88-31) [Aikawa et al., 2011\)](#page-78-8). Then, the system needs to consider subjectivity as a relevant factor in the information retrieval process.

Subjectivity detection can be incorporated as a filter or feature set in a QA system. For instance, [Stoyanov et al.](#page-95-30) [\(2005\)](#page-95-30) modified the conventional QA system by applying a subjectivity filter and an opinion source filter on the initial IR results, which improved the system significantly. On the other hand, [Wan and McAuley](#page-96-32) [\(2016\)](#page-96-32) leveraged subjective features from reviews to provide users with a list of relevance-ranked reviews, which improved the performance of answering binary questions from categories with abundant data.

###*6.9. Summary*Subjectivity detection is a cognitive semantic processing task. It categorizes statements by subjective and objective classes. Theoretical research indicates that subjectivity can be detected by certain subjective elements, e.g., morphological, lexical, and syntactic elements [\(Banfield, 2014\)](#page-79-24). Thus, computational subjectivity research has developed lexical resources, e.g., Subjectivity Clues (Riloff [and Wiebe, 2003\)](#page-93-24), and MPQA Subjectivity Lexicon [\(Wilson](#page-97-29) [et al., 2005\)](#page-97-29). On the other hand, subjectivity can be also explained from the perspectives of pragmatics, e.g., speech acts [\(Austin, 1975\)](#page-79-25) and conceptual metaphors (Lakoff [and Johnson, 1980\)](#page-87-26). Related subjectivity detection works defined the task as classification tasks. Although those classification tasks can be further divided into course-grained and fine-grained classifications, e.g., document-level, sentence-level, and span-level subjectivity detection, there have not been studies aimed at explaining the subjectivity from pragmatic perspectives, e.g., speech acts and metaphors.

The application of subjectivity detection has proven to be supportive in downstream tasks, such as sentiment computing, information retrieval, hate speech detection, and QA systems. This is because these downstream tasks normally aim at mining opinions from subjective expressions. Subjectivity detection can filter out the objective ones, thus yielding the desired input for downstream tasks.

####*6.9.1. Technical Trends*Subjectivity detection is a well-studied sub-problem in affective computing and opinion mining. There are five technical trends in this area, namely individual, context-dependent, cross-lingual, multi-modal subjectivity detection, and bias detection. A summary of the trends can be found in Tables [21](#page-72-0) and [22.](#page-73-0)

For individual subjectivity detection (Table [21\)](#page-72-0), the subjectivity of each sentence or snippet is determined only by the lexical, syntactic, and semantic information of the sentence itself. There are mainly three types of methods for individual subjectivity detection. First, the lexicon-based approaches rely on external lexicons that contain subjective and sentiment clues to predict the subjectivity of a sentence. The weakness of such an approach is that subjective clues are often not extensive and reliable enough to determine the subjectivity of a sentence. Some works attempted to address this issue by utilizing sentence-level features to extract syntactic information [\(Wilson et al., 2004;](#page-97-22) [Xuan](#page-98-27) [et al., 2012;](#page-98-27) [Barbosa and Feng, 2010\)](#page-79-27), or incorporating WSD to identify subjective clues according to context [\(Akkaya](#page-78-7)

<span id="page-72-0"></span>

| Task | Reference                                      | Techniques         | Feature and KB                                           | Framework                          | Dataset        | Score       | Metric   |
|------|------------------------------------------------|--------------------|----------------------------------------------------------|------------------------------------|----------------|-------------|----------|
|      | Riloff and Wiebe (2003)<br>Kim and Hovy (2005) | Statistics<br>Rule | MPQA, WN<br>SCSL                                         | WN distance<br>Logic Rules         | MPQA<br>-      | 65.00%<br>- | Acc<br>- |
|      | Benamara et al. (2011)                         | Statistics         | syntactic, discursive<br>Lexical, stylistic,<br>features | SVM                                | Self-collected | 82.31%      | Acc      |
|      | Xuan et al. (2012)                             | Statistics         | MPQA, syntax-<br>based patterns                          | MaxEnt                             | Movie          | 92.10%      | Acc      |
|      | Remus (2011)                                   | Statistics         | MPQA, readability                                        | SVM                                | Movie          | 84.50%      | F1       |
|      | Barbosa and Feng (2010)                        | Statistics         | MPQA, POS,tweet-<br>specific features                    | SVM                                | Twitter1       | 81.59%      | Acc      |
|      | Sixto et al. (2016)                            | Statistics         | MPQA, tweet-specific<br>features                         | Stacking classifier                | TASS           | 89.80%      | Acc      |
|      | Keshavarz and Saniee Abadeh (2018)             | Statistics         | SCSL                                                     | Genetic algorithm                  | SemEval2013    | 60.90%      | Acc      |
|      | Khatua et al. (2020)                           | DL                 | SenticNet                                                | CNN                                | NET            | 80.70%      | Acc      |
|      | Akkaya et al. (2009)                           | Statistics         | MPQA, SWSD                                               | SVM                                | MPQA           | 81.30%      | Acc      |
| ISD  | Ortega et al. (2013)                           | Rule               | MPQA, SWSD                                               | Clustering, logic rules            | Movie          | 55.68%      | F1       |
|      | Kamil et al. (2018)                            | Statistics         | Pruned ICF                                               | ANFIS                              | Movie          | 91.66%      | Acc      |
|      | Rustamov (2018)                                | Statistics         | Pruned ICF                                               | FCS, ANFIS, HMM                    | Movie          | 92.24%      | Acc      |
|      | Wang and Manning (2013)                        | Statistics         | Word presence                                            | NBSVM                              | Movie          | 93.60%      | Acc      |
|      | Maas et al. (2011)                             | Statistics         | Semantic and sentiment<br>embeddings                     | Probabilistic model,<br>LDA        | MPQA           | 71.20%      | Acc      |
|      | Lin et al. (2011)                              | Statistics         | Sentiment                                                | LDA                                | Movie          | 88.58%      | Acc      |
|      | Zhao et al. (2015)                             | DL                 | Word2Vec                                                 | CNN                                | Movie          | 95.50%      | Acc      |
|      | Chaturvedi et al. (2018)                       | DL                 | MPQA, POS                                                | ELM, RNN, CNN,<br>fuzzy classifier | MPQA Gold      | 75.00%      | Acc      |
|      | Satapathy et al. (2017)                        | DL                 | GloVe, MPQA                                              | CNN, PPR                           | MPQA           | 50.00%      | F1       |
|      | Al Hamoud et al. (2022)                        | DL                 | GloVe                                                    | RNN, Att                           | Movie          | 92.80%      | Acc      |
|      | Kim (2014)                                     | DL                 | Word2Vec                                                 | CNN                                | Movie          | 93.40%      | Acc      |
|      | Huo and Iwaihara (2020)                        | DL                 | BERT                                                     | MTL                                | Movie          | 95.23%      | Acc      |
|      | Satapathy et al. (2022)                        | DL                 | BERT                                                     | MTL, RNN,NTN                       | Movie          | 95.10%      | Acc      |
|      | Sagnika et al. (2020)                          | DL                 | Sentiment-enhanced<br>word embedding                     | CNN, LSTM                          | Movie          | 97.10%      | Acc      |
|      |                                                |                    |                                                          |                                    |                |             |          |

Table 21: A summary of representative subjectivity detection techniques (Part 1). ISD denotes individual subjectivity detection. SCSL denotes self-collected subjectivity lexicon. SWSD denotes subjectivity WSD.

| Tas<br>k | R<br>efer<br>enc<br>e                                                                                   | T<br>ech<br>niq<br>ues           | F<br>eatu<br>re a<br>nd<br>KB                                                                    | F<br>ram<br>ewo<br>rk                                        | D<br>atas<br>et                | S<br>cor<br>e                    | M<br>etri<br>c     |
|----------|---------------------------------------------------------------------------------------------------------|----------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------|--------------------------------|----------------------------------|--------------------|
|          | Pan<br>g a<br>nd<br>Lee<br>(20<br>04)                                                                   | S<br>tati<br>stic<br>s           | S<br>CSL                                                                                         | Na Min<br>ıve<br>¨<br>imu<br>Bay<br>m c<br>es<br>uts<br>,    | Mo<br>vie                      | 8<br>6.4<br>0%                   | A<br>cc            |
|          | Das<br>and<br>Ban<br>dyo<br>pad<br>hya<br>y (2<br>009<br>)                                              | R<br>ule                         | fea<br>MP<br>ture<br>QA<br>, d<br>s, S<br>oc-<br>WN<br>leve<br>l                                 | Log<br>ic r<br>ule<br>s                                      | M<br>PQ<br>A                   | 7<br>9.5<br>4%                   | F<br>1             |
|          | Das<br>and<br>Ban<br>dyo<br>pad<br>hya<br>y (2<br>010<br>)                                              | S<br>tati<br>stic<br>s           | leve<br>MP<br>l fe<br>QA<br>, P<br>atur<br>OS<br>es<br>, d<br>oc-                                | Gen<br>etic<br>alg<br>orit<br>hm                             | M<br>ovi<br>e                  | 9<br>5.6<br>9%                   | F<br>1             |
| CD<br>SD | Biy<br>ani<br>et a<br>l. (2<br>014<br>)                                                                 | S<br>tati<br>stic<br>s           | thre<br>MP<br>QA<br>ad-<br>, S<br>spe<br>ent<br>cifi<br>iStr<br>c fe<br>eng<br>atur<br>th,<br>es | Log<br>isti<br>c re<br>gre<br>ssio<br>n                      | F<br>oru<br>m                  | 7<br>7.0<br>1%                   | A<br>cc            |
|          | Yu<br>and<br>Hat<br>ziva<br>ssil<br>ogl<br>ou<br>(20<br>03)                                             | S<br>tati<br>stic<br>s           | M<br>PQ<br>A,<br>PO<br>S                                                                         | N<br>aıve<br>¨<br>Bay<br>es                                  | T<br>RE<br>C                   | 9<br>7.0<br>0%                   | F<br>1             |
|          | Cha<br>Kar<br>imi<br>turv<br>and<br>edi<br>et a<br>Sha<br>l. (2<br>ker<br>y (2<br>016<br>017<br>a)<br>) | D<br>S<br>tati<br>L<br>stic<br>s | M<br>L<br>ang<br>PQ<br>uag<br>A<br>e m<br>ode<br>l                                               | G<br>R<br>ank<br>BN<br>, C<br>by<br>NN<br>sim<br>ilar<br>ity | M<br>M<br>ovi<br>ovi<br>e<br>e | 9<br>9<br>6.4<br>4.6<br>0%<br>3% | A<br>M<br>cc<br>AP |
|          | Ban<br>ea e<br>t a<br>l. (2<br>010<br>)                                                                 | M<br>L                           | M<br>PQ<br>A                                                                                     | S<br>MT<br>, N<br>aıve<br>¨<br>Bay<br>es                     | M<br>ulti<br>-MP<br>QA<br>EN   | 7<br>4.7<br>2%                   | A<br>cc            |
| CL<br>SD | Mo<br>gad<br>ala<br>and<br>Var<br>ma<br>(20<br>12)                                                      | S<br>tati<br>stic<br>s           | freq<br>Un<br>igra<br>., w<br>m a<br>ord<br>nd<br>len<br>big<br>gth<br>ram                       | Naı<br>¨<br>ve B<br>aye<br>s                                 | M<br>ulti<br>-MP<br>QA<br>EN   | 9<br>2.5<br>0%                   | F<br>1             |
|          | Cha<br>turv<br>edi<br>et a<br>l. (2<br>016<br>b)                                                        | D<br>L                           | M<br>PQ<br>A,<br>WS<br>D                                                                         | S<br>MT<br>, C<br>NN<br>, R<br>NN                            | M<br>PQ<br>A G<br>old          | 8<br>4.0<br>0%                   | F<br>1             |
|          | Mu<br>rray<br>and<br>Car<br>eni<br>ni (<br>201<br>1)                                                    | S<br>tati<br>stic<br>s           | V<br>IN,<br>raw<br>fea<br>ture<br>s                                                              | M<br>axE<br>nt                                               | A<br>MI<br>DA                  | 5<br>2.0<br>0%                   | F<br>1             |
| MM<br>SD | Raa<br>ijm<br>ake<br>rs e<br>t a<br>l. (2<br>008<br>)                                                   | S<br>tati<br>stic<br>s           | pho<br>Lex<br>nem<br>ical<br>, p<br>ic f<br>ros<br>eatu<br>odi<br>res<br>c, a<br>nd              | Boo<br>sTe<br>xte<br>r                                       | A<br>MI<br>DA                  | 7<br>5.4<br>0%                   | A<br>cc            |
|          | Rec<br>ase<br>ns e<br>t a<br>l. (2<br>013<br>)                                                          | S<br>tati<br>stic<br>s           | PO<br>Bia<br>S<br>sed<br>lexi<br>con<br>,                                                        | Log<br>isti<br>c re<br>gre<br>ssio<br>n                      | S<br>elf-<br>col<br>lect<br>ed | 3<br>4.3<br>5%                   | A<br>cc            |
|          | Hub<br>e a<br>nd<br>Fet<br>ahu<br>(20<br>18)                                                            | S<br>tati<br>stic<br>s           | bia<br>Wo<br>sed<br>rd2<br>Vec<br>lexi<br>, P<br>con<br>OS<br>, L<br>IW<br>C,                    | Ran<br>dom<br>F<br>ore<br>st                                 | C<br>ons<br>erv<br>ape<br>dia  | 7<br>4.0<br>0%                   | P<br>rec           |
| BD       | Ale<br>ksa<br>ndr<br>ova<br>et a<br>l. (2<br>019<br>)                                                   | S<br>tati<br>stic<br>s           | W<br>ord<br>freq<br>uen<br>cy                                                                    | B<br>oW                                                      | S<br>elf-<br>col<br>lect<br>ed | 5<br>9.5<br>7%                   | F<br>1             |
|          | Hub<br>e a<br>nd<br>Fet<br>ahu<br>(20<br>19)                                                            | D<br>L                           | G<br>loV<br>e, P<br>OS<br>, L<br>IW<br>C                                                         | R<br>NN                                                      | S<br>elf-<br>col<br>lect<br>ed | 7<br>7.1<br>0%                   | F<br>1             |
|          | Pry<br>zan<br>t e<br>t a<br>l. (2<br>020<br>)                                                           | D<br>L                           | B<br>ER<br>T                                                                                     | L<br>STM                                                     | W<br>NC                        | 4<br>5.8<br>0%                   | A<br>cc            |
|          | Pan<br>t e<br>t a<br>l. (2<br>020<br>)                                                                  | D<br>L                           | B<br>ER<br>T                                                                                     | E<br>nse<br>mb<br>le,<br>BE<br>RT                            | W<br>NC                        | 7<br>1.6<br>1%                   | A<br>cc            |
|          |                                                                                                         |                                  |                                                                                                  |                                                              |                                |                                  |                    |

<span id="page-73-0"></span>MMSD denotes multi-modal subjectivity detection. BD denotes bias detection. SWN denotes SentiWordNet. Table 22: A summary of representative subjectivity detection techniques (Part 2). CDSD denotes concept-dependent subjectivity detection. CLSD denotes cross-lingual subjectivity detection.

[et al., 2009;](#page-78-7) [Ortega et al., 2013\)](#page-91-31). Nonetheless, these methods cannot fully extract the underlying sentence structure and contextual information. Word-frequency-based approaches, on the other hand, predict sentence subjectivity according to the word presence or occurrence in a given corpus, thus being able to adapt to new domains and languages. Additionally, this approach requires little external resources or human effort. However, similar to the lexicon-based approach, word frequency methods lack the ability to capture syntactic information. To address this limitation, deeplearning-based methods utilize neural networks to learn spatial and temporal dependencies. Specifically, PLMs are widely used for their ability to provide universal representations [\(Kim, 2014;](#page-87-33) [Liu et al., 2019a;](#page-89-31) [Sun et al., 2019\)](#page-95-26).

For context-dependent subjectivity detection (Table [22\)](#page-73-0), the subjectivity of a sentence is determined with regards to its surrounding context, e.g., inter-sentence-level [\(Pang and Lee, 2004;](#page-92-27) [Belinkov et al., 2017\)](#page-80-30), document-level [\(Yu](#page-98-28) [and Hatzivassiloglou, 2003;](#page-98-28) [Das and Bandyopadhyay, 2009;](#page-82-31) [Karimi and Shakery, 2017\)](#page-86-30), or discourse-level [\(Biyani](#page-80-28) [et al., 2014\)](#page-80-28) information. In the existing works, such information is typically captured through feature engineering or statistical means.

As a large part of subjectivity detection works to some extent relies on external subjective clues, cross-lingual subjectivity detection aims specifically to solve the lack of lexical resources for non-English languages. There are mainly two branches of thought to address this problem (Table [22\)](#page-73-0). One is to make use of language-independent methods such as word frequency [\(Rustamov et al., 2013;](#page-93-25) [Lin et al., 2011;](#page-88-30) [Kamil et al., 2018;](#page-86-26) [Belinkov et al., 2017\)](#page-80-30) and language modeling [\(Karimi and Shakery, 2017\)](#page-86-30). The other is to generate resources for the target language from English lexicons with the help of SMT systems [\(Banea et al., 2010;](#page-79-26) [Chaturvedi et al., 2016b\)](#page-81-29).

Multi-modal subjectivity detection is a rising field of interest in accordance with the rising need for sentiment analysis in various media (Table [22\)](#page-73-0). Existing works utilized lexical, prosodic, and phonemic features for subjectivity detection in spoken conversations [\(Murray and Carenini, 2011;](#page-90-29) [Raaijmakers et al., 2008\)](#page-93-30). Subjectivity detection in other modalities such as video remains mostly unexplored.

Bias detection is a task that is closely related to subjectivity detection (Table [22\)](#page-73-0). It aims to identify biased statements from supposedly impartial articles such as Wikipedia. Despite its greater complexity, the identification of bias exhibits technical patterns that are akin to those found in subjectivity detection, e.g., lexicon-based [\(Recasens et al.,](#page-93-31) [2013;](#page-93-31) [Hube and Fetahu, 2018\)](#page-86-27), deep learning [\(Hube and Fetahu, 2019;](#page-86-32) [Pryzant et al., 2020\)](#page-92-28), and cross-lingual [\(Alek](#page-79-35)[sandrova et al., 2019\)](#page-79-35) methods.

| Reference               | Downstream Tasks      | Feature | Parser |
|-------------------------|-----------------------|---------|--------|
| Bonzanini et al. (2012) | Sentiment Computing   |         | ✓      |
| Pang and Lee (2004)     | Sentiment Computing   |         | ✓      |
| Das and Sagnika (2020)  | Sentiment Computing   |         | ✓      |
| Kamal (2013)            | Sentiment Computing   |         | ✓      |
| Barbosa and Feng (2010) | Sentiment Computing   |         | ✓      |
| Soong et al. (2019)     | Sentiment Computing   |         | ✓      |
| Zhang et al. (2007)     | Information Retrieval |         | ✓      |
| Wiebe and Riloff (2011) | Information Retrieval |         | ✓      |
| Cohen-Almagor (2011)    | Hate Speech Detection |         | ✓      |
| Gitari et al. (2015)    | Hate Speech Detection | ✓       | ✓      |
| Li et al. (2008a)       | Question Answering    | ✓       | ✓      |
| Li et al. (2008b)       | Question Answering    | ✓       | ✓      |
| Aikawa et al. (2011)    | Question Answering    | ✓       | ✓      |
| Stoyanov et al. (2005)  | Question Answering    |         | ✓      |
| Wan and McAuley (2016)  | Question Answering    | ✓       |        |
|                         |                       |         |        |
*6.9.2. Application Trends*Table 23: A summary of the representative applications of subjectivity detection in downstream tasks.

Due to its filtering nature, subjectivity detection is widely used as a parser for many downstream tasks, e.g., sentiment analysis [\(Pang and Lee, 2004;](#page-92-27) [Barbosa and Feng, 2010;](#page-79-27) [Kamal, 2013;](#page-86-33) [Soong et al., 2019;](#page-95-29) [Das and Sagnika,](#page-82-33) [2020\)](#page-82-33), information retrieval [\(Zhang et al., 2007;](#page-99-15) [Wiebe and Rilo](#page-97-23)ff, [2011\)](#page-97-23), hate speech detection [\(Gitari et al., 2015\)](#page-84-32), and QA systems [\(Stoyanov et al., 2005;](#page-95-30) [Wan and McAuley, 2016\)](#page-96-32). Most existing works take the pipeline approach, using the filtered results from subjectivity detection as the input of the target application. On the other hand, we also observe that subjectivity lexicons can also be useful features to support hate speech detection and QA systems.

A survey of literature pertaining to subjectivity detection reveals that the progress made in this research area has not kept pace with the advancements made in its downstream sentiment computing tasks, e.g., sentiment analysis [\(Gandhi](#page-84-33) [et al., 2023\)](#page-84-33). This is likely because sentiment analysis may deliver more fine-grained classification outputs, which helps to gain business insights, e.g., sentiment polarities on product or service reviews. However, it should be noted that while positive, negative, and neutral sentiment polarities represent subsets of subjective texts, there exists a substantial portion of texts that are objective in nature, presenting factual information. Objective texts are likely to be infrequent in reviews of products or services, as customers often use such platforms to express their opinions. However, in the context of opinion mining on social media, it is crucial to differentiate between subjective and objective statements, given that even statements with neutral sentiment polarities can be indicative of an individual's opinion. Thus, it is still necessary to conduct subjectivity detection before sentiment analysis.

#*6.9.3. Future Works*Fine-grained subjectivity detection. A sentence may contain several clauses with differing subjectivity. For instance, a sentence may present two or more opinions, or contain both opinions and factual information. Therefore, to better assist downstream applications, fine-grained subjectivity detection that identifies the particular opinion-bearing clauses is worthy of investigation. However, there is limited research on this issue. [Benamara et al.](#page-80-29) [\(2011\)](#page-80-29) proposed segment-level subjectivity detection. [Wilson et al.](#page-97-22) [\(2004\)](#page-97-22) proposed a method specifically for classifying the subjectivity of deeply nested clauses. There is scope for additional research to exploit the full potential of the fine-grained subjectivity annotation offered by the MPQA scheme [\(Wiebe et al., 2005\)](#page-97-27).

Multi-modal subjectivity detection. Subjectivity detection using information from multiple modalities remains largely unexplored. There is related multi-modal research that might provide inspiration for future works. [Wrede](#page-98-30) [and Shriberg](#page-98-30) [\(2003\)](#page-98-30) aimed to identify hot spots, which are regions in a meeting where participants are highly involved in the discussion, using solely a set of prosodic features. [Hillard et al.](#page-85-34) [\(2003\)](#page-85-34); [Galley et al.](#page-84-34) [\(2004\)](#page-84-34) both targeted the detection of agreements and disagreements in meetings. The former explored the combination of lexical and prosodic features, whereas the latter incorporated pragmatic features that captured the interactions between speakers. [Neiberg et al.](#page-91-32) [\(2006\)](#page-91-32) recognized positive, negative, and neutral emotions in meetings using lexical and acousticprosodic features. [Somasundaran et al.](#page-95-31) [\(2007\)](#page-95-31) detected sentences and turns in meetings that express sentiment and arguing opinions using lexical and discourse features. [Morency et al.](#page-90-30) [\(2011\)](#page-90-30); [Wollmer et al.](#page-98-29) [\(2013\)](#page-98-29); [Tsai et al.](#page-96-33) [\(2019\)](#page-96-33) ¨ conducted sentiment analysis on review videos using linguistic features, acoustic features, and visual features (face tracking).

Explainable subjectivity detection. While much of the subjectivity detection research has utilized lexical resources such as subjectivity and affective lexicons to explain the subjective nature of text based on individual words, these resources do not capture the pragmatic nuances of words within their contextual environment. This is because the utilized lexical knowledge is context-independent. Theoretical research has explained subjectivity from the perspective of pragmatics [\(Austin, 1975;](#page-79-25) Lakoff [and Johnson, 1980\)](#page-87-26). It would be valuable to study subjectivity detection that detects and explains subjectivity. Explainable subjectivity detection could push the development of more linguisticsinspired models that can account for the complexities of subjectivity and its expression in natural language. Additionally, there is potential for cross-disciplinary collaboration between linguistics, cognitive science, and computer science to further advance our understanding of subjectivity and its detection in various domains.

## <span id="page-75-0"></span>7. Discussion

###*7.1. Interactions between the Surveyed Tasks*In preceding sections, we have provided an introduction to the relationships between our surveyed tasks and downstream applications. Nevertheless, it is important to recognize that these tasks are intrinsically interconnected. For example, WSD and anaphora resolution are mutually supportive for each other. Consider the following sentence:

(21) I observed a colossal mammoth statue on the summit. It's really cool.

In this case, an anaphora resolution model should be capable of linking "it" to the "mammoth statue", assuming the significance of "cool" is interpreted as "a form of approval due to the appealing attributes of the mammoth statue", rather than the low temperature associated with the "summit". Conversely, if the antecedent of "it", denoting the "mammoth statue", is established, the intended meaning of "cool" can be easily discerned. This symbiotic enhancement is also observable in the context of WSD and NER. Within the context of the following sentence, disambiguating the sense of "hit" aids NER in recognizing "King's Arm" as a location rather than a person.

#### (22) I hit the King's Arm yesterday. It's my preferred pub in London.

Likewise, accurately identifying "King's Arm" as a location bolsters WSD models in determining that "hit" should be interpreted as "visited".

In the domain of concept extraction, WSD for multi-word expressions assumes heightened significance. For example, "go bananas", "cloud computing", and "pain killer" are best captured as concepts with multi-word expressions, rather than independent words, since their meanings manifest coherently only when interpreted as integrated wholes. Absent WSD for multi-word expressions, the task of concept extraction struggles to delineate conceptual boundaries within a given sentence. Furthermore, the application of WSD techniques extends to textual subjectivity detection. Taking the adjective "fine" for example, it ordinarily corresponds with the subjective text due to its meaning referring to the subjective feeling of being satisfactory, as seen in "Tesla Model X is a fine car". However, "fine" can also appear in objective contexts, if construed as a monetary penalty, as demonstrated in "I received a fine yesterday for speeding". Another instance is the term "long", which can be employed in an objectively spatial context as well as a negatively subjective sense akin to "tenacious". Integrating a sense-sensitive approach into subjectivity detection can enhance its performance.

The aforementioned interconnectedness and instances highlight the intricate interplay of language. The explication of linguistic interpretations can encompass various dimensions, even though the surveyed tasks pertain to fundamental semantic endeavors. These tasks exhibit interdependencies and mutual dependencies. Consequently, diverse learning methods may be requisite for addressing these multidimensional linguistic interpretation tasks.

####*7.2. The Impacts of Deep Learning on Semantic Processing*In the current neural network models with end-to-end task-processing purposes, the aforementioned linguistic interpretation facets might be encapsulated within a black box, lacking explicit representation. The limitation of these approaches lies in their inability to elucidate how language is employed and construed across divergent semantic facets. While the pursuit of human-like accuracy in deep learning-based systems is prominent, it is essential to acknowledge that the simulation of human cognitive and interpretive mechanisms, akin to human-like intelligence, may just gain a secondary focus in contrast to the endeavor for heightened task accuracy in the NLP domain.

Prior to the era of deep learning, semantic processing tasks often relied on rule-based or symbolic methods [\(Zhang](#page-99-18) [et al., 2021a\)](#page-99-18). These approaches aimed to distill the linguistic intuitions and insights associated with a given semantic processing task by leveraging a variety of linguistic features. Algorithms were devised to capture the specific linguistic nuances of each task, and substantial endeavors were undertaken to unveil the overarching principles governing semantic interpretation [\(Akkaya et al., 2009;](#page-78-7) [Cambria et al., 2014\)](#page-81-24). The process of cross-validating different linguistic features played a prominent role in the pursuit of enhanced predictive accuracy.

Nonetheless, the emergence of neural networks has brought about a convergence in the landscape of semantic learning and representations. Within the domain of semantic learning, a prevalent strategy involves utilizing contextual cues to predict a target word. This is achieved through various learning paradigms such as continuous bag of words (word2vec, [Mikolov et al., 2013\)](#page-90-1), masked word prediction (as seen in models like BERT and RoBERTa, [Devlin](#page-83-0) [et al., 2019;](#page-83-0) [Liu et al., 2019b\)](#page-89-0), or the prediction of the subsequent word (exemplified by the GPT families, [Radford](#page-93-32) [et al., 2018,](#page-93-32) [2019;](#page-93-33) [Brown et al., 2020\)](#page-81-30). This approach has undoubtedly yielded remarkable accomplishments across a wide range of NLP tasks. Neural networks excel at capturing the fundamental meanings of words and sentences within vectorized representations, and their ability to encode contextualized meanings as the network architecture becomes deeper.

In light of the demonstrated efficacy of the aforementioned neural semantic learning paradigms, the emphasis on tailoring models to capture task-specific linguistic intuitions has diminished somewhat, compared to rule-based or symbolic methods. Nevertheless, a pertinent query arises: Is the general unified target word prediction approach of pre-training the optimal strategy for achieving multi-dimensional semantic understanding? Semantic representations in vector form possess the capacity to apprehend spatial correlations among meanings, manifesting through distinct proximities of similar and dissimilar meanings. These spatial relationships are forged through the learning of word associations. Nonetheless, substantial knowledge, such as commonsense, causality, and occurrences that are either unprecedented or infrequent, e.g., novel metaphors [\(Ge et al., 2023\)](#page-84-35), remain beyond the direct purview of contextual understanding. Consequently, the comprehension of intricate constructs like frame semantics [\(Fillmore et al., 2006\)](#page-83-2), narratives, and cognitive mechanisms – which intricately hinge on facets like knowledge representation, commonsense reasoning, social cognition, and learning – presents challenges when solely relying on vector representations for their explication [\(Cambria and White, 2014\)](#page-81-31).

Considering the strong connections between semantic processing tasks and linguistics, it is advisable to direct heightened attention toward the incorporation of linguistic and cognitive intuitions and the exploration of semantic interpretative dimensions via neural networks and neuro-symbolic methods that marry the advantages of neural nets and symbolic knowledge representations. This constitutes a salient characteristic demarcating computational semantics-focused research from pure machine learning-oriented deep learning studies, inspiring a broader exploration of semantic processing.

####*7.3. Semantic Processing and Large Language Models*

ChatGPT and GPT-4 have expanded the reach of LLMs across diverse domains. Their remarkable proficiency in text generation, multitask execution, and complex task handling has garnered significant attention within the NLP community. Meantime, it is evident that there are noteworthy challenges associated with these expansive models, including issues such as hallucinations and complex task reasoning [\(Mao et al., 2023a\)](#page-89-34).

In the context of dialogues, [Cabrera and Neubig](#page-81-32) [\(2023\)](#page-81-32) have introduced an innovative evaluation framework tailored for LLMs. Their study has compared multiple such models and identified a recurring challenge of hallucination – a scenario where the generated content appears plausible but is, in fact, entirely fictional. Embedding semantic knowledge into these models presents an avenue to offer them with a more percise comprehension of real-world information, thereby diminishing the likelihood of generating content that lacks substantiated foundation. For instance, consider the sentence "the horse flew over the barn". A model enriched with semantic acumen would promptly discern the implausibility of such an event, thereby reducing the susceptibility to produce hallucinatory output. The scope of semantic acumen encompasses not solely the literal meaning of "barn", but also encompasses a more widespread understanding of its prevalent dimensions of size and height. Such a semantically informed model can manifest as a system adept at recognizing inconsistencies or deviations from anticipated semantic patterns. Alternatively, it could be a model proficient in grasping ordinary concept associations grounded in the frame semantics. Moreover, semantic processing can assist in reformulating user queries to render them more machine-friendly, thereby mitigating the potential for hallucinations stemming from vague queries.

On the other hand, [Mao et al.](#page-89-34) [\(2023a\)](#page-89-34) have reported that ChatGPT demonstrates satisfactory performance in general scientific knowledge and can effectively address questions necessitating open-ended responses. Nonetheless, it is not without errors, particularly in cases requiring multi-step reasoning. This shortcoming may be attributed to the current practice of employing solely feedforward propagation and fast inference in LLMs [\(Bubeck et al., 2023\)](#page-81-33). The absence of human-like deliberation for complex inquiries impedes the model's capacity for intricate multi-step reasoning tasks. Recent strides in Chain-of-Thought Prompting [\(Wei et al., 2022\)](#page-97-35) underscore the potential of decomposing complex problems into intermediate steps to enhance the complex reasoning capabilities of LLMs. In this context, semantic processing emerges as a valuable asset for task decomposition. It can assist in identifying pivotal concepts and entities, and delineating the principal topic into coherent logical subtopics or sequential steps. Semantic comprehension ensures a seamless and coherent progression of steps, yielding prompts that are not only more efficient but also conducive to the adept reasoning of intricate challenges by LLMs.

#### <span id="page-77-0"></span>8. Conclusion

In this survey, we have reviewed recent semantic processing techniques, e.g., WSD, anaphora resolution, concept extraction, NER, and subjectivity detection. We summarized useful datasets, annotation tools and knowledge bases that can facilitate the research in these domains. We also summarized the technical trends of these techniques, related theoretical research, and their downstream applications. We found that the breadth and depth of semantic processing can be greatly extended, both from the perspective of the needs of theoretical research and downstream applications. This is because current computational semantic processing techniques are limited in their reliance on specific task settings and available datasets. The review of the downstream applications of semantic processing techniques could potentially stimulate further research into fusion methodologies, which seek to enhance the performance of downstream tasks. The semantic processing methods can not only deliver effective features for downstream tasks, but also gain insights into analyzing model behaviors and studying linguistic and cognitive patterns.

As we continue to advance in the field of NLP, using powerful PLMs and LLMs has become increasingly common to tackle more complex NLP tasks. However, it is important to note that there is still great academic value in studying the low-level semantic tasks that these models are built upon. These tasks help us understand how language is presented and received, how semantics relates to human cognition, and how semantic processing tasks are interrelated. We observe that numerous contemporary semantic processing tasks have been translated into machine learning problems, which have somehow diminished linguistic motivations and intuitions from these computational studies. Shaping semantic processing tasks into tasks that are more conducive to machine learning can indeed improve the accuracy of specific tasks. However, improving accuracy in a single-task setting is not the only pursuit of semantic processing. We should pay more attention to how semantic processing techniques can better serve humans and machines to explain language phenomena. We hope that this paper can stimulate more research directions in the field of semantic processing and inspire researchers to place greater emphasis on the nature and cognition of semantics. With the development of more powerful tools such as PLMs and LLMs, it is perhaps valuable for our research to use these tools to address those fundamental linguistic challenges that were previously considered daunting. Regardless of the sophistication of tasks that can be performed by LLMs, basic semantic processing tasks remain crucial for comprehending and utilizing language effectively. These tasks serve as the foundation upon which our understanding of language is built.

#### CRediT Authorship Contribution Statement

Rui Mao: Conceptualization, Methodology, Investigation, Writing-original draft (Introduction, Discussion, and Conclusion) & harmonization. Kai He: Investigation, Writing-original draft (Named Entity Recognition). Xulang Zhang: Investigation, Writing-original draft (Subjectivity Detection). Guanyi Chen: Investigation, Writing-original draft (Anaphora Resolution). Jinjie Ni: Investigation, Writing-original draft (Word Sense Disambiguation). Zonglin Yang: Investigation, Writing-original draft (Concept Extraction). Erik Cambria: Conceptualization, Writing - review & editing, Project administration, Supervision, Funding acquisition.

#### Declaration of Competing Interest

Rui Mao, Zonglin Yang and Erik Cambria report financial support was provided by Continental Automotive Singapore Pte. Ltd.

#### Acknowledgment

This study is supported under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).

#### References

- <span id="page-78-5"></span>Abro, W.A., Aicher, A., Rach, N., Ultes, S., Minker, W., Qi, G., 2022. Natural language understanding for argumentative dialogue systems in the opinion building domain. Knowledge-Based Systems 242, 108318.
- <span id="page-78-2"></span>Adams, R., Nicolae, G., Nicolae, C., Harabagiu, S., 2007. Textual entailment through extended lexical overlap and lexico-semantic matching, in: Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pp. 119–124.

<span id="page-78-6"></span>Adomavicius, G., Tuzhilin, A., 2011. Context-aware recommender systems, in: Recommender Systems Handbook, pp. 217–253.

- <span id="page-78-3"></span>Agichtein, E., Askew, W., Liu, Y., 2008. Combining lexical, syntactic, and semantic evidence for textual entailment classification, in: TAC, pp. 1–6.
- <span id="page-78-0"></span>Agirre, E., Lopez de Lacalle, O., Soroa, A., 2014. Random walks for knowledge-based word sense disambiguation. Computational Linguistics 40, ´ 57–84.
- <span id="page-78-1"></span>Agirre, E., Soroa, A., 2009. Personalizing pagerank for word sense disambiguation, in: Proceedings of the 12th Conference of the European Chapter of the ACL, pp. 33–41.
- <span id="page-78-4"></span>Ahlers, D., 2013. Assessment of the accuracy of GeoNames gazetteer data, in: Proceedings of the 7th Workshop on Geographic Information Retrieval, pp. 74–81.
- <span id="page-78-8"></span>Aikawa, N., Sakai, T., Yamana, H., 2011. Community qa question classification: Is the asker looking for subjective answers or not? IPSJ Online Transactions 4, 160–168.

<span id="page-78-7"></span>Akkaya, C., Wiebe, J., Mihalcea, R., 2009. Subjectivity word sense disambiguation, in: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 190–199.

- <span id="page-79-23"></span>Al Hamoud, A., Hoenig, A., Roy, K., 2022. Sentence subjectivity analysis of a political and ideological debate dataset using LSTM and BiLSTM with attention and GRU models. Journal of King Saud University-Computer and Information Sciences , 7975–7987.
- <span id="page-79-21"></span>Al-Zaidy, R.A., Caragea, C., Giles, C.L., 2019. Bi-lstm-crf sequence labeling for keyphrase extraction from scholarly documents, in: Liu, L., White, R.W., Mantrach, A., Silvestri, F., McAuley, J.J., Baeza-Yates, R., Zia, L. (Eds.), The World Wide Web Conference, ACM. pp. 2551–2557.
- <span id="page-79-20"></span>Alami Merrouni, Z., Frikh, B., Ouhbi, B., 2020. Automatic keyphrase extraction: a survey and trends. Journal of Intelligent Information Systems 54, 391–424.
- <span id="page-79-35"></span>Aleksandrova, D., Lareau, F., Menard, P.A., 2019. Multilingual sentence-level bias detection in Wikipedia, in: Proceedings of the International ´ Conference on Recent Advances in Natural Language Processing, pp. 42–51.
- <span id="page-79-14"></span>Aloraini, A., Poesio, M., 2020. Cross-lingual zero pronoun resolution, in: Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 90–98.
- <span id="page-79-16"></span>Aloraini, A., Poesio, M., 2021. Data augmentation methods for anaphoric zero pronouns, in: Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference, pp. 82–93.
- <span id="page-79-19"></span><span id="page-79-15"></span>Aloraini, A., Pradhan, S., Poesio, M., 2022. Joint coreference resolution for zeros and non-zeros in Arabic. arXiv preprint arXiv:2210.12169 .
- Alvaro, N., Miyao, Y., Collier, N., et al., 2017. TwiMed: Twitter and PubMed comparable corpus of drugs, diseases, symptoms, and their relations. JMIR Public Health and Surveillance 3, e6396.
- <span id="page-79-33"></span>Amini, I., Karimi, S., Shakery, A., 2019. Cross-lingual subjectivity detection for resource lean languages, in: Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pp. 81–90.
- <span id="page-79-29"></span>Anaya-Sanchez, H., Pons-Porrata, A., Berlanga-Llavori, R., 2006. Word sense disambiguation based on word sense clustering, in: Advances in ´ Artificial Intelligence-IBERAMIA-SBIA 2006. Springer, pp. 472–481.
- <span id="page-79-10"></span>Aone, C., Bennett, S.W., . Automated acquisition of anaphora resolution strategies. AAAI , 1–7.
- <span id="page-79-18"></span>Aralikatte, R., Lamm, M., Hardt, D., Søgaard, A., 2021. Ellipsis resolution as question answering: An evaluation, in: Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Association for Computational Linguistics, Online. pp. 810–817. URL: <https://aclanthology.org/2021.eacl-main.68>, doi:[10.18653/v1/2021.eacl-main.68](http://dx.doi.org/10.18653/v1/2021.eacl-main.68).
- <span id="page-79-12"></span>Aralikatte, R., Lent, H., Gonzalez, A.V., Herschcovich, D., Qiu, C., Sandholm, A., Ringaard, M., Søgaard, A., 2019. Rewarding coreference resolvers for being consistent with world knowledge, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1229–1235.
- <span id="page-79-5"></span>Arnold, J.E., 1998. Reference form and discourse patterns. Stanford University.
- <span id="page-79-28"></span>Asher, N., Lascarides, A., 2003. Logics of Conversation. Cambridge University Press.
- <span id="page-79-0"></span>Atkins, B.T., 1992. Tools for computer-aided corpus lexicography: the hector project. Acta Linguistica Hungarica 41, 5–71.
- <span id="page-79-13"></span>Attree, S., 2019. Gendered ambiguous pronouns shared task: Boosting model confidence by evidence pooling, in: Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pp. 134–146.
- <span id="page-79-30"></span>Aue, A., Gamon, M., 2005. Customizing sentiment classifiers to new domains: A case study, in: Proceedings of Recent Advances in Natural Language Processing, Citeseer. pp. 2–1.
- <span id="page-79-7"></span>Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., Ives, Z., 2007. Dbpedia: A nucleus for a web of open data, in: The Semantic Web, pp. 722–735.
- <span id="page-79-25"></span>Austin, J.L., 1975. How to do things with words. Oxford University Press.
- <span id="page-79-4"></span>Baccianella, S., Esuli, A., Sebastiani, F., 2010. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining, in: Proceedings of the Seventh International Conference on Language Resources and Evaluation, pp. 2200–2204.
- <span id="page-79-8"></span>Bagga, A., Baldwin, B., 1998. Algorithms for scoring coreference chains, in: The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pp. 563–566.
- <span id="page-79-11"></span>Bahdanau, D., Cho, K.H., Bengio, Y., 2015. Neural machine translation by jointly learning to align and translate, in: 3rd International Conference on Learning Representations, pp. 1–15.
- <span id="page-79-9"></span>Baldwin, B., 1997. CogNIAC: high precision coreference with limited knowledge and linguistic resources, in: Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts, pp. 38–45.
- <span id="page-79-6"></span>Bamman, D., Lewke, O., Mansoor, A., 2020. An annotated dataset of coreference in English literature, in: Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 44–54.
- <span id="page-79-26"></span>Banea, C., Mihalcea, R., Wiebe, J., 2010. Multilingual subjectivity: Are more languages better?, in: Proceedings of the 23rd international conference on computational linguistics (Coling 2010), pp. 28–36.
- <span id="page-79-32"></span>Banea, C., Mihalcea, R., Wiebe, J., 2011. Multilingual sentiment and subjectivity analysis. Multilingual Natural Language Processing 6, 1–19.
- <span id="page-79-31"></span>Banea, C., Mihalcea, R., Wiebe, J., Hassan, S., 2008. Multilingual subjectivity analysis using machine translation, in: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pp. 127–135.
- <span id="page-79-1"></span>Banerjee, S., Pedersen, T., et al., 2003. Extended gloss overlaps as a measure of semantic relatedness, in: Proceedings of the 18th International Joint Conference on Artificial Intelligence, pp. 805–810.
- <span id="page-79-34"></span><span id="page-79-24"></span>Banfield, A., 2014. Unspeakable Sentences (Routledge Revivals): Narration and Representation in the Language of Fiction. Routledge.
- <span id="page-79-22"></span>Banse, R., Scherer, K.R., 1996. Acoustic profiles in vocal emotion expression. Journal of Personality and Social Psychology 70, 614.
- Bao, H., He, K., Yin, X., Li, X., Bao, X., Zhang, H., Wu, J., Gao, Z., 2021. Bert-based meta-learning approach with looking back for sentiment analysis of literary book reviews, in: Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qingdao, China, October 13–17, 2021, Proceedings, Part II 10, Springer. pp. 235–247.
- <span id="page-79-17"></span>Bar-Haim, R., Dagan, I., Mirkin, S., Shnarch, E., Szpektor, I., Berant, J., Greental, I., 2008. Efficient semantic deduction and approximate matching over compact parse forests, in: TAC, pp. 1–10.
- <span id="page-79-3"></span>Barba, E., Pasini, T., Navigli, R., 2021a. ESC: Redesigning WSD with extractive sense comprehension, in: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4661–4672.
- <span id="page-79-2"></span>Barba, E., Procopio, L., Lacerra, C., Pasini, T., Navigli, R., 2021b. Exemplification modeling: Can you give me an example, please?, in: IJCAI, pp. 3779–3785.
- <span id="page-79-27"></span>Barbosa, L., Feng, J., 2010. Robust sentiment detection on twitter from biased and noisy data, in: Proceedings of the 23rd International Conference

on Computational Linguistics: Posters, pp. 36–44.

<span id="page-80-15"></span>Bartalesi Lenzi, V., Moretti, G., Sprugnoli, R., 2012. CAT: the CELCT annotation tool, in: Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pp. 333–338.

<span id="page-80-0"></span>Barwise, J., Perry, J., 1981. Situations and attitudes. The Journal of Philosophy 78, 668–691.

<span id="page-80-5"></span>Basile, P., Caputo, A., Semeraro, G., 2014. An enhanced lesk word sense disambiguation algorithm through a distributional semantic model, in: Proceedings of the 25th International Conference on Computational Linguistics, pp. 1591–1600.

<span id="page-80-25"></span>Batmaz, Z., Yurekli, A., Bilge, A., Kaleli, C., 2019. A review on deep learning for recommender systems: challenges and remedies. Artificial Intelligence Review 52, 1–37.

<span id="page-80-26"></span><span id="page-80-17"></span>Beaver, D.I., 2004. The optimization of discourse anaphora. Linguistics and Philosophy 27, 3–56.

- Bekoulis, I., Deleu, J., Demeester, T., Develder, C., 2018. Joint entity recognition and relation extraction as a multi-head selection problem. Expert Systems With Applications 114, 34–45.
- <span id="page-80-30"></span>Belinkov, Y., Marquez, L., Sajjad, H., Durrani, N., Dalvi, F., Glass, J., 2017. Evaluating layers of representation in neural machine translation ` on part-of-speech and semantic tagging tasks, in: Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1–10.
- <span id="page-80-29"></span>Benamara, F., Chardon, B., Mathieu, Y., Popescu, V., 2011. Towards context-based subjectivity analysis, in: Proceedings of 5th International Joint Conference on Natural Language Processing, pp. 1180–1188.
- <span id="page-80-18"></span>Bengtson, E., Roth, D., 2008. Understanding the value of features for coreference resolution, in: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pp. 294–303.
- <span id="page-80-21"></span>Bentivogli, L., Clark, P., Dagan, I., Giampiccolo, D., 2009. The fifth PASCAL recognizing textual entailment challenge., in: TAC, Citeseer. pp. 1–18.
- <span id="page-80-2"></span>Bentivogli, L., Pianta, E., 2005. Exploiting parallel texts in the creation of multilingual semantically annotated resources: the multisemcor corpus. Natural Language Engineering 11, 247–261.
- <span id="page-80-10"></span>Berend, G., 2020. Sparsity makes sense: Word sense disambiguation using sparse contextualized word representations, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 8498–8508.
- <span id="page-80-19"></span>Berger, A.L., Della Pietra, S.A., Della Pietra, V.J., 1996. A maximum entropy approach to natural language processing. Computational Linguistics 22, 39–71.
- <span id="page-80-20"></span>Bergler, S., Witte, R., Khalife, M., Li, Z., Rudzicz, F., 2003. Using knowledge-poor coreference resolution for text summarization, in: Workshop on Text Summarization, pp. 1–8.
- <span id="page-80-27"></span>Bethard, S., Savova, G., Chen, W.T., Derczynski, L., Pustejovsky, J., Verhagen, M., 2016. SemEval-2016 Task 12: Clinical TempEval, in: Proceedings of the 10th international workshop on semantic evaluation (SemEval-2016), pp. 1052–1062.
- <span id="page-80-9"></span>Bevilacqua, M., Navigli, R., 2019. Quasi bidirectional encoder representations from transformers for word sense disambiguation, in: Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pp. 122–131.
- <span id="page-80-8"></span>Bevilacqua, M., Navigli, R., 2020. Breaking through the 80% glass ceiling: Raising the state of the art in word sense disambiguation by incorporating knowledge graph information, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2854–2864.
- <span id="page-80-1"></span>Bevilacqua, M., Pasini, T., Raganato, A., Navigli, R., 2021a. Recent trends in word sense disambiguation: A survey, in: Zhou, Z. (Ed.), Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, ijcai.org. pp. 4330–4338. URL: <https://doi.org/10.24963/ijcai.2021/593>, doi:[10.24963/ijcai.2021/593](http://dx.doi.org/10.24963/ijcai.2021/593).
- <span id="page-80-13"></span>Bevilacqua, M., Pasini, T., Raganato, A., Navigli, R., et al., 2021b. Recent trends in word sense disambiguation: A survey, in: Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 4330–4338.
- <span id="page-80-28"></span>Biyani, P., Bhatia, S., Caragea, C., Mitra, P., 2014. Using non-lexical features for identifying factual and opinionative threads in online forums. Knowledge-Based Systems 69, 170–178.
- <span id="page-80-4"></span>Black, W., Elkateb, S., Rodriguez, H., Alkhalifa, M., Vossen, P., Pease, A., Fellbaum, C., et al., 2006. Introducing the Arabic WordNet project, in: Proceedings of the Third International WordNet Conference, pp. 295–300.

<span id="page-80-6"></span>Blei, D.M., Ng, A.Y., Jordan, M.I., 2003. Latent Dirichlet allocation. Journal of Machine Learning Research 3, 993–1022.

- <span id="page-80-7"></span>Blevins, T., Zettlemoyer, L., 2020. Moving down the long tail of word sense disambiguation with gloss informed bi-encoders, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 1006–1017.
- <span id="page-80-11"></span>Blloshmi, R., Pasini, T., Campolungo, N., Banerjee, S., Navigli, R., Pasi, G., 2021. IR like a SIR: Sense-enhanced information retrieval for multiple languages, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1030–1041.
- <span id="page-80-23"></span>Bodenreider, O., 2004. The unified medical language system (UMLS): integrating biomedical terminology. Nucleic Acids Research 32, D267– D270.
- <span id="page-80-14"></span>Bollacker, K., Evans, C., Paritosh, P., Sturge, T., Taylor, J., 2008. Freebase: a collaboratively created graph database for structuring human knowledge, in: Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pp. 1247–1250.
- <span id="page-80-3"></span>Bond, F., Baldwin, T., Fothergill, R., Uchimoto, K., 2012. Japanese semcor: A sense-tagged corpus of japanese, in: Proceedings of the 6th Global WordNet Conference, pp. 56–63.
- <span id="page-80-24"></span>Bontcheva, K., Cunningham, H., Roberts, I., Roberts, A., Tablan, V., Aswani, N., Gorrell, G., 2013. GATE Teamware: a web-based, collaborative text annotation framework. Language Resources and Evaluation 47, 1007–1029.
- <span id="page-80-31"></span>Bonzanini, M., Martinez-Alvarez, M., Roelleke, T., 2012. Opinion summarisation through sentence extraction: An investigation with movie reviews, in: Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 1121– 1122.
- <span id="page-80-16"></span>Bornstein, A., Cattan, A., Dagan, I., 2020. CoRefi: A crowd sourcing suite for coreference annotation, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 205–215.
- <span id="page-80-12"></span>Boroditsky, L., 2011. How language shapes thought. Scientific American 304, 62–65.
- <span id="page-80-22"></span>Borrega, O., Taule, M., Martı, M.A., 2007. What do we mean when we speak about named entities, in: Proceedings of Corpus Linguistics, Citeseer. ´ pp. 1–27.

- <span id="page-81-8"></span>Bos, J., 2003. Implementing the binding and accommodation theory for anaphora resolution and presupposition projection. Computational Linguistics 29, 179–210.
- <span id="page-81-22"></span>Branahl, S., 1998. Das EDGAR (Electronic Data Gathering, Analysis and Retrieval) System der SEC und seine Bedeutung fur die Bereitstellung ¨ von Rechnungslegungsinformationen. diplom. de.
- <span id="page-81-5"></span>Brennan, S.E., 1995. Centering attention in discourse. Language and Cognitive processes 10, 137–167.
- <span id="page-81-14"></span>Brennan, S.E., Friedman, M.W., Pollard, C.J., 1987. A centering approach to pronouns, in: 25th Annual Meeting of the Association for Computational Linguistics, pp. 155–162.
- <span id="page-81-1"></span>Brin, S., Page, L., 1998. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems 30, 107–117.
- <span id="page-81-30"></span>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al., 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems 33, 1877–1901.
- <span id="page-81-0"></span>Bruce, R., Wiebe, J., 1999. Decomposable modeling in natural language processing. Computational Linguistics 25, 195–207.
- <span id="page-81-33"></span>Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y.T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M.T., Zhang, Y., 2023. Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712 .
- <span id="page-81-16"></span><span id="page-81-3"></span>Buring, D., 2005. Binding theory. Cambridge University Press. ¨
- Byron, D.K., Gegg-Harrison, W., Lee, S.H., 2006. Resolving zero anaphors and pronouns in korean. Traitement Automatique des Langues 46, 91–114.
- <span id="page-81-32"></span>Cabrera, A., Neubig, G., 2023. Zeno chatbot report. <https://github.com/zeno-ml/zeno-build/tree/main/tasks/chatbot/report>. Accessed: 18/05/2023.
- <span id="page-81-23"></span>Cambria, E., Liu, Q., Decherchi, S., Xing, F., Kwok, K., 2022a. SenticNet 7: A commonsense-based neurosymbolic AI framework for explainable sentiment analysis, in: Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 3829–3839.
- <span id="page-81-26"></span>Cambria, E., Mao, R., Han, S., Liu, Q., 2022b. Sentic parser: A graph-based approach to concept extraction for sentiment analysis, in: IEEE International Conference on Data Mining Workshops, pp. 413–420.
- <span id="page-81-24"></span>Cambria, E., Olsher, D., Rajagopal, D., 2014. SenticNet 3: A common and common-sense knowledge base for cognition-driven sentiment analysis, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1515–1521.
- <span id="page-81-25"></span>Cambria, E., Poria, S., Bajpai, R., Schuller, B., 2016. SenticNet 4: A semantic resource for sentiment analysis based on conceptual primitives, in: Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers, pp. 2666–2677.
- <span id="page-81-31"></span>Cambria, E., White, B., 2014. Jumping NLP curves: A review of natural language processing research. IEEE Computational intelligence magazine 9, 48–57.
- <span id="page-81-2"></span>Campolungo, N., Martelli, F., Saina, F., Navigli, R., 2022. DiBiMT: A novel benchmark for measuring word sense disambiguation biases in machine translation, in: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4331–4352.
- <span id="page-81-15"></span>Cardie, C., Wagstaff, K., 1999. Noun phrase coreference as clustering, in: 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp. 82–89.
- <span id="page-81-13"></span>Carter, D., 1987. Interpreting Anaphors in Natural Language Texts. Halsted Press.
- <span id="page-81-21"></span><span id="page-81-4"></span>Chafe, W., 1976. Givenness, contrastiveness, definiteness, subjects, topics, and point of view. Subject and topic .
- Chai, H., Strube, M., 2022. Incorporating centering theory into neural coreference resolution, in: Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2996–3002.
- <span id="page-81-6"></span>Chambers, C.G., Smyth, R., 1998. Structural parallelism and discourse coherence: A test of centering theory. Journal of Memory and Language 39, 593–608.
- <span id="page-81-20"></span>Chambers, N., Cer, D., Grenager, T., Hall, D., Kiddon, C., MacCartney, B., de Marneffe, M.C., Ramage, D., Yeh, E., Manning, C.D., 2007. Learning alignments and leveraging natural logic, in: Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pp. 165–170.
- <span id="page-81-28"></span>Chaturvedi, I., Cambria, E., Poria, S., Bajpai, R., 2016a. Bayesian deep convolution belief networks for subjectivity detection, in: 2016 IEEE 16th International Conference on Data Mining workshops (ICDMW), IEEE. pp. 916–923.
- <span id="page-81-29"></span>Chaturvedi, I., Cambria, E., Vilares, D., 2016b. Lyapunov filtering of objectivity for spanish sentiment model, in: 2016 International Joint Conference on Neural Networks, IEEE. pp. 4474–4481.
- <span id="page-81-27"></span>Chaturvedi, I., Ragusa, E., Gastaldo, P., Zunino, R., Cambria, E., 2018. Bayesian network based extreme learning machine for subjectivity detection. Journal of The Franklin Institute 355, 1780–1797.
- <span id="page-81-12"></span>Chen, C., Ng, V., 2013. Chinese zero pronoun resolution: Some recent advances, in: Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics. pp. 1360–1365.
- <span id="page-81-17"></span>Chen, C., Ng, V., 2014. Chinese zero pronoun resolution: An unsupervised probabilistic model rivaling supervised resolvers, in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 763–774.
- <span id="page-81-18"></span>Chen, C., Ng, V., 2015. Chinese zero pronoun resolution: A joint unsupervised discourse-aware model rivaling state-of-the-art resolvers, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 320–326.
- <span id="page-81-19"></span>Chen, C., Ng, V., 2016. Chinese zero pronoun resolution with deep neural networks, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 778–788.
- <span id="page-81-9"></span>Chen, G., 2022. Computational Generation of Chinese Noun Phrases. Ph.D. thesis. Utrecht University.
- <span id="page-81-10"></span>Chen, G., van Deemter, K., 2022. Understanding the use of quantifiers in Mandarin, in: Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, Association for Computational Linguistics, Online only. pp. 73–80. URL: [https://aclanthology.org/2022.](https://aclanthology.org/2022.findings-aacl.7) [findings-aacl.7](https://aclanthology.org/2022.findings-aacl.7).
- <span id="page-81-7"></span>Chen, G., van Deemter, K., Lin, C., 2018a. Modelling pro-drop with the rational speech acts model, in: Proceedings of the 11th International Conference on Natural Language Generation, Association for Computational Linguistics. pp. 57–66.
- <span id="page-81-11"></span>Chen, G., Same, F., van Deemter, K., 2023. Neural referential form selection: Generalisability and interpretability. Computer Speech & Language 79, 101466.

<span id="page-82-9"></span>Chen, H., Fan, Z., Lu, H., Yuille, A., Rong, S., 2018b. PreCo: A large-scale dataset in preschool vocabulary for coreference resolution, in: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 172–181.

<span id="page-82-24"></span>Chen, J., Zhang, X., Wu, Y., Yan, Z., Li, Z., 2018c. Keyphrase generation with correlation constraints, in: Riloff, E., Chiang, D., Hockenmaier, J., Tsujii, J. (Eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4057–4066.

<span id="page-82-17"></span>Chen, S., Gu, B., Qu, J., Li, Z., Liu, A., Zhao, L., Chen, Z., 2021. Tackling zero pronoun resolution and non-zero coreference resolution jointly, in: Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 518–527.

<span id="page-82-25"></span>Chen, W., Gao, Y., Zhang, J., King, I., Lyu, M.R., 2019. Title-guided encoding for keyphrase generation, in: The Thirty-Third AAAI Conference on Artificial Intelligence, pp. 6268–6275.

<span id="page-82-27"></span><span id="page-82-18"></span>Chesley, P., Vincent, B., Xu, L., Srihari, R.K., 2006. Using verbs and adjectives to automatically classify blog sentiment. Training 580, 233–241.

Chinchor, N., Hirschman, L., Lewis, D.D., 1993. Evaluating message understanding systems: an analysis of the third message understanding conference (MUC-3). Computational Linguistics 19, 409–450.

<span id="page-82-8"></span>Chinchor, N.A., 1998. Overview of MUC-7, in: Seventh Message Understanding Conference (MUC-7), pp. 1–4.

<span id="page-82-5"></span>Chinchor, N.A., Sundheim, B., 1995. Message understanding conference (muc) tests of discourse processing, in: Proc. AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, pp. 21–26.

<span id="page-82-1"></span>Chklovski, T., Pantel, P., 2004. Verbocean: Mining the web for fine-grained semantic verb relations, in: Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 33–40.

<span id="page-82-29"></span>Cho, K., van Merrienboer, B., Bahdanau, D., Bengio, Y., 2014. On the properties of neural machine translation: Encoder-decoder approaches, in: Wu, D., Carpuat, M., Carreras, X., Vecchi, E.M. (Eds.), Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pp. 103–111.

<span id="page-82-12"></span>Church, K.W., 1989. A stochastic parts program and noun phrase parser for unrestricted text, in: International Conference on Acoustics, Speech, and Signal Processing,, pp. 695–698.

<span id="page-82-6"></span>Clark, H.H., 1975. Bridging, in: Theoretical Issues in Natural Language Processing, pp. 169–174.

<span id="page-82-26"></span>Clark, K., Luong, M., Le, Q.V., Manning, C.D., 2020. ELECTRA: pre-training text encoders as discriminators rather than generators, in: 8th International Conference on Learning Representations, pp. 1–18.

<span id="page-82-16"></span>Clark, K., Manning, C.D., 2015. Entity-centric coreference resolution with model stacking, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1405–1415.

<span id="page-82-15"></span>Clark, K., Manning, C.D., 2016a. Deep reinforcement learning for mention-ranking coreference models, in: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2256–2262.

<span id="page-82-14"></span>Clark, K., Manning, C.D., 2016b. Improving coreference resolution by learning entity-level distributed representations, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 643–653.

<span id="page-82-10"></span>Cohen, K.B., Lanfranchi, A., Choi, M.J.y., Bada, M., Baumgartner, W.A., Panteleyeva, N., Verspoor, K., Palmer, M., Hunter, L.E., 2017. Coreference annotation and resolution in the colorado richly annotated full text (craft) corpus of biomedical journal articles. BMC Bioinformatics 18, 1–14.

<span id="page-82-34"></span>Cohen-Almagor, R., 2011. Fighting hate and bigotry on the internet. Policy & Internet 3, 1–26.

- <span id="page-82-2"></span>Cohn, T., 2003. Performance metrics for word sense disambiguation, in: Proceedings of the Australasian Language Technology Workshop 2003, pp. 86–93.
- <span id="page-82-19"></span>Collobert, R., Weston, J., 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning, in: Proceedings of the 25th International Conference on Machine Learning, pp. 160–167.

<span id="page-82-20"></span>Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P., 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research 12, 2493–2537.

<span id="page-82-3"></span>Conia, S., Navigli, R., 2021. Framing word sense disambiguation as a multi-label problem for model-agnostic knowledge integration, in: Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pp. 3269–3275.

<span id="page-82-28"></span>Cotelo, J.M., Cruz, F., Ortega, F.J., Troyano, J.A., 2015. Explorando Twitter mediante la integracion de informaci ´ on estructurada y no estructurada. ´ Procesamiento del Lenguaje Natural , 75–82.

<span id="page-82-30"></span>Crawshaw, M., 2020. Multi-task learning with deep neural networks: A survey. arXiv preprint arXiv:2009.09796 , 1–43.

<span id="page-82-0"></span>Croft, W., Cruse, D.A., 2004. Cognitive linguistics. Cambridge University Press.

<span id="page-82-22"></span>Cui, L., Wu, Y., Liu, J., Yang, S., Zhang, Y., 2021. Template-based named entity recognition using bart, in: Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 1835–1845.

<span id="page-82-7"></span>Cybulska, A., Vossen, P., 2014. Using a sledgehammer to crack a nut? lexical diversity and event coreference resolution, in: Proceedings of the Ninth International Conference on Language Resources and Evaluation, pp. 4545–4552.

<span id="page-82-13"></span>Daelemans, W., Zavrel, J., Van Der Sloot, K., Van den Bosch, A., 2004. Timbl: Tilburg memory-based learner. Tilburg University .

<span id="page-82-31"></span>Das, A., Bandyopadhyay, S., 2009. Theme detection an exploration of opinion subjectivity, in: 2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops, pp. 1–6.

<span id="page-82-32"></span>Das, A., Bandyopadhyay, S., 2010. Subjectivity detection using genetic algorithm. Computational Approaches to Subjectivity and Sentiment Analysis , 14–21.

<span id="page-82-33"></span>Das, N., Sagnika, S., 2020. A subjectivity detection-based approach to sentiment analysis, in: Machine Learning and Information Processing. Springer, pp. 149–160.

<span id="page-82-21"></span>Das, S.S.S., Katiyar, A., Passonneau, R., Zhang, R., 2022. CONTaiNER: Few-shot named entity recognition via contrastive learning, in: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6338–6353.

<span id="page-82-4"></span>Dauphin, Y.N., Fan, A., Auli, M., Grangier, D., 2017. Language modeling with gated convolutional networks, in: International Conference on Machine Learning, PMLR. pp. 933–941.

<span id="page-82-23"></span><span id="page-82-11"></span>Davis, E., Morgenstern, L., Ortiz, C.L., 2017. The first Winograd schema challenge at IJCAI-16. AI Magazine 38, 97–98.

De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh, G., Tuytelaars, T., 2021. A continual learning survey: Defying forgetting in classification tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 3366–3385.

<span id="page-83-11"></span>Deemter, K.v., Kibble, R., 2000. On coreferring: Coreference in MUC and related annotation schemes. Computational Linguistics 26, 629–637.

<span id="page-83-19"></span>Demner-Fushman, D., Fung, K.W., Do, P., Boyce, R.D., Goodwin, T.R., 2019. Overview of the TAC 2018 Drug-Drug Interaction Extraction from Drug Labels Track. TAC November, 1–12.

<span id="page-83-18"></span>Deng, Y., Xu, X., Qiu, Y., Xia, J., Zhang, W., Liu, S., 2020. A multimodal deep learning framework for predicting drug–drug interaction events. Bioinformatics 36, 4316–4322.

<span id="page-83-14"></span>Denis, P., Baldridge, J., 2008. Specialized models and ranking for coreference resolution, in: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pp. 660–669.

- <span id="page-83-13"></span>Denis, P., Baldridge, J., 2009. Global joint models for coreference resolution and named entity classification. Procesamiento del lenguaje natural 42.
- <span id="page-83-20"></span>Derczynski, L., Nichols, E., van Erp, M., Limsopatham, N., 2017. Results of the wnut2017 shared task on novel and emerging entity recognition, in: Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 140–147.
- <span id="page-83-6"></span>Dettmers, T., Minervini, P., Stenetorp, P., Riedel, S., 2018. Convolutional 2d knowledge graph embeddings, in: Proceedings of the AAAI conference on artificial intelligence, pp. 1811–1818.
- <span id="page-83-8"></span>Devitt, A., Ahmad, K., 2007. Sentiment polarity identification in financial news: A cohesion-based approach, in: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pp. 984–991.
- <span id="page-83-0"></span>Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2019. BERT: Pre-training of deep bidirectional Transformers for language understanding, in: Proceedings of the 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4171–4186.
- <span id="page-83-5"></span>Dictionary, C.E., 1982. Collins. London & Glasgow .
- <span id="page-83-4"></span>Dictionary, O., 2010. Oxford dictionary of english. Oxford Dictionary of English, 3rd edn. Oxford University Press. China Translation & Printing Services Ltd, China .
- <span id="page-83-28"></span>Dimitrakis, E., Sgontzos, K., Tzitzikas, Y., 2020. A survey on question answering systems over linked data and documents. Journal of Intelligent Information Systems 55, 233–259.
- <span id="page-83-15"></span>Ding, X., Liu, B., 2010. Resolving object and attribute coreference in opinion mining, in: Proceedings of the 23rd International Conference on Computational Linguistics, pp. 268–276.
- <span id="page-83-10"></span>Doddington, G.R., Mitchell, A., Przybocki, M., Ramshaw, L., Strassel, S., Weischedel, R., 2004. The automatic content extraction (ACE) program– tasks, data, and evaluation, in: Proceedings of the Fourth International Conference on Language Resources and Evaluation, pp. 1–4.
- <span id="page-83-22"></span>Donnelly, K., et al., 2006. SNOMED-CT: The advanced terminology and coding system for ehealth. Studies in Health Technology and Informatics 121, 279.
- <span id="page-83-3"></span>Edmonds, P., Cotton, S., 2001. SensEval-2: Overview, in: Proceedings of SENSEVAL-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems, pp. 1–5.
- <span id="page-83-21"></span>Ehrmann, M., Romanello, M., Najem-Meyer, S., Doucet, A., Clematide, S., 2022. Extended overview of HIPE-2022: Named entity recognition and linking in multilingual historical documents, in: Faggioli, G., Ferro, N., Hanbury, A., Potthast, M. (Eds.), Proceedings of the Working Notes of CLEF 2022 - Conference and Labs of the Evaluation Forum, pp. 1038–1063.
- <span id="page-83-12"></span>Emami, A., Trischler, A., Suleman, K., Cheung, J.C.K., 2018. A generalized knowledge hunting framework for the Winograd schema challenge, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 25–31.
- <span id="page-83-31"></span>Fang, I.E., 1966. The "easy listening formula". Journal of Broadcasting & Electronic Media 11, 63–68.
- <span id="page-83-30"></span>Fang, S., Huang, Z., He, M., Tong, S., Huang, X., Liu, Y., Huang, J., Liu, Q., 2021. Guided attention network for concept extraction, in: Zhou, Z. (Ed.), Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, pp. 1449–1455.
- <span id="page-83-29"></span>Fang, Y., Zhang, Y., 2022. Data-efficient concept extraction from pre-trained language models for commonsense explanation generation, in: Goldberg, Y., Kozareva, Z., Zhang, Y. (Eds.), Findings of the Association for Computational Linguistics: EMNLP, pp. 5883–5893.
- <span id="page-83-7"></span>Farooq, U., Dhamala, T.P., Nongaillard, A., Ouzrout, Y., Qadir, M.A., 2015. A word sense disambiguation method for feature level sentiment analysis, in: 2015 9th International Conference on Software, Knowledge, Information Management and Applications, IEEE. pp. 1–8.
- <span id="page-83-17"></span>Fauconnier, G., Turner, M., 2008. The Way We Think: Conceptual Blending and the Mind's Hidden Complexities. Basic Books.
- <span id="page-83-24"></span>Fei, H., Ji, D., Li, B., Liu, Y., Ren, Y., Li, F., 2021. Rethinking boundaries: End-to-end recognition of discontinuous mentions with pointer networks, in: Proceedings of the AAAI conference on artificial intelligence, pp. 12785–12793.
- <span id="page-83-25"></span>Feng, Y., You, H., Zhang, Z., Ji, R., Gao, Y., 2019. Hypergraph neural networks, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 3558–3565.
- <span id="page-83-23"></span>Ferrucci, D., Lally, A., 2004. UIMA: An architectural approach to unstructured information processing in the corporate research environment. Natural Language Engineering 10, 327–348.

<span id="page-83-2"></span>Fillmore, C.J., et al., 2006. Frame semantics. Cognitive Linguistics: Basic Readings 34, 373–400.

- <span id="page-83-16"></span>Finkel, J.R., Grenager, T., Manning, C.D., 2005. Incorporating non-local information into information extraction systems by Gibbs sampling, in: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 363–370.
- <span id="page-83-26"></span>Finkel, J.R., Manning, C.D., 2009. Nested named entity recognition, in: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 141–150.

<span id="page-83-1"></span>Firth, J., 1957. A synopsis of linguistic theory, 1930-1955. Studies in Linguistic Analysis , 10–32.

- <span id="page-83-9"></span>Fligelstone, S., 1992. Developing a scheme for annotating text to show anaphoric relations. New Directions in English Language Corpora: Methodology, Results, Software Developments , 153–170.
- <span id="page-83-32"></span>Frenay, B., Verleysen, M., 2015. Reinforced extreme learning machines for fast robust regression in the presence of outliers. IEEE Transactions ´ on Cybernetics 46, 3351–3363.
- <span id="page-83-33"></span>Friedman, N., Murphy, K.P., Russell, S., 1998. Learning the structure of dynamic probabilistic networks, in: Cooper, G.F., Moral, S. (Eds.), UAI '98: Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann. pp. 139–147.
- <span id="page-83-27"></span>Fritzler, A., Logacheva, V., Kretov, M., 2019. Few-shot classification in named entity recognition task, in: Proceedings of the ACM Symposium on Applied Computing, pp. 993–1000.

<span id="page-84-0"></span>Fu, S., Chen, D., He, H., Liu, S., Moon, S., Peterson, K.J., Shen, F., Wang, L., Wang, Y., Wen, A., et al., 2020. Clinical concept extraction: a methodology review. Journal of Biomedical Informatics 109, 103526.

<span id="page-84-31"></span>Fuller, R., 1995. Neural Fuzzy Systems. Citeseer. ´

- <span id="page-84-34"></span>Galley, M., McKeown, K., Hirschberg, J., Shriberg, E., 2004. Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model pragmatic dependencies, in: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pp. 669–677.
- <span id="page-84-33"></span>Gandhi, A., Adhvaryu, K., Poria, S., Cambria, E., Hussain, A., 2023. Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions. Information Fusion 91, 424–444.
- <span id="page-84-22"></span>Gao, C., Wang, X., He, X., Li, Y., 2022. Graph neural networks for recommender system, in: Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pp. 1623–1625.
- <span id="page-84-26"></span>Gardenfors, P., 2004. Conceptual Spaces: The Geometry of Thought. MIT Press.
- <span id="page-84-6"></span>Garnham, A., 2001. Mental Models and the Interpretation of Anaphora. Psychology Press.
- <span id="page-84-29"></span>Ge, M., Mao, R., Cambria, E., 2022. Explainable metaphor identification inspired by conceptual metaphor theory. Proceedings of the AAAI Conference on Artificial Intelligence 36, 10681–10689.
- <span id="page-84-35"></span>Ge, M., Mao, R., Cambria, E., 2023. A survey on computational metaphor processing techniques: From identification, interpretation, generation to application. Artificial Intelligence Review doi:[10.1007/s10462-023-10564-7](http://dx.doi.org/10.1007/s10462-023-10564-7).
- <span id="page-84-15"></span>Ge, N., Hale, J., Charniak, E., 1998. A statistical approach to anaphora resolution, in: Sixth workshop on very large corpora, pp. 161–170.
- <span id="page-84-27"></span>Gehrmann, S., Dernoncourt, F., Li, Y., Carlson, E.T., Wu, J.T., Welt, J., Foote Jr, J., Moseley, E.T., Grant, D.W., Tyler, P.D., et al., 2018. Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives. PloS one 13, e0192360.
- <span id="page-84-12"></span>Ghaddar, A., Langlais, P., 2016. WikiCoref: An English coreference-annotated corpus of wikipedia articles, in: Proceedings of the Tenth International Conference on Language Resources and Evaluation, pp. 136–142.
- <span id="page-84-13"></span>Girardi, C., Speranza, M., Sprugnoli, R., Tonelli, S., 2014. Cromer: A tool for cross-document event and entity coreference, in: Ninth International Conference on Language Resources and Evaluation, pp. 3204–3208.
- <span id="page-84-32"></span>Gitari, N.D., Zuping, Z., Damien, H., Long, J., 2015. A lexicon-based approach for hate speech detection. International Journal of Multimedia and Ubiquitous Engineering 10, 215–230.
- <span id="page-84-9"></span><span id="page-84-1"></span>Givon, T., 1983. Topic continuity in discourse: The functional domain of switch reference. Switch reference and universal grammar 51, 82. ´
- <span id="page-84-5"></span>Goldberg, A., Suttle, L., 2010. Construction grammar. Wiley Interdisciplinary Reviews: Cognitive Science 1, 468–477.
- Gonzalo, J., Penas, A., Verdejo, F., 1999. Lexical ambiguity and information retrieval revisited, in: 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp. 195–202.
- <span id="page-84-4"></span>Gonzalo, J., Verdejo, F., Chugur, I., Cigarran, J., 1998. Indexing with WordNet synsets can improve text retrieval, in: Usage of WordNet in Natural Language Processing Systems, pp. 38–44.
- <span id="page-84-2"></span>Graves, A., Schmidhuber, J., 2005. Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks 18, 602–610.
- <span id="page-84-23"></span>Grishman, R., Sundheim, B.M., 1996. Message understanding conference-6: A brief history, in: proceeding of the 16th International Conference on Computational Linguistics, pp. 466–471.
- <span id="page-84-14"></span><span id="page-84-7"></span>Grosz, B.J., 1977. The Representation and Use of Focus in Dialogue Understanding. University of California, Berkeley.
- Grosz, B.J., Joshi, A.K., Weinstein, S., 1983. Providing a unified account of definite noun phrases in discourse, in: 21st Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics. pp. 44–50.
- <span id="page-84-8"></span>Grosz, B.J., Joshi, A.K., Weinstein, S., 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics 21, 203–225.
- <span id="page-84-28"></span>Gu, J., Lu, Z., Li, H., Li, V.O.K., 2016. Incorporating copying mechanism in sequence-to-sequence learning, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pp. 1631—-1640.
- <span id="page-84-21"></span>Guillou, L., 2012. Improving pronoun translation for statistical machine translation, in: Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pp. 1–10.
- <span id="page-84-10"></span>Gundel, J.K., Hedberg, N., Zacharski, R., 1993. Cognitive status and the form of referring expressions in discourse. Language , 274–307.
- <span id="page-84-25"></span>Gupta, P., Schutze, H., Andrassy, B., 2016. Table filling multi-task recurrent neural network for joint entity and relation extraction, in: Proceedings ¨ of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pp. 2537–2547.
- <span id="page-84-24"></span>Gurulingappa, H., Rajput, A.M., Roberts, A., Fluck, J., Hofmann-Apitius, M., Toldo, L., 2012. Development of a benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports. Journal of Biomedical Informatics 45, 885–892.
- <span id="page-84-3"></span>Hadiwinoto, C., Ng, H.T., Gan, W.C., 2019. Improved word sense disambiguation using pre-trained contextualized word representations, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5297–5306.
- <span id="page-84-17"></span>Haghighi, A., Klein, D., 2009. Simple coreference resolution with rich syntactic and semantic features, in: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 1152–1161.
- <span id="page-84-19"></span>Han, N.R., 2006. Korean zero pronouns: analysis and resolution. University of Pennsylvania.
- <span id="page-84-30"></span>Han, S., Mao, R., Cambria, E., 2022. Hierarchical attention network for explainable depression detection on Twitter aided by metaphor concept mappings, in: Proceedings of the 29th International Conference on Computational Linguistics, pp. 94–104.
- <span id="page-84-18"></span>Harabagiu, S.M., Bunescu, R.C., Maiorano, S.J., 2001. Text and knowledge mining for coreference resolution, in: Second Meeting of the North American Chapter of the Association for Computational Linguistics, pp. 1–8.
- <span id="page-84-16"></span>Harabagiu, S.M., Maiorano, S.J., 1999. Knowledge-lean coreference resolution and its relation to textual cohesion and coherence, in: The Relation of Discourse/Dialogue Structure and Reference, pp. 29–38.
- <span id="page-84-20"></span>Hardmeier, C., Federico, M., 2010. Modelling pronominal anaphora in statistical machine translation, in: IWSLT (International Workshop on Spoken Language Translation); Paris, France; December 2nd and 3rd, 2010., pp. 283–289.
- <span id="page-84-11"></span>Hasler, L., Orasan, C., Naumann, K., 2006. NPs for events: Experiments in coreference annotation, in: Proceedings of the Fifth International ˇ Conference on Language Resources and Evaluation (LREC'06), pp. 1167–1172.

- <span id="page-85-24"></span>Havasi, C., Speer, R., 2007. ConceptNet 3: A flexible, multilingual semantic network for common sense knowledge, in: Recent advances in natural language processing, pp. 27–29.
- <span id="page-85-15"></span><span id="page-85-1"></span>Haveliwala, T.H., 2002. Topic-sensitive PageRank, in: Proceedings of the 11th International Conference on World Wide Web, pp. 517–526. He, K., Hong, N., Lapalme-Remis, S., Lan, Y., Huang, M., Li, C., Yao, L., 2019a. Understanding the patient perspective of epilepsy treatment

through text mining of online patient support groups. Epilepsy & Behavior 94, 65–71.

- <span id="page-85-20"></span>He, K., Huang, Y., Mao, R., Gong, T., Li, C., Cambria, E., 2023. Virtual prompt pre-training for prototype-based few-shot relation extraction. Expert Systems With Applications 213, 118927.
- <span id="page-85-13"></span>He, K., Mao, B., Zhou, X., Li, Y., Gong, T., Li, C., Wu, J., 2022a. Knowledge enhanced coreference resolution via gated attention, in: 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), IEEE. pp. 2287–2293.
- <span id="page-85-23"></span>He, K., Mao, R., Gong, T., Cambria, E., Li, C., 2022b. JCBIE: a joint continual learning neural network for biomedical information extraction. BMC Bioinformatics 23, 1–20.
- <span id="page-85-28"></span>He, K., Mao, R., Gong, T., Li, C., Cambria, E., 2022c. Meta-based self-training and re-weighting for aspect-based sentiment analysis. IEEE Transactions on Affective Computing .
- <span id="page-85-21"></span>He, K., Wu, J., Ma, X., Zhang, C., Huang, M., Li, C., Yao, L., 2019b. Extracting kinship from obituary to enhance electronic health records for genetic research, in: Proceedings of the Fourth social media mining for health applications (# SMM4H) workshop & shared task, pp. 1–10.
- <span id="page-85-14"></span>He, K., Yao, L., Zhang, J., Li, Y., Li, C., et al., 2021. Construction of genealogical knowledge graphs from obituaries: Multitask neural network extraction system. Journal of Medical Internet Research 23, e25670.
- <span id="page-85-31"></span>He, Y., 2010. Bayesian models for sentence-level subjectivity detection. Technical Report. Knowledge Media Institute.
- <span id="page-85-6"></span>Heim, I.R., 1982. The semantics of definite and indefinite noun phrases. University of Massachusetts Amherst.
- <span id="page-85-29"></span>Helmi, M., AlModarresi, S.M.T., 2009. Human activity recognition using a fuzzy inference system, in: 2009 IEEE International Conference on Fuzzy Systems, pp. 1897–1902.
- <span id="page-85-10"></span>Hendrickx, I., Bouma, G., Coppens, F., Daelemans, W., Hoste, V., Kloosterman, G., Mineur, A.M., Van Der Vloet, J., Verschelde, J.L., 2008. A coreference corpus and resolution system for Dutch, in: Proceedings of the Sixth International Conference on Language Resources and Evaluation, pp. 1–6.
- <span id="page-85-25"></span>Henry, S., Buchan, K., Filannino, M., Stubbs, A., Uzuner, O., 2020. 2018 n2c2 shared task on adverse drug events and medication extraction in electronic health records. Journal of the American Medical Informatics Association 27, 3–12.
- <span id="page-85-17"></span>Herrero-Zazo, M., Segura-Bedmar, I., Mart´ınez, P., Declerck, T., 2013. The DDI corpus: An annotated corpus with pharmacological substances and drug-drug interactions. Journal of Biomedical Informatics 46, 914–920.
- <span id="page-85-34"></span>Hillard, D., Ostendorf, M., Shriberg, E., 2003. Detection of agreement vs. disagreement in meetings: Training with unlabeled data, in: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume of the Proceedings of HLT-NAACL 2003–short papers-Volume 2, pp. 34–36.
- <span id="page-85-30"></span>Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R., 2012. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 , 1–18.
- <span id="page-85-18"></span>Hirsch, J., Nicola, G., McGinty, G., Liu, R., Barr, R., Chittle, M., Manchikanti, L., 2016. ICD-10: history and context. American Journal of Neuroradiology 37, 596–599.
- <span id="page-85-5"></span>Hirschman, L., Chinchor, N., 1998. Appendix F: MUC-7 coreference task definition (version 3.0), in: Seventh Message Understanding Conference (MUC-7), pp. 1–17.
- <span id="page-85-4"></span>Hirschman, L., Robinson, P., Burger, J., Vilain, M., 1997. Automating coreference: The role of annotated training data, in: Proceedings of the AAAI Spring Symposium on Applying Machine Learning to Discourse Processing, pp. 118–121.
- <span id="page-85-8"></span>Hitzeman, J., Black, A.W., Taylor, P., Mellish, C., Oberlander, J., 1998. On the use of automatically generated discourse-level information in a concept-to-speech synthesis system, in: 5th International Conference on Spoken Language Processing, pp. 2763–2766.
- <span id="page-85-11"></span>Hobbs, J.R., 1978. Resolving pronoun references. Lingua 44, 311–338.
- <span id="page-85-12"></span>Hobbs, J.R., Stickel, M., Martin, P., Edwards, D., 1988. Interpretation as abduction, in: 26th Annual Meeting of the Association for Computational Linguistics, pp. 95–103.
- <span id="page-85-22"></span>Hohenecker, P., Mtumbuka, F., Kocijan, V., Lukasiewicz, T., 2020. Systematic comparison of neural architectures and training approaches for open information extraction, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 8554–8565.
- <span id="page-85-32"></span>Holland, J.H., 1992. Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence. MIT press.
- <span id="page-85-33"></span><span id="page-85-0"></span>Hornby, A.S., Cowie, A.P., 1974. Oxford advanced learner's dictionary of current english. Paperback, .
- Hosmer, D.W., Lemeshow, S., 2000. Applied Logistic Regression. 2 ed., Wiley.
- <span id="page-85-9"></span><span id="page-85-7"></span>Hou, Y., Markert, K., Strube, M., 2018. Unrestricted bridging resolution. Computational Linguistics 44, 237–284.
- Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., Weischedel, R., 2006. Ontonotes: the 90% solution, in: Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers, pp. 57–60.
- <span id="page-85-19"></span>Hu, J., Li, Z., Xu, B., 2016. An approach of ontology based knowledge base construction for chinese K12 education, in: 2016 First International Conference on Multimedia and Image Processing, pp. 83–88.
- <span id="page-85-3"></span>Huang, C.T.J., 1984. On the distribution and reference of empty pronouns. Linguistic Inquiry , 531–574.
- <span id="page-85-2"></span>Huang, L., Sun, C., Qiu, X., Huang, X.J., 2019a. GlossBERT: BERT for word sense disambiguation with gloss knowledge, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3509–3514.
- <span id="page-85-27"></span>Huang, L., Ye, Z., Qin, J., Lin, L., Liang, X., 2020. GRADE: Automatic graph-enhanced coherence metric for evaluating open-domain dialogue systems, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 9230–9240.
- <span id="page-85-26"></span>Huang, X., Liu, Q., Wang, C., Han, H., Ma, J., Chen, E., Su, Y., Wang, S., 2019b. Constructing educational concept maps with multiple relationships from multi-source data, in: Wang, J., Shim, K., Wu, X. (Eds.), 2019 IEEE International Conference on Data Mining, IEEE. pp. 1108–1113.
- <span id="page-85-16"></span>Huang, Y., He, K., Wang, Y., Zhang, X., Gong, T., Mao, R., Li, C., 2022a. COPNER: Contrastive learning with prompt guiding for few-shot named entity recognition, in: Proceedings of the 29th International Conference on Computational Linguistics, International Committee on

Computational Linguistics. pp. 2515–2527.

<span id="page-86-20"></span>Huang, Y., He, K., Wang, Y., Zhang, X., Gong, T., Mao, R., Li, C., 2022b. COPNER: contrastive learning with prompt guiding for few-shot named entity recognition, in: Proceedings of the 29th International Conference on Computational Linguistics, pp. 2515–2527.

<span id="page-86-25"></span>Huang, Z., Xu, W., Yu, K., 2015. Bidirectional LSTM-CRF models for sequence tagging. CoRR abs/1508.01991, 1–10.

<span id="page-86-27"></span>Hube, C., Fetahu, B., 2018. Detecting biased statements in Wikipedia, in: Companion Proceedings of the Web Conference 2018, pp. 1779–1786.

- <span id="page-86-32"></span>Hube, C., Fetahu, B., 2019. Neural based statement classification for biased language, in: Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pp. 195–203.
- <span id="page-86-24"></span>Hulth, A., 2003. Improved automatic keyword extraction given more linguistic knowledge, in: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 1–8.
- <span id="page-86-4"></span>Hung, C., Chen, S.J., 2016. Word sense disambiguation based sentiment lexicons for sentiment classification. Knowledge-Based Systems 110, 224–232.
- <span id="page-86-3"></span>Hung, C., Lin, H.K., 2013. Using objective words in SentiWordNet to improve word-of-mouth sentiment classification. IEEE Intelligent Systems 28, 47–54.

<span id="page-86-29"></span>Huo, H., Iwaihara, M., 2020. Utilizing BERT pretrained models with various fine-tune methods for subjectivity detection, in: Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data, Springer. pp. 270–284.

<span id="page-86-2"></span>Iacobacci, I., Pilehvar, M.T., Navigli, R., 2016. Embeddings for word sense disambiguation: An evaluation study, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 897–907.

<span id="page-86-12"></span>Iida, R., Inui, K., Matsumoto, Y., 2007a. Zero-anaphora resolution by learning rich syntactic pattern features. ACM Transactions on Asian Language Information Processing (TALIP) 6, 1–22.

<span id="page-86-8"></span>Iida, R., Komachi, M., Inui, K., Matsumoto, Y., 2007b. Annotating a Japanese text corpus with predicate-argument and coreference relations, in: Proceedings of the Linguistic Annotation Workshop, pp. 132–139.

<span id="page-86-14"></span>Iida, R., Poesio, M., 2011. A cross-lingual ilp solution to zero anaphora resolution, in: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 804–813.

<span id="page-86-22"></span>Iovine, A., Narducci, F., Semeraro, G., 2020. Conversational recommender systems and natural language:: A study through the converse framework. Decision Support Systems 131, 113250.

<span id="page-86-13"></span>Isozaki, H., Hirao, T., 2003. Japanese zero pronoun resolution based on ranking rules and machine learning, in: Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pp. 184–191.

- <span id="page-86-15"></span><span id="page-86-0"></span>Jackendoff, R., 1976. Toward an explanatory semantic representation. Linguistic Inquiry 7, 89–150.
- Jakob, N., Gurevych, I., 2010. Using anaphora resolution to improve opinion target identification in movie reviews, in: Annual Meeting of the Association for Computational Linguistics, pp. 263–268.

<span id="page-86-21"></span>Jiang, T., Zeng, Q., Zhao, T., Qin, B., Liu, T., Chawla, N.V., Jiang, M., 2020. Biomedical knowledge graphs construction from conditional statements. IEEE/ACM Transactions on Computational Biology and Bioinformatics 18, 823–835.

<span id="page-86-17"></span>Jinjie, N., Vlad, P., Tom, Y., Haicang, Z., Erik, C., 2022. HiTKG: Towards goal-oriented conversations via multi-hierarchy learning, in: AAAI Conference on Artificial Intelligence, pp. 11112–11120.

<span id="page-86-18"></span>Johnson, A.E., Pollard, T.J., Shen, L., Lehman, L.w.H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi, L., Mark, R.G., 2016. MIMIC-III, a freely accessible critical care database. Scientific data 3, 1–9.

<span id="page-86-6"></span>Joshi, A., Prasad, R., Miltsakaki, E., 2006. Anaphora resolution: Centering theory approach. Encyclopedia of Language & Linguistics Vol. 1, 223–230.

<span id="page-86-5"></span>Joshi, A.K., Kuhn, S., 1979. Centered logic: The role of entity centered sentence representation in natural language inferencing, in: Proceedings of the 6th international joint conference on Artificial intelligence-Volume 1, pp. 435–439.

<span id="page-86-10"></span>Joshi, M., Chen, D., Liu, Y., Weld, D.S., Zettlemoyer, L., Levy, O., 2020. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics 8, 64–77.

- <span id="page-86-16"></span>Joshi, M., Levy, O., Zettlemoyer, L., Weld, D., 2019. BERT for coreference resolution: Baselines and analysis, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5803–5808.
- <span id="page-86-31"></span>Joulin, A., Grave, E., Bojanowski, P., Mikolov, T., 2017. Bag of tricks for efficient text classification, in: Lapata, M., Blunsom, P., Koller, A. (Eds.), Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Volume 2: Short Papers, Association for Computational Linguistics. pp. 427–431.
- <span id="page-86-1"></span>Kågeback, M., Salomonsson, H., 2016. Word sense disambiguation using a bidirectional LSTM, in: Proceedings of the 5th Workshop on Cognitive ¨ Aspects of the Lexicon (CogALex-V), pp. 51–56.
- <span id="page-86-28"></span>Kalchbrenner, N., Grefenstette, E., Blunsom, P., 2014. A convolutional neural network for modelling sentences, in: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 655–665.
- <span id="page-86-33"></span>Kamal, A., 2013. Subjectivity classification using machine learning techniques for mining feature-opinion pairs from web opinion sources. International Journal of Computer Science Issues 10, 191.

<span id="page-86-11"></span>Kameyama, M., 1985. Zero Anaphora: The Case of Japanese (Discourse Aanalysis, Pronouns, Sytax, Computational Llinguistics, Typology). Stanford University.

<span id="page-86-9"></span>Kameyama, M., 1997. Recognizing referential links: an information extraction prespective, in: Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts, pp. 46–53.

<span id="page-86-26"></span>Kamil, A.z., Rustamov, S., Clements, M.A., Mustafayev, E., 2018. Adaptive neuro-fuzzy inference system for classification of texts, in: Recent Developments and the New Direction in Soft-Computing Foundations and Applications. Springer, pp. 63–70.

<span id="page-86-7"></span>Kamp, H., Reyle, U., 2013. From discourse to logic: Introduction to modeltheoretic semantics of natural language, formal logic and discourse representation theory. volume 42. Springer Science & Business Media.

<span id="page-86-30"></span>Karimi, S., Shakery, A., 2017. A language-model-based approach for subjectivity detection. Journal of Information Science 43, 356–377.

<span id="page-86-23"></span>Kartik Detroja, C.K. Bhensdadia, B.S.B., 2023. A survey on relation extraction. Intelligent Systems with Applications .

<span id="page-86-19"></span>Katiyar, A., Cardie, C., 2018. Nested named entity recognition revisited, in: 2018 Conference of the North American Chapter of the Association

for Computational Linguistics: Human Language Technologies, pp. 861–871.

<span id="page-87-19"></span>Kaya, M., Bilge, H.S¸., 2019. Deep metric learning: A survey. Symmetry 11, 1066.

<span id="page-87-4"></span>Kehler, A., 1997. Current theories of centering for pronoun interpretation: A critical evaluation. Computational linguistics 23, 467–475.

<span id="page-87-31"></span>Keshavarz, H., Saniee Abadeh, M., 2018. MHSubLex: Using metaheuristic methods for subjectivity classification of microblogs. Journal of AI and data mining 6, 341–353.

<span id="page-87-30"></span>Khatua, A., Cambria, E., Ho, S.S., Na, J.C., 2020. Deciphering public opinion of nuclear energy on Twitter, in: 2020 International Joint Conference on Neural Networks, pp. 1–8.

<span id="page-87-32"></span>Kilgarriff, A., Palmer, M., 2000. Introduction to the special issue on senseval. Computers and the Humanities 34, 1–13.

<span id="page-87-21"></span>Kim, A., Song, H.J., Park, S.B., et al., 2018. A two-step neural dialog state tracker for task-oriented dialog processing. Computational Intelligence and Neuroscience 2018, 1–12.

<span id="page-87-6"></span>Kim, J.D., Ohta, T., Tateisi, Y., Tsujii, J., 2003. GENIA corpus—a semantically annotated corpus for bio-textmining. Bioinformatics 19, i180–i182.

<span id="page-87-18"></span>Kim, J.D., Son, J., Baik, D.K., 2012. CA 5W1H onto: ontological context-aware model based on 5W1H. International Journal of Distributed Sensor Networks 8, 247346. Kim, S.B., Seo, H.C., Rim, H.C., 2004. Information retrieval using word senses: root sense tagging approach, in: Proceedings of the 27th Annual

<span id="page-87-3"></span>International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 258–265.

<span id="page-87-29"></span>Kim, S.M., Hovy, E., 2005. Automatic detection of opinion bearing words and sentences, in: Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts, pp. 61–66.

<span id="page-87-36"></span>Kim, S.M., Hovy, E., 2006. Identifying and analyzing judgment opinions, in: Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pp. 200–207.

<span id="page-87-24"></span>Kim, S.N., Medelyan, O., Kan, M., Baldwin, T., 2010. Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles, in: Erk, K., Strapparava, C. (Eds.), Proceedings of the 5th International Workshop on Semantic Evaluation, pp. 21–26.

<span id="page-87-33"></span><span id="page-87-12"></span>Kim, Y., 2014. Convolutional neural networks for sentence classification.

Kirstain, Y., Ram, O., Levy, O., 2021. Coreference resolution without span representations, in: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 14–19.

<span id="page-87-8"></span>Klie, J.C., Bugert, M., Boullosa, B., Eckart de Castilho, R., Gurevych, I., 2018. The INCEpTION platform: Machine-assisted and knowledgeoriented interactive annotation, in: Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pp. 5–9.

<span id="page-87-11"></span>Kocijan, V., Camburu, O.M., Cretu, A.M., Yordanov, Y., Blunsom, P., Lukasiewicz, T., 2019. WikiCREM: A large unsupervised corpus for coreference resolution, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4303–4312.

<span id="page-87-22"></span>Kolluru, K., Adlakha, V., Aggarwal, S., Chakrabarti, S., et al., 2020. OpenIE6: Iterative grid labeling and coordination analysis for open information extraction, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 3748–3761.

<span id="page-87-13"></span>Kong, F., Zhou, G., 2010. A tree kernel-based unified framework for Chinese zero anaphora resolution, in: Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 882–891.

<span id="page-87-17"></span>Krallinger, M., Rabal, O., Lourenc¸o, A., Oyarzabal, J., Valencia, A., 2017. Information Retrieval and Text Mining Technologies for Chemistry. Chemical Reviews 117, 7673–7761.

<span id="page-87-23"></span>Krapivin, M., Autaeu, A., Marchese, M., 2009. Large dataset for keyphrases extraction. Technical Report. University of Trento.

<span id="page-87-15"></span><span id="page-87-2"></span>Kripke, S.A., 1972. Naming and necessity, in: Semantics of natural language. Springer, pp. 253–355.

Krovetz, R., Croft, W.B., 1992. Lexical ambiguity and information retrieval. ACM Transactions on Information Systems (TOIS) 10, 115–141.

<span id="page-87-7"></span>Kubler, S., Zhekova, D., 2011. Singletons and coreference resolution evaluation, in: Proceedings of the International Conference Recent Advances ¨ in Natural Language Processing 2011, pp. 261–267.

<span id="page-87-20"></span>Kulis, B., et al., 2013. Metric learning: A survey. Foundations and Trends® in Machine Learning 5, 287–364.

<span id="page-87-0"></span>Kumar, S., Jat, S., Saxena, K., Talukdar, P., 2019. Zero-shot word sense disambiguation using sense definition embeddings, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5670–5681.

<span id="page-87-34"></span>Lafferty, J., Zhai, C., 2001. Document language models, query models, and risk minimization for information retrieval, in: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 111–119.

<span id="page-87-25"></span>Lafferty, J.D., McCallum, A., Pereira, F.C.N., 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data, in: Brodley, C.E., Danyluk, A.P. (Eds.), Proceedings of the Eighteenth International Conference on Machine Learning, pp. 282–289.

<span id="page-87-26"></span>Lakoff, G., Johnson, M., 1980. Metaphors We Live by. University of Chicago press.

<span id="page-87-27"></span>Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R., 2020. Albert: A lite bert for self-supervised learning of language representations, in: International Conference on Learning Representations, pp. 1–17.

<span id="page-87-28"></span>Lange, L., Adel, H., Strotgen, J., 2020. Closing the gap: Joint de-identification and concept extraction in the clinical domain, in: Jurafsky, D., Chai, ¨ J., Schluter, N., Tetreault, J.R. (Eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics. pp. 6945–6952.

<span id="page-87-16"></span>LaPorte, J., 2006. Rigid designators for properties. Philosophical Studies 130, 321–336.

<span id="page-87-9"></span><span id="page-87-5"></span>Lappin, S., Leass, H.J., 1994a. An algorithm for pronominal anaphora resolution. Computational Linguistics 20, 535–561.

Lappin, S., Leass, H.J., 1994b. An algorithm for pronominal anaphora resolution. Computational Linguistics 20, 535–561.

<span id="page-87-35"></span>Largeron, C., Moulin, C., Gery, M., 2011. Entropy based feature selection for text categorization, in: Proceedings of the 2011 ACM Symposium ´ on Applied Computing, pp. 924–928.

<span id="page-87-10"></span>Lata, K., Singh, P., Dutta, K., 2022. Mention detection in coreference resolution: survey. Applied Intelligence , 1–45.

- <span id="page-87-1"></span>Le, M., Postma, M., Urbani, J., Vossen, P., 2018. A deep dive into word sense disambiguation with lstm, in: Proceedings of the 27th International Conference on Computational Linguistics, pp. 354–365.
- <span id="page-87-14"></span>Le, T.T., Vo, T.H., Mai, D.T., Quan, T.T., Phan, T.T., 2016. Sentiment analysis using anaphoric coreference resolution and ontology inference, in: International Workshop on Multi-disciplinary Trends in Artificial Intelligence, pp. 297–303.

- <span id="page-88-10"></span>Le Nagard, R., Koehn, P., 2010. Aiding pronoun translation with co-reference resolution, in: Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pp. 252–261.
- <span id="page-88-1"></span>Leacock, C., Towell, G., Voorhees, E.M., 1993. Corpus-based statistical sense resolution, in: Human Language Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993, pp. 260–265.
- <span id="page-88-5"></span>Lee, H., Chang, A., Peirsman, Y., Chambers, N., Surdeanu, M., Jurafsky, D., 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics 39, 885–916.
- <span id="page-88-7"></span>Lee, H., Surdeanu, M., Jurafsky, D., 2017a. A scaffolding approach to coreference resolution integrating statistical and rule-based models. Natural Language Engineering 23, 733–762.
- <span id="page-88-8"></span>Lee, K., He, L., Lewis, M., Zettlemoyer, L., 2017b. End-to-end neural coreference resolution, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 188–197.
- <span id="page-88-9"></span>Lee, K., He, L., Zettlemoyer, L., 2018. Higher-order coreference resolution with coarse-to-fine inference, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 687–692.
- <span id="page-88-2"></span>Lesk, M., 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone, in: Proceedings of the 5th Annual International Conference on Systems Documentation, pp. 24–26.
- <span id="page-88-4"></span>Levesque, H., Davis, E., Morgenstern, L., 2012. The Winograd schema challenge, in: Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, pp. 552–561.
- <span id="page-88-0"></span>Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., Zettlemoyer, L., 2020. BART: Denoising sequenceto-sequence pre-training for natural language generation, translation, and comprehension, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880.
- <span id="page-88-28"></span>Li, B., Liu, Y., Agichtein, E., 2008a. CoCQA: Co-training over questions and answers with an application to predicting question subjectivity orientation, in: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pp. 937–946.
- <span id="page-88-31"></span>Li, B., Liu, Y., Ram, A., Garcia, E.V., Agichtein, E., 2008b. Exploring question subjectivity prediction in community qa, in: Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 735–736.
- <span id="page-88-16"></span>Li, C., Donizelli, M., Rodriguez, N., Dharuri, H., Endler, L., Chelliah, V., Li, L., He, E., Henry, A., Stefan, M.I., et al., 2010. BioModels database: An enhanced, curated and annotated resource for published quantitative kinetic models. BMC Systems Biology 4, 1–14.
- <span id="page-88-12"></span>Li, C., Xu, X., Zhou, G., He, K., Qi, T., Zhang, W., Tian, F., Zheng, Q., Hu, J., et al., 2019. Implementation of national health informatization in china: survey about the status quo. JMIR Medical Informatics 7, e12238.
- <span id="page-88-21"></span>Li, F., Zhang, M., Fu, G., Ji, D., 2017a. A neural joint model for entity and relation extraction from biomedical text. BMC Bioinformatics 18, 1–11.
- <span id="page-88-13"></span>Li, J., Sun, A., Han, J., Li, C., 2020a. A survey on deep learning for named entity recognition. IEEE Transactions on Knowledge and Data Engineering 34, 50–70.
- <span id="page-88-22"></span><span id="page-88-19"></span>Li, K., Zha, H., Su, Y., Yan, X., 2018. Concept mining via embedding, in: IEEE International Conference on Data Mining, pp. 267–276.
- Li, L., Wang, P., Yan, J., Wang, Y., Li, S., Jiang, J., Sun, Z., Tang, B., Chang, T.H., Wang, S., et al., 2020b. Real-world data medical knowledge graph: construction and applications. Artificial Intelligence in Medicine 103, 101817.
- <span id="page-88-23"></span>Li, P., Huang, H., 2016. UTA DLNLP at semeval-2016 task 12: Deep learning based natural language processing system for clinical information identification from clinical notes and pathology reports, in: Bethard, S., Cer, D.M., Carpuat, M., Jurgens, D., Nakov, P., Zesch, T. (Eds.), Proceedings of the 10th International Workshop on Semantic Evaluation, pp. 1268–1273.
- <span id="page-88-24"></span>Li, W., Zhu, L., Mao, R., Cambria, E., 2023. SKIER: A symbolic knowledge integrated model for conversational emotion recognition. Proceedings of the AAAI Conference on Artificial Intelligence , 13121–13129.
- <span id="page-88-20"></span>Li, X., Chen, Y.N., Li, L., Gao, J., Celikyilmaz, A., 2017b. End-to-end task-completion neural dialogue systems, in: Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 733–743.
- <span id="page-88-17"></span>Li, X., Feng, J., Meng, Y., Han, Q., Wu, F., Li, J., 2020c. A unified MRC framework for named entity recognition, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5849–5859.
- <span id="page-88-11"></span>Li, Y., Ma, X., Zhou, X., Cheng, P., He, K., Li, C., 2021. Knowledge enhanced lstm for coreference resolution on biomedical texts. Bioinformatics 37, 2699–2705.
- <span id="page-88-6"></span>Liang, T., Wu, D.S., 2003. Automatic pronominal anaphora resolution in English texts, in: Proceedings of Research on Computational Linguistics Conference, pp. 111–127.
- <span id="page-88-30"></span>Lin, C., He, Y., Everson, R., 2011. Sentence subjectivity detection with weakly-supervised learning, in: Proceedings of 5th International Joint Conference on Natural Language Processing, pp. 1153–1161.
- <span id="page-88-15"></span>Lipscomb, C.E., 2000. Medical subject headings (MeSH). Bulletin of the Medical Library Association 88, 265.
- <span id="page-88-29"></span>Litkowski, K., 2004. SensEval-3 task: Word sense disambiguation of wordnet glosses, in: Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pp. 13–16.
- <span id="page-88-27"></span>Liu, B., et al., 2010. Sentiment analysis and subjectivity. Handbook of Natural Language Processing 2, 627–666.
- <span id="page-88-26"></span>Liu, C.W., Lowe, R., Serban, I.V., Noseworthy, M., Charlin, L., Pineau, J., 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation, in: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2122–2132.
- <span id="page-88-3"></span>Liu, F., Lu, H., Neubig, G., 2018a. Handling homographs in neural machine translation, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Association for Computational Linguistics. pp. 1336–1345.
- <span id="page-88-14"></span>Liu, P., Guo, Y., Wang, F., Li, G., 2022a. Chinese named entity recognition: The state of the art. Neurocomputing 473, 37–53.
- <span id="page-88-25"></span>Liu, Q., Huang, Z., Huang, Z., Liu, C., Chen, E., Su, Y., Hu, G., 2018b. Finding similar exercises in online education systems, in: Guo, Y., Farooq, F. (Eds.), Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1821–1830.
- <span id="page-88-18"></span>Liu, Q., Mao, R., Geng, X., Cambria, E., 2023a. Semantic matching in machine reading comprehension: An empirical study. Information Processing & Management 60, 103145.

- <span id="page-89-13"></span>Liu, R., Chen, G., Mao, R., Cambria, E., 2023b. A multi-task learning model for gold-two-mention co-reference resolution, in: 2023 International Joint Conference on Neural Networks, pp. 1–8.
- <span id="page-89-15"></span><span id="page-89-7"></span>Liu, R., Mao, R., Luu, A.T., Cambria, E., 2023c. A brief survey on recent advances in coreference resolution. Artificial Intelligence Review , 1–43. Liu, T., Cui, Y., Yin, Q., Zhang, W.N., Wang, S., Hu, G., 2017a. Generating and exploiting large-scale pseudo training data for zero pronoun
- resolution, in: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 102– 111.
- <span id="page-89-31"></span>Liu, X., He, P., Chen, W., Gao, J., 2019a. Multi-task deep neural networks for natural language understanding, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4487–4496.
- <span id="page-89-23"></span>Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., Tang, J., 2022b. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks, in: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 61–68.
- <span id="page-89-20"></span>Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., Tang, J., 2021a. GPT understands, too. arXiv preprint arXiv:2103.10385 .
- <span id="page-89-0"></span>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V., 2019b. RoBERTa: A robustly optimized BERT pretraining approach. arXiv e-prints , arXiv–1907.
- <span id="page-89-25"></span>Liu, Y., Wu, H., Huang, Z., Wang, H., Ma, J., Liu, Q., Chen, E., Tao, H., Rui, K., 2020. Technical phrase extraction for patent mining: A multi-level approach, in: 20th IEEE International Conference on Data Mining, pp. 1142–1147.
- <span id="page-89-16"></span>Liu, Z., Shi, K., Chen, N., 2021b. Coreference-aware dialogue summarization, in: Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pp. 509–519.
- <span id="page-89-27"></span>Liu, Z., Yang, M., Wang, X., Chen, Q., Tang, B., Wang, Z., Xu, H., 2017b. Entity recognition from clinical texts via recurrent neural network. BMC Medical Informatics Decis. Mak. 17, 53–61.
- <span id="page-89-33"></span><span id="page-89-28"></span>Lohr, G., 2022. What are abstract concepts? on lexical ambiguity and concreteness ratings. Review of Philosophy and Psychology 13, 549–566. ¨
- Lopez, A., 2008. Statistical machine translation. ACM Computing Surveys 40, 1–49.
- <span id="page-89-3"></span>Loureiro, D., Jorge, A., 2019. Language modelling makes sense: Propagating representations through wordnet for full-coverage word sense disambiguation, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5682–5691.
- <span id="page-89-14"></span>Luo, H., Glass, J., 2018. Learning word representations with cross-sentence dependency for end-to-end co-reference resolution, in: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4829–4833.
- <span id="page-89-9"></span>Luo, X., 2005. On coreference resolution performance metrics, in: Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pp. 25–32.
- <span id="page-89-10"></span>Luo, X., Pradhan, S., 2016. Evaluation metrics, in: Anaphora Resolution. Springer, pp. 141–163.
- <span id="page-89-22"></span>Ma, R., Zhou, X., Gui, T., Tan, Y., Li, L., Zhang, Q., Huang, X., 2022a. Template-free prompt tuning for few-shot NER, in: Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5721–5732.
- <span id="page-89-24"></span>Ma, Y., Hiraoka, T., Okazaki, N., 2022b. Named entity recognition and relation extraction using enhanced table filling by contextualized representations. Journal of Natural Language Processing 29, 187–223.
- <span id="page-89-30"></span>Maas, A., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y., Potts, C., 2011. Learning word vectors for sentiment analysis, in: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142–150.
- <span id="page-89-18"></span>Mao, B., Jia, C., Huang, Y., He, K., Wu, J., Gong, T., Li, C., 2022a. Uncertainty-guided mutual consistency training for semi-supervised biomedical relation extraction, in: 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), IEEE. pp. 2318–2325.
- <span id="page-89-34"></span>Mao, R., Chen, G., Zhang, X., Guerin, F., Cambria, E., 2023a. GPTEval: A survey on assessments of ChatGPT and GPT-4. arXiv:2308.12488 .
- <span id="page-89-32"></span>Mao, R., Li, X., 2021. Bridging towers of multi-task learning with a gating mechanism for aspect-based sentiment analysis and sequential metaphor identification. Proceedings of the AAAI Conference on Artificial Intelligence 35, 13534–13542.
- <span id="page-89-6"></span>Mao, R., Li, X., Ge, M., Cambria, E., 2022b. MetaPro: A computational metaphor processing model for text pre-processing. Information Fusion 86-87, 30–43.
- <span id="page-89-26"></span>Mao, R., Li, X., He, K., Ge, M., Cambria, E., 2023b. MetaPro Online: A computational metaphor processing online system, in: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 127–135.
- <span id="page-89-5"></span>Mao, R., Lin, C., Guerin, F., 2018. Word embedding and WordNet based metaphor identification and interpretation, in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL), Association for Computational Linguistics. pp. 1222–1231.
- <span id="page-89-29"></span>Mao, R., Lin, C., Guerin, F., 2019. End-to-end sequential metaphor identification inspired by linguistic theories, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 3888–3898.
- <span id="page-89-21"></span>Mao, R., Liu, Q., He, K., Li, W., Cambria, E., 2022c. The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection. IEEE Transactions on Affective Computing , 1–11.
- <span id="page-89-19"></span>Marrero, M., Urbano, J., Sanchez-Cuadrado, S., Morato, J., G ´ omez-Berb ´ ´ıs, J.M., 2013. Named entity recognition: fallacies, challenges and opportunities. Computer Standards & Interfaces 35, 482–489.
- <span id="page-89-8"></span>Martin, S., 2015. The role of salience ranking in anaphora resolution.
- <span id="page-89-17"></span>Martschat, S., Strube, M., 2014. Recall error analysis for coreference resolution, in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2070–2081.
- <span id="page-89-2"></span>Maru, M., Scozzafava, F., Martelli, F., Navigli, R., 2019. Syntagnet: Challenging supervised word sense disambiguation with lexical-semantic combinations, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3534–3540.
- <span id="page-89-4"></span>Marvin, R., Koehn, P., 2018. Exploring word sense disambiguation abilities of neural machine translation systems, in: Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track), Association for Machine Translation in the Americas. pp. 125–131.
- <span id="page-89-1"></span>Mayor, M., 2009. Longman dictionary of contemporary English. Pearson Education India.
- <span id="page-89-12"></span>McCallum, A., Wellner, B., 2003. Object consolidation by graph partitioning with a conditionally-trained distance metric, in: KDD Workshop on Data Cleaning, Record Linkage and Object Consolidation, pp. 1–6.
- <span id="page-89-11"></span>McCallum, A., Wellner, B., 2004. Conditional models of identity uncertainty with application to noun coreference. Advances in neural information processing systems 17, 1–8.

- <span id="page-90-12"></span>McCarthy, J.F., Lehnert, W.G., 1995. Using decision trees for conference resolution, in: Proceedings of the 14th international joint conference on Artificial intelligence-Volume 2, pp. 1050–1055.
- <span id="page-90-31"></span>McCowan, I., Carletta, J., Kraaij, W., Ashby, S., Bourban, S., Flynn, M., Guillemot, M., Hain, T., Kadlec, J., Karaiskos, V., et al., 2005. The AMI meeting corpus, in: Proceedings of the 5th International Conference on Methods and Techniques in Behavioral Research, p. 100.
- <span id="page-90-8"></span>McCoy, K.E., Strube, M., 1999. Generating anaphoric expressions: pronoun or definite description? The Relation of Discourse/Dialogue Structure and Reference .
- <span id="page-90-26"></span>Medin, D.L., Schaffer, M.M., 1978. Context theory of classification learning. Psychological Review 85, 207.
- <span id="page-90-27"></span>Meng, R., Zhao, S., Han, S., He, D., Brusilovsky, P., Chi, Y., 2017. Deep keyphrase generation, in: Barzilay, R., Kan, M. (Eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pp. 582–592.
- <span id="page-90-15"></span>Miculicich, L.M., Popescu-Belis, A., 2017. Using coreference links to improve Spanish-to-English machine translation, in: Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017), pp. 30–40.
- <span id="page-90-28"></span>Mihalcea, R., Banea, C., Wiebe, J., 2007. Learning multilingual subjective language via cross-lingual projections, in: Proceedings of the 45th annual meeting of the association of computational linguistics, pp. 976–983.
- <span id="page-90-1"></span>Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J., 2013. Distributed representations of words and phrases and their compositionality. Advances in Neural Information Processing Systems 26.
- <span id="page-90-10"></span>Miller, G.A., 1998. WordNet: An electronic lexical database. MIT press.
- <span id="page-90-4"></span>Miller, G.A., Beckwith, R., Fellbaum, C., Gross, D., Miller, K.J., 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography 3, 235–244.
- <span id="page-90-2"></span>Miller, G.A., Leacock, C., Tengi, R., Bunker, R.T., 1993. A semantic concordance, in: Human Language Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993, pp. 303–308.
- <span id="page-90-19"></span>van Miltenburg, E., Lu, W.T., Krahmer, E., Gatt, A., Chen, G., Li, L., van Deemter, K., 2020. Gradations of error severity in automatic image descriptions, in: Proceedings of the 13th International Conference on Natural Language Generation, pp. 398–411.
- <span id="page-90-25"></span>Miner, G., Elder IV, J., Fast, A., Hill, T., Nisbet, R., Delen, D., 2012. Practical text mining and statistical analysis for non-structured text data applications. Academic Press.
- <span id="page-90-18"></span>Mirkin, S., Dagan, I., Pado, S., 2010. Assessing the role of discourse references in entailment inference, in: Proceedings of the 48th Annual ´ Meeting of the Association for Computational Linguistics, pp. 1209–1219.
- <span id="page-90-7"></span>Mitkov, R., 2014. Anaphora Resolution. Routledge.
- <span id="page-90-17"></span><span id="page-90-6"></span>Mitkov, R., 2022. The Oxford Handbook of Computational Linguistics. Oxford University Press.
- Mitkov, R., Evans, R., Orasan, C., Ha, L.A., Pekar, V., 2007. Anaphora resolution: To what extent does it help nlp applications?, in: Anaphora: ˘ Analysis, Algorithms and Applications: 6th Discourse Anaphora and Anaphor Resolution Colloquium, pp. 179–190.
- <span id="page-90-21"></span>Miwa, M., Bansal, M., 2016. End-to-end relation extraction using lstms on sequences and tree structures, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1105–1116.
- <span id="page-90-22"></span>Miwa, M., Sasaki, Y., 2014. Modeling joint entity and relation extraction with table representation, in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pp. 1858–1869.
- <span id="page-90-32"></span>Mogadala, A., Varma, V., 2012. Language independent sentence-level subjectivity analysis with feature selection, in: Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation, pp. 171–180.
- <span id="page-90-23"></span>Monaikul, N., Castellucci, G., Filice, S., Rokhlenko, O., 2021. Continual learning for named entity recognition, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 13570–13577.
- <span id="page-90-24"></span>Montgomery, C.A., 1982. Concept extraction. American Journal of Computational Linguistics 8, 70–73.
- <span id="page-90-0"></span>Montoyo, A., Mart´ınez-Barco, P., Balahur, A., 2012. Subjectivity and sentiment analysis: An overview of the current state of the area and envisaged developments. Decision Support Systems 53, 675–679.
- <span id="page-90-11"></span>Moosavi, N.S., Strube, M., 2016. Which coreference evaluation metric do you trust? a proposal for a link-based entity aware metric, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 632–642.
- <span id="page-90-30"></span>Morency, L.P., Mihalcea, R., Doshi, P., 2011. Towards multimodal sentiment analysis: Harvesting opinions from the web, in: Proceedings of the 13th International Conference on Multimodal Interfaces, pp. 169–176.
- <span id="page-90-33"></span>Moro, A., Cecconi, F., Navigli, R., 2014a. Multilingual word sense disambiguation and entity linking for everybody, in: Proceedings of the 2014 International Conference on Posters & Demonstrations Track-Volume 1272, pp. 25–28.
- <span id="page-90-3"></span>Moro, A., Navigli, R., 2015. SemEval-2015 task 13: Multilingual all-words sense disambiguation and entity linking, in: Proceedings of the 9th International Workshop on Semantic Evaluation, pp. 288–297.
- <span id="page-90-5"></span>Moro, A., Raganato, A., Navigli, R., 2014b. Entity linking meets word sense disambiguation: a unified approach. Transactions of the Association for Computational Linguistics 2, 231–244.
- <span id="page-90-20"></span>Muis, A.O., Lu, W., 2017. Labeling gaps between words: Recognizing overlapping mentions with mention separators, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2608–2618.
- <span id="page-90-34"></span>Murray, G., Carenini, G., 2009. Predicting subjectivity in multimodal conversations, in: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 1348–1357.
- <span id="page-90-29"></span><span id="page-90-9"></span>Murray, G., Carenini, G., 2011. Subjectivity detection in spoken and written conversations. Natural Language Engineering 17, 397–418.
- Muzerelle, J., Lefeuvre, A., Antoine, J.Y., Schang, E., Maurel, D., Villaneau, J., Eshkol, I., 2013. ANCOR, the first large French speaking corpus of conversational speech annotated in coreference to be freely available (ANCOR, premier corpus de franc¸ais parle d'envergure annot ´ e en ´ coref´ erence et distribu ´ e librement) [in French], in: Proceedings of TALN 2013 (Volume 2: Short Papers), pp. 555–563. ´
- <span id="page-90-16"></span>Nakaiwa, H., Ikehara, S., 1992. Zero pronoun resolution in a machine translation system by using japanese to english verbal semantic attributes., in: Third Conference on Applied Natural Language Processing, pp. 201–208.
- <span id="page-90-14"></span>Nakaiwa, H., Shirai, S., 1996. Anaphora resolution of Japanese zero pronouns with deictic reference, in: The 16th International Conference on Computational Linguistics, pp. 812–817.
- <span id="page-90-13"></span>Nakaiwa, H., Shirai, S., Ikehara, S., Kawaoka, T., 1995. Extrasentential resolution of japanese zero pronouns using semantic and pragmatic constraints, in: Proceedings of the AAAI 1995 Spring Symposium Series: Empirical Methods in Discourse Interpretation and Generation, pp.

99–105.

- <span id="page-91-30"></span>Nakov, P., Rosenthal, S., Kozareva, Z., Stoyanov, V., Ritter, A., Wilson, T., 2013. SemEval-2013 task 2: Sentiment analysis in Twitter, in: Second Joint Conference on Lexical and Computational Semantics (\*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pp. 312–320.
- <span id="page-91-26"></span>Nasar, Z., Jaffry, S.W., Malik, M.K., 2021. Named entity recognition and relation extraction: State-of-the-art. ACM Computing Surveys (CSUR) 54, 1–39.
- <span id="page-91-8"></span>Nassirtoussi, A.K., Aghabozorgi, S., Wah, T.Y., Ngo, D.C.L., 2015. Text mining of news-headlines for FOREX market prediction: A multi-layer dimension reduction algorithm with semantics and sentiment. Expert Systems with Applications 42, 306–324.
- <span id="page-91-15"></span>Nastase, V., Strube, M., Boerschinger, B., Zirn, C., Elghafari, A., 2010. WikiNet: A very large scale multi-lingual concept network, in: Proceedings of the Seventh International Conference on Language Resources and Evaluation, pp. 1015–1022.
- <span id="page-91-2"></span>Navigli, R., 2009. Word sense disambiguation: A survey. ACM computing surveys (CSUR) 41, 1–69.
- <span id="page-91-4"></span>Navigli, R., Jurgens, D., Vannella, D., 2013. SemEval-2013 task 12: Multilingual word sense disambiguation, in: Second Joint Conference on Lexical and Computational Semantics (\* SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pp. 222–231.
- <span id="page-91-6"></span>Navigli, R., Lapata, M., 2007. Graph connectivity measures for unsupervised word sense disambiguation, in: Proceedings of the 20th International Joint Conference on Artifical Intelligence, pp. 1683–1688.
- <span id="page-91-1"></span>Navigli, R., Ponzetto, S.P., 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence 193, 217–250.
- <span id="page-91-5"></span>Neale, S., Silva, J., Branco, A., 2015. A flexible tool for manual word sense annotation, in: Proceedings of the 11th Joint ACL-ISO Workshop on Interoperable Semantic Annotation (ISA-11), pp. 1–5.
- <span id="page-91-14"></span>Nedoluzhko, A., M´ırovsky, J., Fu ` cˇ´ıkova, E., Pergler, J., 2014. Annotation of Coreference in Prague Czech-English Dependency Treebank. Tech- ´ nical Report. Technical report 2014/57. Prague: UFAL MFF UK. ´
- <span id="page-91-25"></span>Nedoluzhko, A., Novak, M., Popel, M., ´ Zabokrtsk ˇ y, Z., Zeldes, A., Zeman, D., 2022. CorefUD 1.0: Coreference meets universal dependencies, ` in: Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 4859–4872.
- <span id="page-91-32"></span>Neiberg, D., Elenius, K., Karlsson, I., Laskowski, K., 2006. Emotion recognition in spontaneous speech, in: Proceedings of Fonetik, Citeseer. pp. 101–104.

<span id="page-91-17"></span>Neves, M., Seva, J., 2021. An extensive review of tools for manual annotation of documents. Briefings in Bioinformatics 22, 146–163. ˇ

- <span id="page-91-10"></span><span id="page-91-3"></span>Ng, H.T., 1997. Getting serious about word sense disambiguation, in: Tagging Text with Lexical Semantics: Why, What, and How?, pp. 1–7.
- Ng, H.T., Lee, H.B., 1996. Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach, in: Proceedings of the 34th annual meeting on Association for Computational Linguistics, pp. 40–47.
- <span id="page-91-21"></span>Ng, V., 2010. Supervised noun phrase coreference research: The first fifteen years, in: Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 1396–1411.
- <span id="page-91-19"></span>Ng, V., Cardie, C., 2002a. Combining sample selection and error-driven pruning for machine learning of coreference rules, in: Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pp. 55–62.
- <span id="page-91-18"></span>Ng, V., Cardie, C., 2002b. Improving machine learning approaches to coreference resolution, in: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 104–111.
- <span id="page-91-28"></span>Nguyen, T.D., Kan, M., 2007. Keyphrase extraction in scientific publications, in: Goh, D.H., Cao, T.H., Sølvberg, I., Rasmussen, E.M. (Eds.), 10th International Conference on Asian Digital Libraries, pp. 317–326.
- <span id="page-91-27"></span>Ni, J., Young, T., Pandelea, V., Xue, F., Cambria, E., 2022. Recent advances in deep learning based dialogue systems: A systematic survey. Artificial Intelligence Review , 3055–3155.
- <span id="page-91-20"></span>Nicolae, C., Nicolae, G., 2006. BESTCUT: A graph algorithm for coreference resolution, in: Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pp. 275–283.
- <span id="page-91-24"></span>Nicolov, N., Salvetti, F., Ivanova, S., 2008. Sentiment analysis: Does coreference matter, in: AISB 2008 Convention Communication, Interaction and Social Intelligence, p. 37.
- <span id="page-91-0"></span>Noyes, G.E., 1943. The first English dictionary, Cawdrey's table alphabeticall. Modern Language Notes 58, 600–605.
- <span id="page-91-16"></span>Oberle, B., 2018. SACR: A Drag-and-Drop Based Tool for Coreference Annotation, in: Proceedings of the Eleventh International Conference on Language Resources and Evaluation, pp. 389–394.
- <span id="page-91-13"></span>Ogrodniczuk, M., Głowinska, K., Kope ´ c, M., Savary, A., Zawisławska, M., 2013. Polish coreference corpus, in: Language and Technology ´ Conference, Springer. pp. 215–226.
- <span id="page-91-9"></span>Ohana, B., Tierney, B., 2009. Sentiment classification of reviews using SentiWordNet. Proceedings of IT&T 8.
- <span id="page-91-22"></span>Okumura, M., Tamura, K., 1996. Zero pronoun resolution in Japanese discourse based on centering theory, in: COLING 1996 Volume 2: The 16th International Conference on Computational Linguistics, pp. 871–876.
- <span id="page-91-23"></span>Orasan, C., 2007. The influence of pronominal anaphora resolution on term-based summarisation. Recent Advances in Natural Language Processing V: Selected Papers from RANLP , 291–300.
- <span id="page-91-11"></span>Orita, N., Feldman, N., Boyd-Graber, J., Vornov, E., 2014. Quantifying the role of discourse topicality in speakers' choices of referring expressions, in: Proceedings of the Fifth Workshop on Cognitive Modeling and Computational Linguistics, pp. 63–70.
- <span id="page-91-12"></span>Orita, N., Vornov, E., Feldman, N., Daume III, H., 2015. Why discourse a ´ ffects speakers' choice of referring expressions, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1639–1649.
- <span id="page-91-31"></span>Ortega, R., Fonseca, A., Gutierrez, Y., Montoyo, A., 2013. Improving subjectivity detection using unsupervised subjectivity word sense disam- ´ biguation. Procesamiento del Lenguaje Natural 51, 179–186.
- <span id="page-91-7"></span>O'Hara, T., Bruce, R., Donner, J., Wiebe, J., 2004. Class-based collocations for word sense disambiguation, in: Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pp. 199–202.
- <span id="page-91-29"></span>Paivio, A., 1965. Abstractness, imagery, and meaningfulness in paired-associate learning. Journal of Verbal Learning and Verbal Behavior 4, 32–38.

<span id="page-92-34"></span><span id="page-92-0"></span>Palmer, F.R., Frank Robert, P., 1981. Semantics. Cambridge university press.

- Paltoglou, G., Giachanou, A., 2014. Opinion retrieval: Searching for opinions in social media, in: Professional Search in the Modern World. Springer, pp. 193–214.
- <span id="page-92-25"></span>Pan, L., Wang, X., Li, C., Li, J., Tang, J., 2017a. Course concept extraction in MOOCs via embedding-based graph propagation, in: Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 875–884.
- <span id="page-92-26"></span>Pan, L., Wang, X., Li, C., Li, J., Tang, J., 2017b. Course concept extraction in moocs via embedding-based graph propagation, in: Kondrak, G., Watanabe, T. (Eds.), Proceedings of the Eighth International Joint Conference on Natural Language Processing, pp. 875–884.
- <span id="page-92-27"></span>Pang, B., Lee, L., 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts, in: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pp. 271–278.
- <span id="page-92-33"></span>Pant, K., Dadu, T., Mamidi, R., 2020. Towards detection of subjective bias using contextualized word embeddings, in: Companion Proceedings of the Web Conference 2020, pp. 75–76.
- <span id="page-92-7"></span>Pasini, T., Elia, F., Navigli, R., 2018. Huge automatically extracted training-sets for multilingual word sense disambiguation, in: Proceedings of the Eleventh International Conference on Language Resources and Evaluation, pp. 1694–1698.
- <span id="page-92-6"></span>Pasini, T., Navigli, R., 2017. Train-o-matic: Large-scale supervised word sense disambiguation in multiple languages without manual training data, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 78–88.
- <span id="page-92-8"></span>Pasini, T., Navigli, R., 2020. Train-o-matic: Supervised word sense disambiguation with no (manual) effort. Artificial Intelligence 279, 103215.
- <span id="page-92-18"></span><span id="page-92-11"></span>Passonneau, R., 1997. Instructions for applying discourse reference annotation for multiple applications (drama). Unpublished Manuscript , 46.
- Pearson, J., Stevenson, R., Poesio, M., 2001. The effects of animacy, thematic role, and surface position on the focusing of entities in discourse, in: Proceedings of the First Workshop on Cognitively Plausible Models of Semantic Processing, pp. 1472–1504.
- <span id="page-92-24"></span>Peng, Y., Yan, S., Lu, Z., 2019. Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. BioNLP 2019 , 58.
- <span id="page-92-32"></span>Pennebaker, J.W., Francis, M.E., Booth, R.J., 2001. Linguistic inquiry and word count: LIWC 2001. Mahway: Lawrence Erlbaum Associates 71, 2001.
- <span id="page-92-2"></span>Pennington, J., Socher, R., Manning, C.D., 2014. Glove: Global vectors for word representation, in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pp. 1532–1543.
- <span id="page-92-23"></span>Petroni, F., Rocktaschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., Miller, A., 2019. Language models as knowledge bases?, in: Proceedings ¨ of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463–2473.
- <span id="page-92-3"></span>Petruck, M.R., 1996. Frame semantics. Handbook of pragmatics 2.
- <span id="page-92-4"></span>Pianta, E., Bentivogli, L., Girardi, C., 2002. MultiWordNet: developing an aligned multilingual database, in: First International Conference on Global WordNet, pp. 293–302. Poesio, M., 2000. Annotating a corpus to develop and evaluate discourse entity realization algorithms: Issues and preliminary results, in: Proceed-
- <span id="page-92-15"></span>ings of the Second International Conference on Language Resources and Evaluation, pp. 1–8.
- <span id="page-92-19"></span>Poesio, M., 2003. Associative descriptions and salience: A preliminary investigation, in: Proceedings of the 2003 EACL Workshop on The Computational Treatment of Anaphora, pp. 31–38.
- <span id="page-92-13"></span>Poesio, M., 2004. The MATE/GNOME proposals for anaphoric annotation, revisited, in: Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue at HLT-NAACL 2004, pp. 154–162.
- <span id="page-92-20"></span>Poesio, M., Alexandrov-Kabadjov, M., 2004. A general-purpose, off the shelf anaphoric resolver, in: Proceedings of Language Resources and Evaluation Conference, pp. 653–656.
- <span id="page-92-17"></span><span id="page-92-16"></span>Poesio, M., Artstein, R., 2008a. Anaphoric annotation in the ARRAU corpus. Technical Report. University of Southern California Los Angeles. Poesio, M., Artstein, R., 2008b. Anaphoric annotation in the ARRAU corpus, in: Proceedings of the Sixth International Conference on Language Resources and Evaluation, pp. 1–5.
- <span id="page-92-12"></span>Poesio, M., Bruneseaux, F., Romary, L., 1999a. The MATE meta-scheme for coreference in dialogues in multiple languages, in: ACL'99 Workshop Towards Standards and Tools for Discourse Tagging, pp. 65–74.
- <span id="page-92-21"></span>Poesio, M., Henschel, R., Hitzeman, J., Kibble, R., Montague, S., van Deemter, K., 1999b. Towards an annotation scheme for noun phrase generation. Technical Report. European Chapter of the Association for Computational Linguistics.
- <span id="page-92-1"></span>Poesio, M., Yu, J., Paun, S., Aloraini, A., Lu, P., Haber, J., Cokal, D., 2023. Computational models of anaphora. Annual Review of Linguistics 9, 561–587.
- <span id="page-92-30"></span>Polanyi, L., Zaenen, A., 2006. Contextual valence shifters, in: Computing Attitude and Affect in Text: Theory and Applications. Springer, pp. 1–10.
- <span id="page-92-22"></span>Ponzetto, S.P., Strube, M., 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution, in: Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pp. 192–199.
- <span id="page-92-10"></span>Popov, A., 2017. Word sense disambiguation with recurrent neural networks, in: Proceedings of the Student Research Workshop associated with RANLP 2017, pp. 25–34.
- <span id="page-92-9"></span>Postma, M., Van Miltenburg, E., Segers, R., Schoen, A., Vossen, P., 2016. Open Dutch WordNet, in: Proceedings of the 8th Global WordNet Conference (GWC), pp. 302–310.
- <span id="page-92-5"></span>Pradhan, S., Loper, E., Dligach, D., Palmer, M., 2007. SemEval-2007 task-17: English lexical sample, SRL and all words, in: Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pp. 87–92.
- <span id="page-92-14"></span>Pradhan, S., Moschitti, A., Xue, N., Uryupina, O., Zhang, Y., 2012. CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes, in: Joint Conference on EMNLP and CoNLL-Shared Task, pp. 1–40.
- <span id="page-92-29"></span>Preiss, J., Yarowsky, D., 2001. Proceedings of senseval-2 second international workshop on evaluating word sense disambiguation systems, in: Proceedings of SENSEVAL-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems, pp. 1–163.
- <span id="page-92-28"></span>Pryzant, R., Martinez, R.D., Dass, N., Kurohashi, S., Jurafsky, D., Yang, D., 2020. Automatically neutralizing subjective bias in text, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 480–489.
- <span id="page-92-31"></span>Raaijmakers, S., 2007. Sentiment classification with interpolated information diffusion kernels, in: Proceedings of the 1st International Workshop

on Data Mining and Audience Intelligence for Advertising, pp. 34–39.

- <span id="page-93-30"></span>Raaijmakers, S., Truong, K.P., Wilson, T., 2008. Multimodal subjectivity analysis of multiparty conversation, in: Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pp. 466–474.
- <span id="page-93-32"></span>Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al., 2018. Improving language understanding by generative pre-training. Technical Report. OpenAI.
- <span id="page-93-33"></span>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., 2019. Language Models are Unsupervised Multitask Learners. Technical Report. OpenAI.
- <span id="page-93-5"></span>Raganato, A., Bovi, C.D., Navigli, R., 2017a. Neural sequence learning models for word sense disambiguation, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1156–1167.
- <span id="page-93-2"></span>Raganato, A., Camacho-Collados, J., Navigli, R., 2017b. Word sense disambiguation: A unified evaluation framework and empirical comparison, in: Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pp. 99–110.
- <span id="page-93-8"></span>Raganato, A., Scherrer, Y., Tiedemann, J., 2019. The MuCoW test suite at WMT 2019: Automatically harvested multilingual contrastive word sense disambiguation test sets for machine translation, in: Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), Association for Computational Linguistics. pp. 470–480.
- <span id="page-93-14"></span>Raghunathan, K., Lee, H., Rangarajan, S., Chambers, N., Surdeanu, M., Jurafsky, D., Manning, C., 2010. A multi-pass sieve for coreference resolution, in: Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 492–501.
- <span id="page-93-23"></span>Rahimi, A., Li, Y., Cohn, T., 2019. Massively multilingual transfer for NER, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 151–164.
- <span id="page-93-15"></span>Rahman, A., Ng, V., 2011. Narrowing the modeling gap: A cluster-ranking approach to coreference resolution. Journal of Artificial Intelligence Research 40, 469–521.
- <span id="page-93-10"></span>Rahman, A., Ng, V., 2012. Resolving complex cases of definite pronouns: The Winograd schema challenge, in: Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 777–789.
- <span id="page-93-0"></span>Ransing, R., Gulati, A., 2022. A survey of different approaches for word sense disambiguation, in: ICT Analysis and Applications: Proceedings of ICT4SD 2022. Springer, pp. 435–445.
- <span id="page-93-31"></span>Recasens, M., Danescu-Niculescu-Mizil, C., Jurafsky, D., 2013. Linguistic models for analyzing and detecting biased language, in: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1650–1659.

<span id="page-93-12"></span>Recasens, M., Hovy, E., 2011. BLANC: Implementing the rand index for coreference evaluation. Natural Language Engineering 17, 485–510.

<span id="page-93-13"></span><span id="page-93-9"></span>Reinhart, T., 1983. Coreference and bound anaphora: A restatement of the anaphora questions. Linguistics and Philosophy , 47–88.

- <span id="page-93-27"></span>Reiter, N., 2018. CorefAnnotator - a new annotation tool for entity references, in: Abstracts of EADH: Data in the Digital Humanities, pp. 1–4. Remus, R., 2011. Improving sentence-level subjectivity classification through readability measurement, in: Proceedings of the 18th Nordic Conference of Computational Linguistics, pp. 168–174.
- <span id="page-93-22"></span>Ren, F., Zhang, L., Yin, S., Zhao, X., Liu, S., Li, B., Liu, Y., 2021. A novel global feature-oriented relational triple extraction model based on table filling, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 2646–2656.
- <span id="page-93-3"></span>Resnik, P., Yarowsky, D., 1999. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation. Natural Language Engineering 5, 113–133.

<span id="page-93-26"></span>Riloff, E., 1996. Automatically generating extraction patterns from untagged text, in: Proceedings of the national conference on artificial intelligence, pp. 1044–1049.

- <span id="page-93-24"></span>Riloff, E., Wiebe, J., 2003. Learning extraction patterns for subjective expressions, in: Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pp. 105–112.
- <span id="page-93-21"></span>Rim, K., 2016. MAE2: Portable annotation tool for general natural language use, in: Proc 12th Joint ACL-ISO Workshop on Interoperable Semantic Annotation, pp. 75–80.
- <span id="page-93-16"></span>Ringland, N., Dai, X., Hachey, B., Karimi, S., Paris, C., Curran, J.R., 2019a. Nne: A dataset for nested named entity recognition in english newswire, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5176–5181.

<span id="page-93-20"></span>Ringland, N., Dai, X., Hachey, B., Karimi, S., Paris, C., Curran, J.R., 2019b. NNE: A dataset for nested named entity recognition in english newswire, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5176–5181.

<span id="page-93-7"></span>Rios Gonzales, A., Mascarell, L., Sennrich, R., 2017. Improving word sense disambiguation in neural machine translation with sense embeddings, in: Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics. pp. 11–19.

<span id="page-93-18"></span>Rosch, E., Mervis, C.B., 1975. Family resemblances: Studies in the internal structure of categories. Cognitive Psychology 7, 573–605.

<span id="page-93-19"></span><span id="page-93-17"></span>Rosch, E., Mervis, C.B., Gray, W.D., Johnson, D.M., Boyes-Braem, P., 1976. Basic objects in natural categories. Cognitive Psychology 8, 382–439. Rosch, E.H., 1973. Natural categories. Cognitive psychology 4, 328–350.

- <span id="page-93-4"></span>Rothe, S., Schutze, H., 2015. Autoextend: Extending word embeddings to embeddings for synsets and lexemes, in: Proceedings of the 53rd Annual ¨ Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1793–1803.
- <span id="page-93-11"></span>Rudinger, R., Naradowsky, J., Leonard, B., Van Durme, B., 2018. Gender bias in coreference resolution, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 8–14.
- <span id="page-93-1"></span>Ruppenhofer, J., Ellsworth, M., Schwarzer-Petruck, M., Johnson, C.R., Scheffczyk, J., 2016. FrameNet II: Extended theory and practice. Technical Report. International Computer Science Institute.

<span id="page-93-28"></span>Rustamov, S., 2018. A hybrid system for subjectivity analysis. Advances in Fuzzy Systems 2018.

- <span id="page-93-25"></span>Rustamov, S., Mustafayev, E., Clements, M.A., 2013. Sentence-level subjectivity detection using neuro-fuzzy models, in: Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pp. 108–114.
- <span id="page-93-6"></span>Saggionα, H., Funk, A., 2010. Interpreting SentiWordNet for opinion classification, in: Proceedings of the Seventh Conference on International Language Resources and Evaluation, pp. 1129–1133.

<span id="page-93-29"></span>Sagnika, S., Mishra, B.S.P., Meher, S.K., 2020. Improved method of word embedding for efficient analysis of human sentiments. Multimedia Tools

and Applications 79, 32389–32413.

- <span id="page-94-31"></span>Sagnika, S., Mishra, B.S.P., Meher, S.K., 2021. An attention-based CNN-LSTM model for subjectivity detection in opinion-mining. Neural Computing and Applications 33, 17425–17438.
- <span id="page-94-8"></span>Sakaguchi, K., Bras, R.L., Bhagavatula, C., Choi, Y., 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM 64, 99–106.
- <span id="page-94-10"></span>Salgado, D., Krallinger, M., Depaule, M., Drula, E., Tendulkar, A.V., Leitner, F., Valencia, A., Marcelle, C., 2012. MyMiner: a web application for computer-assisted biocuration and text annotation. Bioinformatics 28, 2285–2287.
- <span id="page-94-1"></span>Salloum, S.A., Khan, R., Shaalan, K., 2020. A survey of semantic analysis approaches, in: Proceedings of the International Conference on Artificial Intelligence and Computer Vision (AICV2020), Springer. pp. 61–70.
- <span id="page-94-17"></span>Same, F., Chen, G., Van Deemter, K., 2022. Non-neural models matter: a re-evaluation of neural referring expression generation systems, in: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Dublin, Ireland. pp. 5554–5567. URL: <https://aclanthology.org/2022.acl-long.380>, doi:[10.18653/v1/2022.acl-long.380](http://dx.doi.org/10.18653/v1/2022.acl-long.380).
- <span id="page-94-7"></span>Sanderson, M., 1994. Word sense disambiguation and information retrieval, in: SIGIR'94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, Springer. pp. 142–151.
- <span id="page-94-18"></span>Sang, E.T.K., De Meulder, F., 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition, in: Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pp. 142–147.
- <span id="page-94-14"></span>Sasano, R., Kawahara, D., Kurohashi, S., 2008. A fully-lexicalized probabilistic model for Japanese zero anaphora resolution, in: Proceedings of the 22nd International Conference on Computational Linguistics, pp. 769–776.
- <span id="page-94-15"></span>Sasano, R., Kurohashi, S., 2011. A discriminative approach to Japanese zero anaphora resolution with large-scale lexicalized case frames, in: Proceedings of 5th International Joint Conference on Natural Language Processing, pp. 758–766.
- <span id="page-94-32"></span>Sastry, K., Goldberg, D., Kendall, G., 2005. Genetic algorithms, in: Search Methodologies. Springer, pp. 97–125.
- <span id="page-94-27"></span>Satapathy, R., Chaturvedi, I., Cambria, E., Ho, S.S., Na, J.C., 2017. Subjectivity detection in nuclear energy tweets. Computacion y Sistemas 21, ´ 657–664.
- <span id="page-94-30"></span>Satapathy, R., Pardeshi, S.R., Cambria, E., 2022. Polarity and subjectivity detection with multitask learning and bert embedding. Future Internet 14, 191–201.
- <span id="page-94-16"></span>Saunders, D., Sallis, R., Byrne, B., 2020. Neural machine translation doesn't translate gender coreference right unless you make it, in: Proceedings of the Second Workshop on Gender Bias in Natural Language Processing, pp. 35–43.
- <span id="page-94-2"></span>Scarlini, B., Pasini, T., Navigli, R., 2019. Just "onesec" for producing multilingual sense-annotated data, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 699–709.
- <span id="page-94-5"></span>Scarlini, B., Pasini, T., Navigli, R., 2020a. SensEmBERT: Context-enhanced sense embeddings for multilingual word sense disambiguation, in: Proceedings of the AAAI conference on artificial intelligence, pp. 8758–8765.
- <span id="page-94-6"></span>Scarlini, B., Pasini, T., Navigli, R., 2020b. With more contexts comes better performance: Contextualized sense embeddings for all-round word sense disambiguation, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 3528–3539.
- <span id="page-94-33"></span>Schapire, R.E., Singer, Y., 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning 39, 135–168.
- <span id="page-94-22"></span>Schick, T., Schutze, H., 2021. It's not just size that matters: Small language models are also few-shot learners, in: Proceedings of the 2021 ¨ Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2339–2352.
- <span id="page-94-3"></span>Scozzafava, F., Maru, M., Brignone, F., Torrisi, G., Navigli, R., 2020. Personalized pagerank with syntagmatic information for multilingual word sense disambiguation, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 37–46.
- <span id="page-94-25"></span><span id="page-94-12"></span>Seel, N.M., 2011. Encyclopedia of the Sciences of Learning. Springer Science & Business Media.
- Seki, K., Fujii, A., Ishikawa, T., 2001. A probabilistic model for japanese zero pronoun resolution integrating syntactic and semantic features., in: NLPRS, pp. 403–410.
- <span id="page-94-13"></span>Seki, K., Fujii, A., Ishikawa, T., 2002. A probabilistic method for analyzing japanese anaphora integrating zero pronoun detection and resolution, in: Proceedings of the 19th International Conference on Computational Linguistics, pp. 1–7.
- <span id="page-94-26"></span>Serrano-Guerrero, J., Olivas, J.A., Romero, F.P., Herrera-Viedma, E., 2015. Sentiment analysis: A review and comparative analysis of web services. Information Sciences 311, 18–38.
- <span id="page-94-24"></span>Shafqat, S., Majeed, H., Javaid, Q., Ahmad, H.F., 2022. Standard NER tagging scheme for big data healthcare analytics built on unified medical corpora. Journal of Artificial Intelligence and Technology 2, 152–157.
- <span id="page-94-19"></span>Shibuya, T., Hovy, E., 2020. Nested named entity recognition via second-best sequence learning and decoding. Transactions of the Association for Computational Linguistics 8, 605–620.
- <span id="page-94-11"></span>Sidner, C.L., 1979. Towards a Computational Theory of Definite Anaphora Comprehension in English Discourse. Technical Report. Massachusetts Inst of Tech Cambridge Artificial Intelligence lab.
- <span id="page-94-23"></span>Silvestri, S., Gargiulo, F., Ciampi, M., 2022. Iterative annotation of biomedical ner corpora with deep neural networks and knowledge bases. Applied Sciences 12, 5775.

<span id="page-94-0"></span>Simpson, J., Weiner, E., 1989. The Oxford English Dictionary. 2 ed., Oxford University Press.

<span id="page-94-9"></span>Singh, P., 2002. The open mind common sense project. KurzweilAI.net 143, 1–12.

- <span id="page-94-4"></span>Singh, R.L., Ghosh, K., Nongmeikapam, K., Bandyopadhyay, S., 2014. A decision tree based word sense disambiguation system in manipuri language. Advanced Computing 5, 17.
- <span id="page-94-29"></span>Sixto, J., Almeida, A., Lopez-de Ipi ´ na, D., 2016. An approach to subjectivity detection on Twitter using the structured information, in: International ˜ Conference on Computational Collective Intelligence, Springer. pp. 121–130.
- <span id="page-94-20"></span>Skylaki, S., Oskooei, A., Bari, O., Herger, N., Kriegman, Z., 2020. Named entity recognition in the legal domain using a pointer generator network. arXiv preprint arXiv:2012.09936 .

<span id="page-94-28"></span>Smith, E.A., 1961. Devereux readability index. The Journal of Educational Research 54, 298–303.

<span id="page-94-21"></span>Snell, J., Swersky, K., Zemel, R., 2017. Prototypical networks for few-shot learning, in: Proceedings of the 31st International Conference on

Neural Information Processing Systems, p. 4080–4090.

<span id="page-95-20"></span>Snow, R., Jurafsky, D., Ng, A.Y., 2006. Semantic taxonomy induction from heterogenous evidence, in: Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pp. 801–808.

- <span id="page-95-1"></span>Snyder, B., Palmer, M., 2004. The English all-words task, in: Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pp. 41–43.
- <span id="page-95-27"></span>Socher, R., Chen, D., Manning, C.D., Ng, A., 2013. Reasoning with neural tensor networks for knowledge base completion. Advances in Neural Information Processing Systems 26.
- <span id="page-95-31"></span>Somasundaran, S., Ruppenhofer, J., Wiebe, J., 2007. Detecting arguing and sentiment in meetings, in: Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pp. 26–34.
- <span id="page-95-22"></span>Somasundaran, S., Wiebe, J., 2010. Recognizing stances in ideological on-line debates, in: Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text, pp. 116–124.
- <span id="page-95-14"></span>Sonawane, S., Kulkarni, P., 2016. The role of coreference resolution in extractive summarization, in: 2016 International Conference on Computing, Analytics and Security Trends (CAST), pp. 351–356.
- <span id="page-95-16"></span>Song, B., Li, F., Liu, Y., Zeng, X., 2021. Deep learning methods for biomedical named entity recognition: a survey and qualitative comparison. Briefings in Bioinformatics 22, bbab282.
- <span id="page-95-11"></span>Song, L., Xu, K., Zhang, Y., Chen, J., Yu, D., 2020. ZPR2: Joint zero pronoun recovery and resolution using multi-task learning and BERT, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5429–5434.
- <span id="page-95-9"></span>Soon, W.M., Ng, H.T., Lim, D.C.Y., 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics 27, 521–544.
- <span id="page-95-29"></span>Soong, H.C., Jalil, N.B.A., Ayyasamy, R.K., Akbar, R., 2019. The essential of sentiment analysis and opinion mining in social media: Introduction and survey of the recent approaches and techniques, in: 2019 IEEE 9th Symposium on Computer Applications & Industrial Electronics, pp. 272–277.
- <span id="page-95-25"></span>Soria-Olivas, E., Gomez-Sanchis, J., Martin, J.D., Vila-Frances, J., Martinez, M., Magdalena, J.R., Serrano, A.J., 2011. BELM: Bayesian extreme learning machine. IEEE Transactions on Neural Networks 22, 505–509.
- <span id="page-95-21"></span>Speer, R., Chin, J., Havasi, C., 2017. Conceptnet 5.5: An open multilingual graph of general knowledge, in: Proceedings of the AAAI conference on artificial intelligence, pp. 4444–4451.
- <span id="page-95-13"></span>Steinberger, J., Poesio, M., Kabadjov, M.A., Jezek, K., 2007. Two uses of anaphora resolution in summarization. Information Processing & ˇ Management 43, 1663–1680.
- <span id="page-95-8"></span>Stenetorp, P., Pyysalo, S., Topic, G., Ohta, T., Ananiadou, S., Tsujii, J., 2012. Brat: a web-based tool for nlp-assisted text annotation, in: Proceedings ´ of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pp. 102–107.
- <span id="page-95-5"></span>Stevenson, R.J., Crawley, R.A., Kleinman, D., 1994. Thematic roles, focus and the representation of events. Language and Cognitive Processes 9, 519–548.
- <span id="page-95-12"></span>Stojanovski, D., Fraser, A., 2018. Coreference and coherence in neural machine translation: A study using oracle experiments, in: Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 49–60.
- <span id="page-95-3"></span>Stokoe, C., Oakes, M.P., Tait, J., 2003. Word sense disambiguation in information retrieval revisited, in: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, pp. 159–166.
- <span id="page-95-23"></span>Stone, P.J., Dunphy, D.C., Smith, M.S., 1966. The general inquirer: A computer approach to content analysis. MIT press.
- <span id="page-95-30"></span>Stoyanov, V., Cardie, C., Wiebe, J., 2005. Multi-perspective question answering using the opqa corpus, in: Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pp. 923–930.
- <span id="page-95-7"></span>Stoyanov, V., Gilbert, N., Cardie, C., Riloff, E., 2009. Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art, in: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 656–664.
- <span id="page-95-18"></span>Strakova, J., Straka, M., Hajic, J., 2019. Neural architectures for nested NER through linearization, in: Proceedings of the 57th Annual Meeting of ´ the Association for Computational Linguistics, pp. 5326–5331.
- <span id="page-95-24"></span>Strapparava, C., Valitutti, A., 2004. WordNet-Affect: an affective extension of WordNet, in: Proceedings of the 4th International Conference on Language Resources and Evaluation, pp. 1083–1086.
- <span id="page-95-10"></span>Strube, M., Rapp, S., Muller, C., 2002. The influence of minimum edit distance on reference resolution, in: Proceedings of the 2002 Conference ¨ on Empirical Methods in Natural Language Processing (EMNLP 2002), pp. 312–319.
- <span id="page-95-17"></span>Stubbs, A., Uzuner, O., 2015. Annotating longitudinal clinical narratives for de-identification: The 2014 I2B2 ¨ /uthealth corpus. Journal of Biomedical Informatics 58, S20–S29.
- <span id="page-95-15"></span>Su, J., Murtadha, A., Pan, S., Hou, J., Sun, J., Huang, W., Wen, B., Liu, Y., 2022. Global pointer: Novel efficient span-based approach for named entity recognition. arXiv preprint arXiv:2208.03054 .
- <span id="page-95-28"></span>Subrahmanya, N., Shin, Y.C., 2009. Sparse multiple kernel learning for signal processing applications. IEEE Transactions on Pattern Analysis and Machine Intelligence 32, 788–798.

<span id="page-95-6"></span><span id="page-95-4"></span>Suchanek, F.M., Kasneci, G., Weikum, G., 2008. Yago: A large ontology from wikipedia and WordNet. Journal of Web Semantics 6, 203–217. Sukthanker, R., Poria, S., Cambria, E., Thirunavukarasu, R., 2020. Anaphora and coreference resolution: A review. Information Fusion 59,

- 139–162.
- <span id="page-95-26"></span>Sun, C., Qiu, X., Xu, Y., Huang, X., 2019. How to fine-tune BERT for text classification?, in: China National Conference on Chinese Computational Linguistics, Springer. pp. 194–206.
- <span id="page-95-19"></span>Sun, Y., Wang, S., Li, Y., Feng, S., Tian, H., Wu, H., Wang, H., 2020. ERNIE 2.0: A continual pre-training framework for language understanding, in: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 8968–8975.
- <span id="page-95-0"></span>Taghipour, K., Ng, H.T., 2015. One million sense-tagged instances for word sense disambiguation and induction, in: Proceedings of the Nineteenth Conference on Computational Natural Language Learning, pp. 338–344.
- <span id="page-95-2"></span>Talukdar, P.P., Crammer, K., 2009. New regularized algorithms for transductive learning, in: Machine Learning and Knowledge Discovery in Databases: European Conference, Springer. pp. 442–457.

- <span id="page-96-14"></span>Tan, X., Kuang, S., Xiong, D., 2019. Detecting and translating dropped pronouns in neural machine translation, in: Natural Language Processing and Chinese Computing: 8th CCF International Conference, NLPCC 2019, Dunhuang, China, October 9–14, 2019, Proceedings, Part I 8, pp. 343–354.
- <span id="page-96-27"></span>Tang, D., Wei, F., Yang, N., Zhou, M., Liu, T., Qin, B., et al., 2014. Learning sentiment-specific word embedding for Twitter sentiment classification, in: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, Citeseer. pp. 1555–1565.
- <span id="page-96-5"></span>Tang, G., Sennrich, R., Nivre, J., 2019. Encoders help you disambiguate word senses in neural machine translation, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1429–1435.
- <span id="page-96-7"></span>Taule, M., Mart ´ ´ı, M.A., Recasens, M., 2008. AnCora: Multilevel annotated corpora for catalan and spanish, in: Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08), pp. 1–6.
- <span id="page-96-20"></span>Tedeschi, S., Conia, S., Cecconi, F., Navigli, R., 2021a. Named entity recognition for entity linking: What works and what's next, in: Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2584–2596.
- <span id="page-96-22"></span>Tedeschi, S., Maiorca, V., Campolungo, N., Cecconi, F., Navigli, R., 2021b. WikiNEuRal: Combined neural and knowledge-based silver data creation for multilingual ner, in: Findings of the Association for Computational Linguistics: EMNLP, pp. 2521–2533.

<span id="page-96-18"></span>Tedeschi, S., Navigli, R., 2022. MultiNERD: A multilingual, multi-genre and fine-grained dataset for named entity recognition (and disambiguation), in: Findings of the Association for Computational Linguistics: NAACL 2022, pp. 801–812.

<span id="page-96-8"></span>Telljohann, H., Hinrichs, E., Kubler, S., K ¨ ubler, R., 2004. The T ¨ uBa-D ¨ /Z treebank: Annotating german with a context-free backbone, in: In Proceedings of the Fourth International Conference on Language Resources and Evaluation, Citeseer. pp. 2229–2232.

<span id="page-96-12"></span>Tetreault, J.R., 2001. A corpus-based evaluation of centering and pronoun resolution. Computational Linguistics 27, 507–520.

- <span id="page-96-30"></span>Thelwall, M., Buckley, K., Paltoglou, G., 2012. Sentiment strength detection for the social web. Journal of the American Society for Information Science and Technology 63, 163–173.
- <span id="page-96-4"></span>Tong, H., Faloutsos, C., Pan, J.Y., 2006. Fast random walk with restart and its applications, in: Sixth International Conference on Data Mining, IEEE. pp. 613–622.
- <span id="page-96-31"></span>Toutanvoa, K., Manning, C.D., 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger, in: 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora, pp. 63–70.
- <span id="page-96-26"></span>Traugott, E.C., 2010. Revisiting subjectification and intersubjectification. Subjectification, Intersubjectification and Grammaticalization 29, 71.
- <span id="page-96-3"></span>Tripodi, R., Navigli, R., 2019. Game theory meets embeddings: a unified framework for word sense disambiguation, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 88–99.
- <span id="page-96-33"></span>Tsai, Y.H.H., Bai, S., Liang, P.P., Kolter, J.Z., Morency, L.P., Salakhutdinov, R., 2019. Multimodal transformer for unaligned multimodal language sequences, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, p. 6558.
- <span id="page-96-25"></span>Tu, Z., Lu, Z., Liu, Y., Liu, X., Li, H., 2016. Modeling coverage for neural machine translation, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pp. 76––85.
- <span id="page-96-0"></span>Turney, P.D., Pantel, P., 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research 37, 141–188.
- <span id="page-96-29"></span>Ulrich, J., Murray, G., Carenini, G., 2008. A publicly available annotated corpus for supervised email summarization, in: AAAI08 email workshop, pp. 1–6.
- <span id="page-96-15"></span>Uma, A., Almanea, D., Poesio, M., 2022. Scaling and disagreements: Bias, noise, and ambiguity. Frontiers in Artificial Intelligence 5, 1–11.
- <span id="page-96-16"></span>Uma, A.N., Fornaciari, T., Hovy, D., Paun, S., Plank, B., Poesio, M., 2021. Learning from disagreement: A survey. Journal of Artificial Intelligence Research 72, 1385–1470.
- <span id="page-96-21"></span>Upadhyay, C., Abu-Rasheed, H., Weber, C., Fathi, M., 2021. Explainable job-posting recommendations using knowledge graphs and named entity recognition, in: 2021 IEEE International Conference on Systems, Man, and Cybernetics, pp. 3291–3296.
- <span id="page-96-6"></span>Uryupina, O., Artstein, R., Bristot, A., Cavicchio, F., Delogu, F., Rodriguez, K.J., Poesio, M., 2020. Annotating a broad range of anaphoric phenomena, in a variety of genres: the ARRAU corpus. Natural Language Engineering 26, 95–128.
- <span id="page-96-10"></span>Uzuner, O., Bodnari, A., Shen, S., Forbush, T., Pestian, J., South, B.R., 2012. Evaluating the state of the art in coreference resolution for electronic medical records. Journal of the American Medical Informatics Association 19, 786–791.
- <span id="page-96-24"></span>Uzuner, O., South, B.R., Shen, S., DuVall, S.L., 2011. 2010 i2b2 ¨ /va challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association 18, 552–556.
- <span id="page-96-1"></span>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need. Advances in neural information processing systems 30.
- <span id="page-96-2"></span>Vial, L., Lecouteux, B., Schwab, D., 2019. Sense vocabulary compression through the semantic knowledge of WordNet for neural word sense disambiguation, in: Proceedings of the 10th Global WordNet Conference, pp. 108–117.
- <span id="page-96-13"></span>Vieira, R., Poesio, M., 2000. An empirically-based system for processing definite descriptions. Computational Linguistics 26, 539–593.
- <span id="page-96-23"></span>Vijay, S., Priyanshu, A., 2022. NERDA-Con: Extending NER models for continual learning–integrating distinct tasks and updating distribution shifts. arXiv preprint arXiv:2206.14607 .
- <span id="page-96-11"></span>Vilain, M., Burger, J.D., Aberdeen, J., Connolly, D., Hirschman, L., 1995. A model-theoretic coreference scoring scheme, in: Sixth Message Understanding Conference (MUC-6): Proceedings of a Conference Held in Columbia, Maryland, November 6-8, 1995, pp. 45–52.
- <span id="page-96-28"></span>Villena, J., Garc´ıa-Morera, J., Garc´ıa-Cumbreras, M., Mart´ınez-Camara, E., Mart ´ ´ın-Valdivia, M., Lopez, L., 2015. Overview of TASS 2015, in: ´ Proceedings of TASS 2015: Workshop on Sentiment Analysis at SEPLN co-located with 31st SEPLN Conference, pp. 13–21.

<span id="page-96-9"></span>Vrandeciˇ c, D., Kr ´ otzsch, M., 2014. Wikidata: a free collaborative knowledgebase. Communications of the ACM 57, 78–85. ¨

<span id="page-96-32"></span><span id="page-96-17"></span>Walker, C., Strassel, S., Medero, J., Maeda, K., 2006. ACE 2005 multilingual training corpus. Linguistic Data Consortium, Philadelphia 57, 45.

Wan, M., McAuley, J., 2016. Modeling ambiguity, subjectivity, and diverging viewpoints in opinion question answering systems, in: 2016 IEEE 16th International Conference on Data Mining, IEEE. pp. 489–498.

<span id="page-96-19"></span>Wan, Q., Wei, L., Chen, X., Liu, J., 2021. A region-based hypergraph network for joint entity-relation extraction. Knowledge-Based Systems 228, 107298.

- <span id="page-97-33"></span>Wan, X., 2009. Co-training for cross-lingual sentiment classification, in: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language, pp. 235–243.
- <span id="page-97-13"></span>Wang, B., Lu, W., 2018. Neural segmental hypergraphs for overlapping mention recognition, in: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 204–214.
- <span id="page-97-16"></span>Wang, H., Zhang, F., Wang, J., Zhao, M., Li, W., Xie, X., Guo, M., 2019a. Exploring high-order user preference on the knowledge graph for recommender systems. ACM Transactions on Information Systems 37, 1–26.
- <span id="page-97-10"></span>Wang, L., Tu, Z., Wang, X., Shi, S., 2019b. One model to learn both: Zero pronoun prediction and translation, in: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 921–930.
- <span id="page-97-1"></span>Wang, M., Wang, Y., 2020. A synset relation-enhanced framework with a try-again mechanism for word sense disambiguation, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 6229–6240.
- <span id="page-97-3"></span>Wang, S., Bond, F., 2013. Building the Chinese open Wordnet (COW): Starting from core synsets, in: Proceedings of the 11th Workshop on Asian Language Resources, pp. 10–18.

<span id="page-97-31"></span>Wang, S., Manning, C., 2013. Fast dropout training, in: International Conference on Machine Learning, PMLR. pp. 118–126.

- <span id="page-97-32"></span>Wang, S.I., Manning, C.D., 2012. Baselines and bigrams: Simple, good sentiment and topic classification, in: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 90–94.
- <span id="page-97-24"></span>Wang, W., He, L., Wu, Y.J., Goh, M., 2021. Signaling persuasion in crowdfunding entrepreneurial narratives: the subjectivity vs objectivity debate. Computers in Human Behavior 114, 106576.
- <span id="page-97-20"></span>Wang, X., Feng, W., Tang, J., Zhong, Q., 2018a. Course concept extraction in MOOC via explicit/implicit representation, in: Third IEEE International Conference on Data Science in Cyberspace, pp. 339–345.
- <span id="page-97-17"></span>Wang, Y., Guo, Y., Zhu, S., 2020. Slot attention with value normalization for multi-domain dialogue state tracking, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 3019–3028.
- <span id="page-97-19"></span>Wang, Y., Liu, Q., Qin, C., Xu, T., Wang, Y., Chen, E., Xiong, H., 2018b. Exploiting topic-based adversarial neural network for cross-domain keyphrase extraction, in: IEEE International Conference on Data Mining, pp. 597–606.
- <span id="page-97-0"></span>Wang, Y., Tong, H., Zhu, Z., Li, Y., 2022. Nested named entity recognition: A survey. ACM Transactions on Knowledge Discovery from Data 16, 1–29.
- <span id="page-97-34"></span>Warner, W., Hirschberg, J., 2012. Detecting hate speech on the world wide web, in: Proceedings of the Second Workshop on Language in Social Media, pp. 19–26.
- <span id="page-97-6"></span>Webber, B.L., 1988. Discourse deixis: Reference to discourse segments, in: 26th Annual Meeting of the Association for Computational Linguistics, pp. 113–122.
- <span id="page-97-4"></span>Webber, B.L., 2016. A formal approach to discourse anaphora. Routledge.
- <span id="page-97-5"></span>Webster, K., Recasens, M., Axelrod, V., Baldridge, J., 2018. Mind the GAP: A balanced corpus of gendered ambiguous pronouns. Transactions of the Association for Computational Linguistics 6, 605–617.
- <span id="page-97-35"></span>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al., 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35, 24824–24837.
- <span id="page-97-15"></span>Wei, Z., Su, J., Wang, Y., Tian, Y., Chang, Y., 2019. A novel hierarchical binary tagging framework for joint extraction of entities and relations. arXiv preprint arXiv:1909.03227 .
- <span id="page-97-11"></span>Weischedel, R., Palmer, M., Marcus, M., Hovy, E., Pradhan, S., Ramshaw, L., Xue, N., Taylor, A., Kaufman, J., Franchini, M., et al., 2013. Ontonotes release 5.0. Linguistic Data Consortium, Philadelphia, PA 23.
- <span id="page-97-12"></span>Wheeler, D.L., Barrett, T., Benson, D.A., Bryant, S.H., Canese, K., Chetvernin, V., Church, D.M., DiCuccio, M., Edgar, R., Federhen, S., et al., 2007. Database resources of the national center for biotechnology information. Nucleic Acids Research 36, D13–D21.
- <span id="page-97-21"></span>Wiebe, J., 1994. Tracking point of view in narrative. Computational Linguistics 20, 233–287.
- <span id="page-97-30"></span>Wiebe, J., Riloff, E., 2005. Creating subjective and objective sentence classifiers from unannotated texts, in: International conference on intelligent text processing and computational linguistics, Springer. pp. 486–497.
- <span id="page-97-23"></span>Wiebe, J., Riloff, E., 2011. Finding mutual benefit between subjectivity analysis and information extraction. IEEE Transactions on Affective Computing 2, 175–191.
- <span id="page-97-27"></span>Wiebe, J., Wilson, T., Cardie, C., 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation 39, 165–210.
- <span id="page-97-26"></span>Wiebe, J., et al., 2000. Learning subjective adjectives from corpora. AAAI/IAAI 20, 0.
- <span id="page-97-25"></span><span id="page-97-18"></span>Wiebe, J.M., 1990. Recognizing Subjective Sentences: A Computational Investigation of Narrative Text. State University of New York at Buffalo. Wierzbicka, A., 1972. Semantic Primitives. Athena um-Verl.
- <span id="page-97-2"></span>Wilks, Y., 1973. Preference semantics. Technical Report. Computer Science Department, Stanford University.
- <span id="page-97-28"></span>Wilson, T., 2008. Annotating subjective content in meetings, in: Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08), pp. 2738–2745.
- <span id="page-97-29"></span>Wilson, T., Wiebe, J., Hoffmann, P., 2005. Recognizing contextual polarity in phrase-level sentiment analysis, in: Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, pp. 347–354.

<span id="page-97-22"></span>Wilson, T., Wiebe, J., Hwa, R., 2004. Just how mad are you? finding strong and weak opinion clauses, in: AAAI, pp. 761–769.

<span id="page-97-7"></span>Winograd, T., 1972. Understanding natural language. Cognitive Psychology 3, 1–191.

- <span id="page-97-8"></span>Wiseman, S., Rush, A.M., Shieber, S., Weston, J., 2015. Learning anaphoricity and antecedent ranking features for coreference resolution, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1416–1426.
- <span id="page-97-9"></span>Wiseman, S., Rush, A.M., Shieber, S.M., 2016. Learning global features for coreference resolution, in: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 994–1004.
- <span id="page-97-14"></span>Wiseman, S., Stratos, K., 2019. Label-agnostic sequence labeling by copying nearest neighbors, in: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 5363–5369.

- <span id="page-98-11"></span>Wishart, D.S., Feunang, Y.D., Guo, A.C., Lo, E.J., Marcu, A., Grant, J.R., Sajed, T., Johnson, D., Li, C., Sayeeda, Z., et al., 2018. DrugBank 5.0: a major update to the DrugBank database for 2018. Nucleic Acids Research 46, D1074–D1082.
- <span id="page-98-9"></span>Witte, R., Bergler, S., 2003. Fuzzy coreference resolution for summarization, in: Proceedings of 2003 International Symposium on Reference Resolution and Its Applications to Question Answering and Summarization (ARQAS), Citeseer. pp. 43–50.
- <span id="page-98-30"></span>Wrede, B., Shriberg, E., 2003. Spotting "hot spots" in meetings: human judgments and prosodic cues, in: Proceedings of EUROSPEECH, pp. 2805–2808.
- <span id="page-98-20"></span>Wu, C.S., Hoi, S.C., Socher, R., Xiong, C., 2020a. TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 917–929.
- <span id="page-98-19"></span>Wu, J., He, K., Mao, R., Li, C., Cambria, E., 2023. Megacare: Knowledge-guided multi-view hypergraph predictive framework for healthcare. Information Fusion 100, 101939.
- <span id="page-98-1"></span>Wu, W., Wang, F., Yuan, A., Wu, F., Li, J., 2020b. CorefQA: Coreference resolution as query-based span prediction, in: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 6953–6963.
- <span id="page-98-29"></span>Wollmer, M., Weninger, F., Knaup, T., Schuller, B., Sun, C., Sagae, K., Morency, L.P., 2013. Youtube movie reviews: Sentiment analysis in an ¨ audio-visual context. IEEE Intelligent Systems 28, 46–53.
- <span id="page-98-21"></span>Xia, Y., Wang, Q., Lyu, Y., Zhu, Y., Wu, W., Li, S., Dai, D., 2022. Learn and review: Enhancing continual named entity recognition via reviewing synthetic samples, in: Findings of the Association for Computational Linguistics: ACL 2022, pp. 2291–2300.
- <span id="page-98-22"></span>Xiong, C., Power, R., Callan, J., 2017. Explicit semantic ranking for academic search via knowledge graph embedding, in: Barrett, R., Cummings, R., Agichtein, E., Gabrilovich, E. (Eds.), Proceedings of the 26th International Conference on World Wide Web, pp. 1271–1279.
- <span id="page-98-26"></span>Xiong, S., Tan, Y., Wang, G., 2021. Explore visual concept formation for image classification, in: Meila, M., Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, PMLR. pp. 11470–11479.
- <span id="page-98-27"></span>Xuan, H.N.T., Le, A.C., et al., 2012. Linguistic features for subjectivity classification, in: 2012 International Conference on Asian Language Processing, IEEE. pp. 17–20.
- <span id="page-98-10"></span>Yadav, V., Bethard, S., 2018. A survey on recent advances in named entity recognition from deep learning models, in: Proceedings of the 27th International Conference on Computational Linguistics, pp. 2145–2158.
- <span id="page-98-12"></span>Yan, H., Gui, T., Dai, J., Guo, Q., Zhang, Z., Qiu, X., 2021a. A unified generative framework for various NER subtasks, in: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5808–5822.
- <span id="page-98-14"></span>Yan, Y., Song, S., 2022. Local hypergraph-based nested named entity recognition as query-based sequence labeling. arXiv preprint arXiv:2204.11467 .
- <span id="page-98-16"></span>Yan, Z., Zhang, C., Fu, J., Zhang, Q., Wei, Z., 2021b. A partition filter network for joint entity and relation extraction, in: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 185–197.
- <span id="page-98-3"></span>Yang, C.L., Gordon, P.C., Hendrick, R., Wu, J.T., 1999. Comprehension of referring expressions in chinese. Language and Cognitive Processes 14, 715–743.
- <span id="page-98-13"></span>Yang, S., Tu, K., 2022. Bottom-up constituency parsing and nested named entity recognition with pointer networks, in: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2403–2416.
- <span id="page-98-24"></span><span id="page-98-0"></span>Yang, X., Bian, J., Hogan, W.R., Wu, Y., 2020. Clinical concept extraction using transformers. J. Am. Medical Informatics Assoc. 27, 1935–1942. Yang, X., Zhou, G., Su, J., Tan, C.L., 2003. Coreference resolution using competition learning approach, in: Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 176–183.
- <span id="page-98-15"></span>Yang, Y., Katiyar, A., 2020. Simple and effective few-shot named entity recognition with structured nearest neighbor learning, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pp. 6365–6375.
- <span id="page-98-31"></span>Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., Hovy, E., 2016. Hierarchical attention networks for document classification, in: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1480–1489.
- <span id="page-98-18"></span>Yao, Y., Zhang, Z., Xu, Y., Li, C., 2022. Data augmentation for few-shot knowledge graph completion from hierarchical perspective, in: Proceedings of the 29th International Conference on Computational Linguistics, pp. 2494–2503.
- <span id="page-98-2"></span>Ye, D., Lin, Y., Du, J., Liu, Z., Li, P., Sun, M., Liu, Z., 2020. Coreferential Reasoning Learning for Language Representation, in: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 7170–7186.
- <span id="page-98-23"></span>Ye, H., Wang, L., 2018. Semi-supervised learning for neural keyphrase generation, in: Riloff, E., Chiang, D., Hockenmaier, J., Tsujii, J. (Eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4142–4153.
- <span id="page-98-4"></span>Yin, Q., Zhang, W., Zhang, Y., Liu, T., 2017a. A deep neural network for chinese zero pronoun resolution, in: Proceedings of the 26th International Joint Conference on Artificial Intelligence, pp. 3322–3328.
- <span id="page-98-5"></span>Yin, Q., Zhang, W., Zhang, Y., Liu, T., 2019. Chinese zero pronoun resolution: A collaborative filtering-based approach. ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP) 19, 1–20.
- <span id="page-98-7"></span>Yin, Q., Zhang, Y., Zhang, W., Liu, T., 2017b. Chinese zero pronoun resolution with deep memory network, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1309–1318.

<span id="page-98-6"></span>Yin, Q., Zhang, Y., Zhang, W., Liu, T., Wang, W.Y., 2018a. Zero pronoun resolution with attention-based neural network, in: Proceedings of the 27th International Conference on Computational Linguistics, pp. 13–23.

- <span id="page-98-8"></span>Yin, Q., Zhang, Y., Zhang, W.N., Liu, T., Wang, W.Y., 2018b. Deep reinforcement learning for Chinese zero pronoun resolution, in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 569–578.
- <span id="page-98-25"></span>Young, T., Cambria, E., Chaturvedi, I., Zhou, H., Biswas, S., Huang, M., 2018. Augmenting end-to-end dialogue systems with commonsense knowledge, in: Proceedings of the AAAI conference on artificial intelligence, pp. 4970–4977.
- <span id="page-98-17"></span>Yu, B., Zhang, Z., Shu, X., Liu, T., Wang, Y., Wang, B., Li, S., 2020. Joint extraction of entities and relations based on a novel decomposition strategy, in: ECAI 2020. IOS Press, pp. 2282–2289.
- <span id="page-98-28"></span>Yu, H., Hatzivassiloglou, V., 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences, in: Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pp. 129–136.

<span id="page-99-2"></span>Yuan, D., Richardson, J., Doherty, R., Evans, C., Altendorf, E., 2016. Semi-supervised word sense disambiguation with neural models, in: Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pp. 1374–1385.

<span id="page-99-6"></span><span id="page-99-3"></span>Zeldes, A., 2017. The GUM corpus: Creating multilayer resources in the classroom. Language Resources and Evaluation 51, 581–612. Zhang, H., Song, Y., Song, Y., Yu, D., 2019. Knowledge-aware pronoun coreference resolution, in: Proceedings of the 57th Annual Meeting of the

<span id="page-99-18"></span>Association for Computational Linguistics, pp. 867–876. Zhang, J., Chen, B., Zhang, L., Ke, X., Ding, H., 2021a. Neural, symbolic and neural-symbolic reasoning on knowledge graphs. AI Open 2, 14–35.

<span id="page-99-13"></span>Zhang, J., Yang, Y., Chen, C., He, L., Yu, Z., 2021b. KERS: A knowledge-enhanced framework for recommendation dialog systems with multiple subgoals, in: Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 1092–1101.

- <span id="page-99-10"></span>Zhang, M., Zhang, Y., Fu, G., 2017a. End-to-end neural relation extraction with global optimization, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 1730–1740.
- <span id="page-99-14"></span>Zhang, Q., Wang, Y., Gong, Y., Huang, X., 2016. Keyphrase extraction using deep recurrent neural networks on twitter, in: Su, J., Carreras, X., Duh, K. (Eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 836–845.
- <span id="page-99-5"></span>Zhang, R., Nogueira dos Santos, C., Yasunaga, M., Xiang, B., Radev, D., 2018. Neural coreference resolution with deep biaffine attention by joint mention detection and mention clustering, in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 102–107.
- <span id="page-99-15"></span>Zhang, W., Yu, C., Meng, W., 2007. Opinion retrieval from blogs, in: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pp. 831–840.
- <span id="page-99-0"></span>Zhang, X., Mao, R., Cambria, E., 2023. A survey on syntactic processing techniques. Artificial Intelligence Review 56, 5645–5728. doi:[10.1007/](http://dx.doi.org/10.1007/s10462-022-10300-7) [s10462-022-10300-7](http://dx.doi.org/10.1007/s10462-022-10300-7).
- <span id="page-99-9"></span>Zhang, Y., Zhong, V., Chen, D., Angeli, G., Manning, C.D., 2017b. Position-aware attention and supervised data improve slot filling, in: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 35–45.
- <span id="page-99-17"></span>Zhang, Z., Li, Z.N., Drew, M.S., 2010. AdaMKL: A novel biconvex multiple kernel learning approach, in: 2010 20th International Conference on Pattern Recognition, pp. 2126–2129.
- <span id="page-99-16"></span>Zhao, H., Lu, Z., Poupart, P., 2015. Self-adaptive hierarchical sentence model, in: Twenty-fourth International Joint Conference on Artificial Intelligence, pp. 4069–4076.
- <span id="page-99-4"></span>Zhao, J., Wang, T., Yatskar, M., Ordonez, V., Chang, K.W., 2018. Gender bias in coreference resolution: Evaluation and debiasing methods, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 15–20.
- <span id="page-99-7"></span>Zhao, S., Ng, H.T., 2007. Identification and resolution of Chinese zero pronouns: A machine learning approach, in: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 541–550.
- <span id="page-99-11"></span>Zheng, S., Wang, F., Bao, H., Hao, Y., Zhou, P., Xu, B., 2017. Joint extraction of entities and relations based on a novel tagging scheme, in: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1227–1236.
- <span id="page-99-8"></span>Zhong, Z., Chen, D., 2021. A frustratingly easy approach for entity and relation extraction, in: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics. pp. 50–61.
- <span id="page-99-1"></span>Zhong, Z., Ng, H.T., 2010. It makes sense: A wide-coverage word sense disambiguation system for free text, in: Proceedings of the ACL 2010 System Demonstrations, pp. 78–83.
- <span id="page-99-12"></span>Zhou, K., Zhao, W.X., Bian, S., Zhou, Y., Wen, J.R., Yu, J., 2020. Improving conversational recommender systems via knowledge graph based semantic fusion, in: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1006–1014.
