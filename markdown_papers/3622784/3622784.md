![](_page_0_Picture_0.jpeg)

# **Lessons Learnt from a Multimodal Learning Analytics Deployment In-the-Wild**

[ROBERTO MARTINEZ-MALDONADO](https://orcid.org/0000-0002-8375-1816) and [VANESSA ECHEVERRIA,](https://orcid.org/0000-0002-2022-9588) Monash University, Australia and Escuela Superior Politécnica del Litoral, Ecuador [GLORIA](https://orcid.org/0000-0003-2681-4451)[FERNANDEZ-NIETO,](https://orcid.org/0000-0003-2681-4451) [LIXIANG YAN,](https://orcid.org/0000-0003-3818-045X) [LINXUAN ZHAO,](https://orcid.org/0000-0001-5564-0185) [RIORDAN ALFREDO,](https://orcid.org/0000-0001-5440-6143) XINYU LI, [SAMANTHA DIX,](https://orcid.org/0000-0003-4414-7445) [HOLLIE JAGGARD,](https://orcid.org/0000-0002-6741-3024) [ROSIE](https://orcid.org/0000-0001-8887-3188)[WOTHERSPOON, and](https://orcid.org/0000-0001-8887-3188) ABRA OSBORNE, Monash University, Australia [SIMON BUCKINGHAM SHUM,](https://orcid.org/0000-0002-6334-7429) University of Technology Sydney, Australia [DRAGAN GAŠEVIĆ,](https://orcid.org/0000-0001-9265-1908) Monash University, Australia

Multimodal Learning Analytics (MMLA) innovations make use of rapidly evolving sensing and artificial intelligence algorithms to collect rich data about learning activities that unfold in physical spaces. The analysis of these data is opening exciting new avenues for both studying and supporting learning. Yet, practical and logistical challenges commonly appear while deploying MMLA innovations "in-the-wild". These can span from technical issues related to enhancing the learning space with sensing capabilities, to the increased complexity of teachers' tasks. These practicalities have been rarely investigated. This article addresses this gap by presenting a set of lessons learnt from a 2-year human-centred MMLA in-the-wild study conducted with 399 students and 17 educators in the context of nursing education. The lessons learnt were synthesised into topics related to (i) technological/physical aspects of the deployment; (ii) multimodal data and interfaces; (iii) the design process; (iv) participation, ethics and privacy; and (v) sustainability of the deployment.

CCS Concepts: • **Human-centered computing** → **Empirical studies in ubiquitous and mobile computing**; **Collaborative and social computing systems and tools;**

Additional Key Words and Phrases: Learning analytics, sensors, ubiquitous computing, human-centred design, CSCW

© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.

1073-0516/2023/11-ART8 \$15.00 <https://doi.org/10.1145/3622784>

R. Martinez-Maldonado and V. Echeverria contributed equally to this research.

This research was funded partially by the Australian Government through the Australian Research Council (project number DP210100060). Roberto Martinez-Maldonado's research is partly funded by Jacobs Foundation.

Authors' addresses: R. Martinez-Maldonado, G. Fernandez-Nieto, L. Yan, L. Zhao, R. Alfredo, X. Li, and D. Gašević, Faculty of Information Technology, Monash University, 25 Exhibition Walk, Clayton VIC 3168, Australia; e-mails: roberto. martinezmaldonado@monash.edu, gloriamilena.fernandeznieto@monash.edu, lixiang.yan@monash.edu, linxuan.zhao@ monash.edu, riordan.alfredo@monash.edu, xinyu.li@monash.edu, dragan.gasevic@monash.edu; V. Echeverria, Faculty of Information Technology, Monash University, 25 Exhibition Walk, Clayton VIC 3168, Australia and Escuela Superior Politécnica del Litoral, Vía Perimetral 5, Guayaquil, Guayas 090112, Ecuador; e-mail: vanessa.echeverria@monash.edu; S. Dix, H. Jaggard, R. Wotherspoon, and A. Osborne, Faculty of Medicine, Nursing And Health Sciences, Monash University, McMahons Road, Frankston VIC 3199 3168, Australia; e-mails: samantha.dix@monash.edu, hollie.jaggard@monash.edu, rosie.wotherspoon@monash.edu, abra.osborne@monash.edu; S. Buckingham Shum, Connected Intelligence Centred, University of Technology Sydney, Bldg 22, 2 Blackfriars Street, Chippendale NSW 2008, Australia; e-mail: simon. buckinghamshum@uts.edu.au.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from [permissions@acm.org.](mailto:permissions@acm.org)

#### **ACM Reference format:**

Roberto Martinez-Maldonado, Vanessa Echeverria, Gloria Fernandez-Nieto, Lixiang Yan, Linxuan Zhao, Riordan Alfredo, Xinyu Li, Samantha Dix, Hollie Jaggard, Rosie Wotherspoon, Abra Osborne, Simon Buckingham Shum, and Dragan Gašević. 2023. Lessons Learnt from a Multimodal Learning Analytics Deployment In-the-Wild. *ACM Trans. Comput.-Hum. Interact.* 31, 1, Article 8 (November 2023), 41 pages. <https://doi.org/10.1145/3622784>

### **1 INTRODUCTION**

**Multimodal Learning Analytics** (**MMLA**) is a relatively new area that formally emerged as such in 2012 at the ACM International Conference of Multimodal Interaction (ICMI'12) [\[90\]](#page-38-0). Since then, MMLA innovations have been opening exciting new avenues for supporting and generating a deep understanding of human learning by embracing the complexity of learners and their learning activities. MMLA systems are pushing the boundaries in educational research by de-emphasising the computational analysis of student interactions via online learning systems based on input devices such as the mouse and the keyboard [\[113\]](#page-39-0). In contrast, MMLA emphasises the many ways in which students interact with other students—fully mediated [\[109\]](#page-39-0), partly mediated [\[93\]](#page-38-0) or unmediated [\[104\]](#page-39-0) by technology—with both teachers [\[28\]](#page-35-0), and with physical learning environments [\[114\]](#page-39-0). MMLA research also investigates learner attributes that are hard to automatically analyse without the use of specialised sensing systems, such as student emotions [\[38\]](#page-36-0), cognitive states [\[58\]](#page-37-0), distraction [\[51\]](#page-36-0), and stress [\[85\]](#page-38-0). In practice, MMLA endeavours often make use of sensors, such as eye-trackers, positioning systems, wearable microphones and physiological writs/chest bands, and advanced audio and video processing algorithms [\[7,](#page-34-0) [117\]](#page-40-0) that generate large amounts of multimodal data that can be analysed to gain a more holistic view of intrinsically complex human learning phenomena such as problem-based learning [\[54\]](#page-36-0), effective collocated collaboration [\[92,](#page-38-0) [102\]](#page-39-0) and teamwork [\[34\]](#page-35-0), self-regulated learning [\[8\]](#page-34-0), engagement in adaptive online learning [\[79\]](#page-38-0), socially-shared regulated learning [\[70\]](#page-37-0), effective public speech [\[73\]](#page-37-0), motor learning [\[26\]](#page-35-0), and classroom teaching [\[3,](#page-34-0) [82\]](#page-38-0). In a sense, MMLA aims at crystallising Mark Weiser's vision [\[110\]](#page-39-0) in educational contexts by creating learning spaces enhanced with ubiquitous computing capabilities to augment teachers' and students' activities. This can be achieved by creating applications that automate the capture of the lived experiences occurring in the learning space to allow later access to those experiences by educational stakeholders [\[1\]](#page-34-0) for the purpose of supporting reflection and learning [\[16\]](#page-34-0). Yet, enhancing learning environments with such sensing capabilities can easily impose practical challenges, such as elevating the cost of implementation and making it harder to scale up the MMLA deployments in relation to fully digital **learning analytics** (**LA**) solutions.

Although recent literature reviews suggest that most MMLA solutions rely on video and audio analysis [\[7,](#page-34-0) [117\]](#page-40-0) (which should not be, in principle, too costly), MMLA researchers consistently report that most current MMLA systems only reach the prototype stage [\[24,](#page-35-0) [72\]](#page-37-0). Several practical challenges have been identified as potential threats to the broader adoption of MMLA that can ultimately impact the effectiveness of such innovations in supporting teaching and learning. Challenges include those related to the complexity of some sensor installation requirements and lack of technology readiness to enable full-scale deployments [\[21,](#page-35-0) [72\]](#page-37-0); lack of maturity from an analysis perspective to find causal relationships in multimodal data that can translate into actual improvements in learning outcomes [\[7\]](#page-34-0); misalignment between the technological MMLA innovation and the learning design [\[97\]](#page-39-0); and ethical concerns that can ultimately dissuade educational stakeholders from adopting such complex solutions [\[7,](#page-34-0) [23,](#page-35-0) [24\]](#page-35-0). While the number of small-scale laboratory studies—conducted under controlled conditions—is similar to the number of small-scale ecological (authentic) studies that have deployed sensors at the classroom level [\[20\]](#page-35-0), the majority of MMLA

<span id="page-2-0"></span>![](_page_2_Picture_1.jpeg)

Fig. 1. Sensors deployed in the high-fidelity simulation with a team of four students and two educators enacting the roles of a patient's relative and a doctor who provides some information.

studies (more than two-thirds, according to a recent literature review by Yan et al. [\[117\]](#page-40-0)) do not report enough methodological details to allow other researchers to replicate or learn from such deployments. Moreover, there is a lack of relatively large-scale studies conducted under authentic conditions that close the "data loop", that is, transitioning from multimodal data collection and analysis to the provision of some form of end-user interface [\[117\]](#page-40-0), which is one of the ultimate goals in LA research [\[111\]](#page-39-0). This makes it hard for MMLA researchers to fully understand the extent of the challenges involved in deploying MMLA innovations *in-the-wild* (i.e., a deployment that is as naturalistic as possible). In-the-wild studies are commonly conducted in **human–computer interaction** (**HCI**) research with the aim of investigating the implications of embedding new technology interventions in everyday situations [\[22,](#page-35-0) [84\]](#page-38-0). Conducting MMLA in-the-wild studies can thus contribute to understanding the challenges that need to be addressed to maximise the adoption of MMLA innovations.

This article addresses the above knowledge gap by synthesising a set of lessons learnt from a large human-centred MMLA study conducted in-the-wild (e.g., see Figure 1) in the context of nursing education. To the best of our knowledge, and according to the most recent MMLA systematic literature review [\[117\]](#page-40-0), this is the first large, complex MMLA study that closes the LA loop by providing direct feedback to students on a group task using MMLA-based visual interfaces. A total of 399 higher-education nursing students and 17 educators, who gave their consent, participated in the research within the context of their regular classes. A range of sensing devices to capture multimodal data was used, namely, video recorders, microphones, physiological sensors, positioning sensors, and an educator's logging system. The study spanned two years in which two deployment iterations were conducted. The first iteration focused on collecting multimodal data for research and design purposes. The second iteration focused on the deployment of a MMLA dashboard interface to support teacher-led, team reflection in the classroom. Evidence about the deployment included a set of interviews, surveys and focus groups with teachers, students and researchers to understand more about the practical challenges that emerged. This study offers pivotal findings for HCI and LA researchers concerning the deployment of MMLA systems in genuine educational contexts, tackling key elements such as stakeholder concerns, technological preparedness, data integrity, and learner-centric design, with the aim of augmenting the learning experience and fostering sustainability.

### **2 RELATED WORK**

In this section, we first provide an overview of MMLA and its evolution over the past decade. Next, we examine recent systematic literature reviews on MMLA and related works, which highlight key logistical, privacy, and ethical challenges that must be tackled for this promising field to continue being relevant and positively influence educational practices.

### <span id="page-3-0"></span>**2.1 A Historical Overview of MMLA**

As an interdisciplinary research field, MMLA has evolved significantly over the past decades, combining theoretical foundations and applications from various fields, such as the learning sciences, LA, **artificial intelligence** (**AI**), and HCI to gain insights into teaching and learning processes [\[75\]](#page-38-0). The emergence of LA in the early 2010s [\[100\]](#page-39-0) laid the foundation for MMLA, which emphasised the importance of interdisciplinary collaboration and the analysis of multiple sources of educational data [\[29\]](#page-35-0). The first international workshop dedicated to MMLA was organised at an HCI conference in 2012 (ICMI'12) [\[90\]](#page-38-0). Yet, MMLA was later formalised as a distinct research field in 2013 when Blikstein [\[14\]](#page-34-0) explicitly identified the opportunity of using high-frequency data collection technologies and machine learning analysis techniques to capture and analyse learning and teaching beyond computer-mediated contexts (e.g., physical and physiological traces that unfolds in face-to-face classrooms). Back then, MMLA studies were mostly limited to studying small group collaborations that unfold around a table and under a controlled environment (e.g., in lab settings) [\[63\]](#page-37-0). The sources of data were also mostly video and audio recordings due to technological limitations. As sensing technologies become more mature over the years, MMLA has experienced rapid growth and diversification, incorporating new modalities of data sources and exploring various application scenarios [\[112\]](#page-39-0). For instance, wearable positioning tracing systems and biometric sensors have become available for capturing spatial [\[88\]](#page-38-0) and physiological data [\[27\]](#page-35-0), enabling the modelling of complex educational constructs (e.g., students' metacognitive experiences). In recent years, MMLA research has started to be applied to more authentic educational situations with in-the-wild studies being conducted across different learning environments, such as early education classrooms [\[88\]](#page-38-0), school libraries [\[83\]](#page-38-0), open-plan primary schools [\[114\]](#page-39-0), nursing simulation classrooms [\[34\]](#page-35-0), and exhibition centres [\[55\]](#page-36-0). This shift from controlled lab environments to realworld settings has revealed several significant challenges in incorporating MMLA technologies into routine educational practices. We will explore these challenges in more detail in the following subsections.

### **2.2 Logistical Challenges**

Most MMLA studies so far have primarily focused on developing prototypes and testing the functionality of different combinations of sensors and analytics approaches [\[64,](#page-37-0) [71,](#page-37-0) [96\]](#page-39-0). Yet, many concerns have been raised regarding the logistical challenges that can emerge when moving from controlled settings to in-the-wild MMLA deployments such as the added intrusiveness of sensing devices and complexity in their installation and orchestration [\[19,](#page-35-0) [20,](#page-35-0) [78\]](#page-38-0). Yan et al. [\[117\]](#page-40-0) systematically reviewed these logistical issues and identified a relatively low level of technology readiness regarding existing MMLA innovations, resulting in heavy reliance on the onsite support of researchers or technicians. This undermines the sustainability of these systems and unnecessarily increases the complexity of the learning situation from the teachers' perspective. While most of the sensing technologies used in MMLA research can be purchased off-the-shelf, implementing these technologies in authentic physical learning spaces often requires extensive technical background for tasks such as physical installation, system integration, and modalities synchronisation [\[19,](#page-35-0) [23,](#page-35-0) [64,](#page-37-0) [96\]](#page-39-0). There is also a tradeoff between data quality and affordability as most of the MMLA innovations that rely on mature sensing technologies, such as location sensors, eye-trackers, and biometric sensors, can be financially unscalable due to the high unit prices [\[117\]](#page-40-0). Although lowcost alternatives are emerging (e.g., [\[74,](#page-37-0) [88\]](#page-38-0)), these technologies remain in the prototype and validation stages and often sacrifice accuracy or portability for affordability.

Likewise, the lack of MMLA studies that have closed the LA loop by providing some form of end-user interface makes it harder for educational stakeholders to weigh the benefits against the potential added complexity to their already rich educational ecologies [\[117\]](#page-40-0). Although the alignment between MMLA innovations and learning design should be one of the foundations for developing MMLA innovations [\[24,](#page-35-0) [72\]](#page-37-0), such alignment is rarely considered or reported in the existing literature, as noted in recent reviews [\[81,](#page-38-0) [97\]](#page-39-0). This can undermine teacher and student confidence, if they do not understand how the MMLA system aligns with their teaching practices or learning outcomes.

All of these challenges are hallmarks of emerging HCI infrastructures that must be co-evolved with work practices. Our in-the-wild MMLA deployment offered the opportunity to study how both educational and technical stakeholders learnt to work together to address the challenges.

### **2.3 Privacy Challenges**

As a research area that benefits from the data collection opportunities enabled by various sensing technologies, the privacy issues surrounding the adoption of MMLA innovations are the focus of critical debate. Crescenzi-Lanna [\[23\]](#page-35-0) emphasised the need to consider the privacy implications of using sensing technologies to generate analytics about children's activity. Such implications have also been identified by students and teachers who have expressed concerns regarding the security of their data [\[47,](#page-36-0) [57\]](#page-37-0). These privacy implications of MMLA innovations have been under-investigated in the literature [\[7,](#page-34-0) [78,](#page-38-0) [117\]](#page-40-0). Specifically, while most works published in MMLA mention that informed consent was obtained from participants, none of the existing works has elaborated on the consenting strategies they adopted, which could contribute valuable insights regarding data security measures for protecting individual privacy and maximising data autonomy (e.g., individuals' autonomy of removing their data from the database) [\[12\]](#page-34-0). Additionally, while most of MMLA innovations endeavour to provide dashboards and visualisations for supporting educational practices, privacy issues regarding who has the right to see these visualisations remain unclear, especially in the contexts of collaborative learning where, in most cases, individuals' personal trace data, even anonymised (e.g., masking students' identity with numbers or colours), could remain identifiable when used for provoking reflections at a group-level, since other students typically have the contextual knowledge to decode anonymised representations [\[7,](#page-34-0) [57\]](#page-37-0). Providing additional empirical evidence on educational stakeholders' perspectives of these privacy-related issues could potential benefit the on-going development of MMLA, and is a particular focus of this study.

### **2.4 Ethical Challenges**

Beyond logistical and privacy issues, the potential biases in analytics, and cognitive dissonances that may be caused by the inconsistency between individuals' observations and generated insights, could also undermine the potential benefits of MMLA innovations [\[33,](#page-35-0) [78\]](#page-38-0). Such issues are vital as the accuracy of the existing MMLA-based predictive models and early-warning systems are far from suitable for practical deployment (e.g., rarely above 80% accuracy), and these models have mostly been developed and evaluated based on relatively small sample sizes (i.e., with *n* < 50) [\[117\]](#page-40-0). These small sample sizes combined with the poor reporting standards found in the existing MMLA literature could also mask potential algorithmic biases that may disadvantage certain minority groups of students as replicating these studies remain difficult without adequately reported methodologies [\[53,](#page-36-0) [117\]](#page-40-0). Additionally, significant concerns have been highlighted regarding the need to enhance trust and data transparency within MMLA systems [\[7\]](#page-34-0) and in AIpowered educational technologies more generally [\[67,](#page-37-0) [68\]](#page-37-0). Moreover, Yan et al. [\[114\]](#page-39-0) suggested that more research needs to be done to assess the potential risk of making decisions with incomplete multimodal data. Consequently, understanding the ethical practices of using these analytics

<span id="page-5-0"></span>is also essential but rarely considered in prior literature [\[94\]](#page-38-0) and requires the participation of key educational stakeholders such as students and educators [\[78\]](#page-38-0). A large-scale in-the-wild study opens new opportunities to study approaches to these ethical challenges under more authentic conditions than has been reported to date.

### **2.5 Contribution to HCI and Research Question**

Against the literature reviewed above, we formulate the following **research question** (**RQ**) that guided our study:

*RQ: What logistical, privacy and ethical challenges emerge from a complex MMLA, in-the-wild study that closes the analytics loop by providing direct feedback to students?*

In addressing this question, the contribution of this article is a set of lessons learnt regarding how such challenges were or could have been, addressed in the context of a two-year deployment of a MMLA system in an authentic educational scenario. The implications of this study should assist researchers, developers and designers in making informed decisions about the effective deployment of innovations that involve the use of ubiquitous computing technologies, sensing devices and AI algorithms to augment teaching and learning in physical spaces. The closest work to ours was presented in a position paper authored by Chejara et al. [\[19\]](#page-35-0), which discusses, from the authors' perspective, the logistical obstacles they faced while introducing their MMLA innovation into the classroom setting. Our research extends beyond this work by conducting an in-depth analysis of our iterative MMLA deployment, integrating feedback from teachers and students and addressing concerns that exceed the researchers' viewpoint of deployment, specifically focusing on challenges related to teaching, learning, ethics, privacy, and sustainability.

### **3 STUDY IN THE WILD**

### **3.1 Context**

In this article, we present a study that followed a human-centered LA approach [\[17\]](#page-35-0). A collaboration between a team of four teachers (*senior teaching team*) and four LA researchers (*researcher team*) was formed to gradually co-create an MMLA innovation for regular classes in an undergraduate nursing course at Monash University. Students were also consulted to understand: (i) how their learning experiences could be affected by sensing technologies, (ii) their ethical and privacy concerns, and (iii) how effectively data insights could support their learning. While detailed information about the co-creation process is beyond the scope of this paper and can be found elsewhere [\[31,](#page-35-0) [35\]](#page-35-0), essential details about educators' and students' involvement are provided below.

In the targeted course, high-fidelity, immersive team simulations are typically conducted to help students develop effective collaboration and communication skills while learning from errors in a safe environment [\[89\]](#page-38-0). High-fidelity simulation is a healthcare education methodology, conducted in a realistic but simulated health setting environment, where clinical situations that students may encounter in the workplace are reproduced using sophisticated manikins as patients [\[59\]](#page-37-0). In these simulations, students are often posed with a situation that they need to address without the instruction of a teacher, followed by a reflective *debrief*, facilitated by a senior teacher, in which students reflect on their actions and learning. While video analysis is often used as support material for debriefs, the literature suggests that pre-selecting relevant video clips is more effective than watching the entire video [\[50,](#page-36-0) [91\]](#page-38-0). However, watching full videos or pre-selecting clips may not be feasible for scaling up feedback in time-limited situations, such as in university settings. This constraint led the senior teaching team to establish the educational goal of the MMLA deployment: to support student feedback during debriefs by using the data to highlight important aspects of team activity.

### <span id="page-6-0"></span>**3.2 Study Iterations and Participants**

The MMLA study had two iterations. The first, conducted in 2021, focused on (i) collecting a rich multimodal dataset, (ii) enhancing the understanding of the senior teaching team about the possibilities enabled by the multimodal data, and (iii) asking students about envisaged uses of their data and potential concerns regarding the use of sensors in their regular learning spaces. The second, conducted in 2022, focused on (i) closing the LA loop by deploying MMLA visual interfaces to support the reflective debrief, and (ii) expanding the multimodal dataset. Both iterations were conducted under almost identical conditions: the same course, learning goals, senior teaching team, and lesson design.

A total of 399 students (avg. age: 24 years, std. dev: 5.6) agreed to take part in the study (261 – 196 females—consenting out of 461 enrolled students in iteration 1; and 138 – 114 females—out 358 in iteration 2). Some students were invited to follow-up activities for them to provide their feedback based on their lived experiences (see details in the next section). Besides the four senior teachers, another 13 teachers were involved in both years facilitating the samelesson plan.

### **3.3 The Authentic Learning Situation**

Each 3-hour class was typically attended by 10–15 students and was conducted across two learning spaces: a regular classroom and the specialised simulation classroom. The latter featured four beds with a patient manikin in each of them, as shown in Figure [1.](#page-2-0) Two consecutive simulations would be conducted during the class, both focused on prioritising care and identifying the deteriorating patient who required urgent attention. Students were given important information called a *handover* before commencing, and then asked to provide care in teams according to the assessment they conducted on each manikin. Each team was made up of four students who volunteered to play either a graduate or ward nurse. These students were also asked to optionally consent to be part of the study. Two teachers enacted the roles of a patient's family member and a doctor assisting with patient care after students identified a problem. Other students were invited to be observers, watching the simulation unfold. Immediately after each simulation, a whole class debrief conversation was led by a teacher in the regular classroom.

Verbal communication is essential in learning about patient care management and coordination in nursing simulations [\[11\]](#page-34-0). Research has shown that even non-verbal metadata from students' speech can offer insights into team dynamics [\[119\]](#page-40-0). Nurses often need to strategically position themselves for effective patient care [\[118\]](#page-40-0), and indoor positioning data has shown potential in assessing team dynamics in nursing education by providing insights on nurses' positioning relative to patients and colleagues [\[34\]](#page-35-0). Physiological data can also help teachers support nursing students, who frequently experience high stress levels [\[66\]](#page-37-0), during debriefs and assist in developing stresscoping strategies [\[5\]](#page-34-0). These identified educational needs and our previous work on separate data streams [\[5,](#page-34-0) [34,](#page-35-0) [66,](#page-37-0) [119\]](#page-40-0) inspired the development of our ecology of multimodal sensors that is described in the following section.

### **3.4 Apparatus**

Before entering the simulation classroom, consenting students were asked to wear a number of devices, namely, (1) a wireless headset with an unidirectional microphone; (2) a physiological Empatica E4 wristband, with built-in sensors to capture heart rate variability, electrodermal skin activity among other physiological measures that can be related to stress and arousal; and (3) a Pozyx indoor positioning locator, with built-in sensors that capture *x-y* position and body orientation of each student in the learning space (see Figure [2\)](#page-7-0). Each set of sensors was colour-coded according to the role enacted by each student (i.e., red and blue for graduate nurses, and yellow and green

<span id="page-7-0"></span>![](_page_7_Picture_1.jpeg)

Fig. 2. MMLA deployed in an authentic healthcare education setting. Left: Illustration of a student wearing the sensors during a team simulation. Right: Each sensor set to be worn by each student within a team (A– an indoor positioning locator inside a belly bag, B–a wireless microphone, and C–a physiological wristband) was placed by the teaching team on coloured trays to enable easy access to and organisation of the sensors during and between classes.

for ward nurses). The simulation room was already equipped with a set of built-in video cameras, and an additional 180-degree video camera was added to the set-up.

All the data were captured and synchronised in *real-time* using our open-source MMLA infrastructure called [YarnSense.](https://teamwork-analytics.github.io/yarn-sense) This employs a microservice architectural pattern, creating a collection of small, loosely coupled services that can be independently developed, deployed, and scaled. Each microservice is responsible for specific functionality, promoting better separation of concerns, ease of maintenance, and scalability. The microservices for each data stream, including video, position, audio, and physiological data, were developed using the Java Spring Framework and managed by an Eureka server. Due to the large volume of data, a Kafka server was implemented as middleware to queue position and Empatica data. This ensures efficient data handling and seamless integration between microservices.

To synchronise the timeline of all data channels, timestamps are recorded for each data collection action (e.g., a signal is sent across all channels once the teacher or a technical assistant presses a button indicating that a team session has commenced). The physiological data service utilises the vendor's software for each wristband to send data via Bluetooth to a centralised server that can then make the data available to our infrastructure. However, in practice, this third-party software is not regularly maintained and it is unstable, leading to constant data loss. After testing, we had to collect physiological data directly into the wristband, making these data unavailable for use in the debrief.

It should be noted that despite these devices, this experience was not completely novel to students, since these simulations are commonly video recorded and students often wear lapel microphones for the rest of the class to observe the simulation from the regular classroom. After each simulation, the reflective debrief was conducted with both the team who participated in the simulation and the rest of the class in the regular classroom. A dashboard displaying representations of the multimodal data collected during the simulation was deployed in the debriefs, as detailed next.

### **3.5 Potential Benefits to Learning and Teaching**

In the first iteration, numerous potential learning and teaching benefits were identified through data analysis and stakeholder evaluations. For supporting formative assessment in simulationbased learning, it was found that students' task prioritisation and team collaboration behaviours, captured by indoor positioning sensors, correlated strongly with their team performance, and could distinguish between low and high-performing teams (see Reference [\[115\]](#page-39-0) (for more details). Teachers believed that such behavioural evidence could be valuable for motivating and guiding students' self-reflection in post-simulation debriefs, particularly if differences in team behaviours could be identified in comparison to previous high-performing teams. These differences might help students better understand the prioritisation and collaboration strategies of high-performing teams, such as which bed to focus on and how often to collaborate on various tasks.

Likewise, we also identified that students' verbal communication captured using microphones also contained rich information for providing evidence-based feedback to students (see Reference [\[119\]](#page-40-0) for more details). For example, while task coordination was important, verbal evidence suggested that low-performing teams were overly focused on discussing task planning and task allocation at the expense of other important behaviours (e.g., assessing patient's situation and sharing gathered evidence among the team). Such insights could foster students' understanding of the key feature of highly-effective teams in healthcare across different situations.

Moreover, insights gained from students' physiological data could potentially enhance teachers' understanding of the hidden effects of learning design on students' cognitive and emotional responses (see Reference [\[116\]](#page-39-0) for more details)). Specifically, we found that in learning contexts where students were required to collaborate (e.g., being asked by a patient's relative played by a staff member), the act of collaboration might not have necessarily led to higher collaboration satisfaction, as students could feel they had little control over such behaviours.

In summary, during the first iteration, we conducted various modelling studies to extract meaningful insights from students' physical and physiological behaviour traces. Although teachers considered these insights potentially useful, we had not yet synthesised them into a student and teacher-facing dashboard or used them during debriefs to support formative assessments and evidence-based feedback.

### **3.6 The MMLA Dashboard**

In the second iteration, we designed a dashboard (depicted in Figure [3\)](#page-9-0) that incorporated some of the analytics discussed above. This dashboard was displayed automatically after each simulation on the main screen in the regular classroom (Figure [3,](#page-9-0) left) and was used by teachers to facilitate reflective debriefing. The senior teaching team helped design a set of three MMLA visualisations, each utilising multiple data modalities. These visualisations represented critical teamwork dynamics related to positioning, prioritisation, and team communication, as described in Section [3.3.](#page-6-0)

For instance, Figure [3](#page-9-0) (right) displays one of these visualisations, inspired by a technique for representing multimodal, spatial data [\[37\]](#page-36-0). This visualisation uses hexagons to indicate the locations of colour-coded students in the simulation space when they were actively conversing with each other or the patient. We automatically triangulated x-y positions from indoor positioning sensors and determined speech presence using a **voice activity detection** (**VAD**) algorithm applied to the multi-channel microphone signals. Each hexagonal data point summarizes which student was primarily speaking in that position, providing evidence of their positioning and communication during patient care.

The second visualisation is a bar chart that summarises x-y positioning data automatically coded according to primary and secondary tasks (see details in Reference [\[115\]](#page-39-0)), which is crucial for understanding teamwork prioritisation. The third visualisation is a sociogram (a network-based chart) often used to depict the extent of verbal communication among team members (e.g., Reference [\[43\]](#page-36-0)). It is important to note that while physiological data was captured in this study and analysed for research purposes [\[116\]](#page-39-0), we were not able to obtain real-time data and analytics as explained above. The impact of technical issues on the learning experience is discussed in detail

# <span id="page-9-0"></span>8:10 R. Martinez-Maldonado et al.

![](_page_9_Picture_1.jpeg)

Fig. 3. Left: A teacher leading a team debrief using positioning and audio data in the MMLA dashboard. Right: The MMLA dashboard providing a menu of visualisations at the top. The selected example visualisation shows whether students, according to their role/colour, were speaking or not at certain positions of the learning space (see hexagons filled with student's colour or in grey, respectively).

later in Section [5.1.3.](#page-13-0) Further details about these visualisations go beyond the purpose of this article, which is focused on reporting the lessons learnt from the iterative deployment as a whole. Nonetheless, details and the source-code can also be found in our [YarnSense](https://teamwork-analytics.github.io/yarn-sense) repository.

# **4 METHODS**

### **4.1 Research "In-the-Wild"**

The phrase "research in-the-wild" is used in HCI studies to differentiate between research conducted in lab-based environments and research that involves embedding new technology interventions in everyday situations [\[22\]](#page-35-0). Arguably [\[86\]](#page-38-0), a key tenet of studies conducted in-the-wild is that they can provide more ecologically valid findings compared with typical measures collected under controlled conditions [\[9\]](#page-34-0). Indeed, in-the-wild studies often have to deal with a number of practical and ethical challenges and uncertainties rarely discussed in published studies [\[84\]](#page-38-0). Carefully identifying these can reveal the biases and the logistics that researchers and designers should consider to design and deploy emerging technologies in a specific context. Based on Rogers and Marshall's proposed framework [\[84\]](#page-38-0) to design for and analyse in-the-wild research, other authors, such as Balestrini et al. [\[9\]](#page-34-0), have suggested analysis approaches to identify practical challenges that can arise in an in-the-wild study. Moreover, in-the-wild research is often iterative since the valuable insights gained from real-world deployments that can inform subsequent redesigns (e.g., References [\[87,](#page-38-0) [101,](#page-39-0) [105\]](#page-39-0)). Inspired by this work [\[9\]](#page-34-0), we explored the main logistic, privacy and ethical challenges when designing and deploying MMLA research in-the-wild, organised around five main themes: (1) *space and place*—what is the impact of the computational system on the existing setting?; (2) *technology*—how was the technology used?—in this case, the data and the analytics; (3) *design*—what was the impact of the design approach?—in this case, human-centred design; (4) *social factors*—what are the stakeholders' concerns and expectations?; and (5) *sustainability* – what is needed for the new technology to be used continuously over time?

# **4.2 Sources of Evidence**

Table [1](#page-10-0) summarises the sources of evidence captured from students, teachers, and the researcher team, using a set of interviews and surveys. This section describes how the questions asked in these cover the five themes introduced above.

In the first iteration of the study, all participating students were asked to rate their perception of the intrusiveness of the sensing technology in the learning space (theme 1) using a seven-point

| Iteration | Participants | Sources of evidence    | Themes explored        |
|-----------|--------------|------------------------|------------------------|
| 1         | Students     | Survey (N<br>= 253)    | (1) Space and place    |
|           |              |                        | (1) Space and place    |
|           | Students     | Interviews (N<br>= 20) | (2) Data and analytics |
|           |              |                        | (4) Social factors     |
| 2         | Students     | Survey (N<br>= 47)     | (2) Data and analytics |
|           | Teachers     | Survey (N<br>= 11)     | (2) Data and analytics |
|           | Teachers     | Interviews (N<br>= 4)  | (1) Space and place    |
|           |              |                        | (2) Data and analytics |
|           |              |                        | (3) Human-centredness  |
|           |              |                        | (4) Social factors     |
|           |              |                        | (5) Sustainability     |
|           | Researchers  | Survey (N<br>= 5)      | (1) Space and place    |
|           |              |                        | (3) Human-centredness  |
|           |              |                        | (4) Social factors     |
|           |              |                        | (5) Sustainability     |

<span id="page-10-0"></span>Table 1. Sources of Evidence and Themes Explored Inspired by Balestrini et al's Approach [\[9\]](#page-34-0)

Likert scale. In addition, twenty volunteering students (18 females, avg. age: 22.21, std. dev.: 4.40— S1–S20) who participated in the first iteration also participated in 1-hour post-hoc individual interviews to explore their perceptions and experiences more in-depth in relation to the themes *1 space and place* (i.e., their perceptions on the intrusiveness of devices used and the data collection); *2- data and analytics* (their perceptions on the MMLA dashboard); and *4- social factors* (their concerns regarding the deployment). Moreover, students were presented with early prototypes of the visualisations that ended up in the MMLA dashboard in the second iteration using their own data. Each interview was recorded using an online video conferencing platform (i.e., Zoom) and lasted about 60 minutes.

In the second iteration, the four senior teaching team members (T1–T4, all females) and a total of 7 additional supporting teachers (T5–11, also all females) led the debriefs after each simulation across all classes. All teachers who led the debrief using the MMLA dashboard were asked to complete a survey immediately after each class. The survey comprised questions related to the second theme of the study in-the-wild (*technology—data and analytics*) to inquire about the helpfulness (e.g., *How the visualisations assisted you in the reflective debrief?*) and integration of the MMLA dashboard into the learning experience (*e.g., How did you integrate the visualisations into the debrief?)*. Moreover, the four senior teaching team members, who were also part of the teachers leading the debrief (T1–T4), participated in a post-hoc reflective interview session to gain insights into their experiences and challenges in using the MMLA dashboard in an authentic setting. We asked questions related to the five themes presented above to explore their perceptions of the study in-the-wild (e.g., regarding *1- space and place*: *How intrusive was it to equip students, teaching staff and the simulation space with various sensors? What kind of unexpected issues did you face that may have affected the learning goals of the simulations?*; *2- technology*: *How and for what purpose did you use the tool during the debrief?*; *3- design and human-centredness*: *To what extent do you value the collaboration with researchers to design this technology with them?*; *4- social factors*: *Did you perceive or hear any concerns from students regarding the study?*; and *5- sustainability*: *What steps would be needed to make the current system into a real-world application without the help of researchers behind it?*). Two interviews were conducted with two teachers at a time, each lasting about 40 minutes. Video recordings of all the interviews were fully transcribed for further analysis.

In addition, students who consented to participate in the second iteration were invited to participate in a survey to gather their perceptions on the trust of the MMLA dashboard information and their comments about their experience when navigating the information presented in the MMLA dashboard (2 - data and analytics). The survey showed the visualisations that were included in the dashboard using students' own data. Students were asked to rate their perception of trust using a five-point Likert scale (1 = *I would completely trust this information* - 5: *I would not trust this information*). They were also asked to explain their rate and give comments on their experience with the MMLA dashboard. A total of 47 students (40 females, avg. age: 23.81, std. dev: 5.61 - S21-67) completed this survey.

Finally, five members of the research team (R1–R5), who were the ones mostly involved in the deployment, were asked to fill in a survey to reflect on and document their challenges and experiences during both the first and second data collection. The questions were related to four themes (except technology since it is about how teachers and students used the technology) and were similar to those asked to teachers as presented above. The complete protocols for teachers' postreflection sessions and the research team survey can be found in the supplementary material (see Appendices [A.1-](#page-29-0)5, in the order, the surveys and interviews were described above).

### **4.3 Analysis**

We derived a set of lessons learnt by following a hybrid deductive and inductive thematic analysis approach [\[32,](#page-35-0) [106\]](#page-39-0). The *research team* and the *senior teaching team* convened multiple times to address challenges and concerns related strategies that might have affected students' learning experiences and the overall teaching process. The first deductive phase involved using the literature on research in-the-wild [\[9,](#page-34-0) [84\]](#page-38-0) to identify the *five themes*: (1) space and place; (2) data and analytics; (3) human-centredness; (4) social factors, and (5) sustainability. These themes also facilitated the gathering of additional evidence regarding the MMLA deployment, as previously described (Section [4.2\)](#page-9-0).

During the subsequent inductive phase, three researchers collaborated to identify a total of 337 quotes from interview transcripts and open-ended survey questions, which were extracted and assembled on a collaborative board for analysis and synthesis. They then organized all the quotes according to the five themes and inductively grouped similar quotes within each theme to discover emerging topics [\[15\]](#page-34-0). Redefining and merging topics occurred simultaneously and through consensus during multiple discussion sessions, adhering to recommended practices for qualitative analysis in HCI and CSCW research [\[61\]](#page-37-0). None of the three researchers participated in responding to the research team survey. By the end of this phase, 66 quotes were coded in relation to Space and Place; 188 in relation to Data and Analytics; 19 in relation to Human-centeredness; 64 in relation to Social Factors; and 33 in relation to Sustainability. Subsequently, all authors convened to discuss the lessons learned and how the evidence could illustrate the challenges encountered, taking into account the perspectives of various educational stakeholders.

### **5 LESSONS LEARNT**

### **5.1 Space and Place**

This theme included the following four topics: (i) intrusiveness of sensors; (ii) technology readiness; (iii) unexpected issues during the deployment; and (iv) the tradeoff among data quality, portability of sensors and affordability.

*5.1.1 Intrusiveness of Sensors.* This topic focuses on teachers' and students' perceptions of the intrusiveness of the MMLA deployment as a whole, and the wearables sensors in particular. All teachers agreed that the whole MMLA deployment was less intrusive than what they had initially expected. As expressed by T3: *when we look back at the previous year, we thought it was going to be a lot more intrusive than it turned out to be*. Yet, two teachers also mentioned that the deployment may have caused **distraction to some students**, as expressed by T1: *students may be distracted by the research happening instead of being aware of the actual learning*. For example, one observation made by the same teacher was that *sometimes researchers were entering and leaving the debriefing room to solve some technical issues*.

Teachers had mixed reactions about the intrusiveness of the sensors. Two teachers (T3 and T4) explained that the sensors were not intrusive to students while two other teachers (T1 and T2) believed that **sensors may have been uncomfortable for some students to wear**. T2 mentioned that *the belly bag was too tight for some students* and that *the microphone fell off for some students*. These issues may cause students to be *self-aware about the research* (T2). In contrast, most of the 261 students who participated in the first iteration reported that they felt comfortable wearing the sensors (Q: *I felt comfortable wearing the sensors during the simulation*, Median = 6, mean = 6.166, std = 1.07, min = 1, max = 7). In the more in-depth interviews with students, most of them (*N* = 18 out of 20) also expressed that it was fine to wear the sensors during the simulation (e.g., *It was fine wearing the sensors. I didn't mind wearing them* - S9). Some students explained that, at the beginning, they felt *conscious* (S20), *nervous* (S9) or *stressed* (S12) about being monitored. However, once they started the simulation, they focused more on their learning task than on the sensors. One student stated: *I was focused on my simulation. So, after I had them in and went into the simulation, it was fine. I didn't notice them.* (S17). Students also pointed out that it is normal for nurses to wear various devices or instruments. Thus, wearing sensors was not different from their usual practice. One student explained this as follows: *In placements, we're so used to wearing a pickpocket on the side. So it didn't matter to me because I was already used to that* (S19).

When students were asked if they had any **concerns related to health and safety** while wearing the sensors), all the interviewed students (except one) expressed that they did not feel any concern about this. Only one student pointed out the following: *when I was putting the equipment on, I was wondering whether it was clean* (S20). Students mentioned they knew that the University had strict safety protocols for running these simulations and that, in clinical environments, it is a regular practice to have all equipment clean after every use.

In sum, although some teachers were somewhat concerned about the intrusiveness of adding wearables to the learning environment in terms of students' comfort and potential distraction, most students reported that they were comfortable with wearing the sensors, citing that they commonly wear various devices in simulation-based learning sessions and at their placements. Yet, it is critical to follow strict hygiene protocols, which was the case in both iterations.

*5.1.2 Technology Readiness.* This topic focuses on the potential impact of the lack of MMLA technology readiness. From a research perspective, two members of our research team (R1 and R2) explained that whilst the off-the-shelf sensors utilised in this setting were easy to install, there still is complexity in maintaining the ecology of devices during the deployment. Although wearable microphones are common in healthcare simulation, smartwatches are already being worn by students to monitor themselves and indoor positioning technologies based on smart-phones already exist (typically less precise than the one used) [\[40\]](#page-36-0), the **technology was not yet ready to enable bring-your-own-device (BYOD) strategies** to work smoothly in our MMLA deployments. Hence, we still needed to provide specific devices to students and they needed to be worn correctly to maximise the quality of the data collected. Moreover, all sensors were connected to a data infrastructure running on more than one computer, since different sensor vendors required particular specifications in terms of hardware and operating system configurations. This made it hard, if not impossible, to easily connect all sensors to a single machine. Consequently, a team with technical

<span id="page-13-0"></span>expertise was still required to closely monitor the data infrastructure and to help students to put the sensors on.

Teachers noted the **potential negative impact on the lesson plan** as a result of asking students to wear the sensors correctly during a regular class. T1 noted: *If we factor in more time for sensor equipping, we still only have a three-hour session, which means that we cut into time for other things. We need to think about ways that we can reduce the amount of time spent equipping students.* However, T4 had a contrasting point of view, expressing that *there was no great time-saving difference between teams wearing the devices compared to those who did not*, suggesting that this is not a time-consuming task. Nevertheless, from the experience in the first iteration, the senior teaching team devised ways to equip students with the sensors more efficiently. Thus, the lead course coordinator placed coloured trays (blue, yellow, green, and red) per each student's role to put the devices inside the tray (e.g., see Figure [2,](#page-7-0) right) so students could easily equip themselves. This new configuration was well received by T1, T3, and T4, stating that it made it easier to identify the equipment and the roles (e.g., *The coloured trays worked really well, I think that was great.* - T1; *It was easier to equip students this year [second iteration] than last year [first iteration]*. - T3).

In the second iteration, we observed teachers instructing students on how to wear the devices properly. This illustrates how **teachers became involved in developing their own strategies to appropriate the MMLA sensing technology** and configured the space without the intervention of the researcher team. Teachers also proposed ideas to overcome issues regarding technology readiness by suggesting that, in the future, clear instructions could be provided to students to wear the equipment correctly without any assistance, for example, using *posters showing pictures of how the devices should be used properly* (T1). In terms of readiness of the infrastructure, teachers were eager to use *on-the-go* packed and ready-to-use sensors (T1 and T2) to adopt the tool in their classes without the researchers' intervention: *We can use the system without the need of the research team, we want to appropriate the use of it without needing assistance (T2)*. This is linked to the fifth theme on the sustainability of the deployment further explained later.

*5.1.3 Unexpected Technical Issues During the Deployment.* This topic focuses on unenvisaged problems with the sensing technologies that impacted data collection and the analytics. The research team reported various**technological issues when configuring the devices**. For example, while setting up the space before the second iteration, they found that a third-party driver for the positioning sensors had to be updated thus making the installation time longer than expected for the MMLA system to work exactly as it did in the first iteration. Other issues emerged during the study itself. R2, R3, and R5 reported that, although the physiological wristbands used in the study are marketed as high-end sensors and are relatively new, some stopped working unexpectedly (e.g., *Two physiological wristbands were not working or they turned off in the middle of some classes*-R5). The research team had to investigate the cause of this malfunction and re-install the firmware in between classes with mixed results. Also, during the study, the University-owned computers restarted unexpectedly due to software updates controlled centrally by the University's technical support team. R3 mentioned that: *the [positioning sensors] laptop restarted due to University device update* and R5 complemented that due to this issue, the visualisations could not be generated: *one of these devices restarted on its own even in the middle of data collection, and all visualisations are depending on this device*. The computers that were used during data collection are operated by the University, leaving the research team with limited freedom to configure them as the need arises.

Another unexpected issue was that some **students wore the sensors incorrectly** causing data loss. As noticed by R5, some students did not wear the microphone properly (e.g., "placing the mic far from the mouth"), even though the instructions given to them were clear. This could have affected the audio quality for a few students: *The talking volume of students actually have great* *influence on audio data collection, if the volume is high, then the audio signals might be harmed, if the volume is low, then some audio might not be clearly recorded* (R3). These unexpected issues hindered the correct generation of visualisations or yielded to an incomplete dataset collection for some teams.

Most of the issues described above are expected given the particular requirements imposed by multiple third-party technologies used in our MMLA deployment. While some of these challenges go beyond the control of the research team, researchers reflected and provided suggestions to minimise these in the future, such as replacing the current physiological wristbands with alternative wearables even though they may collect less granular data (i.e., a Fitbit sense) (R5) and reduce the number of computers used in the deployment to minimise connectivity or operating system related issues (R2 and R5): *In total, we have 3 computers for data collection. It would be better to merge to two* (R2).

*5.1.4 Tradeoff among Data Quality, Portability of Sensors, and Affordability.* This topic focuses on how the sensing technology can impact data quality, and the portability and affordability of the MMLA system. A tradeoff among these three aspects of the deployment can be illustrated in terms of audio data collection. Six pairs of inexpensive, minimalist wireless microphones were used in the first iteration with the aim of minimising intrusiveness caused by the wearables. These were consisted of one slim headset without any further equipment or antenna, thus, making them highly affordable and portable. However, the lack of a less portable signal transmitter in these devices made the signal inconsistent and subject to interference. This resulted in only 23 out of 57 simulation sessions being fully recorded for all team members. A second quality issue was the voice overlap, since other students' voices were easily captured when in close proximity. Without special audio post-processing, this led to inaccurate data analysis results. To address these issues, the microphones were upgraded in the second iteration. The new microphones (Shure BLX14P31 Wireless Headsets) resolved the first quality issue about signal interference, as they provided multiple frequency bands for signal transmission. The second issue about voice overlap was also resolved, as these microphones were unidirectional, minimising the chance of getting voice overlap. Although the new microphones resolved the quality issues, it made the MMLA system less affordable as the price of each pair is around 17 times more than the previous microphone, and they included a bodypack transmitter that students had to wear on their hip additionally to the headpiece connected through a cable, making them slightly more complex to wear. Each microphone also required a receiver in the form of a black box connected to the audio infrastructure, making the setup more complex too.

In sum, prioritising high-quality data in a MMLA deployment may require using more expensive and bulkier sensing devices, a tradeoff that must be balanced against anticipated instructional benefits. For instance, improved audio quality enables a detailed analysis of students' conversations, which can help support learning more effectively (e.g., Reference [\[119\]](#page-40-0)).

### **5.2 Data and Analytics**

This theme included the following three topics: (i) the purpose of capturing multimodal data; (ii) data incompleteness and trustworthiness; and (iii) emerging issues related to the MMLA Dashboard.

*5.2.1 Purpose of Capturing Multimodal Data.* This topic focuses on students' and teachers' perceptions of the educational purpose of the MMLA deployment. Regarding whether students understood **what data was being collected**, all interviewed students (*N* = 20) remembered they wore a bracelet (i.e., *a watch, a little hand band, a wristwatch*), a microphone (i.e., "a headphones", "a headset"), and an indoor positioning device (i.e., *a fanny pack, belly bags, a tag at the front*). Yet,

only two students realised that video data were also recorded (*i.e., you recorded us and there was video*) possibly because video recording has been a staple in this educational context [\[62\]](#page-37-0). In the first iteration, when students were asked about their perceptions regarding the purpose behind **why their data were collected**, half of them (*N* = 10) indicated that data was collected for others (researchers or teachers) to understand how students perform during the simulation scenario and assist future students (e.g., *researchers can suggest ways that the teachers can change to help or assist students in the simulation, the common things that students are struggling with* (S17). Likewise, S4 noted that the data was collected to *help future students have better learning resources, using current students as guinea pigs to see what needed to be improved for the learning outcomes for future nurses*. Various students (*N* = 11) concretely explained that the new technology was to support them directly. For instance, students claimed that it was *to see how everyone reacts when there's a situation going on and how people prioritise* (S7) and *to provide some feedback about the whole scenario to students and educators* (S1). Other purposes that students mentioned were related to the assessment of the simulation effectiveness (*N* = 4) (e.g., *to see the effectiveness of the simulation, or whether it's working and if it is beneficial to our learning* S1), the support for future nursing professionals (*N* = 4) (e.g., *to see what we experience for improvement of future nurses*—S10), and only two of them were not sure about the purpose of the MMLA deployment.

In relation to the second iteration, **teachers felt satisfied with the use of the debrief MMLA dashboard**. All senior teachers highlighted that the debriefing tool helped to *reinforce and back up the discussion during the debrief*, stating that the tool *ignited some light bulb moments around this discussion* (T4). As stated by T4, the tool can draw attention to key points and spark discussions towards improvement in their practice: *I used the data to draw attention to improving communication. I also used it to discuss the most useful aspects of teamwork and communication students needed to learn for a deteriorating patient*. Specifically, the tool was used to validate and emphasise some teamwork aspects previously discussed with students (e.g., *[the data] validated some of the points that were made in the debrief* - T5; *[the data] highlighted the team communication that we had discussed, and only discussions that we previously had with the group*, - T4. The debrief tool was also used to **reflect on a team's reaction in critical moments**, as mentioned by T2: *for example, from students' positioning, I could see how they reacted, after the MET [medical emergency team] call, if they took a 2-2 approach or a 3-1 approach*. Teachers also stated that the visualisations were helpful during the debrief because they showed visual evidence of the team's dynamics and performance (e.g., *it provided visual evidence of team dynamics* - T7; *it helped to give visual confirmation of excellent collaboration the team showed*- T1). All interviewed senior teachers (T1-T4) highlighted how the tool was used to provide what they referred to as "objective feedback", explaining that teachers often focus on one student when observing the simulation, which may later introduce bias towards that student during the debrief (we discuss perceived "objectivity" in the next section on data trustworthiness). For two teachers, *the data [visualisation] was way less subjective* (T3 and T4). The same teachers (T3 and T4) also mentioned how the tool could benefit students' learning by providing alternative sources for reflection (e.g., *another way of telling students what they did*—T3; *another way of seeing their performance*—T4) and by highlighting positive aspects of students' performance, especially for cases when students assess or judge their own performance too harshly (e.g., *I highlighted to students they did a good job although they thought had done poorly*—T4).

In sum, from the first iteration (that involved data collection only) we learnt that when the educational purpose is clearly communicated to students, they may see the value of participating in a complex MMLA intervention even if they do not receive direct benefits from it (e.g., to help improve current teaching practices or improve the technology that will be used by future students). Moreover, from the second iteration, we learnt that teachers perceived the potential benefits of the richness of multimodal data, rendered into data visualisations, to support students' reflection but

Lessons Learnt from a Multimodal Learning Analytics Deployment In-the-Wild 8:17

they need to ultimately develop the strategies to optimise the effective use of data for educational purposes.

*5.2.2 Data Incompleteness and Trustworthiness.* This topic focuses on the potential impact of incomplete multimodal data on the perceived trustworthiness of the MMLA system gathered from students and teachers. When students from the first iteration were asked about trust of their own data presented to them during the interview, 15 out of 20 students indicated that they would **trust the multimodal information**. Their responses can be organised into two groups. Students in the first group (9 students out of 15) would trust the data because they were able to identify themselves or their perceived team outcomes from the visualisations. For example, S2 explained the following: *I trust the data because it presents what we did in the simulation, by looking at the data I can see who I was in that scenario*. Likewise, student S3O indicated the following: *when I looked at the [visualisations], I could tell that this is what I did, at this time [pointing the visuals], and the actions we performed were very familiar*. The second group of students (6 students out of 15) responses indicated that they trusted in the multimodal data based on the belief that sensors provide, what they referred to as: "objective and accurate data". For instance, one of the students described this as follows: *I wore the sensor so I know that the information came from pretty reliable sources* (S3). However, 4 of the 5 students, who did not trust the data, were **skeptical about the trustworthiness of the multimodal information** because at the beginning they found it difficult to read. For instance, S10 indicated that he *do not trust the information much because [the visualisations] are not as simple and straightforward* (S10). In the same way, S22 suggested the following: *I think I would trust the data for the most part but I think [other people] would require knowledge about the context as well*. However, when students got familiarised with the visualisations two of them changed their opinions (*N* = 2). For example, S20 suggested that the visualisation was *a bit hard to interpret. But, once you sort of look at it, and when you read the data as well, when you combine it all together it is trusted and the data presented definitely makes sense*. In the end, only 3 students would not trust the multimodal data at all as they considered it did not represent what they did during the simulation (e.g., *I don't think it was accurate enough considering how my team performed*—S7).

Similarly to the students in the first iteration, most students who looked at their data in the MMLA dashboard in the second iteration (43 out of the 47 survey respondents) indicated that they would generally trust the multimodal visual representations. However, some students were also aware that sometimes the visualisations in the MMLA dashboard were generated with incomplete data. For example, four students (*N* = 47) recognised that the **incompleteness of data affected their sense of trust**. One student explained this, as follows: *This visualisation allows the students to reflect and be accountable in communication when working in collaborative practice. However, I can only trust to some extent as I know that a line is missing between the student wearing blue colour and the doctor for communication* (S51). Another student expressed a similar idea: *I cannot judge my communication from these data as the majority of my communication was between myself and the other primary nurse, and her data was not collected* (S29). As noted by S46, having incomplete data could hinder the validity of results: *My only comment would be to ensure that the equipment is working and recording as required, as this, unfortunately, affects the validity of results*.

The four senior teachers also reflected on the trustworthiness of the data and the system. T2 and T3 pointed out that trust is built over time. T2 expressed this as follows: *For the first week at least, I really didn't understand the visualisations to a degree. Once I knew what each visualisation was about, I did trust in them in later weeks for sure*. T1 and T3 indicated that trust is not coming from the data but from their confidence about making a good interpretation of the information presented in the tool: *It's about how I learned to use the system that changed my trust. It is not that I'm not trusting the data. I am not trusting what I have to say about it; I don't know what it means* (T1). Moreover, T3

and T4 mentioned that they **over-trusted** the information. Even though their understanding did not seem aligned with the data, they wanted to make an explanation (i.e., *Sometimes I was trying to force an explanation based on the visualisation but I shouldn't be because it didn't make sense to me. I was seeing things that I wasn't expecting or that I couldn't explain*—T4).

Teachers indicated their **strong preference for using the debrief tool when the data was complete** (i.e., data from *all four* students in a team). If the information was about less than four students, as it was the case for teams in which not all members consented to participate in the study, using the tool can still be relevant. However, teachers had concerns about data incompleteness and a deeper explanation in the debrief was needed to complete the whole picture of what was expected from an entire team according to the expected learning outcomes. T1 explained this as follows: *If teachers don't go through the [incomplete information], [students] don't understand what they're looking at, and the wider context. So I walked through how the visualisation would have looked like if we had a full set of students*. T2 also stressed that visualisations generated from the incomplete data of a team might cause confusion to students: *giving [incomplete information] to the students without explaining it probably would have just made them confused. I think with three or four [team members] the debrief tool was generally well used*. But also using incomplete data could be harmful in general as teachers could also make wrong assumptions as T2 explained: *Sometimes the data would confusing to me. A student reminded me that there were only three [team members] then I was 'yes, that does explain it exactly'*.

In sum, we discovered that both students and teachers might make incorrect assumptions due to their tendency to trust data simply because it is automatically collected through sensors. Teachers and students may also believe that sensor data is objective and free from bias. Yet, learning data is intrinsically incomplete [\[49\]](#page-36-0), especially when captured via sensors, and data representations are not necessarily objective as they are unavoidably imbued with subjective design decisions [\[13\]](#page-34-0). It is essential to establish socio-technical mechanisms that ensure MMLA systems are reliable. This involves teachers and students attaining an appropriate level of understanding that multimodal data is incomplete and susceptible to various biases. Such incompleteness and biases can be both *intrinsic*, due to the computational modelling of any social activity (i.e., all data and algorithms offer partial perspectives on human activity), and *extrinsic*, stemming from local contextual factors (e.g., technical malfunctions or students not consenting to be tracked).

*5.2.3 Emerging Issues Related to Visualising Multimodal Data.* This topic focuses on issues experienced by students and teachers while interacting with the MMLA dashboard. A recurrent challenge highlighted by all the senior teachers and other teachers (T5, T7, and T9) was that **they did not feel in the position to make a rapid interpretation of the visualisations** and lead a reflection immediately after. Teachers explained that this was often due to the challenge of aligning what they observed in the simulation with the information presented in the dashboard. As described by T2: *sometimes I tried to make sense and align the observed behaviours with the data, but then, during the debrief, I was thinking, Oh no, I don't think this might be correct*. Teachers also indicated that they needed more time to familiarise themselves with the MMLA dashboard in order to devise ways to use it for moderating the class reflection (e.g., *it was challenging because I have not done it before*—T7, and *it was challenging due to unfamiliarity with the technology*—T9). However, teachers recognised that using the MMLA dashboard involves a learning curve process: *once we got more familiarised with the tool, it got easier* (T3). One potential solution is to factor in a preparation or formal training period with the whole teaching team for them to create strategies that they can follow to interpret the data visualisations generated immediately after each team session, as suggested by T4: *having some standard [moderation] with the teachers would be really useful*. Another solution suggested by T3 was to enrich the MMLA dashboard with teachers' observations and learning expectations to create a stronger sense of trustworthiness: *I think we can articulate the data better having some kind of moderation where we [the teachers] manually check if students achieved a learning outcome and transfer this into the learning tool*.

Some teachers also mentioned that their lack of understanding was due to not knowing how the data was captured and translated into the visualisations (e.g., *how are you coming up with the yellow bar? What is being used to create that?*—T4). The four senior teachers suggested that helping teachers to develop their **data literacy in relation to the particular MMLA dashboard** would contribute to interpreting the visualisations better and also to understanding their limitations, especially when using it for the first time. T1 explained this as follows: *knowing where the data is coming from and how the information is being calculated would help us to understand the specific parts of it*.

Similarly, some students in the second iteration also pointed at the challenge of connecting the complex multimodal data with specific learning constructs or performance metrics. Five out of the 32 students (*N* = 47) who indicated that the information in the MMLA dashboard was relatively easy to read, also explained that it was hard for them to make a clear connection of its meaning with their learning experience (e.g., *I understand what each part of the data is showing. But I think I need more explanation to understand the meaning behind each one and the reasoning on why this occurs*— S54). One student highlighted **the need for contextual information for them to be in the position of interpreting the multimodal data**: *I think more of a prompt before the visualisation is shown, such as knowing how many patients we have and the critical information of the patient, then I could have more understanding and confidence of my performance in the simulation* (S47). Another student expressed her desire for **detailed explanations about the team's performance**: *I didn't really understand what I was looking at, at first, and there is still some confusion in the last section about task transition. I don't know if being higher than the average is considered a good or bad thing* (S64). Another student recognised the need for clear instructions to adjust their practice: *it would be good to see what you would expect to see from a highly effective team* (S58).

In sum, teachers may find it hard to interpret MMLA visual interfaces given the complexity of the intertwined data underpinning them. For effective in-the-wild deployments, teachers need to be supported to develop relevant data literacy skills to understand the basic inner-workings of the analytics and for them to develop pedagogical strategies around the use of the MMLA systems. Students also emphasised the need for design elements in the MMLA visual interface to *explain* the meaning of the data and for them to understand what were the performance expectations.

### **5.3 Design and Human-Centredness**

This theme included the following two topics: (i) human-centred design, teaching and learning; and (ii) human-centred design and research innovation.

*5.3.1 Human-Centred Design, Teaching and Learning.* This topic focuses on teachers' perceived benefits of being part of a human-centred process in the development of the MMLA system in terms of their teaching practices and students' learning. The senior teachers felt generally **satisfied with a design process in which their voices could be reflected in the resulting end-user interface**. For example, T1 appreciated the partnership with the researchers throughout the design process, as follows: *We're all involved in this because we can see that there are benefits to the students and to the teachers for doing this, that it alleviates how we do our debrief and can potentially change the way we do our sims*. T3 also recognised the value of bringing different expertise to innovate in terms of technology innovation and pedagogical practice: *I think by combining our teams we can make something that's new, and we'll help students learn*. T1 and T2 expressed that they felt involved in the design process and acknowledged that their lived experiences and suggestions were considered in the final prototype of the MMLA dashboard: *[the research team] was very flexible in working with us. They have taken a lot of our suggestions on board* (T2); and *We felt we had contributed to it. So I thought the two teams were able to add value, and I found that really satisfying* (T4). Teachers also felt that both researchers and teachers were working towards the common goal of supporting students' learning. T1 and T2 elaborated on this idea, as follows: *It's good that we are going towards the same goal, but with different perspectives* (T2); *I think it's important that we've been able to have those priorities to see that there's a common goal. We just want to work at it from different points of view* (T1). In short, teachers appreciated partnering with researchers in the MMLA design process. This can lead to creating MMLA visual interfaces aligned with existing teaching practices and learning goals.

*5.3.2 Human-Centred Design and Research Innovation.* This topic focuses on the views of both teachers and researchers on partnering to foster MMLA research innovation. The lead senior teacher (T1) expressed that **at the beginning of the design process the teaching team was a bit uncertain about the outcome** of the MMLA deployment given its novelty, but the experience after the two MMLA iterations made these feelings fade: *I really had no idea exactly what we were doing, and I had to see it and be part of it before I could really understand it*. Linked to this idea, T4 explained that, because the research is considerably innovative, teachers' involvement in the design process enabled them to consider students' feelings and reactions and be careful about how students were invited to be part of the research study: *[the deployment] was very innovative. Because of this, I think I was a bit more worried about the [students], thinking that we have to do it correctly. We were given the opportunity to be diligent about it.*

Also, **teachers value research collaboration if the common goal benefits students' learning**. T2 expressed this positive relationship by highlighting the openness, easiness and willingness of the research team to work with the nursing teaching team: *It's the fact that there are some researchers who are just as passionate about this as the educators, it's actually easy for us to keep wanting to do this because they want to work with us as much as we want to work with them. And although they aren't nurses, actually we can work together and share points of view, so that we can try and understand what the research side is. They are willing to work, and be quite flexible with what's going on so that we can make it work for nursing*. T3 also stressed the strengths of combining both, the LA research and the nursing teaching teams towards building innovative learning tools: *I think by combining our teams, we can make something that's new, and you know we'll help students learn*.

Researchers' perspectives resonated with teachers' perspectives. The research team indicated that it was critical to consider **teachers' voice since they have the lived experience in deploying pedagogical interventions**. For example, T4 described how teachers would indicate whether certain logistic decisions were *unfeasible* or not. R4 explained this as follows: *When collaborating or interacting with teachers, I think the most important thing is to put teachers' and students' educational needs before our research desires*. R3 also explained that a benefit of collaborating with teachers in the design of the MMLA deployment was that *they could be invited to an interview, to get what they valued or missed in the visualisation which can show what can be improved or developed in the future to help them better*. R4 also explained how important was to collaborate with teachers to **understand the meaning of the data** in light of the characteristics of the learning design: *This collaboration has led to changes in my algorithm for detecting task prioritisation. There are some minor changes in the learning design that we were unaware of but significantly impacted the analytics*. Finally, R1 also mentioned the importance of **partnering with students** to validate the MMLA dashboard: *Doing a cognitive walkthrough while teachers and students think aloud has benefited my research in validating how useful or misleading the [multimodal] visualisation may be*.

Lessons Learnt from a Multimodal Learning Analytics Deployment In-the-Wild 8:21

In sum, we learnt about the importance of involving teachers and students in the design process to not only validate the highly innovative MMLA end-user interfaces but also to expand understanding of the learning design, values that must be endorsed, and the lived experiences that can affect the logistics of the deployment.

### **5.4 Social Factors**

This theme included the following two topics: (i) consenting and participation strategies; and (ii) data privacy and sharing.

*5.4.1 Consenting and Participation Strategies.* This topic focuses on the consenting strategies applied in both iterations of the study. Teachers recognised that, proportionally, more students consented to participate in the first iteration (2021) than in the second (2022). As mentioned by T4: *it didn't seem to work as well the second time. We were trying very much to keep on time, and I think [the consenting process] actually became a little bit more complicated*. While some teachers tried to explore potential explanations for this difference, citing potential seasonal differences (e.g., *students may just be so much more burnt out than they were last year. They're disengaged on everything, not just the simulation*—T1); teachers tried to optimise the consenting process. In preparation for the second iteration, consenting information was sent to students beforehand as pre-class material through an online consent form and an explanatory video. Teachers recognised this strategy may have caused confusion since students often do not access pre-class materials, as explained by T4: *we also know that they don't access the pre-class stuff, so that will limit their exposure to it. But I did feel it was just a little bit complicated for them to take on, and then agreed to consent to*. T3 also explained that **the MMLA deployment may be too novel in the eyes of the students** and it involves several layers in relation to data (i.e., capture, analysis and visualisation) that students cannot fully comprehend: *I think the video was really helpful. We played it in class, and it was very sound, very personable and short. Perhaps the way it communicated made it sound a bit complicated*.

Due to the challenges in getting participants for the second data collection, the senior teaching team shifted to the same strategy as in the first iteration (2021). This consisted in inviting a researcher to give a short 1–2 minute speech just before the class to communicate the research and ask them to participate. Students could ask any clarification questions. As mentioned by T1: *I think it worked really well when one of the researchers gave the speech from the research point of view. And then one of the academics back that up*. This space also served to explain to students that the data was de-identified and that participating in the research would not affect their grades. As mentioned by T2, this strategy was crucial to highlight to students that the data was being de-identified: *We noticed that they were worried like if their faces or names will be shown in the screen*. However, packing all ideas in two minutes is challenging. It may cause some pressure on the research team. T4 indicated that: *you're trying to [inform students] in a very short space coming in and trying to convey this information. So I think [the researcher] was under a lot of pressure to do it* (T4).

Moreover, all the senior teaching team agreed that the consenting strategy should be improved. After teachers experienced two in-the-wild MMLA deployments, they reflected on the value of the research, the usefulness of the MMLA debrief tool and its potential impact on students' learning. They thus indicated they may want to move the deployment forward to happen as a part of the regular tasks in the classroom. They proposed a **"business-as-usual" use** of the MMLA dashboard if the maturity of the system allows it as in video-based debriefs in healthcare simulation [\[62\]](#page-37-0), so all students would have the same learning opportunities. This way, the consenting would be just about optionally *recording* the data for research purposes, otherwise, deleting it immediately after the class. This was explained by T1 as follows: *I still think our best bet is that the [MMLA system] is*

*inbuilt in all simulations. Perhaps this is what we will do next year. Maybe we have only one or two sessions where the technology is not used at all. This is definitely the way to go*.

R3 and R5 agreed that the lower participation in the study was due to the intrinsic complexity of the multimodality aspects of the data collection. R3 expressed that *some students didn't seem to actually understand what would happen in our data collection and didn't want to participate. Students might see the digital version of the consenting form as more work and then ignore that*. The lack of understanding could come from technical words embedded during the explanation of the study. R5 indicated that *[students] didn't actually understand what's 'multimodal data' and didn't ask about it either.* As reflected by R5, there should be a balance between details (e.g., over-explaining details of the data and technology that students without formal analysis training and AI literacy would not easily understand) and simplicity (e.g., omitting key information about the potential implications related to the data collection that may be relevant to make an informed decision) to minimise students misunderstandings: *over-explaining technologies, explaining from what kind of data and showing what kind of visualisations they'll see in debriefing, resulted in a lower number of students participating in the study. However, explaining it too simply and straightforward resulted in several students withdrawing from the study*. For future studies, researchers suggested **simplifying the explanations and words** in the consent form and during the description of the research. R5 explained this as follows: *I reckon we need to improve some wording in the consent form by simplifying some words, for example, instead of using the word 'multimodal' we should use words they can understand such as 'audio, position, and health data'*.

When reflecting on consenting strategies from both iterations, R5 indicated that the first iteration was easier because it was **focused only on data collection**. Therefore, teachers were not much involved as in the second iteration: *The consenting [from the previous year] was smoother than this year because we only focused on data collection. Teachers didn't provide much help besides helping students wear sensors.* R4 also reflected that *although the consenting strategy in the first year was non-environmentally friendly, as printed consent forms were handed to each student, it was very successful in getting several students volunteering to the study*. It seems that an in-person strategy to explain to students about the MMLA deployment is better than an online strategy because clarifications can be made in person, and there is a direct engagement with students: "an online strategy can be confusing because students do not only consent to the study but also to the online confidentiality agreement they need to sign as a part of their course" (R3).

In sum, it is challenging to explain to students what a complex MMLA study entails as it involves various types of heterogeneous data sources each pointing at multiple analysis and visualisation approaches. Yet, providing too many technical details in advance may not necessarily contribute to clarity. Explaining the complexity of the MMLA deployment *in person* can enable students to ask clarification questions and then provide informed consent.

*5.4.2 Data Privacy and Sharing.* This topic focuses on students' perspectives on multimodal data privacy and sharing. All interviewed students (*N* = 20) agreed to **share their data for educational purposes in ways they could not be identified** by others outside their class. Yet, they had different perspectives about who should benefit from looking at and using the visualisations. For instance, 10 students indicated that it would be only beneficial for those students who own the multimodal data to use it. As one of the students argued: *you just take more knowledge from your own experience than from someone else's experience* (S5). This was supported by another student who considered that showing their data *will make more sense to the team, who was performing the simulation* (S18). The rest of the students (*N* = 10) reflected on the opportunities of sharing the data with others and its benefits for learning. For instance, S20 envisaged that *if he could see their teams' visualisations, compared to another team, it would be quite interesting to see maybe why did*

Lessons Learnt from a Multimodal Learning Analytics Deployment In-the-Wild 8:23

*this team recognise aspects earlier than them*. Likewise, other students (*N* = 5) indicated that teachers can benefit in the way they *can work on improving critical aspects of students performance and the scenario* (S10), *can learn a lot about student thinking and performance* (S18), and *can help them understand how well a student did* (S3).

In sum, students did not show signs of being concerned about sharing their data with others, which may be explained by a low level of understanding of the multimodal data itself as mentioned in the previous subsection. Yet, half of the interviewed students thought that the data would be meaningful and relevant only to the students who were in the same class where the data was collected.

### **5.5 Sustainability**

This theme included two topics: (i) technological sustainability; and (ii) MMLA appropriation in the classroom.

*5.5.1 Technological Sustainability.* The first topic relates to the sustainability of the technological infrastructure. R1 and R5 suggested a flexible "detachable" architecture capable of **running through microservices as a centralised process**. The idea of this architecture is to minimise data loss in case one of the data sources is not correctly running (e.g., *the architecture should provide an easy way to de-attached pieces of the application presenting issues. That way, it would be possible to reduce any misleading data visualisation [caused by issues during data collection]*, R1). Microservices should also allow running the MMLA solution with minimum requirements: *These systems should be able to run with minimum hardware and software installations. For example, we have one document that provides a list of recommended hardware that we use and a list of software that is needed to be installed* (R5).

Special care should also be given to automatically **communicate to teachers about any issues that may have been detected during the data collection** (e.g., sensors that may have been accidentally disconnected) so they can decide if they will use the tool in the debrief or not: *teachers should be informed that issues can happen and it can help them to decide whether they want to use the technology or not during their classes* (R1). R4 also explained that technological sustainability depends on the readiness of sensing technologies, switching towards user-friendly versions of the hardware: *The sustainability of MMLA research studies largely depends on the readiness of the sensing technologies. For example, the positioning tracking system and analytics will become sustainable for teachers to use on their own when fully enterprise solutions are available, as the level of automation from data collection to analytics will be higher*. In short, a potential strategy to maximise long-term technical sustainability is a light-weight microservices-based architecture that can enable attaching and detaching heterogeneous sensors as required. Such technical properties of the MMLA infrastructure are tied to **building and sustaining stakeholders' trust** in it.

*5.5.2 MMLA Appropriation in the Classroom.* The second topic concerns the requirements to sustainably integrate the MMLA system into regular teachers' practices. R1 and R2 expressed that for teachers to appropriate the MMLA system, this should move towards a toolkit (of hardware and software) that can be easily deployed and **used by non-experts users**, such as the teaching team or nursing students. R2 suggested that *all the software should be reconstructed to generate a toolkit which can be easily deployed and used*. R1 further expanded on strategies required to embed the multimodal sensors into the existing classroom ecology, as follows: *a classroom can be equipped with charging stations where teachers and students can collect/leave the devices they are wearing, and teachers have a computer in the classroom where they can easily start/stop the data collection and generate data visualisations*.

The senior teachers' perspectives were also aligned with the idea of running the MMLA system without requiring too much technical support: *in terms of using the system, running the tool by yourself without any help of the team of researchers. Right? I think that would be the ideal end goal that we can just run it.* (T2); and *there should be some way of being able for it to collect data automatically* (T4). However, both T2 and T4 also suggested that **minimal technical support is still required**: *it will always require a sort of a technical person* (T4) and *if we get to the point where we only need one researcher there on the day to troubleshoot, just in case things go wrong to get the system up* (T2). R1 mentioned that *having a dedicated space (e.g., a specific classroom) to run this multimodal data collection on a regular basis would help other universities if they want to implement these solutions*. T4 supported this idea: *concerning the technology, I think that the infrastructure in the room could be improved, the sensors need to be embedded in the room*. R3 mentioned that it would be ideal to use the equipment that is already part of the simulation room, such as ceiling microphones and 360 video cameras: *If we can use the devices they already have, like microphones, the price of the whole system would be cheaper*.

Finally, researchers and teachers suggested **a training period for teachers**. For example, T4 explained the following: *teachers could definitely be better educated about it and be more autonomous in that regard. It is about training the teaching staff to be able to attach the wearables and all the rest or running this the MMLA system*. In sum, a potential strategy to maximise adoption and technology appropriation by non-technical end-users includes (i) embedding sensing capabilities into the classroom; (ii) providing teachers with a high degree of user control; (iii) providing basic training for teachers to not only use the system but also to learn how to interpret and act upon the multimodal data representations effectively; and (iv) providing readily available technical support in case something goes wrong.

### **6 DISCUSSION**

In this section, we summarise the lessons learnt from our MMLA in-the-wild deployment; then discuss the implications of these findings for practice, identify various limitations of our in-thewild study, and suggest some potential directions for future research and development.

### **6.1 Summary of Lessons Learnt and Contributions to HCI Research**

This section summarises the main key logistical, privacy and ethical challenges that emerged from our complex MMLA, in-the-wild study and a set of implications for advancing HCI and LA research, as follows.

*Space and place*

- **Intrusiveness**—While students did not report discomfort in wearing sensors, teachers can still get concerned about their potential *distracting factor* and some students can feel *stressed* about being monitored.
- **MMLA Technology readiness**—The lack of MMLA technology readiness can severely impact the lesson plan. Teachers need to play an active role to create *strategies to moderate* the sensing/analytics technologies, and minimise potential disruptions and setup time.
- **Unexpected issues during the MMLA deployment**—While several technical issues that can emerge during the MMLA deployment are beyond the control of the research team, reducing the number of devices used can minimise potential technical failures. Some highend sensors may need to be replaced with less expensive sensors, that may capture coarser data, if the change increases *reliability*.
- **Multimodal data quality, portability of sensors and affordability**—At least currently, a tradeoff may exist between capturing *high quality* data and the portability and affordability of the sensing technology.

These findings supply empirical evidence that offers specific directions to address the challenge outlined by Oviatt [\[78\]](#page-38-0) and Chejara et al. [\[19\]](#page-35-0) (see Section [2.2\)](#page-3-0) concerning the need to create unobtrusive MMLA infrastructures for large-scale data collection. As suggested by our results, crucial actions for deployment to consider comprise: (1) creating a comprehensive understanding of how educational stakeholders perceive the intrusiveness of specific sensing devices; (2) investigating the readiness of technology to evaluate the feasibility of seamless integration; (3) formulating strategies to address unexpected technical challenges (very likely to arise in heterogeneous sensing ecologies [\[10\]](#page-34-0)); and (4) examining the interplay between data quality, sensor portability, and affordability to enhance educational environments. The extent of intrusiveness and readiness of sensing technologies to be deployed in authentic learning situations is highly context-dependent [\[57\]](#page-37-0), a facet that warrants further exploration in MMLA research [\[36,](#page-35-0) [117\]](#page-40-0).

*Technology: data and analytics*

- **Purpose of capturing multimodal data**—If communicated clearly, students are willing to participate in a complex MMLA study and contribute their data for the purpose of helping their teachers or future students. Teachers can and need to develop strategies to optimise the use of multimodal data to support students.
- **Multimodal data incompleteness and trustworthiness**—Although multimodal data is required to build analytical representations of an embodied learning experience, multimodal sensor data are intrinsically incomplete and subject to bias. Thus, mechanisms to ensure MMLA systems are *trustworthy* and designing for data incompleteness are required.
- **Visualisation of multimodal data**—Teachers need to be supported to develop relevant *data literacy skills* for them to develop pedagogical *strategies around the effective use* of the intrinsically complex MMLA visual interfaces. Students may also require visualisation guidance or explanatory features for them to understand the meaning of the data in educational terms.

These findings provide insights for HCI researchers working with data and analytics in educational contexts. Key deployment actions to consider include: (1) effectively communicating the educational purpose of MMLA deployments to educational stakeholders (for example, empowering rather than replacing the teacher) to maximise the chances of user acceptance; (2) addressing concerns related to data incompleteness and trustworthiness to minimise potential harm due to data misinterpretation; (3) understanding the impact of data visualisation design on user trust; and (4) supporting teachers and students in developing data literacy skills and providing contextual and explainable information to enable effective interpretation and use of MMLA visual interfaces. These actions are connected to emerging research in the areas of Human-Centred AI, seeking to develop mechanisms to ensure the development of trustworthy AI systems [\[67,](#page-37-0) [68,](#page-37-0) [93\]](#page-38-0); and AI literacy in HCI [\[52\]](#page-36-0) education [\[69\]](#page-37-0), seeking to equip people and educational stakeholders with the skills needed to understand how to effectively make use of data.

### *Design: human-centredness*

- **Human-centred design, teaching and learning**—Teachers' appreciation of partnering with researchers in the design process can lead to creating MMLA systems aligned with teaching practices and learning goals.
- **Human-centred MMLA and research innovation**—Involving teachers and students in the design process contributes to the validation of the MMLA interfaces according to the learning design and to the improvement of the logistics of the MMLA research study.

Aligning technical aspects of tool design with the pedagogical design remains a persistent challenge in learning analytics [\[46\]](#page-36-0). HCI-related areas like Human-Centered Design [\[77\]](#page-38-0) and, more

specifically, Human-Centered AI [\[98\]](#page-39-0) can provide with tools and conceptual frameworks to effectively engage with educational stakeholders' views and values with respect to culturally aware uses of AI [\[107\]](#page-39-0). Our findings address Oviatt's [\[78\]](#page-38-0) call for the establishment of collaborative relationships with educational stakeholders in the design of learner-focused MMLA systems. Key actions to consider include: (1) involving teachers in the design process to align MMLA visual interfaces with teaching practices and learning goals; (2) considering students' perspectives and reactions in the design process for ethical technology deployment; (3) combining expertise from various fields, such as LA and educational research, and the area of application (i.e., nursing teaching in our study), to develop contextualised learning tools; and (4) prioritising educational needs over data-driven innovation while collaborating with teachers and students in MMLA design for valuable insights in algorithm development and learning design characteristics.

### *Social factors*

- **Consenting and participation strategies**—It is challenging to explain to students what a complex MMLA study entails. Providing too many technical details about the sensors and the analytics in advance does not necessarily contribute to clarity. Explaining the complexity of the MMLA deployment *in person* can enable students to ask clarification questions and then provide informed consent.
- **Data privacy and sharing**—Students were willing to share their multimodal data with others if their privacy is preserved and the purpose is limited to supporting learning. While most students see their multimodal data as only beneficial to themselves, some students can see the potential benefit to make their data available to other students to learn from their experiences or for teachers to improve the design of the learning tasks.

Preliminary work, such as that of Beardsley et al. [\[12\]](#page-34-0), has started to delve into the unique challenges of securing consent from students interacting with MMLA tools. However, our study illuminates further how students can make informed decisions about their participation in MMLA studies and share their multimodal data based on the types of data used in a specific MMLA innovation. These findings can provide key insights for HCI researchers examining the sociotechnical aspects of data-intensive educational systems. Crucial actions to contemplate include: (1) further investigating the enhancement of fully informed consenting and participation strategies to optimise student engagement and comprehension of AI and sensing technologies' implications; (2) striking a balance between technical complexity and simplicity in consent forms and research descriptions; (3) utilising in-person approaches for enhanced clarity; and (4) considering students' perspectives on data privacy and sharing to respect their preferences and enrich learning experiences.

### *Sustainability*

- **Technological sustainability**—A potential strategy to maximise long-term technical sustainability is a lightweight *microservices-based architecture* that can enable attaching and detaching heterogeneous sensors as required.
- **MMLA appropriation in the classroom**—A potential strategy to maximise adoption and technology appropriation includes embedding sensing capabilities into the classroom, providing a high degree of user control, providing training to teachers on system usage and data interpretation, and keeping the need for support from a technical actor to a minimum extent.

These findings correspond with recent efforts within HCI to emphasise human-centeredness in designing sustainable data applications for the built environment [\[4,](#page-34-0) [108\]](#page-39-0). They also respond to a clear need identified in the current literature for the development of more sustainable MMLA systems [\[117\]](#page-40-0). To underline our contributions towards sustainability in MMLA systems, we propose the following initiatives: (1) developing flexible, detachable microservices-based architectures to support adaptability and minimise data loss; (2) designing easily deployable systems tailored for non-expert users, facilitating seamless integration into teaching practices; (3) if possible, embedding multimodal sensors within the classroom environment, ensuring high user control; and (4) provide essential training and accessible technical support, empowering teachers to effectively use and interpret multimodal data representations.

### **6.2 Implications for Practice**

The lessons learnt from our in-the-wild MMLA study have several implications for practice. We summarise these into the following three recommendations to provide guidance for researchers, developers and designers to make informed decisions about the effective deployment of MMLA in-the-wild.

*Forging design partnerships with teachers and students***.** The more sensors are used to capture activity in complex educational scenarios that involve non-computer mediated interactions, or ill-defined, open tasks such as in teamwork, the more complex the meaning-making process becomes to move from data to insights [\[30\]](#page-35-0). Thus, as rich data infrastructures become more commonplace in educational contexts [\[41\]](#page-36-0), it is also becoming critical to forge strong partnership relationships among teachers, students, educational decision-makers, researchers and developers. This has the potential to ensure that algorithmic outputs and data representations are meaningful and aligned to local learning objectives and pedagogical values [\[2\]](#page-34-0). Indeed, some educational researchers have started to utilise the body of knowledge and practice from design communities, such as participatory design and co-design, in data-intensive educational contexts [\[17\]](#page-35-0). However, following human-centred design approaches is yet to be seen in MMLA according to the most recent review [\[117\]](#page-40-0).

In our study, several practical challenges in the MMLA deployment demanded expertise from a wide range of areas (such as LA, interaction design, and information visualisation), plus knowledge from stakeholders contributing insights and evidence from their lived experiences. By giving an active voice to students and involving teachers in the design process, we were able to identify the key practical challenges that can easily undermine adoption if they are not addressed in a timely manner. Teacher/student involvement was also critical to give meaning to the complex multimodal data streams both for research purposes, and to design the MMLA dashboard aimed at end-users. An indicator of the success of the teachers' partnering experience, is that once they reflected on the value of the MMLA deployment, they wanted to move the deployment to happen as a part of their regular classes, potentially making the transition from research to practice an immediate possibility.

Yet, much work is still required to develop specific guidelines to create human-centred MMLA systems. For example, the rapidly growing human-centred AI [\[98\]](#page-39-0) movement within and beyond HCI has much to offer to the design and development of MMLA systems to ensure that novel AI tools are effectively in service of students and teachers. Moreover, researchers and developers may want to address the complexity of visual interfaces of multimodal data by grounding their designs in key Information Visualisation principles aimed at scaffolding the interpretation of large amounts of data by non-technical users (e.g., by applying data visualisation guidance [\[18\]](#page-35-0) or data storytelling [\[60\]](#page-37-0) principles).

*Designing MMLA considering data imperfection and teacher control***.** In Jeffrey Heer's view [\[42\]](#page-36-0), *AI methods can be applied to helpfully reshape, rather than replace, human labor*. In our study, the ultimate aim is not to replace the teacher but augment their repertoire of tools they can use to

support students' reflective thinking through data interfaces. For example, multimodal behaviour visualisations can simplify teachers' analysis of video recordings, enabling more efficient use by highlighting meaningful clips without the need to watch the entire session. Yet, the data captured from the physical world through sensing devices are often incomplete, noisy, and unreliable [\[10\]](#page-34-0). Moreover, beyond the use of multimodal data, it has been reported that there is commonly a disconnection between any logged data and higher-order educational constructs [\[30,](#page-35-0) [56\]](#page-37-0). This means that the design of effective MMLA interfaces needs to deal with data incompleteness and partial models of the actual learning activity. Creating MMLA systems that perform fully automated actions based on these incomplete data is risky, and cannot be recommended at this level of MMLA maturity.

A primary finding from our MMLA in-the-wild study is that teachers see that a key requirement to maximise the sustainability of the complex computational system is to provide a high degree of user control. The debate around the balance between human agency and AI automation is not new in HCI (e.g., [\[99\]](#page-39-0)), yet it is nascent in the context of MMLA. Nonetheless, Ogan [\[76\]](#page-38-0) suggested that once sensing technologies mature to the extent that they enable capturing a variety of behaviours in the classroom, we should let teachers empower themselves to use data for making informed decisions and improving their own classroom practices.

Moreover, we learnt that if the MMLA interface does not provide any visual cue about potential data incompleteness, both teachers and students can attempt to make potentially misleading inferences from the data. More problematically, decisions can be made and actions can be taken without sufficient recognition that logged student data is, by definition, imperfect [\[49\]](#page-36-0). In the long term, this can erode their trust in the system. This resonates with literature on AI-powered educational technologies, which suggests that offering educational users insights into the system's inner workings can alleviate their concerns and contribute to building trust [\[67\]](#page-37-0).

Future work can consider at least two potential ways to address these challenges. First, as suggested by some of the teachers in our study, it may be possible to identify gaps in teachers' knowledge around the use of data in their practice such as whether they are aware of how the multimodal data are collected, what educational constructs are being modelled, the limitations of algorithmic outputs, and the kinds of insights that can be derived from them. Professional development programs can be created to increase teachers' AI literacy [\[52\]](#page-36-0) and visualisation literacy [\[80\]](#page-38-0) for them to understand, to some extent, how they can integrate the MMLA interfaces into their existing practices or how they can adapt their current practices to the new possibilities enabled by the use of such multimodal data. Alternatively or in parallel, the teachers in our study also suggested that the MMLA user interface can be designed to provide visual cues that alert teachers about the reliability of the data so they can make informed data interpretations or decide not to use the MMLA system for a session with uncertain data. To address this, researchers and developers of this kind of innovations may want to consider elements from the emerging literature on the human aspects of AI explainability [\[45,](#page-36-0) [48\]](#page-36-0) to design MMLA systems that, for example, reveal their assumptions and biases in ways that make sense to non-specialist users so they can keep in control of the potential pedagogical actions that can be taken [\[94\]](#page-38-0).

*Ensuring teachers' and students' safety***.** Enhancing physical learning spaces with rich sensing capabilities unavoidably raises critical questions about the potentially harmful effects of excessive surveillance and potential threats to students' and teachers' privacy rather than supporting learning. Preserving human safety in increasingly autonomous smart environments has been identified as one of the main HCI grand challenges [\[103\]](#page-39-0). Selwyn [\[94\]](#page-38-0) explains that even LA systems intended to only support students' learning run the risk of being utilised for broader purposes: *the concern here lies with the secondary (re)uses of learning analytics data by institutions and other 'third parties'* (p.3). Multimodal learning data can raise particular concerns since analysing a combination

of on-skin and under-skin sensor data can lead to richer user models that could be used for student profiling or for performance measurement of teachers, which may have negative consequences for the individuals concerned [\[95\]](#page-38-0). Unfortunately, the ethical implications of using MMLA systems have been seldom mentioned in the literature, as has been flagged in recent scoping works [\[24,](#page-35-0) [113\]](#page-39-0) and reviews [\[7,](#page-34-0) [23\]](#page-35-0).

Our findings flagged some further concerns. Teachers and students may not easily grasp all the potential ways in which their data can be exploited. Yet, they had sufficient awareness to confirm that their data should only be used by themselves or by other educational stakeholders to support other students. Strict guidelines about data privacy and data ownership should be established for systems that use students' multimodal data since some of these data can be highly sensitive. For example, designers could explore ways in which end-users can indicate to the MMLA system to forget their multimodal data totally or partially after it has been used for educational purposes [\[65\]](#page-37-0). Visualising multimodal data also raised another set of potential concerns. In our second iteration, students' inclinations to participate in a MMLA study changed as they seemed to be more willing to participate in a study that only involved data collection but were not sure about all the implications related to having a user interface showing their data in front of their classmates. In this regard, future MMLA work aimed at closing the LA loop by providing end-user data interfaces would benefit from building upon the long-standing HCI research focused on designing for sharing personal data through group interfaces [\[39\]](#page-36-0).

### **6.3 Limitations**

Our study has various limitations. First, the lessons learnt are not generalisable as MMLA studies cannot be treated as a generic type of analytics. Our study involved the use of video, physiological wristbands, audio, and indoor positioning sensing. Although these cover most types of sensors used in MMLA studies [\[117\]](#page-40-0), students' and teachers' perceptions towards sensing technologies can vary across learning situations and technical setups. For example, in other studies where laboratory-grade EEG headsets have been worn by students, their perceptions towards potential negative effects related to sensor intrusiveness have been more prominent compared to those of the students in our study [\[57\]](#page-37-0).

A second limitation is that the teachers and students in our study were, to some extent, accustomed to technology-equipped learning spaces, such as the simulation rooms. Thus, our MMLA sensors were added to an existing ecology of devices and educational practices that involve the use of technologies of various kinds. Nonetheless, most of the existing technologies are not used for the purpose of monitoring and data-intensive reflection, thus, the lived experiences of the educational stakeholders were novel in relation to the MMLA innovation.

A third limitation is that the students who participated in the study and the interviews were those who were more willing to participate and often highly motivated as participation was optional. We could not interview participants who were less inclined to experience the MMLA study which prevented us from gaining a deeper understanding of the factors considered by nonconsenting students or potential further concerns about the deployment. Besides the comments from students and teachers, we also reported some of the lessons learnt from a researcher's perspective with the aim of sharing the particular experiences and insights we gained from this in-the-wild experience. Readers are encouraged to interpret these as such rather than as generalisable claims.

A fourth limitation is the inconsistent evidence collection between iterations 1 and 2 of our study (e.g., students were interviewed about intrusiveness in iteration 1 but not in iteration 2). This occurred due to the study being conducted in real-world conditions, where research objectives had to adapt to the needs and availability of teachers, students, and planned educational activities. We chose not to disrupt these factors to maintain the authentic nature of the study.

<span id="page-29-0"></span>Finally, we adopted a deductive analysis lens based on in-the-wild HCI research. There are other similar approaches that can be considered for examining the implications of deploying novel technologies in authentic learning spaces. For instance, **technology acceptance models** (**TAM**) commonly used in both LA and HCI research (e.g., References [\[6,](#page-34-0) [25\]](#page-35-0)) can provide valuable insights into usage beliefs, such as perceptions of ease-of-use and usefulness of LA tools, which can be linked to the intention to adopt the tool. Yet, these models might overlook the genuine concerns that students and teachers have regarding data and AI when deployed in-the-wild. Ignoring these concerns cannot only threaten the tool's adoption but also the integrity of data usage. Nonetheless, some models at the intersection of TAM and UX (i.e., see review in Reference [\[44\]](#page-36-0)) include *constructs* that can be directly linked to the in-the-wild constructs we explored, such as the potential impact of *social influence*, the *system characteristics*, the *task characteristics*, and *experience on use* as a result of introducing a new tool in a specific context. Researchers could consider these alternative analysis lenses for in-the-wild studies. Moreover, an inductive approach can also be beneficial in identifying other important aspects of the deployment that might not have been covered by the in-the-wild constructs we explored.

### **7 CONCLUSION**

MMLA hold great potential for enhancing the support of teachers and learners, but with various challenges. In this article, we explored the challenges that arose in an in-the-wild MMLA study. Some challenges can be addressed by refining study design and logistics and adapting educational practices. However, other challenges call for a human-centered approach to designing ethical sensing and analytics technologies. This article serves as a starting point for further work aimed at gaining a deeper understanding of effective practices for deploying sensing and MMLA technologies in real-world learning situations while maintaining integrity.

### **APPENDIX**

### **A SURVEY AND INTERVIEW GUIDE**

In this section, we provide the survey questions and interview guide for our study.

### **A.1 Student's Interview**

A total of 20 students participated in the interview during Iteration 1 of the study. Students were asked the following questions:

### **Theme 1: Space and Place**

- What data do you recall was collected during the simulation?
- Why do you think the data was collected during the simulation?
- How did you feel wearing sensors during the simulation?
- Did you have any concerns about wearing additional sensors and the microphone?

### **Theme 2: Data and Analytics**

- Would you trust the information presented in the visualisations?

### **Theme 4: Social Factors**

- Who do you think should look at these data?

- Would you be willing to share this data for academic purposes? E.g.,: for the teacher to guide the debriefing session?

### **A.2 Teaching Team Survey**

The following survey questions were handled to the teaching team at the end of the debrief session. A total of 11 teachers answered the survey in the Iteration 2 of the study.

# **Theme 2: Data and Analytics**

- How did you integrate the data into the debrief?
- Did you think the data assisted student learning?
- Did you find using the data during the debrief challenging? Why?

# **A.3 Senior Teaching Team Interview**

In this interview, a total of 4 teachers participated in Iteration 2 of the study. Teachers were asked the following questions:

# **Theme 1: Space and Place**

- How intrusive was it to equip students, teaching staff and the simulation space with various sensors?

- What kind of unexpected (technical and/or logistic) issues did you face that may have affected the learning goals of the simulations?

- How do you think those unexpected issues (technical and/or logistic) can be minimised in the future?

- Do you think you (or other teachers) may need some training with the tool beforehand?

# **Theme 2: Data and Analytics**

- How did you use the "tool" (slides) during the debrief? For what purposes?

- To what extent do you think that using the tool may have been helpful for you or the students during the debrief?

- Do you think any of the data presented may have been misleading? If yes, explain how?

- Did you trust the information presented to you through the tool during the debrief?

Did you also use the visualisations for cases where the data were incomplete (for example, when not all the students were tracked)?

- How can we improve the system so you can trust more on the data?

# **Theme 3: Human-Centredness**

- To what extent do you value the collaboration with LA researchers to design this technology with them? What motivates you to do that?

# **Theme 4: Social Factors**

- What do you think about the consenting strategy from last year's study (iteration 1) in comparison to the one for this year's study (iteration 2)?

- What do you think can be done differently regarding the consenting strategies for a future study?

- Did you perceive or hear any concerns from students regarding the study? If so, can you explain?

- Besides the students and the teacher leading the debrief, who do you think would benefit from looking at the visualisations shown in the tool used in the debrief?

# **Theme 5: Sustainability**

- How can we run our research studies in the future in a more sustainable way?

- What steps would be needed to make our current system into a real-world application without the help of a team of researchers behind it?

# **A.4 Researchers' Survey**

At the end of Iteration 2 of the study, five researchers from our team were asked to fill in a survey, comprised of the following questions:

# **Theme 1: Space and Place**

- How complex was it to transport, install, and configure the equipment before the data collection location?

- Did you face any specific challenges/problems?

- What kind of unexpected technical challenges/problems did you face regarding the sensors/equipment during the study/data collection?

- What kind of unexpected logistic issues did you face that could affect the study?

- How do you think those unexpected issues (technical and/or logistic) can be minimised in the future?

### **Theme 3: Human-Centredness**

- What do you value the most when collaborating or interacting with teachers or students to plan, analyse or validate your research ideas?

### **Theme 4: Social Factors**

- What do you think about the consenting strategy from last year's data collection (iteration 1)?

- What do you think about the consenting strategy from this year's data collection (iteration 2)?

- Please share your ideas on what you think can be done differently regarding the consenting strategies for a future study/data collection.

# **Theme 5: Sustainability**

- How can we run our research studies in the future in a more sustainable way?

- How can we recover from failure and debug issues to provide reliability to our systems?

- What steps would be needed to make our current system into a real-world application without the help of a team of researchers behind it?

# **A.5 Student's Survey**

A total of 47 students completed a survey during Iteration 2 of the study. Students were asked the following questions about the visualisations presented in the MMLA dashboard:

### **Theme 2: Data and Analytics**

### *Visualisation 1: Team Communication*

Please review the following visualisation that represents the data we collected during your simulation. Reflect on what it may represent and answer the questions below.

![](_page_31_Figure_22.jpeg)

*Note: this visualisation IS representing YOUR data.*

Lessons Learnt from a Multimodal Learning Analytics Deployment In-the-Wild 8:33

- To what extent you would trust this visualisation to judge or reflect on your own performance?

- (1) I'd absolutely trust on it
- (2) I'd trust it to some extent
- (3) neutral/borderline
- (4) I'd not trust it to some extent
- (5) I'd absolutely not trust on it
- Please, briefly explain your response.

- Do you have any comments on how this visualisation can be improved, or would you add something to the visualisation to make it more straightforward?

## *Visualisation 2: Team Speaking and Positioning*

Please review the following visualisation that represents the data we collected during your simulation. Reflect on what it may represent and answer the questions below.

![](_page_32_Figure_11.jpeg)

*Note: this visualisation IS representing YOUR data.*

- To what extent you would trust this visualisation to judge or reflect on your own performance?

- (1) I'd absolutely trust on it
- (2) I'd trust it to some extent
- (3) neutral/borderline
- (4) I'd not trust it to some extent
- (5) I'd absolutely not trust on it
- Please, briefly explain your response.

- Do you have any comments on how this visualisation can be improved, or would you add something to the visualisation to make it more straightforward?

# *Visualisation 3: Team Prioritisation*

Please review the following visualisation that represents the data we collected during your simulation. Reflect on what it may represent and answer the questions below.

![](_page_33_Figure_3.jpeg)

*Note: this visualisation IS representing YOUR data.*

- To what extent you would trust this visualisation to judge or reflect on your own performance?

- (1) I'd absolutely trust on it
- (2) I'd trust it to some extent
- (3) neutral/borderline
- (4) I'd not trust it to some extent
- (5) I'd absolutely not trust on it
- Please, briefly explain your response.

- Do you have any comments on how this visualisation can be improved, or would you add something to the visualisation to make it more straightforward?

### **AUTHORS' STATEMENT**

The content and contribution of the manuscript are unique in relation to our previous publications. In the manuscript, we cite our own papers where details that are not related to the main contribution of the current manuscript can be found. More specifically, these are two other papers, cited in Section [3,](#page-5-0) where some details about the human-centred design approach we followed can be consulted:

*Vanessa Echeverria, Roberto Martinez-Maldonado, Lixiang Yan, Linxuan Zhao, Gloria Fernandez-Nieto, Dragan Gašević, and Simon Buckingham Shum. 2022. HuCETA: A Framework for Human-Centered Embodied Teamwork Analytics. IEEE Pervasive Computing (2022), 1–11.*

*Gloria Milena Fernandez Nieto, Kirsty Kitto, Simon Buckingham Shum, and Roberto Martinez-Maldonado. 2022. Beyond the Learning Analytics Dashboard: Alternative Ways to Communicate Student Data Insights Combining Visualisation, Narrative and Storytelling. In 12th International*

<span id="page-34-0"></span>*Learning Analytics and Knowledge Conference (Online, USA) (LAK22). ACM New York, NY, USA, 219–229.*

### **REFERENCES**

- [1] Gregory D. Abowd and Elizabeth D. Mynatt. 2000. Charting past, present, and future research in ubiquitous computing. *[ACM Transactions on Computer–Human Interaction \(TOCHI\)](https://doi.org/10.1145/344949.344988)* 7, 1 (Mar. 2000), 29–58. DOI:https://doi.org/10. 1145/344949.344988
- [2] June Ahn, Fabio Campos, Maria Hays, and Daniela Digiacomo. 2019. Designing in context: Reaching beyond usability [in learning analytics dashboard design.](https://doi.org/10.18608/jla.2019.62.5) *Journal of Learning Analytics* 6, 2 (Jul. 2019), 70–85. DOI:https://doi.org/10. 18608/jla.2019.62.5
- [3] Karan Ahuja, Dohyun Kim, Franceska Xhakaj, Virag Varga, Anne Xie, Stanley Zhang, Jay Eric Townsend, Chris Harrison, Amy Ogan, and Yuvraj Agarwal. 2019. EduSense: Practical classroom sensing at scale. *Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies* 3, 3 (2019), 1–26. DOI:<https://doi.org/10.1145/3351229>
- [4] Hamed S. Alavi, Elizabeth F. Churchill, Mikael Wiberg, Denis Lalanne, Peter Dalsgaard, Ava Fatah gen Schieck, and Yvonne Rogers. 2019. Introduction to human-building interaction (HBI): Interfacing HCI with architecture and urban design. *[ACM Transactions on Computer-Human Interaction \(TOCHI\)](https://doi.org/10.1145/3309714)*. 26, 2, Article 6 (Mar 2019), 10 pages. https: //doi.org/10.1145/3309714
- [5] Riordan Dervin Alfredo, Lanbing Nie, Paul Kennedy, Tamara Power, Carolyn Hayes, Hui Chen, Carolyn McGregor, Zachari Swiecki, Dragan Gašević, and Roberto Martinez-Maldonado. 2023. "That student should be a Lion Tamer!" StressViz: Designing a stress analytics dashboard for teachers. In *Proceedings of the 13th International Learning Analytics and Knowledge Conference* (Arlington, TX, USA) *(LAK'23)*. Association for Computing Machinery, New York, NY, 57–67. DOI:<https://doi.org/10.1145/3576050.3576058>
- [6] Liaqat Ali, Mohsen Asadi, Dragan Gašević, Jelena Jovanović, and Marek Hatala. 2013. Factors influencing beliefs for adoption of a learning analytics tool: An empirical study. *Computers & Education* 62 (March 2013), 130–148. DOI:<https://doi.org/10.1016/j.compedu.2012.10.023>
- [7] Haifa Alwahaby, Mutlu Cukurova, Zacharoula Papamitsiou, and Michail Giannakos. 2022. *The Evidence of Impact and Ethical Considerations of Multimodal Learning Analytics: A Systematic Literature Review*. Springer International Publishing, Cham, 289–325. DOI:[https://doi.org/10.1007/978-3-031-08076-0\\_12](https://doi.org/10.1007/978-3-031-08076-0_12)
- [8] Roger Azevedo and Dragan Gašević. 2019. Analyzing multimodal multichannel data about self-regulated learning with advanced learning technologies: Issues and challenges. *Computers in Human Behavior* 96 (July 2019), 207–210. DOI:<https://doi.org/10.1016/j.chb.2019.03.025>
- [9] Mara Balestrini, Sarah Gallacher, and Yvonne Rogers. 2020. Moving HCI outdoors: Lessons learned from conducting research in the wild. In *Proceedings of the HCI Outdoors: Theory, Design, Methods and Applications*. Springer, 83–98. DOI:[https://doi.org/10.1007/978-3-030-45289-6\\_4](https://doi.org/10.1007/978-3-030-45289-6_4)
- [10] Oluwaseun Bamgboye, Xiaodong Liu, and Peter Cruickshank. 2018. Towards modelling and reasoning about uncertain data of sensor measurements for decision support in smart spaces. In *Proceedings of the 2018 IEEE 42nd Annual [Computer Software and Applications Conference \(COMPSAC'18\)](https://doi.org/10.1109/COMPSAC.2018.10330)*, Vol. 2. IEEE, 744–749. DOI:https://doi.org/10.1109/ COMPSAC.2018.10330
- [11] Glenn Barton, Anne Bruce, and Rita Schreiber. 2018. Teaching nurses teamwork: Integrative review of competency[based team training in nursing education.](https://doi.org/10.1016/j.nepr.2017.11.019) *Nurse Education in Practice* 32 (September 2018), 129–137. https://doi.org/ 10.1016/j.nepr.2017.11.019
- [12] Marc Beardsley, Judit Martínez Moreno, Milica Vujovic, Patricia Santos, and Davinia Hernández-Leo. 2020. Enhancing consent forms to support participant decision making in multimodal learning data research. *British Journal of Educational Technology* 51, 5 (2020), 1631–1652. DOI:<https://doi.org/10.1111/bjet.12983>
- [13] Steve Benford, Chris Greenhalgh, Bob Anderson, Rachel Jacobs, Mike Golembewski, Marina Jirotka, Bernd Carsten Stahl, Job Timmermans, Gabriella Giannachi, Matt Adams, Ju Row Farr, Nick Tandavanitj, and Kirsty Jennings. 2015. The ethical implications of HCI's turn to the cultural. *ACM Transactions on Computer–Human Interaction (TOCHI)* 22, 5, Article 24 (Aug. 2015), 37 pages. DOI:<https://doi.org/10.1145/2775107>
- [14] [Paulo Blikstein. 2013. Multimodal learning analytics. In](https://doi.org/10.1145/2460296.2460316) *Proceedings of the LAK'13*. 102–106. DOI:https://doi.org/10. 1145/2460296.2460316
- [15] [Virginia Braun and Victoria Clarke. 2012. Thematic analysis. APA, Washington, DC, 57–71.](https://doi.org/10.1037/13620-004) DOI:https://doi.org/10. 1037/13620-004
- [16] Jason A. Brotherton and Gregory D. Abowd. 2004. Lessons learned from EClass: Assessing automated capture and access in the classroom. *ACM Transactions on Computer–Human Interaction (TOCHI)* 11, 2 (Jun. 2004), 121–155. DOI:<https://doi.org/10.1145/1005361.1005362>

- <span id="page-35-0"></span>[17] Simon Buckingham Shum, Rebecca Ferguson, and Roberto Martinez-Maldonado. 2019. Human-centred learning analytics. *Journal of Learning Analytics* 6, 2 (Jul. 2019), 1–9. DOI:<https://doi.org/10.18608/jla.2019.62.1>
- [18] Davide Ceneda, Theresia Gschwandtner, Thorsten May, Silvia Miksch, Hans-Jörg Schulz, Marc Streit, and Christian Tominski. 2016. Characterizing guidance in visual analytics. *IEEE Transactions on Visualization and Computer Graphics* 23, 1 (2016), 111–120. DOI:<https://doi.org/10.1109/TVCG.2016.2598468>
- [19] Pankaj Chejara, Reet Kasepalu, Luis P. Prieto, María Jesús Rodríguez-Triana, Adolfo Ruiz-Calleja, and Shashi Kant Shankar. 2023. Multimodal learning analytics research in the wild: Challenges and their potential solutions. In *Proceedings of the CrossMMLA'23 Workshop: Leveraging Multimodal Data for Generating Meaningful Feedback*. 1–7.
- [20] Yi Han Victoria Chua, Justin Dauwels, and Seng Chee Tan. 2019. Technologies for automated analysis of co-located, real-life, physical learning spaces: Where are we now?. In *Proceedings of the 9th International Learning Analytics and Knowledge Conference*. 11–20. DOI:<https://doi.org/10.1145/3303772.3303811>
- [21] Hector Cornide-Reyes, René Noël, Fabián Riquelme, Matías Gajardo, Cristian Cechinel, Roberto Mac Lean, Carlos Becerra, Rodolfo Villarroel, and Roberto Munoz. 2019. Introducing low-cost sensors into the classroom settings: [Improving the assessment in agile practices with multimodal learning analytics.](https://doi.org/10.3390/s19153291) *Sensors* 19, 15 (2019), 3291. DOI:https: //doi.org/10.3390/s19153291
- [22] Andy Crabtree, Alan Chamberlain, Rebecca E. Grinter, Matt Jones, Tom Rodden, and Yvonne Rogers. 2013. Introduction to the special issue of "The Turn to The Wild". *ACM Transactions on Computer–Human Interaction (TOCHI)* 20, 3 (2013), 1–4. DOI:<https://doi.org/10.1145/2491500.2491501>
- [23] Lucrezia Crescenzi-Lanna. 2020. Multimodal learning analytics research with young children: A systematic review. *British Journal of Educational Technology* 51, 5 (2020), 1485–1504. DOI:<https://doi.org/10.1111/bjet.12959>
- [24] Mutlu Cukurova, Michail Giannakos, and Roberto Martinez-Maldonado. 2020. The promise and challenges of multimodal learning analytics. *[British Journal of Educational Technology](https://doi.org/10.1111/bjet.13015)* 51, 5 (Sep 2020), 1441–1449. DOI:https: //doi.org/10.1111/bjet.13015
- [25] Fred D. Davis. 1989. Perceived usefulness, perceived ease of use, and user acceptance of information technology. *MIS Quarterly* 13, 3 (1989), 319–340. <https://doi.org/10.2307/249008>
- [26] Daniele Di Mitri, Jan Schneider, and Hendrik Drachsler. 2021. Keep me in the loop: Real-time feedback with multimodal data. *[International Journal of Artificial Intelligence in Education](https://doi.org/10.1007/s40593-021-00281-z)* 13 (2021), 1–26. DOI:https://doi.org/10.1007/ s40593-021-00281-z
- [27] Muhterem Dindar, Sanna Järvelä, and Eetu Haataja. 2020. What does physiological synchrony reveal about metacognitive experiences and group performance? *British Journal of Educational Technology* 51, 5 (2020), 1577–1596.
- [28] Sidney K. D'Mello, Andrew M. Olney, Nathan Blanchard, Borhan Samei, Xiaoyi Sun, Brooke Ward, and Sean Kelly. 2015. Multimodal capture of teacher-student interactions for automated dialogic analysis in live classrooms. In *Pro[ceedings of the ACM on International Conference on Multimodal Interaction](https://doi.org/10.1145/2818346.2830602)*. ACM, 557–566. DOI:https://doi.org/10. 1145/2818346.2830602
- [29] Sidney D'Mello and Art Graesser. 2012. Dynamics of affective states during complex learning. *Learning and Instruction* 22, 2 (2012), 145–157.
- [30] Vanessa Echeverria, Roberto Martinez-Maldonado, and Simon Buckingham Shum. 2019. Towards collaboration translucence: Giving meaning to multimodal group data. In *Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI'19)*. ACM, 1–16. DOI:<https://doi.org/10.1145/3290605.3300269>
- [31] Vanessa Echeverria, Roberto Martinez-Maldonado, Lixiang Yan, Linxuan Zhao, Gloria Fernandez-Nieto, Dragan Gašević, and Simon Buckingham Shum. 2023. HuCETA: A framework for human-centered embodied teamwork analytics. *IEEE Pervasive Computing* 22, 1 (2023), 39–49. DOI:<https://doi.org/10.1109/MPRV.2022.3217454>
- [32] Jennifer Fereday and Eimear Muir-Cochrane. 2006. Demonstrating rigor using thematic analysis: A hybrid approach of inductive and deductive coding and theme development. *International Journal of Qualitative Methods* 5, 1 (2006), 80–92. DOI:<https://doi.org/10.1177/160940690600500107>
- [33] Rebecca Ferguson, Tore Hoel, Maren Scheffel, and Hendrik Drachsler. 2016. Guest editorial: Ethics and privacy in learning analytics. *Journal of Learning Analytics* 3, 1 (2016), 5–15. DOI:<https://doi.org/10.18608/jla.2016.31.2>
- [34] Gloria Fernandez-Nieto, Roberto Martinez-Maldonado, Vanessa Echeverria, Kirsty Kitto, Pengcheng An, and Simon Buckingham Shum. 2021. What can analytics for teamwork proxemics reveal about positioning dynamics in clinical simulations? *[Proceedings of the ACM on Human–Computer Interaction](https://doi.org/10.1145/3449284)* 5, CSCW1 (2021), 1–24. DOI:https://doi.org/ 10.1145/3449284
- [35] Gloria Milena Fernandez Nieto, Kirsty Kitto, Simon Buckingham Shum, and Roberto Martinez-Maldonado. 2022. Beyond the learning analytics dashboard: Alternative ways to communicate student data insights combining visualisation, narrative and storytelling. In *Proceedings of the 12th International Learning Analytics and Knowledge Conference*. ACM, 219–229. DOI:<https://doi.org/10.1145/3506860.3506895>
- [36] Michail Giannakos, Daniel Spikol, Daniele Di Mitri, Kshitij Sharma, Xavier Ochoa, and Rawad Hammad. 2022. *Introduction to Multimodal Learning Analytics*[. Springer International Publishing, Cham, 3–28.](https://doi.org/10.1007/978-3-031-08076-0_1) DOI:https://doi.org/10. 1007/978-3-031-08076-0\_1

- <span id="page-36-0"></span>[37] Kirk Goldsberry. 2012. Courtvision: New visual and spatial analytics for the nba. In *Proceedings of the 2012 MIT Sloan Sports Analytics Conference*, Vol. 9. 12–15.
- [38] Jamie Gorson, Kathryn Cunningham, Marcelo Worsley, and Eleanor O'Rourke. 2022. Using electrodermal activity measurements to understand student emotions while programming. In *Proceedings of the 2022 ACM Conference on International Computing Education Research-Volume 1*. 105–119. DOI:<https://doi.org/10.1145/3501385.3543981>
- [39] Saul Greenberg, Michael Boyle, and Jason LaBerge. 1999. PDAs and shared public displays: Making personal infor[mation public, and public information personal.](https://doi.org/10.1007/BF01305320) *Personal Technologies* 3, 1 (1999), 54–64. DOI:https://doi.org/10.1007/ BF01305320
- [40] David Gualda, María Carmen Pérez-Rubio, Jesús Ureña, Sergio Pérez-Bachiller, José Manuel Villadangos, Álvaro Hernández, Juan Jesús García, and Ana Jiménez. 2021. LOCATE-US: Indoor positioning for mobile devices using [encoded ultrasonic signals, inertial sensors and graph-matching.](https://doi.org/10.3390/s21061950) *Sensors* 21, 6 (2021), 1950. DOI:https://doi.org/10. 3390/s21061950
- [41] Carolina Guzmán-Valenzuela, Carolina Gómez-González, Andrés Rojas-Murphy Tagle, and Alejandro Lorca-Vyhmeister. 2021. Learning analytics in higher education: A preponderance of analytics but very little learning? *Inter[national Journal of Educational Technology in Higher Education](https://doi.org/10.1186/s41239-021-00258-x)* 18, 1 (2021), 1–19. DOI:https://doi.org/10.1186/s41239- 021-00258-x
- [42] Jeffrey Heer. 2019. Agency plus automation: Designing artificial intelligence into interactive systems. *Proceedings of the National Academy of Sciences* 116, 6 (2019), 1844–1850. DOI:<https://doi.org/10.1073/pnas.1807184115>
- [43] Bernie Hogan, Joshua R. Melville, Gregory Lee Phillips II, Patrick Janulis, Noshir Contractor, Brian S. Mustanski, and Michelle Birkett. 2016. Evaluating the paper-to-screen translation of participant-aided sociograms with high-risk participants. In *Proceedings of the SIGCHI Conference on Human Factors in Computing Systems*. 5360–5371.
- [44] Kasper Hornbæk and Morten Hertzum. 2017. Technology acceptance and user experience: A review of the experiential component in HCI. *ACM Transactions on Computer–Human Interaction (TOCHI)* 24, 5 (2017), 1–30.
- [45] Jinglu Jiang, Surinder Kahai, and Ming Yang. 2022. Who needs explanation and when? Juggling explainable AI and user epistemic uncertainty. *International Journal of Human–Computer Studies* 165 (September 2022), 102839. DOI:<https://doi.org/10.1016/j.ijhcs.2022.102839>
- [46] Rogers Kaliisa, Anders Kluge, and Anders I Mørch. 2022. Overcoming challenges to the adoption of learning analytics at the practitioner level: A critical analysis of 18 learning analytics frameworks. *Scandinavian Journal of Educational Research* 66, 3 (2022), 367–381.
- [47] Reet Kasepalu, Pankaj Chejara, Luis P. Prieto, and Tobias Ley. 2021. Do teachers find dashboards trustworthy, actionable and useful? A vignette study using a logs and audio dashboard. *Technology, Knowledge and Learning* (2021), 1–19. DOI:<https://doi.org/10.1007/s10758-021-09522-5>
- [48] Hassan Khosravi, Simon Buckingham Shum, Guanliang Chen, Cristina Conati, Yi-Shan Tsai, Judy Kay, Simon Knight, Roberto Martinez-Maldonado, Shazia Sadiq, and Dragan Gašević. 2022. Explainable artificial intelligence in education. *Computers and Education: Artificial Intelligence* 3 (2022), 100074. <https://doi.org/10.1016/j.caeai.2022.100074>
- [49] Kirsty Kitto, Simon Buckingham Shum, and Andrew Gibson. 2018. Embracing imperfection in learning analytics. In *Proceedings of the 8th ACM International Conference on Learning Analytics and Knowledge*. ACM, New York, NY, 451–460. DOI:<https://doi.org/10.1145/3170358.3170413>
- [50] Kristian Krogh, Margaret Bearman, and Debra Nestel. 2015. Expert practice of video-assisted debriefing: An Australian qualitative study. *Clinical Simulation in Nursing* 11, 3 (2015), 180–187.
- [51] Chen-Hsuan Liao and Jiun-Yu Wu. 2022. Deploying multimodal learning analytics models to explore the impact of digital distraction and peer learning on student performance. *Computers & Education* 190 (December 2022), 104599. DOI:<https://doi.org/10.1016/j.compedu.2022.104599>
- [52] Duri Long and Brian Magerko. 2020. What is AI literacy? Competencies and design considerations. In *Proceedings [of the 2020 SIGCHI Conference on Human Factors in Computing Systems](https://doi.org/10.1145/3313831.3376727)*. ACM, 1–16. DOI:https://doi.org/10.1145/ 3313831.3376727
- [53] Gonzalo Luzardo, Bruno Guamán, Katherine Chiluiza, Jaime Castells, and Xavier Ochoa. 2014. Estimation of presentations skills based on slides and audio features. In *Proceedings of the 2014 ACM Workshop on Multimodal Learning Analytics Workshop and Grand Challenge*. ACM, 37–44. DOI:<https://doi.org/10.1145/2666633.2666639>
- [54] Yingbo Ma, Mehmet Celepkolu, and Kristy Elizabeth Boyer. 2022. Detecting impasse during collaborative problem solving with multimodal learning analytics. In *Proceedings of the 12th International Learning Analytics and Knowledge Conference*. ACM, New York, NY, 45–55. DOI:<https://doi.org/10.1145/3506860.3506865>
- [55] Aditi Mallavarapu, Leilah Lyons, and Stephen Uzzo. 2022. Exploring the utility of social-network-derived collaborative opportunity temperature readings for informing design and research of large-group immersive learning environments. *Journal of Learning Analytics* 9, 1 (2022), 53–76.

### <span id="page-37-0"></span>8:38 R. Martinez-Maldonado et al.

- [56] Katerina Mangaroska and Michail Giannakos. 2018. Learning analytics for learning design: A systematic literature review of analytics-driven design to enhance learning. *IEEE Transactions on Learning Technologies* 12, 4 (2018), 516– 534. DOI:<https://doi.org/10.1109/TLT.2018.2868673>
- [57] Katerina Mangaroska, Roberto Martinez-Maldonado, Boban Vesin, and Dragan Gašević. 2021. Challenges and opportunities of multimodal data in human learning: The computer science students' perspective. *Journal of Computer Assisted Learning* 37, 4 (2021), 1030–1047. <https://doi.org/10.1111/jcal.12542>
- [58] Katerina Mangaroska, Kshitij Sharma, Dragan Gašević, and Michail Giannakos. 2022. Exploring students' cognitive and affective states during problem solving through multimodal data: Lessons learned from a programming activity. *Journal of Computer Assisted Learning* 38, 1 (2022), 40–59. DOI:<https://doi.org/10.1111/jcal.12590>
- [59] Nikki J. Maran and Ronnie J. Glavin. 2003. Low-to high-fidelity simulation–a continuum of medical education? *Medical Education* 37, S1 (2003), 22–28. DOI:<https://doi.org/10.1046/j.1365-2923.37.s1.9.x>
- [60] Roberto Martinez-Maldonado, Vanessa Echeverria, Gloria Fernandez Nieto, and Simon Buckingham Shum. 2020. From data to insights: A layered storytelling approach for multimodal learning analytics. In *Proceedings of the SIGCHI Conference on Human Factors in Computing Systems* (Honolulu, HI, USA) *(CHI '20)*. ACM, New York, NY, 1–15. DOI:<https://doi.org/10.1145/3313831.3376148>
- [61] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and inter-rater reliability in qualitative research: Norms and guidelines for CSCW and HCI practice. *Proceedings of the ACM on Human–Computer Interaction* 3, CSCW (2019), 1–23. DOI:<https://doi.org/10.1145/3359174>
- [62] Mary Erickson Megel, Cynthia Bailey, Annette Schnell, Dina Whiteaker, and Angela Vogel. 2013. High-fidelity sim[ulation: How are we using the videos?](https://doi.org/10.1016/j.ecns.2012.04.003) *Clinical Simulation in Nursing* 9, 8 (2013), e305–e310. DOI:https://doi.org/10. 1016/j.ecns.2012.04.003
- [63] Louis-Philippe Morency, Sharon Oviatt, Stefan Scherer, Nadir Weibel, and Marcelo Worsley. 2013. ICMI 2013 grand challenge workshop on multimodal learning analytics. In *Proceedings of the 15th ACM on International Conference on Multimodal Interaction*. 373–378.
- [64] Su Mu, Meng Cui, and Xiaodi Huang. 2020. Multimodal data fusion in learning analytics: A systematic review. *Sensors* 20, 23 (2020), 6856. DOI:<https://doi.org/10.3390/s20236856>
- [65] Michael Muller and Angelika Strohmayer. 2022. Forgetting practices in the data sciences. In *Proceedings of the 2022 [SIGCHI Conference on Human Factors in Computing Systems](https://doi.org/10.1145/3491102.3517644)*. ACM, New York, NY, Article 323, 19 pages. DOI:https: //doi.org/10.1145/3491102.3517644
- [66] Natsuki Nakayama, Naoko Arakawa, Harumi Ejiri, Reiko Matsuda, and Tsuneko Makino. 2018. Heart rate variability can clarify students' level of stress during nursing simulation. *PLoS One* 13, 4 (2018), e0195280.
- [67] Tanya Nazaretsky, Moriah Ariely, Mutlu Cukurova, and Giora Alexandron. 2022. Teachers' trust in AI-powered educational technology and a professional development program to improve it. *British Journal of Educational Technology* 53, 4 (2022), 914–931. DOI:<https://doi.org/10.1111/bjet.13232>
- [68] Tanya Nazaretsky, Mutlu Cukurova, and Giora Alexandron. 2022. An instrument for measuring teachers' trust in AIbased educational technology. In *Proceedings of the 12th International Learning Analytics and Knowledge Conference* (Online, USA) *(LAK22)*. ACM, New York, 56–66. DOI:<https://doi.org/10.1145/3506860.3506866>
- [69] Davy Tsz Kit Ng, Jac Ka Lok Leung, Maggie Jiahong Su, Iris Heung Yue Yim, Maggie Shen Qiao, and Samuel Kai Wah Chu. 2022. *AI Literacy from Educators' Perspectives*[. Springer International Publishing, Cham, 131–139.](https://doi.org/10.1007/978-3-031-18880-0_10) DOI:https: //doi.org/10.1007/978-3-031-18880-0\_10
- [70] Omid Noroozi, Iman Alikhani, Sanna Järvelä, Paul A. Kirschner, Ilkka Juuso, and Tapio Seppänen. 2019. Multimodal data to design visual learning analytics for understanding regulation of learning. *Computers in Human Behavior* 100 (2019), 298–304. DOI:<https://doi.org/10.1016/j.chb.2018.12.019>
- [71] Omid Noroozi, Héctor J. Pijeira-Díaz, Marta Sobocinski, Muhterem Dindar, Sanna Järvelä, and Paul A. Kirschner. 2020. Multimodal data indicators for capturing cognitive, motivational, and emotional learning processes: A systematic literature review. *[Education and Information Technologies](https://doi.org/10.1007/s10639-020-10229-w)* 25, 6 (2020), 5499–5547. DOI:https://doi.org/10.1007/s10639- 020-10229-w
- [72] Xavier Ochoa. 2022. *Multimodal Learning Analytics—Rationale, Process, Examples, and Direction* (2nd ed.). SoLAR, Vancouver, Canada, 54–65. Retrieved from <https://www.solaresearch.org/publications/hla-22/hla22-chapter6/>
- [73] Xavier Ochoa and Federico Dominguez. 2020. Controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting. *[British Journal of Educational Technology](https://doi.org/10.1111/bjet.12987)* 51, 5 (2020), 1615–1630. DOI:https: //doi.org/10.1111/bjet.12987
- [74] Xavier Ochoa, Federico Domínguez, Bruno Guamán, Ricardo Maya, Gabriel Falcones, and Jaime Castells. 2018. The RAP system: Automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors. In *Proceedings of the 8th ACM International Conference on Learning Analytics and Knowledge*. ACM, Sydney New South Wales Australia, 360–364. DOI:<https://doi.org/10.1145/3170358.3170406>

- <span id="page-38-0"></span>[75] Xavier Ochoa and Marcelo Worsley. 2016. Augmenting learning analytics with multimodal sensory data. *Journal of Learning Analytics* 3, 2 (2016), 213–219.
- [76] [Amy Ogan. 2019. Reframing classroom sensing: Promise and peril.](https://doi.org/10.1145/3358902) *Interactions* 26, 6 (Oct. 2019), 26–32. DOI:https: //doi.org/10.1145/3358902
- [77] Leif Oppermann, Alexander Boden, Britta Hofmann, Wolfgang Prinz, and Stefan Decker. 2019. Beyond HCI and CSCW: Challenges and useful practices towards a human-centred vision of AI and IA. In *Proceedings of the Halfway to the Future Symposium 2019*. 1–5.
- [78] Sharon Oviatt. 2018. Ten opportunities and challenges for advancing student-centered multimodal learning analytics. In *Proceedings of the 20th ACM International Conference on Multimodal Interaction*. ACM, New York, NY, 87–94. DOI:<https://doi.org/10.1145/3242969.3243010>
- [79] Zacharoula Papamitsiou, Ilias O. Pappas, Kshitij Sharma, and Michail N. Giannakos. 2020. Utilizing multimodal data through fsQCA to explain engagement in adaptive learning. *IEEE Transactions on Learning Technologies* 13, 4 (2020), 689–703. DOI:<https://doi.org/10.1109/TLT.2020.3020499>
- [80] Stanislav Pozdniakov, Roberto Martinez-Maldonado, Yi-Shan Tsai, Vanessa Echeverria, Namrata Srivastava, and Dragan Gašević. 2023. How do teachers use dashboards enhanced with data storytelling elements according to their data visualisation literacy skills?. In *Proceedings of the 13th International Learning Analytics and Knowledge Conference*. 1–14.
- [81] Sambit Praharaj, Maren Scheffel, Hendrik Drachsler, and Marcus Specht. 2021. Literature review on co-located collaboration modeling using multimodal learning analytics–can we go the whole nine yards? *IEEE Transactions on Learning Technologies* 14, 3 (2021), 367–385. DOI:<https://doi.org/10.1109/TLT.2021.3097766>
- [82] Luis Pablo Prieto, Kshitij Sharma, Łukasz Kidzinski, María Jesús Rodríguez-Triana, and Pierre Dillenbourg. 2018. Multimodal teaching analytics: Automated extraction of orchestration graphs from wearable sensor data. *Journal of Computer Assisted Learning* 34, 2 (2018), 193–203. DOI:<https://doi.org/10.1111/jcal.12232>
- [83] Fabián Riquelme, Rene Noel, Hector Cornide-Reyes, Gustavo Geldes, Cristian Cechinel, Diego Miranda, Rodolfo Villarroel, and Roberto Munoz. 2020. Where are you? Exploring micro-location in indoor learning environments. *IEEE Access* 8 (2020), 125776–125785. <https://doi.org/10.1109/ACCESS.2020.3008327>
- [84] Yvonne Rogers and Paul Marshall. 2017. *Research in the Wild*. Synthesis Lectures on Human-Centered Informatics, Vol. 10. Morgan & Claypool Publishers. i–97 pages. DOI:<https://doi.org/10.1007/978-3-031-02220-3>
- [85] Miguel A. Ronda-Carracao, Olga C. Santos, Gloria Fernandez Nieto, and Roberto Martínez Maldonado. 2021. Towards exploring stress reactions in teamwork using multimodal physiological data. In *Proceedings of the First International Workshop on Multimodal Artificial Intelligence in Education (MAIED'21)*. 49–60.
- [86] John Rooksby. 2013. Wild in the laboratory: A discussion of plans and situated actions. *ACM Transactions on Computer–Human Interaction (TOCHI)* 20, 3 (2013), 1–17. DOI:<https://doi.org/10.1145/2491500.2491507>
- [87] Selma Šabanović, Sarah M. Reeder, and Bobak Kechavarzi. 2014. Designing robots in the wild: In situ prototype evaluation for a break management robot. *Journal of Human–Robot Interaction* 3, 1 (2014), 70–88.
- [88] Nazmus Saquib, Ayesha Bose, Dwyane George, and Sepandar Kamvar. 2018. Sensei: Sensing educational interaction. *[Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies](https://doi.org/10.1145/3161172)* 1, 4 (Jan 2018), 1–27. DOI:https: //doi.org/10.1145/3161172
- [89] Aleksandra Sarcevic, Ivan Marsic, and Randal S. Burd. 2012. Teamwork errors in trauma resuscitation. *ACM Transactions on Computer–Human Interaction (TOCHI)* 19, 2 (2012), 1–30. DOI:<https://doi.org/10.1145/2240156.2240161>
- [90] Stefan Scherer, Marcelo Worsley, and Louis-Philippe Morency. 2012. 1st international workshop on multimodal learning analytics. In *Proceedings of the 14th ACM International Conference on Multimodal Interaction*. 609–610. DOI:<https://doi.org/10.1145/2388676.2388803>
- [91] Kimberly Schertzer and Muhammad Waseem. 2020. Use of video during debriefing in medical simulation. *StatPearls. Treasure Island (FL): StatPearls Publishing; 2020* (2020).
- [92] Bertrand Schneider, Kshitij Sharma, Sébastien Cuendet, Guillaume Zufferey, Pierre Dillenbourg, and Roy Pea. 2016. Using mobile eye-trackers to unpack the perceptual benefits of a tangible user interface for collaborative learning. *[ACM Transactions on Computer–Human Interaction \(TOCHI\)](https://doi.org/10.1145/3012009)* 23, 6, Article 39 (Dec. 2016), 23 pages. DOI:https://doi. org/10.1145/3012009
- [93] Bertrand Schneider, Kshitij Sharma, Sebastien Cuendet, Guillaume Zufferey, Pierre Dillenbourg, and Roy Pea. 2018. Leveraging mobile eye-trackers to capture joint visual attention in co-located collaborative learning groups. *Inter[national Journal of Computer-Supported Collaborative Learning](https://doi.org/10.1007/s11412-018-9281-2)* 13, 3 (2018), 241–261. DOI:https://doi.org/10.1007/ s11412-018-9281-2
- [94] Neil Selwyn. 2019. What's the problem with learning analytics? *Journal of Learning Analytics* 6, 3 (2019), 11–19. DOI:<https://doi.org/10.18608/jla.2019.63.3>
- [95] Neil Selwyn and Luci Pangrazio. 2018. Doing data differently? Developing personal data tactics and strategies [amongst young mobile media users.](https://doi.org/10.1177/2053951718765021) *Big Data & Society* 5, 1 (2018), 2053951718765021. DOI:https://doi.org/10.1177/ 2053951718765021

### <span id="page-39-0"></span>8:40 R. Martinez-Maldonado et al.

- [96] Shashi Kant Shankar, Luis P. Prieto, María Jesús Rodríguez-Triana, and Adolfo Ruiz-Calleja. 2018. A review of multimodal learning analytics architectures. In *Proceedings of the IEEE 18th International Conference on Advanced Learning Technologies*. IEEE, 212–214. DOI:<https://doi.org/10.1109/ICALT.2018.00057>
- [97] Kshitij Sharma and Michail Giannakos. 2020. Multimodal data capabilities for learning: What can multimodal data tell us about learning? *[British Journal of Educational Technology](https://doi.org/10.1111/bjet.12993)* 51, 5 (2020), 1450–1484. DOI:https://doi.org/10.1111/ bjet.12993
- [98] Ben Shneiderman. 2022. *Human-Centered AI*. Oxford.
- [99] Ben Shneiderman and Pattie Maes. 1997. Direct manipulation vs. interface agents. *Interactions* 4, 6 (1997), 42–61. DOI:<https://doi.org/10.1145/267505.267514>
- [100] George Siemens. 2012. Learning analytics: Envisioning a research discipline and a domain of practice. In *Proceedings of the 2nd International Conference on Learning Analytics and Knowledge*. 4–8.
- [101] Jocelyn Spence, Benjamin Bedwell, Michelle Coleman, Steve Benford, Boriana N. Koleva, Matt Adams, Ju Row Farr, Nick Tandavanitj, and Anders Sundnes Løvlie. 2019. Seeing with new eyes: Designing for in-the-wild museum gifting. In *Proceedings of the SIGCHI Conference on Human Factors in Computing Systems*. 1–13.
- [102] Daniel Spikol, Emanuele Ruffaldi, Lorenzo Landolfi, and Mutlu Cukurova. 2017. Estimation of success in collaborative learning based on multimodal learning analytics features. In *Proceedings of the IEEE 17th International Conference on Advanced Learning Technologies*. IEEE, 269–273. DOI:<https://doi.org/10.1109/ICALT.2017.122>
- [103] Chairs Constantine Stephanidis, Gavriel Salvendy, Members of the Group Margherita Antona, Jessie Y. C. Chen, Jianming Dong, Vincent G. Duffy, Xiaowen Fang, Cali Fidopiastis, Gino Fragomeni, Limin Paul Fu, Yinni Guo, Don Harris, Andri Ioannou, Kyeong-ah (Kate) Jeong, Shin'ichi Konomi, Heidi Krömker, Masaaki Kurosu, James R. Lewis, Aaron Marcus, Gabriele Meiselwitz, Abbas Moallem, Hirohiko Mori, Fiona Fui-Hoon Nah, Stavroula Ntoa, Pei-Luen Patrick Rau, Dylan Schmorrow, Keng Siau, Norbert Streitz, Wentao Wang, Sakae Yamamoto, Panayiotis Zaphiris, and Jia Zhou. 2019. Seven HCI grand challenges. *International Journal of Human–Computer Interaction* 35, 14 (2019), 1229–1269. DOI:<https://doi.org/10.1080/10447318.2019.1619259>
- [104] Ömer Sümer, Patricia Goldberg, Sidney D'Mello, Peter Gerjets, Ulrich Trautwein, and Enkelejda Kasneci. 2023. Multimodal engagement analysis from facial videos in the classroom. *IEEE Transactions on Affective Computing* 14, 2 (2023), 1012–1027. <https://doi.org/10.1109/TAFFC.2021.3127692>
- [105] Nick Taylor, Keith Cheverst, Peter Wright, and Patrick Olivier. 2013. Leaving the wild: Lessons from community technology handovers. In *Proceedings of the SIGCHI Conference on Human Factors in Computing Systems*. 1549–1558.
- [106] James Thomas and Angela Harden. 2008. Methods for the thematic synthesis of qualitative research in systematic reviews. *BMC Medical Research Methodology* 8, 1 (2008), 1–10. DOI:<https://doi.org/10.1186/1471-2288-8-45>
- [107] Olga Viberg, Ioana Jivet, and Maren Scheffel. 2023. *Designing Culturally Aware Learning Analytics: A Value Sensitive Perspective*. Springer International Publishing, Cham, 177–192. DOI:[https://doi.org/10.1007/978-3-031-27646-0\\_10](https://doi.org/10.1007/978-3-031-27646-0_10)
- [108] Clara Vite, Anca-Simona Horvath, Gina Neff, and Naja L. Holten Møller. 2021. Bringing human-centredness to technologies for buildings: An agenda for linking new types of data to the challenge of sustainability. In *Proceedings of the 14th Biannual Conference of the Italian SIGCHI Chapter*. 1–8.
- [109] Hana Vrzakova, Mary Jean Amon, Angela Stewart, Nicholas D. Duran, and Sidney K. D'Mello. 2020. Focused or stuck together: Multimodal patterns reveal triads' performance in collaborative problem solving. In *Proceedings of the 10th International Learning Analytics and Knowledge Conference*. 295–304. DOI:<https://doi.org/10.1145/3375462.3375467>
- [110] Mark Weiser. 1991. The computer for the 21 st century. *Scientific American* 265, 3 (1991), 94–105.
- [111] Alyssa F. Wise, Simon Knight, and Xavier Ochoa. 2021. What makes learning analytics research matter. *Journal of Learning Analytics* 8, 3 (2021), 1–9. DOI:<https://doi.org/10.18608/jla.2021.7647>
- [112] Marcelo Worsley and Paulo Blikstein. 2015. Leveraging multimodal learning analytics to differentiate student learning strategies. In *Proceedings of the 5th International Conference on Learning Analytics and Knowledge*. 360–367.
- [113] Marcelo Worsley, Roberto Martinez-Maldonado, and Cynthia D'Angelo. 2021. A new era in multimodal learning analytics: Twelve core commitments to ground and grow MMLA. *Journal of Learning Analytics* 8, 3 (2021), 10–27. DOI:<https://doi.org/10.18608/jla.2021.7361>
- [114] Lixiang Yan, Roberto Martinez-Maldonado, Beatriz Gallo Cordoba, Joanne Deppeler, Deborah Corrigan, Gloria Fernandez Nieto, and Dragan Gašević. 2021. Footprints at school: Modelling in-class social dynamics from students' physical positioning traces. In *Proceedings of the 11th International Learning Analytics and Knowledge Conference*. ACM, 43–54. DOI:<https://doi.org/10.1145/3448139.3448144>
- [115] Lixiang Yan, Roberto Martinez-Maldonado, Linxuan Zhao, Samantha Dix, Hollie Jaggard, Rosie Wotherspoon, Xinyu Li, and Dragan Gašević. 2023. The role of indoor positioning analytics in assessment of simulation-based learning. *British Journal of Educational Technology* 54, 1 (2023), 267–292.
- [116] Lixiang Yan, Roberto Martinez-Maldonado, Linxuan Zhao, Xinyu Li, and Dragan Gašević. 2023. Physiological synchrony and arousal as indicators of stress and learning performance in embodied collaborative learning. In *Proceedings of the 24th Conference on Artificial Intelligence in Education*. in press.

- <span id="page-40-0"></span>[117] Lixiang Yan, Linxuan Zhao, Dragan Gašević, and Roberto Martinez-Maldonado. 2022. Scalability, sustainability, and ethicality of multimodal learning analytics. In *Proceedings of the 12th International Learning Analytics and Knowledge Conference*. ACM, New York, NY, 13–23. DOI:<https://doi.org/10.1145/3506860.3506862>
- [118] Zhan Zhang and Aleksandra Sarcevic. 2015. Constructing awareness through speech, gesture, gaze and movement during a time-critical medical task. In *Proceedings of the 14th European Conference on Computer Supported Cooperative Work, 19–23 September 2015, Oslo, Norway*. Springer, 163–182.
- [119] Linxuan Zhao, Zachari Swiecki, Dragan Gasevic, Lixiang Yan, Samantha Dix, Hollie Jaggard, Rosie Wotherspoon, Abra Osborne, Xinyu Li, Riordan Alfredo, and Roberto Martinez-Maldonado. 2023. METS: Multimodal learning analytics of embodied teamwork learning. In *Proceedings of the 13th International Learning Analytics and Knowledge Conference*. in press. DOI:<https://doi.org/10.1145/3576050.3576076>

Received 18 December 2022; revised 24 August 2023; accepted 24 August 2023