---
cite_key: words_2021
title: Morescient GAI for Software Engineering (Extended Version)
authors: Additional Key Words, Marcus Kessel
year: 2021
doi: 10.1145/3359591.3359735
date_processed: '2025-07-02'
phase2_processed: true
original_folder: arxiv_2406.04710_Morescient_GAI_for_Software_Engineering
images_total: 3
images_kept: 3
images_removed: 0
tags:
- Machine Learning
- Natural Language Processing
- Personal Health
- Semantic Web
---

# Morescient GAI for Software Engineering (Extended Version)

MARCUS KESSEL and COLIN ATKINSON, University of Mannheim, Germany

The ability of Generative AI (GAI) technology to automatically check, synthesize and modify software engineering artifacts promises to revolutionize all aspects of software engineering. Using GAI for software engineering tasks is consequently one of the most rapidly expanding fields of software engineering research, with over a hundred LLM-based code models having been published since 2021. However, the overwhelming majority of existing code models share a major weakness – they are exclusively trained on the syntactic facet of software, significantly lowering their trustworthiness in tasks dependent on software semantics. To address this problem, a new class of "Morescient" GAI is needed that is "aware" of (i.e., trained on) both the semantic and static facets of software. This, in turn, will require a new generation of software observation platforms capable of generating large quantities of execution observations in a structured and readily analyzable way. In this paper, we present a vision and roadmap for how such "Morescient" GAI models can be engineered, evolved and disseminated according to the principles of open science.

# CCS Concepts: • Software and its engineering → Software notations and tools; • Computing methodologies → Artificial intelligence; Natural language processing.

Additional Key Words and Phrases: generative AI, morescience, semantics, dynamic, analysis, behavior-aware, observation, dataset, vision, roadmap

## ACM Reference Format:

Marcus Kessel and Colin Atkinson. 2024. Morescient GAI for Software Engineering (Extended Version). ACM Trans. Softw. Eng. Methodol. 1, 1, Article 1 (November 2024), [17](#page-16-0) pages. <https://doi.org/XXXXXXX.XXXXXXX>

## 1 INTRODUCTION

Generative-AI-based tools, and particularly Large Language Models (LLMs) of the kind made famous by OpenAI's ChatGPT, are set to become one of the most disruptive technologies in software engineering for decades. They are not only able to synthesize software artifacts (including source code) from simple, natural language prompts [\[22\]](#page-14-0), they can automatically perform many other software engineering tasks such as repairing, refactoring and explaining code, generating test cases and comprehending programs. Training and using LLMs for software engineering tasks, which Hou et al. [\[27\]](#page-14-1) call LLM4SE and Lo calls AI4SE [\[43\]](#page-15-0), has therefore rapidly become one of the largest research fields in software engineering with well over 100 different models for code synthesis having already been published and/or described in the literature [\[39,](#page-15-1) [42\]](#page-15-2).

Despite their huge potential, however, most LLM4SE applications share one major Achilles' heel that significantly reduces their potential to improve productivity in mainstream software engineering projects – they are exclusively trained on the syntactic facet of software. Static properties of code are not only the most tangible manifestation of software, they are the most amenable to the traditional, static analysis approaches of computer science and the statistical NLP techniques of data science (cf. the naturalness of code principle [\[2\]](#page-13-0)). However, code has another equally important

Authors' address: Marcus Kessel, marcus.kessel@uni-mannheim.de; Colin Atkinson, colin.atkinson@uni-mannheim.de, University of Mannheim, Mannheim, Germany.

© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.

<https://doi.org/XXXXXXX.XXXXXXX>

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

ACM 1049-331X/2024/11-ART1

facet, its run-time behavior (a.k.a. as its dynamic semantics), which is much less tangible and cannot, in general, be deduced analytically from the syntactic facet (cf. Rice's theorem [\[53\]](#page-16-1)). As a result, the "true" or "de facto" dynamic semantics of a non-trivial piece of code can only be determined by observing its behavior in response to known stimuli under known conditions (i.e., through software testing) [\[3,](#page-13-1) [5\]](#page-13-2).

Since the vast majority of code LLMs created to date are exclusively trained on the syntactic facet of software, they have no direct knowledge of the semantic facet, and are thus "unaware" of the actual behavior of software. Thus, when an LLM exclusively trained on syntactic data is asked to create a procedure to "sort" an array of elements, for example, it can only use the linguistic (i.e., identifier-based) information it has collected syntactically about procedures related to "sort" rather than procedures that demonstrably deliver "sort" behavior at run-time, because it has never been exposed to data that captures the "de facto" behavior of such procedures. In addition to their ignorance of true functional properties, LLMs are also unaware of another critical aspect of software: non-functional properties such as performance (e.g., speed and resource utilization) which can only be obtained at run-time in a target execution environment. Functional and non-functional properties are crucial aspects of software development that play a significant role in ensuring the quality, reliability, and overall success of software systems [\[6\]](#page-13-3). Neglecting any of them can have significant consequences such as higher risks and operational costs (e.g., blindly choosing a generated code artifact that turns out to have poor resource usage behavior in a cloud-based environment).

This ignorance of de facto behavior directly contributes to the low trustworthiness of the current generation of code LLMs. For example, in a recent experiment (a replication) [\[10\]](#page-13-4) we showed that code synthesis solutions for Java coding problems returned by OpenAI's Davinci model are, on average, correct only 42% of the time [\[32\]](#page-14-2), and the solutions returned by two other popular models, Codegen and InCoder, are correct only 22% and 9% of the time, respectively. On the other hand, although more recent studies have shown substantial gains on widely used, Python-based benchmarks like HumanEval and MBPP and their improved derivatives[1](#page-1-0) like EvalPlus [\[39\]](#page-15-1), these advances come with a major caveat: the benchmarks are becoming increasingly saturated. In fact, some models are already converging to optimal precision, reducing their utility for evaluating model performance for practitioners.

These problems have contributed to growing skepticism about the representativeness of popular benchmarks, and the potential for data leakage [\[45\]](#page-15-3) (i.e., models being trained on solutions to the code challenges used to evaluate them). As a result, there is an increasing concern that current benchmarks may not accurately reflect likely performance in real-world coding tasks, but merely indicate their performance on artificially-simple coding problems. Researchers and practitioners alike are therefore increasingly skeptical about the reliability of AI-generated code, with major tech companies like Microsoft, Google and Meta reporting that their developers typically reject around two-thirds of AI-powered code recommendations (e.g., GitHub Copilot [\[23\]](#page-14-3)). This widespread untrustworthiness of GAI raises concerns that potential productivity gains from automated code synthesis may be offset by the need for additional verification and validation effort to ensure the quality of the resulting software system [\[3\]](#page-13-1).

To address the lack of awareness of de facto run-time behavior, we believe a completely new class of code LLMs is required that "knows about" the true dynamic semantics of code, alongside the usual syntactic properties, because they have been trained on both facets of software rather just the latter. We characterize such LLMs as "morescient" – a neologism derived from the Latin words "mores" (behavior) and "scire" (to know). Our underlying hypothesis is that morescient LLMs will

<span id="page-1-0"></span><sup>1</sup>Even the benchmarks were criticized for missing true behavior awareness, since they contained weak tests

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: November 2024.

deliver much more trustworthy results [\[34,](#page-14-4) [43\]](#page-15-0) than traditional trained LLMs, and therefore will be much more useful in practical software engineering projects[\[21\]](#page-14-5).

The outline of the paper is as follows. First, in Section [2,](#page-2-0) we discuss the current state of the art in morescient GAI and the main obstacles to achieving it. Next, in Section [3,](#page-6-0) we present the types of data structures and platforms necessary for realizing morescient GAI. This is followed by Section [4,](#page-8-0) where we outline our vision for an open, continual approach to maintaining an evolving dataset of observations of the same order of magnitude as current syntactic code datasets. Finally, in Section [5,](#page-10-0) we provide a roadmap for integrating true-behavior awareness into generative AI for software engineering. We conclude this work with some closing remarks in Section [6.](#page-12-0)

## <span id="page-2-0"></span>2 STATE OF THE ART

At the time of writing, the overwhelming majority of GAI code models have been trained on statically derived data and raw code. There has been a small number of attempts to include de facto (i.e., observed) run-time behavior in the training or use of code LLMs, but since they are highly limited in their scope and scale, we refer to them as semi-morescient. In the following subsection we describe these preliminary attempts at morescience, before summarizing the main obstacles that stand in the way of achieving this in a comprehensive and systematic way in the subsequent subsection.

## 1 Semi-Morescient Approaches

To date, researchers have explored three main lines of research to improve the morescience of GAIderived code. The first line, which we refer to as "morescient training", aims to include observations of code modules' run-time behavior in the collection of data used to train and fine-tune code LLMs. The second line, which we refer to as "morescient usage", aims to improve the results gained from code LLM queries by selecting and refining results using feedback from the compilation and/or execution of returned candidates. The third line of research, which we refer to as "morescient benchmarking", uses execution-based feedback to improve the benchmarking of code LLMs. Each of these is elaborated further below.

Morescient Training. This line of research leverages execution information in the initial training and fine-tuning of execution-aware code models. Notable examples include CodeExecutor [\[38\]](#page-15-4) and TRACED [\[17\]](#page-14-6). The latter, in particular, incorporates run-time trace information into its training data to enhance understanding of program behavior across multiple downstream tasks: semantic clone retrieval, vulnerability detection, and static execution estimation (e.g., predicting which paths or code elements will be covered).

More recent research has also attempted to train models to comprehend and reason about operational semantics in a step-by-step execution style, including scratchpad [\[51\]](#page-15-5), NeXt [\[49\]](#page-15-6), and SemCoder [\[16\]](#page-14-7). While scratchpad and NeXt employ trace reasoning formats of different kinds, SemCoder utilizes a monologue reasoning technique, where each execution step is verbalized to provide a human-readable representation of the code's execution. At the model construction stage, learning-based techniques can be employed to improve the quality of the code LLMs generate for certain software engineering tasks. By incorporating behavioral feedback obtained from compilers and/or tests [\[40,](#page-15-7) [55\]](#page-16-2) within reinforcement learning frameworks, it is possible to identify functionally incorrect generated code and take corrective action.

GAI-based oracle recommendation approaches like TOGA [\[15\]](#page-13-5) or program repair approaches [\[61\]](#page-16-3) are specifically trained using test code (e.g., harvested or generated unit test methods). TOGA, for instance, predicts oracle values by generating assertion statements for a provided test. Since test code embodies abstract semantics (i.e., high-level understanding of code module properties), such

as inputs and outputs, they have elements of morescience. However, they can only be classified as semi-morescient because their ability to generate oracle values is critically dependent on accurate predictions of the code that is supposed to deliver the described behavior, including the prediction of the actual "focal method" – that is, the portion of the code that is being tested by a test.

Morescient Usage. This line of work uses execution information gathered through testing to select the best solution from multiple code generations. This can be accomplished by sampling from a set of potential solutions, such as those provided by CodeT [\[11\]](#page-13-6) or LEVER [\[50\]](#page-15-8) and selecting the one that reveals interesting discrepancies [\[31\]](#page-14-8). Some authors (e.g., [\[12\]](#page-13-7)) apply this idea using the run-time feedback to iteratively improve the generated solution through so called "self-debugging".

The authors of [\[35\]](#page-14-9) further showcase the effectiveness of execution-based reasoning as a prompting technique by introducing the Chain of Code (CoC) approach, a novel prompting technique that generates code to reason through problems. By executing this generated code using either a code interpreter or a language model simulating the execution, CoC can tackle intricate coding problems.

Morescient Benchmarking. Existing benchmarks that reason about the properties of LLM generated code have recently also become more aware of code behavior. While existing popular benchmarks like HumanEval and MBPP [\[39\]](#page-15-1), or CodeContests [\[37\]](#page-14-10), include tests that describe the desired behavior in terms of abstract semantics (i.e., test inputs and expected outputs), they are only used to evaluate the performance of models and their code generation capabilities with respect to the property of functional correctness. However, they do not assess the other fundamental dimension of code models – their ability to reason about the de facto behavior of code. CRUXEval [\[24\]](#page-14-11) is an "execution benchmark" for code reasoning, understanding, and execution evaluation that attempts to address this problem by using 800 Python functions with corresponding input-output pairs to assess their input value prediction and output value prediction capabilities.

## 2 Obstacles to Morescient GAI

Although the aforementioned approaches have delivered useful insights how GAI technology can be made more aware of run-time behavior, they are all limited by a number of fundamental obstacles to the systematic, comprehensive and open development of morescient GAI, as summarized in the following sections.

Lack of Suitable Data Structures. Current approaches are typically ad hoc solutions employing improvised data structures (and representations) as well as manually curated data sets, or data sets synthesized with the assistance of LLMs (e.g., [\[16,](#page-14-7) [24\]](#page-14-11)). Although things have improved slightly, the availability of dynamic (observational) data remains significantly limited compared to syntactic data, hindering the development of fully morescient models. This is because real-world, high-quality corpora of run-time behavior (i.e., morescient data sets) are challenging to obtain and curate at a large scale [\[9,](#page-13-8) [52\]](#page-15-9). A prerequisite is parsable and executable code. However, even if code meets these criteria, observing its executable behavior poses a significant challenge due to the lack of documentation needed for testing (i.e., lack of tests).

To achieve the desired scaling effects of LLMs for run-time behavior [\[29\]](#page-14-12), it is essential to develop more advanced LLM models supported by efficient data structures. These structures must be capable of storing or linking vast quantities of observation data – both functional and non-functional – and associating it with existing syntactic code models. Additionally, they must be designed with interoperability in mind, enabling seamless integration for the training and continuous improvement of LLMs.

Morescient GAI for Software Engineering (Extended Version) 1:5

The diversity of code semantics – including behavioral and static – can be captured by relationships over distinct dimensions. To effectively manage these relationships, they must be cleanly and explicitly stored. Consequently, the required data structures should support a unified framework that enables navigation over, and tracking of, these relationships (e.g., linking code modules to tests that executed them and to the observed behavior).

Fragmented and Poor Quality of Training Data. The performance of LLMs is only as good as the quality of its training data, which depends on several factors, particularly when it comes to natural language text and code. It has been shown that LLMs replicate vulnerabilities, bugs, and poor code quality present in their training data [\[4,](#page-13-9) [22,](#page-14-0) [41\]](#page-15-10). For example, while the Codex LLM powering GitHub Copilot can help prevent some types of "simple, stupid" bugs, it also generates code containing these bugs at twice the rate of correct code [\[28\]](#page-14-13).

One essential factor affecting quality is deduplication of data and code, which is considered critical for the quality of training data sets [\[1,](#page-13-10) [33\]](#page-14-14). Unfortunately, since there is no commonly accepted data structure to represent run-time behavior and its various representations in terms of dynamic semantics, there is a lack of understanding about what constitutes high-quality dynamic training data, beyond general guidelines learned from general-purpose, syntactic training data.

The current generation of code models suffers from poor data quality, primarily due to a critical oversight: unverified training code. Many data sets lack executable code, as checks for compilability (i.e., syntax and static semantics) and executability are often skipped because of the high costs and complexity involved (e.g., missing build information). Desirable properties like self-contained code modules (i.e., standalone modules that can run without external dependencies) are also difficult to identify automatically. As a result, there have been ad hoc attempts to manually curate or synthesize training data to bypass the challenges of collecting dynamic data, which has led to a fragmented data landscape. For example, Ding et al. found that roughly a quarter of the Python solutions in OSS-Instruct [\[60\]](#page-16-4), which were synthetically created from code LLMs, were not executable [\[16\]](#page-14-7).

If this fundamental issue could be addressed, generated code would likely exhibit similar characteristics (i.e., executability), making large-scale data structures and platforms more feasible.

Lack of Structured Representations of Semantics. The availability of specialized data structures for storing and linking semantic information represents just one aspect of the broader challenge of reasoning about functional and non-functional properties of software. Equally crucial is the consideration of how observed semantics are actually represented within these structures, effectively capturing their nuances and complexities. The development of fully morescient GAI approaches is hindered by the lack of formal representations of run-time behavior, especially in how the true behavior of software is linked to the syntax in the data used to train LLMs. For operational semantics that describe actions in terms of step-by-step computations (e.g., debugging), effectively representing execution traces has been a persistent challenge in dynamic analysis. In the case of LLMs, even representing abstract semantics presents difficulties, such as selecting an appropriate encoding scheme to serialize input and output objects for abstract semantics (e.g., a suitable string representation for Java objects). Ideally, these semantic representations should be language-agnostic, enabling more efficient knowledge transfer across multiple programming languages. However, current approaches (e.g., [\[16,](#page-14-7) [17\]](#page-14-6)) are limited, as they are typically designed for specific programming languages, restricting their applicability.

The choice of behavior representation has a substantial impact on how the model processes and generates results, including the textual output. Understanding how run-time behavior is represented in LLMs is essential for designing effective architectures, accurately evaluating performance metrics, and creating applications tailored to specific user needs based on perspectives on run-time behavior.

Developing a clear definition of software semantics is a challenging task on its own. However, the complexity increases significantly when considering the wide range of software engineering tasks that must be addressed. To make progress in developing fully morescient GAI approaches, what is needed is a clear representation of the various dynamic semantics, as well as an understanding of how they are represented across training data sets used to train morescient code models and how they are aligned with the syntactic code model.

Lack of Scalable Testing Approaches. In the preceding subsection on data quality, we touched upon general issues related to ensuring compilability and executability of code. Since true behavior of code modules can only be observed through run-time observations (i.e., software testing), suitable sets of tests are required that exercise the behavior of code in a way that accurately captures its behavior.

Current benchmarking approaches rely on a combination of traditional test generation techniques and LLM-based methods, such as synthesis of test inputs or sequences, exemplified by EvalPlus [\[39\]](#page-15-1). These methods focus primarily on improving coverage metrics (e.g., mutation score) through defect detection. However, they have two significant limitations: (1) they lack a systematic and structured approach to test case design, instead relying on the assumption that defect testing can effectively capture run-time behavior; and (2) their scalability is compromised by the use of classic test representation techniques, such as unit test representations and drivers, which are designed for single-code-module testing and fail to scale to ultra-large execution scenarios. Moreover, these test drivers usually do not have the capability to capture the other interesting run-time behavior – that is the measurement of non-functional properties including performance and execution time.

To address these limitations, there is a need for novel test case representation languages that can effectively discern subtle nuances in run-time behavior. Additionally, scalable test drivers are required to efficiently execute large corpora of executable code through mass-execution, allowing for the collection of comprehensive run-time data.

Fragmentation and Superfluity of Benchmarks. The community has acknowledged the limitations of traditional NLP-based benchmarks, such as the BLEU score [\[19\]](#page-14-15), which evaluates generated code by comparing it to canonical ground truth solutions based on textual similarity. Popular benchmarks like HumanEval and MBPP, therefore, use unit-test-driven evaluation with the pass@k metric [\[22\]](#page-14-0) (i.e., the probability that at least one of the top k generated code samples for a given problem passes all the unit tests).

Benchmark approaches are often applied manually (i.e., through the manual curation of coding problems in terms of desired behavior and tests) and thus are time-consuming and error-prone [\[39\]](#page-15-1), and/or are used together with arguably "risky" LLM-based code synthesis technology (i.e., because models trained on synthetic data generated by GAI may eventually collapse [\[56\]](#page-16-5)). The proliferation of these factors has resulted in the fragmentation, superfluity and excessive diversity of benchmarks. Some authors have consequently been driven to propose novel, typically small-scale benchmarks that cater to idiosyncratic preferences, often as a means to sidestep the inherent challenges in evaluating their own approaches. One of the biggest challenges is curating benchmarks at large scale that contain coding problems and descriptions of desired behavior at appropriate abstraction levels, including black-box (e.g., tests) and white-box behavior (e.g., traces). To overcome these challenges, there is a need for large-scale benchmarking approaches that can automatically create new coding problems (i.e., functionality) and corresponding descriptions of run-time behavior.

Lack of Suitable Platforms. A crucial requirement for developing fully morescient LLMs is the availability of robust platforms that can collect, store, and manage vast quantities of high-quality observation data, encompassing both functional and non-functional aspects. These platforms must be specifically designed to handle ultra-large data sets and provide suitable data structures for training and improving LLMs. To achieve parity between behavior data sets and code data sets, these platforms must effectively address the limitations and challenges outlined earlier, ensuring the resulting data sets are comprehensive, accurate, and large enough for reliable model development.

Challenges in Obtaining Observation Data at Large Scale. Achieving scalable testing poses two key challenges. First, as mentioned above, there is a need for optimized test drivers that can execute potentially large numbers of tests on large numbers of code modules in parallel, without compromising performance. Second, obtaining precise observation data at scale within a given time budget is essential, but comes with its own set of complexities. A significant obstacle to efficient testing is the need for advanced analysis and tracing techniques. Extensive tracing through analysis tools and measurement calipers can provide valuable insights into code behavior, but introduces a substantial overhead in execution time. This can lead to significantly slower code execution due to the added complexity of instrumentation or introspection techniques. To overcome these challenges, improved techniques that balance scalability with precision are needed.

In general, there is a trade-off between the amount of data traced (i.e., the level of precision of the data), and the overhead caused at execution [\[13\]](#page-13-11). The lower the precision of the traced data, the lower the overhead. To mitigate this overhead, two key developments are essential: (a) the continued advancement of dynamic analysis techniques to improve their efficiency, such as through hybrid approaches combining static and dynamic analysis [\[20\]](#page-14-16), and (b) a deeper understanding of the aforementioned trade-offs to support more informed design decisions when capturing morescient data at appropriate precision levels for the software engineering tasks at hand.

## <span id="page-6-0"></span>3 CAPTURING BEHAVIORAL SEMANTICS

Computer scientists have developed a range of different techniques for describing the semantics of software (i.e., programming languages) at different levels of abstraction, including denotational techniques, where run-time meaning is defined by mappings to well-known mathematical domains, axiomatic or algebraic techniques, where run-time meaning is defined by the cumulative behavior of component actions, and natural language (sometimes called approximate) techniques, where run-time meaning is defined in textual prose (e.g., code documentation) [\[25,](#page-14-17) [26\]](#page-14-18). In practice, the three most important levels of abstraction are —

- abstract semantics: which describe the black-box behavior of functional abstractions (e.g., classes or methods) in terms of their externally visibly behavior (i.e., properties),
- operational semantics: which describe the white-box behavior of the implementation of a functional abstraction in terms of the individual, step-wise operations that realize its overall behavior (e.g., the method or class body),
- concrete semantics: which describe the detailed execution semantics of a compiled method or class body at the level of the machine code or intermediate code of the execution platform.

To achieve full morescience, GAI code models will ultimately need to be trained on descriptions at all levels and in all the different forms in which they are available. Languages used to capture observation-based descriptions of abstract semantics are currently the most inadequate, as they tend to focus on unit testing rather than data analysis or GAI training. The primary obstacle in addressing these challenges is the lack of suitable data structures that can systematically and scalably capture the abstract semantics of functional abstractions in terms of their actual responses to valid stimuli.

<span id="page-7-0"></span>![](_page_7_Figure_1.jpeg)
<!-- Image Description: The image depicts a software reliability model (SRM) represented as a matrix. Rows represent tests (Tᵢ), columns represent implementations (Sᵢ). A shaded cell indicates a specific test-implementation execution. Dashed lines connect this cell to resulting functional (output values, function calls) and non-functional (time, memory, branch coverage) observation tables. The diagram illustrates how an SRM tracks and records the results of executing various tests on different software versions. -->

|   | Output | Operation | Inputs |   |
|---|--------|-----------|--------|---|
|   |        |           |        |   |
|   | A      | B         | C      | D |
| 1 |        | create    | 𝑄𝑖     |   |
| 2 | TRUE   | enqueue   | A1     | 1 |
| 3 | TRUE   | enqueue   | A1     | 2 |
| 4 | 1      | peek      | A1     |   |
| 5 | 2      | size      | A1     |   |
| 6 | 1      | dequeue   | A1     |   |
| 7 | 1      | size      | A1     |   |

(a) Sequence Sheets and Stimulus-Response Matrix (SRM)

(b) Sequence Sheet for a (stateful) Queue Abstraction

![](_page_7_Figure_5.jpeg)
<!-- Image Description: The image is a diagram illustrating a multi-dimensional test matrix. A 3D coordinate system represents "Tests," "Repetitions," and "Functional Abstractions," with a dashed line connecting the system to a series of stacked matrices ($S_1, S_{...}, S_n$). Each matrix represents an "Implementation," showing results across the three dimensions. The diagram visualizes the organization and structure of test data across various implementations, abstractions, and repetitions within the paper's experimental methodology. -->

(b) Stimulus-Response Hypercube (SRH)

Fig. 1. Proposed Data Structures for Behavior Representation

## 1 Data Structures

To address this obstacle we propose three new data structures – sequence sheets, which capture sequences of stimulus-response interactions (cf. tests in terms of triples of inputs, operation invocation and corresponding outputs [\[5\]](#page-13-2)) in a tabular form, stimulus-response matrices (SRMs), which capture collections of sequence sheets representing multiple tests of multiple implementations of a given functional abstraction (i.e., functionality or coding problem), and stimulus-response hypercubes (SRHs) which capture collections of SRMs representing multiple repetitions of multiple tests of multiple functional abstractions.

Figure [1](#page-7-0) (a) provides a schematic representation of the first two of these. The bottom left-hand side shows a sequence sheet that captures the response of an implementation of a code module , 2 , to a test <sup>3</sup> , which entails one invocation of the functionality provided by (a single "sum" operation for adding two integers). This corresponds to the single (grey) cell of the SRM, which stores multiple tests of multiple implementations of . As well as the functional information stored in a sequence sheet, each cell also provides access to relevant non-functional information about the test, such as execution time, memory usage and trace information (here branches covered).

Figure (b) presents a more complex sequence sheet for a (stateful) queue abstraction. Unlike the "sum" abstraction, which comprises a single statement, the queue test sequence showcases a series of statements to test core queue behavior – specifically, the first-in-first-out (FIFO) behavior. Notably, all intermediate observational states are captured (i.e., execution flow), providing a comprehensive

view of an implementation's execution over time. SRMs can universally store or link observational records of arbitrary semantic types, encompassing both abstract and operational semantics.

Complex inputs and outputs, such as stateful objects, can be stored in sequence sheets using multiple approaches. The choice depends on the structure of the object. For objects where properties (i.e., class attributes) can be represented by a simple key-value hierarchy, their entire state can be serialized into a suitable representation such as a JSON document, where all properties are accessible by key. Alternatively, when only specific data is needed, inspector methods can be identified that access relevant properties (i.e., state data) of interest. For binary data inputs such as input streams, several strategies exist: either the storage location of the physical copy can be directly linked into the sequence sheet, or suitable metadata like hash signatures can be employed to identify and reference equivalent or similar data. This latter approach reduces storage redundancy while enabling efficient comparison and retrieval of identical or near-identical data.

Finally, Figure (c) gives a schematic representation of an SRH, the third data structure, which collects multiple SRMs applied to multiple code module implementations multiple times in potentially different conditions (e.g., execution environments). SRHs, therefore, provide a multidimensional way of navigating over and analyzing observations from many executions of many implementations under many controlled conditions, including different target execution environments in which observations are obtainable. Since they are representable in the ubiquitous data frame structures supported by popular data processing languages like Python and R, they also lend themselves to analysis by mainstream data analytics tools (i.e., are highly accessible and offer interoperability).

## 2 Platform and Test Driver

The key to obtaining more sophisticated data sets containing (1) executable code and (2) runtime data, at scale, lies in automation. The roadmap envisions a platform that not only realizes the data structures outlined above, but also offers flexible, domain-specific languages to design automatic, data-driven workflows on morescient data sets (e.g., SRHs) based on individual selection criteria tailored to the software engineering task at hand. Our prototype implementation of such a platform, the Large-Scale Software Observatorium (LASSO) [2](#page-8-1) [\[32\]](#page-14-2), features a domain-specific scripting language to design analysis pipelines for generating high-quality data sets based on individual preferences and functional/non-functional properties of interest [\[30\]](#page-14-19). It achieves this by integrating a test driver and additional measurement tools to facilitate the mass-execution of code modules and record their run-time functional and non-functional behavior in SRMs. LASSO employs a dedicated "arena" test driver that efficiently executes large sets of tests and code module implementations (i.e., specified in stimulus matrices) harvested from repositories or synthesized from LLMs, in a distributed manner (using vertical and horizontal parallelization).

## <span id="page-8-0"></span>4 CONTINUALLY EVOLVING, OPEN STIMULUS-RESPONSE HYPERCUBES

To achieve data quantities for morescient run-time data comparable to those of syntactic code datasets and realize the desired scaling effects of LLMs, we propose developing an ever-growing morescient dataset grounded in the SRH concept introduced earlier. We refer to this concept as an open, continual SRH, which we will describe in the remainder of this section.

## 1 Dimensional Extensibility

There are many potential approaches to developing a continually evolving SRH. However, to ensure its trustworthiness from the outset, we believe it must be designed to be open and accessible. This aligns with the principles of open science and aims to "democratize" access to the data, similar

<span id="page-8-1"></span><sup>2</sup>project is freely available on GitHub: <https://softwareobservatorium.github.io/>

<span id="page-9-0"></span>![](_page_9_Figure_1.jpeg)
<!-- Image Description: The image diagrams a "Continually Evolving SRH" system using a series of tables representing models (Morescient LLMs). Each table shows multiple implementations ($S_1$ to $S_n$) evolving across three axes: repetitions, functional abstractions, and tests. The tables are linked to executable code, illustrating a model's iterative development and refinement through repeated testing and implementation changes. The curved lines represent the continuous evolution of the system over time. -->

Fig. 2. An Open, Continually Evolving SRH of de facto Behavior Data

to current open-scientific collaborations such as the Big Project and its open code dataset, The Stack [\[44\]](#page-15-11). Such an open, community-driven SRH of the kind visualized in Figure [2](#page-9-0) would be continuously expanded and enriched by many contributors over time, including researchers and industry practitioners alike.

At the beginning, such an SRH would be a sparse data structure. The subsequent expansion [\[32\]](#page-14-2) would not only continually refine the core dimensions of the SRH (cf. Section [3\)](#page-6-0), incorporating additional harvested/synthesized tests and implementations for each functional abstraction over time, but would also grow to accommodate new dimensions. New data sources (i.e., code repositories), tools and GAI models could be used to harvest or synthesize these. Such a growing SRH would also facilitate the future integration of diverse, behavior data sets by community members using their own morescient GAI approaches in the models dimension. Over time, we anticipate that the community will converge on a common, effective representation of run-time behavior for morescient GAI, agreeing upon them across various levels of abstraction (abstract and operational semantics). This shared understanding will serve as a foundation for further advancements around the open, evolving SRH.

Similar to today's state-of-the-art code LLMs, an evolving SRH must support polyglot programming. This capability will allow the community to leverage a wealth of knowledge from diverse codebases written in various relevant programming languages. To accelerate the scaling effects of morescient LLMs, the community must focus on developing common and effective representations of run-time behavior. To maintain a clear separation of concerns and ensure interoperability, we assume that run-time data and syntactic data (i.e., executable code) will be kept in separate datasets. Thus, SRHs will essentially maintain a link to code module implementations. Since tests are represented as sequence sheets, they are also explicitly stored in SRHs.

With a diverse community helping to evolving SRHs, it is reasonable to assume that the overall quality of the data will improve. As more users engage with the system, they are likely to identify discrepancies or inaccuracies in the data. This collective scrutiny can lead to the detection and correction of errors, ultimately resulting in a more reliable and trustworthy morescient data set (including executable code data sets).

## 2 Unified Benchmarking

The open, evolving SRH concept described above not only provides a continuously growing dataset of training data, but also offers a vast repository of sample cases for unified benchmarking. Given that benchmarks are specific to software engineering tasks, individual benchmarks can be derived from the SRH, as long as standard practices in machine learning training like train-validation-test splits are followed to prevent data leakage.

As such an SRH continuously evolves, new revisions (versions) of the data can be created to facilitate ongoing benchmarking with fresh data. This means that not only is the SRH itself expanding and evolving, but the benchmarks used to assess emerging morescient GAI approaches are evolving as well. Consequently, the results shown by current initiatives such as leaderboards can be directly linked to the specific versions of morescient data that produced those results.

## <span id="page-10-0"></span>5 A ROADMAP FOR MORESCIENT GAI

Sanitized stimulus-response data, stored in the aforementioned data structures, can facilitate morescient GAI in several ways. Below, we outline a roadmap for the next few years, highlighting key milestones and research directions for leveraging observational data to drive progress toward fully morescient GAI approaches.

## 1 Curating Data Sets

Initially, the key enabling factor for morescient GAI approaches will be the availability of large, high-quality data sets. A foundational version of an open, evolving SRH described in the previous section will therefore be created early on. More specifically, to obtain high-quality datasets for morescient GAI, it is essential to first focus on developing high-quality, syntax-driven code models that are executable and testable. This foundation is crucial, as morescient GAI relies on these models to capture run-time observations of functional and non-functional properties in terms of the SRHs discussed before. Therefore, the quality of the syntactic code models will directly impact the accuracy of the morescient data curation process. Thus, the creation of morescient data sets will not only complement the syntactic models that currently exist, it will also further improve their quality since non-executable (non-testable) code can be identified and filtered out.

Scalable code analysis platforms [\[18\]](#page-14-20), such as software observatories (e.g., LASSO) that realize the aforementioned data structures are essential for enabling the mass-execution of code modules at ultra-large scales, thereby generating the vast quantities of run-time data needed to grow the open SRH. Furthermore, automated build systems such as continuous integration platforms can provide a supplementary source of run-time data by continuously building and running code. Despite its potential value, the role and quality of collected build data remain understudied [\[7\]](#page-13-12) and warrant further research to fully realize their exploitation. SRHs can be generated from the build and run-time data collected by these platforms, thereby enhancing the value of what would otherwise be discarded build data. By repurposing this data, SRHs offer a means to preserve the knowledge gained.

## 2 Training

Once a foundational, evolving SRH has been established, the first and most crucial step is to include it in LLMs' training data, either during the initial pre-training stage when new LLMs are created or in subsequent transfer learning steps, where existing models are further trained on SRHs to "fine-tune" their behavioral awareness. In both scenarios, the observational data encapsulated within SRHs can be made available for training in either a direct or indirect form. In direct approaches, LLMs are presented with "raw" (cleaned) SRH data containing uncensored descriptions of numerous executions of a wide array of code modules.

While this is likely to give the best results, the scale of the data sets involved presents numerous practical problems to researchers, including the need for significant processing power. In indirect approaches, the SRH data is (pre)processed in some way as preparation for training. For example, reusable vector representations (e.g., embeddings such as Word2vec [\[48\]](#page-15-12) and BERT [\[14\]](#page-13-13)), can be generated from the data in which information with "close" vector values is assumed to be more closely related than that with more distant values. Another indirect approach is to construct knowledge graphs from SRHs in order to encode relationships efficiently and to train models to predict missing links [\[54\]](#page-16-6).

State-of-the-art architectures used to train LLMs have achieved impressive performance on natural language texts, including code, leveraging their syntactic capabilities. However, incorporating observational data alongside syntactic data as training inputs may necessitate architectural modifications. The choice of representation for observational data has a significant impact on the construction of more sophisticated models that integrate both facets. Observational data can be treated either as textual content or as an additional modality, similar to image information. Given our assumption that both syntactic and observational data will co-evolve, combining models in the spirit of multi-modal architectures may yield benefits. With the availability of diverse run-time data, it may be advantageous to specify multiple representations of observational data, encompassing various levels of abstraction and semantic precision (Section [2\)](#page-2-0). Given the preliminary findings on observational data presented in Section [2,](#page-2-0) we anticipate that further research will focus on developing suitable architectures and representations for training morescient code models.

## 3 Augmented Generation

The second way is to use an evolving SRH, along with a test driver like LASSO's arena, as an external knowledge source to provide a variety of services that existing, syntactically-trained code models can call to improve their accuracy and relevance. These generation augmentation services [\[47\]](#page-15-13) can range from simple testing services, where the LLM can ask whether a generated piece of code has the expected functionality, to oracle services, where the LLM can ask for oracle values for the desired functionality (see [\[31\]](#page-14-8) for further details). The external knowledge source therefore essentially provides an external fact-checking mechanism that can identify "incorrect" code generations. Recently, researchers have been exploring fact-checking models [\[57\]](#page-16-7) that specialize in verifying the output of LLMs, helping to detect false claims and hallucinations. These models leverage factual information to verify whether an LLM's output is factually accurate. Morescient versions of these models can take advantage of knowledge sources like SRHs to validate a claim by comparing it against established facts (here observational data).

## 4 Prompting

Prompting LLMs involves designing the input prompts to elicit the most accurate, relevant and helpful responses from a model. The design of the prompts used to invoke LLM models, therefore, plays a huge role in their perceived performance [\[36,](#page-14-21) [59\]](#page-16-8). The aforementioned stimulus-response data structures in Section [3](#page-6-0) can help improve prompting in three main ways. First, the minimalistic and structured test representation approach offered by sequence sheets can reduce user errors and misunderstandings when they serve as prompt templates. Secondly, since tests are frequently included in prompts to code LLMs to improve precision, users can enrich their prompts to nonmorescient models using information from SRHs. Thirdly, the service APIs (e.g., functions) of the

platforms that support morescient GAI can themselves be included in prompts to encourage LLMs to obtain external factual information [\[47\]](#page-15-13).

## 5 Test-driven Software Experimentation

Finally, the adoption and development of any new technology, including morescient GAI, is critically dependent on experimental evidence of its strengths and weaknesses. Stimulus-response data structures coupled with a scalable test driver of the kind offered by LASSO, are a key foundation for evaluating and comparing code LLMs in a generalizable way. Only by performing large scale test-driven experiments can subtle differences in the generation of alternative code solutions or tests be detected and "n-version" comparisons of the different implementations be performed (e.g., strengthened through differential testing [\[59\]](#page-16-8), or differential GAI in general [\[31\]](#page-14-8)), for example to fine-tune LLM "hyperparameters".

Moreover, making detailed information about the continual SRH openly accessible in accordance with the principles of open science [\[46\]](#page-15-14) facilitates the development of new and improved evaluation metrics, such as pass@k [\[22\]](#page-14-0), or BLEU scores, which rely on a canonical, ground truth implementation [\[19\]](#page-14-15).

## 6 AI-driven Software and Decision-Making

Orthogonal to the milestones previously described, the integration of generative AI into software products is on the rise, blurring the lines between algorithmic and probabilistic reasoning. Components of a software system can now comprise imperative code and probabilistic models, making it increasingly difficult to distinguish between them. This shift contrasts with retrieval augmented generation, where a model is enhanced by external tools.

AI-driven software tools, such as smart IDEs (e.g., VSCode and GitHub Copilot), can leverage the standardized data structures (i.e., sequence sheets and SRHs) to achieve interoperability between LLM representations and run-time information obtained in IDEs. This will enable seamless use of algorithmic or LLM-based components for software engineering tasks and decision-making. These tools will likely employ multiple models and meta-modeling, making standard data structures and semantic representations even more essential to ensure interoperability between them.

For now, the developer remains actively engaged in the development process. Data structures, particularly sequence sheets, not only serve as a means to understand and communicate behavior but also facilitate collaboration among stakeholders – like code. However, with advancements in AI-driven software engineering, it is likely that some of the tasks will soon be fully automated. There is growing interest in LLM-based agents like SWE-agent [\[62\]](#page-16-9) and AutoDev [\[58\]](#page-16-10), which utilize a combination of models and tools to accomplish specific tasks within a given cost budget, thereby achieving complete autonomy. As LLM-based agents evolve, the integration of sophisticated data structures like SRHs out-of-the-box will become even more important, further accelerating the automation of software engineering tasks.

In an ideal future, where morescient GAI has reached its full potential, it may become possible for highly advanced morescient models to accurately predict the execution of software, making traditional verification and validation processes largely obsolete or redundant. With developers and autonomous agents increasingly reliant on data-driven decision-making, an open, evolving SRH would provide a robust foundation for informed decision-making over time (e.g., reduce debugging activities or log analysis).

## <span id="page-12-0"></span>6 CONCLUSION

The hypothesis of this paper is that the trustworthiness, and thus the utility, of GAI for software engineering tasks will be significantly boosted by creating morescient code models trained on the semantic (i.e., dynamic) as well as the syntactic (i.e., static) aspects of software. Among other things, making LLMs morescient will allow them to make much better judgments about the run-time behavior of the software they synthesize in response to prompts. However, creating, organizing and representing the vast quantities of observation data needed to realize this vision will require new kinds of data structures and software observation platforms that can populate them while respecting important data set properties, such as avoiding duplication [\[1\]](#page-13-10).

We have introduced the new data structures and the platform we envisage and have discussed the different ways in which they can help foster morescient GAI in an open, community-driven way. In a seminal "road map" for source code analysis published in 2007 [\[8\]](#page-13-14), Brinkley predicted that by 2025 software analysis tools will "appear to understand algorithms and can automatically suggest superior solutions to specific problems". By applying the ideas, technologies and roadmap presented in this paper, we believe Brinkley's vision is well on track to realization, and fully morescient GAI will soon facilitate the practical use of GAI in mainstream software projects.

## REFERENCES

- <span id="page-13-10"></span>[1] Miltiadis Allamanis. 2019. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Athens, Greece) (Onward! 2019). Association for Computing Machinery, New York, NY, USA, 143–153. <https://doi.org/10.1145/3359591.3359735>
- <span id="page-13-0"></span>[2] Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles Sutton. 2018. A Survey of Machine Learning for Big Code and Naturalness. ACM Comput. Surv. 51, 4, Article 81 (jul 2018), 37 pages. <https://doi.org/10.1145/3212695>
- <span id="page-13-1"></span>[3] Paul Ammann and Jeff Offutt. 2016. Introduction to software testing. Cambridge University Press.
- <span id="page-13-9"></span>[4] Owura Asare, Meiyappan Nagappan, and N. Asokan. 2023. Is GitHub's Copilot as bad as humans at introducing vulnerabilities in code? Empirical Software Engineering 28, 6 (Sept. 2023), 129. [https://doi.org/10.1007/s10664-023-](https://doi.org/10.1007/s10664-023-10380-1) [10380-1](https://doi.org/10.1007/s10664-023-10380-1)
- <span id="page-13-2"></span>[5] Earl T. Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo. 2015. The Oracle Problem in Software Testing: A Survey. IEEE Transactions on Software Engineering 41, 5 (2015), 507–525. [https://doi.org/10.1109/TSE.2014.](https://doi.org/10.1109/TSE.2014.2372785) [2372785](https://doi.org/10.1109/TSE.2014.2372785)
- <span id="page-13-3"></span>[6] Victor R Basili. 1994. Goal question metric paradigm. Encyclopedia of software engineering (1994), 528–532.
- <span id="page-13-12"></span>[7] Moritz Beller, Georgios Gousios, and Andy Zaidman. 2017. TravisTorrent: Synthesizing Travis CI and GitHub for Full-Stack Research on Continuous Integration. In 2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR). 447–450. <https://doi.org/10.1109/MSR.2017.24>
- <span id="page-13-14"></span>[8] David Binkley. 2007. Source Code Analysis: A Road Map. In Future of Software Engineering (FOSE '07). 104–119. <https://doi.org/10.1109/FOSE.2007.27>
- <span id="page-13-8"></span>[9] Islem Bouzenia, Bajaj Piyush Krishan, and Michael Pradel. 2024. DyPyBench: A Benchmark of Executable Python Software. Proc. ACM Softw. Eng. 1, FSE, Article 16 (jul 2024), 21 pages. <https://doi.org/10.1145/3643742>
- <span id="page-13-4"></span>[10] F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin, D. Pinckney, M. Yee, Y. Zi, C. Anderson, M. Q. Feldman, A. Guha, M. Greenberg, and A. Jangda. 2023. MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation. IEEE Transactions on Software Engineering 49, 07 (jul 2023), 3675–3691. [https://doi.org/10.1109/TSE.](https://doi.org/10.1109/TSE.2023.3267446) [2023.3267446](https://doi.org/10.1109/TSE.2023.3267446)
- <span id="page-13-6"></span>[11] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. CodeT: Code Generation with Generated Tests. arXiv[:2207.10397](https://arxiv.org/abs/2207.10397) [cs.CL] <https://arxiv.org/abs/2207.10397>
- <span id="page-13-7"></span>[12] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching Large Language Models to Self-Debug. arXiv[:2304.05128](https://arxiv.org/abs/2304.05128) [cs.CL] <https://arxiv.org/abs/2304.05128>
- <span id="page-13-11"></span>[13] Bas Cornelissen, Andy Zaidman, Arie van Deursen, Leon Moonen, and Rainer Koschke. 2009. A Systematic Survey of Program Comprehension through Dynamic Analysis. IEEE Transactions on Software Engineering 35, 5 (2009), 684–702. <https://doi.org/10.1109/TSE.2009.28>
- <span id="page-13-13"></span>[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 4171–4186. <https://doi.org/10.18653/v1/N19-1423>
- <span id="page-13-5"></span>[15] Elizabeth Dinella, Gabriel Ryan, Todd Mytkowicz, and Shuvendu K. Lahiri. 2022. TOGA: a neural method for test oracle generation. In Proceedings of the 44th International Conference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE

ACM Trans. Softw. Eng. Methodol., Vol. 1, No. 1, Article 1. Publication date: November 2024.

Morescient GAI for Software Engineering (Extended Version) 1:15

<span id="page-14-7"></span>'22). Association for Computing Machinery, New York, NY, USA, 2130–2141. <https://doi.org/10.1145/3510003.3510141> [16] Yangruibo Ding, Jinjun Peng, Marcus J. Min, Gail Kaiser, Junfeng Yang, and Baishakhi Ray. 2024. SemCoder: Training

- Code Language Models with Comprehensive Semantics. arXiv[:2406.01006](https://arxiv.org/abs/2406.01006) [cs.CL] <https://arxiv.org/abs/2406.01006> [17] Yangruibo Ding, Benjamin Steenhoek, Kexin Pei, Gail Kaiser, Wei Le, and Baishakhi Ray. 2024. TRACED: Execution-
- <span id="page-14-6"></span>aware Pre-training for Source Code. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering (Lisbon, Portugal) (ICSE '24). Association for Computing Machinery, New York, NY, USA, Article 36, 12 pages. [https:](https://doi.org/10.1145/3597503.3608140) [//doi.org/10.1145/3597503.3608140](https://doi.org/10.1145/3597503.3608140)
- <span id="page-14-20"></span>[18] Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N. Nguyen. 2013. Boa: A language and infrastructure for analyzing ultra-large-scale software repositories. In 2013 35th International Conference on Software Engineering (ICSE). 422–431. <https://doi.org/10.1109/ICSE.2013.6606588>
- <span id="page-14-15"></span>[19] Aryaz Eghbali and Michael Pradel. 2023. CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code. In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering (ASE '22). Association for Computing Machinery, New York, NY, USA, Article 28, 12 pages. <https://doi.org/10.1145/3551349.3556903>
- <span id="page-14-16"></span>[20] Michael D Ernst. 2003. Static and dynamic analysis: Synergy and duality. In WODA 2003: ICSE Workshop on Dynamic Analysis. New Mexico State University Portland, OR, 24–27.
- <span id="page-14-5"></span>[21] Lichao Sun et al. 2024. TrustLLM: Trustworthiness in Large Language Models. arXiv[:2401.05561](https://arxiv.org/abs/2401.05561) [cs.CL]
- <span id="page-14-0"></span>[22] Mark Chen et al. 2021. Evaluating Large Language Models Trained on Code. arXiv[:2107.03374](https://arxiv.org/abs/2107.03374) [cs.LG]
- <span id="page-14-3"></span>[23] GitHub Blog. 2024. Research: Quantifying GitHub Copilot's impact in the enterprise with Accenture. [https://github.blog/](https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-in-the-enterprise-with-accenture/) [news-insights/research/research-quantifying-github-copilots-impact-in-the-enterprise-with-accenture/](https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-in-the-enterprise-with-accenture/)
- <span id="page-14-11"></span>[24] Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang. 2024. CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. arXiv[:2401.03065](https://arxiv.org/abs/2401.03065) [cs.SE] [https://arxiv.org/abs/2401.](https://arxiv.org/abs/2401.03065) [03065](https://arxiv.org/abs/2401.03065)
- <span id="page-14-17"></span>[25] Carl A Gunter. 1992. Semantics of programming languages: structures and techniques. MIT press.
- <span id="page-14-18"></span>[26] Robert M. Hierons, Kirill Bogdanov, Jonathan P. Bowen, Rance Cleaveland, John Derrick, Jeremy Dick, Marian Gheorghe, Mark Harman, Kalpesh Kapoor, Paul Krause, Gerald Lüttgen, Anthony J. H. Simons, Sergiy Vilkomir, Martin R. Woodward, and Hussein Zedan. 2009. Using Formal Specifications to Support Testing. ACM Comput. Surv. 41, 2, Article 9 (Feb. 2009), 76 pages. <https://doi.org/10.1145/1459352.1459354>
- <span id="page-14-1"></span>[27] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. 2024. Large Language Models for Software Engineering: A Systematic Literature Review. arXiv[:2308.10620](https://arxiv.org/abs/2308.10620) [cs.SE]
- <span id="page-14-13"></span>[28] Kevin Jesse, Toufique Ahmed, Premkumar T. Devanbu, and Emily Morgan. 2023. Large Language Models and Simple, Stupid Bugs. In 2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR). 563–575. <https://doi.org/10.1109/MSR59073.2023.00082>
- <span id="page-14-12"></span>[29] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models. arXiv[:2001.08361](https://arxiv.org/abs/2001.08361) [cs.LG] <https://arxiv.org/abs/2001.08361>
- <span id="page-14-19"></span>[30] Marcus Kessel and Colin Atkinson. 2019. Automatically Curated Data Sets. In 2019 19th International Working Conference on Source Code Analysis and Manipulation (SCAM). 56–61. <https://doi.org/10.1109/SCAM.2019.00015>
- <span id="page-14-8"></span>[31] Marcus Kessel and Colin Atkinson. 2024. N-Version Assessment and Enhancement of Generative AI. IEEE Software (2024), 1–8. <https://doi.org/10.1109/MS.2024.3469388> arXiv[:2409.14071](https://arxiv.org/abs/2409.14071)
- <span id="page-14-2"></span>[32] Marcus Kessel and Colin Atkinson. 2024. Promoting open science in test-driven software experiments. Journal of Systems and Software Special Issue - Open Science in Software Engineering Research 212 (2024), 111971. [https:](https://doi.org/10.1016/j.jss.2024.111971) [//doi.org/10.1016/j.jss.2024.111971](https://doi.org/10.1016/j.jss.2024.111971)
- <span id="page-14-14"></span>[33] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating Training Data Makes Language Models Better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 8424–8445. [https://doi.org/10.18653/](https://doi.org/10.18653/v1/2022.acl-long.577) [v1/2022.acl-long.577](https://doi.org/10.18653/v1/2022.acl-long.577)
- <span id="page-14-4"></span>[34] Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, and Bowen Zhou. 2023. Trustworthy AI: From Principles to Practices. ACM Comput. Surv. 55, 9, Article 177 (jan 2023), 46 pages. <https://doi.org/10.1145/3555803>
- <span id="page-14-9"></span>[35] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. 2024. Chain of Code: Reasoning with a Language Model-Augmented Code Emulator. arXiv[:2312.04474](https://arxiv.org/abs/2312.04474) [cs.CL] <https://arxiv.org/abs/2312.04474>
- <span id="page-14-21"></span>[36] Xuan Li, Shuai Yuan, Xiaodong Gu, Yuting Chen, and Beijun Shen. 2024. Few-shot code translation via task-adapted prompt learning. Journal of Systems and Software 212 (2024), 112002. <https://doi.org/10.1016/j.jss.2024.112002>
- <span id="page-14-10"></span>[37] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz,

Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competitionlevel code generation with AlphaCode. Science 378, 6624 (2022), 1092–1097. <https://doi.org/10.1126/science.abq1158> arXiv[:https://www.science.org/doi/pdf/10.1126/science.abq1158](https://arxiv.org/abs/https://www.science.org/doi/pdf/10.1126/science.abq1158)

- <span id="page-15-4"></span>[38] Chenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan, and Nan Duan. 2023. Code Execution with Pre-trained Language Models. In Findings of the Association for Computational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 4984–4999. <https://doi.org/10.18653/v1/2023.findings-acl.308>
- <span id="page-15-1"></span>[39] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. 2023. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. In Thirty-seventh Conference on Neural Information Processing Systems. <https://openreview.net/forum?id=1qvx610Cu7>
- <span id="page-15-7"></span>[40] Jiate Liu, Yiqin Zhu, Kaiwen Xiao, QIANG FU, Xiao Han, Yang Wei, and Deheng Ye. 2023. RLTF: Reinforcement Learning from Unit Test Feedback. Transactions on Machine Learning Research (2023). [https://openreview.net/forum?](https://openreview.net/forum?id=hjYmsV6nXZ) [id=hjYmsV6nXZ](https://openreview.net/forum?id=hjYmsV6nXZ)
- <span id="page-15-10"></span>[41] Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, Chakkrit Tantithamthavorn, Li Li, Xuan-Bach D. Le, and David Lo. 2024. Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues. ACM Trans. Softw. Eng. Methodol. 33, 5, Article 116 (jun 2024), 26 pages. <https://doi.org/10.1145/3643674>
- <span id="page-15-2"></span>[42] Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming. 2024. EvalPlus Leaderboard. [https:](https://evalplus.github.io/leaderboard.html) [//evalplus.github.io/leaderboard.html](https://evalplus.github.io/leaderboard.html)
- <span id="page-15-0"></span>[43] David Lo. 2023. Trustworthy and Synergistic Artificial Intelligence for Software Engineering: Vision and Roadmaps. In 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE). 69–85. <https://doi.org/10.1109/ICSE-FoSE59343.2023.00010>
- <span id="page-15-11"></span>[44] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2024. StarCoder 2 and The Stack v2: The Next Generation. arXiv[:2402.19173](https://arxiv.org/abs/2402.19173) [cs.SE] <https://arxiv.org/abs/2402.19173>
- <span id="page-15-3"></span>[45] Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, and Matthias Gallé. 2024. On Leakage of Code Generation Evaluation Datasets. arXiv[:2407.07565](https://arxiv.org/abs/2407.07565) [cs.CL] <https://arxiv.org/abs/2407.07565>
- <span id="page-15-14"></span>[46] Daniel Méndez Fernández, Martin Monperrus, Robert Feldt, and Thomas Zimmermann. 2019. The open science initiative of the Empirical Software Engineering journal. Empirical Software Engineering 24, 3 (01 Jun 2019), 1057–1060. <https://doi.org/10.1007/s10664-019-09712-x>
- <span id="page-15-13"></span>[47] Grégoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented Language Models: a Survey. Transactions on Machine Learning Research (2023). [https://openreview.](https://openreview.net/forum?id=jh7wH2AzKK) [net/forum?id=jh7wH2AzKK](https://openreview.net/forum?id=jh7wH2AzKK) Survey Certification.
- <span id="page-15-12"></span>[48] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. arXiv[:1301.3781](https://arxiv.org/abs/1301.3781) [cs.CL]
- <span id="page-15-6"></span>[49] Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2024. NExT: Teaching Large Language Models to Reason about Code Execution. arXiv[:2404.14662](https://arxiv.org/abs/2404.14662) [cs.LG] [https://arxiv.org/](https://arxiv.org/abs/2404.14662) [abs/2404.14662](https://arxiv.org/abs/2404.14662)
- <span id="page-15-8"></span>[50] Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-tau Yih, Sida I Wang, and Xi Victoria Lin. 2023. Lever: Learning to verify language-to-code generation with execution. In Proceedings of the 40th International Conference on Machine Learning (ICML'23).
- <span id="page-15-5"></span>[51] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show Your Work: Scratchpads for Intermediate Computation with Language Models. arXiv[:2112.00114](https://arxiv.org/abs/2112.00114) [cs.LG] [https://arxiv.org/abs/](https://arxiv.org/abs/2112.00114) [2112.00114](https://arxiv.org/abs/2112.00114)
- <span id="page-15-9"></span>[52] Jens Palsberg and Cristina V. Lopes. 2018. NJR: a normalized Java resource. In Companion Proceedings for the ISSTA/ECOOP 2018 Workshops (Amsterdam, Netherlands) (ISSTA '18). Association for Computing Machinery, New York, NY, USA, 100–106. <https://doi.org/10.1145/3236454.3236501>

<span id="page-16-0"></span>Morescient GAI for Software Engineering (Extended Version) 1:17

- <span id="page-16-1"></span>[53] H. G. Rice. 1953. Classes of Recursively Enumerable Sets and Their Decision Problems. Trans. Amer. Math. Soc. 74, 2 (1953), 358–366. <http://www.jstor.org/stable/1990888>
- <span id="page-16-6"></span>[54] Andrea Rossi, Denilson Barbosa, Donatella Firmani, Antonio Matinata, and Paolo Merialdo. 2021. Knowledge Graph Embedding for Link Prediction: A Comparative Analysis. ACM Trans. Knowl. Discov. Data 15, 2, Article 14 (jan 2021), 49 pages. <https://doi.org/10.1145/3424672>
- <span id="page-16-2"></span>[55] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K. Reddy. 2023. Execution-based Code Generation using Deep Reinforcement Learning. Transactions on Machine Learning Research (2023). [https://openreview.net/forum?id=](https://openreview.net/forum?id=0XBuaxqEcG) [0XBuaxqEcG](https://openreview.net/forum?id=0XBuaxqEcG)
- <span id="page-16-5"></span>[56] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. 2024. AI models collapse when trained on recursively generated data. Nature 631, 8022 (2024), 755–759.
- <span id="page-16-7"></span>[57] Liyan Tang, Philippe Laban, and Greg Durrett. 2024. MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents. arXiv[:2404.10774](https://arxiv.org/abs/2404.10774) [cs.CL] <https://arxiv.org/abs/2404.10774>
- <span id="page-16-10"></span>[58] Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian Moghaddam, and Neel Sundaresan. 2024. AutoDev: Automated AI-Driven Development. arXiv[:2403.08299](https://arxiv.org/abs/2403.08299) [cs.SE] <https://arxiv.org/abs/2403.08299>
- <span id="page-16-8"></span>[59] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. 2024. Software Testing With Large Language Models: Survey, Landscape, and Vision. IEEE Transactions on Software Engineering 50, 4 (2024), 911–936. <https://doi.org/10.1109/TSE.2024.3368208>
- <span id="page-16-4"></span>[60] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2024. Magicoder: Empowering Code Generation with OSS-Instruct. arXiv[:2312.02120](https://arxiv.org/abs/2312.02120) [cs.CL] <https://arxiv.org/abs/2312.02120>
- <span id="page-16-3"></span>[61] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated Program Repair in the Era of Large Pretrained Language Models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). 1482–1494. <https://doi.org/10.1109/ICSE48619.2023.00129>
- <span id="page-16-9"></span>[62] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024. SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering. arXiv[:2405.15793](https://arxiv.org/abs/2405.15793) [cs.SE] <https://arxiv.org/abs/2405.15793>
