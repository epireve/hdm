---
cite_key: ilievski_2018
title: Identifying and Consolidating Knowledge Engineering Requirements
authors: Filip Ilievski, Saurav Joshi, Bradley P. Allen
year: 2018
doi: 10.1145/3442188.
date_processed: '2025-07-02'
phase2_processed: true
original_folder: arxiv_arxiv_2306.15124_Identifying_and_Consolidating_Knowledge_Engineering_Requirements
images_total: 1
images_kept: 1
images_removed: 0
tags:
- Decision Support
- Knowledge Graph
- Machine Learning
- Semantic Web
---

# Identifying and Consolidating Knowledge Engineering Requirements

Bradley P. Allen University of Amsterdam Amsterdam, The Netherlands b.p.allen@uva.nl

Filip Ilievski USC Information Sciences Institute Marina del Rey, CA, USA ilievski@isi.edu

Saurav Joshi USC Information Sciences Institute Marina del Rey, CA, USA syjoshi@isi.edu

## ABSTRACT

Knowledge engineering is the process of creating and maintaining knowledge-producing systems. Throughout the history of computer science and AI, knowledge engineering workflows have been widely used because high-quality knowledge is assumed to be crucial for reliable intelligent agents. However, the landscape of knowledge engineering has changed, presenting four challenges: unaddressed stakeholder requirements, mismatched technologies, adoption barriers for new organizations, and misalignment with software engineering practices. In this paper, we propose to address these challenges by developing a reference architecture using a mainstream software methodology. By studying the requirements of different stakeholders and eras, we identify 23 essential quality attributes for evaluating reference architectures. We assess three candidate architectures from recent literature based on these attributes. Finally, we discuss the next steps towards a comprehensive reference architecture, including prioritizing quality attributes, integrating components with complementary strengths, and supporting missing socio-technical requirements. As this endeavor requires a collaborative effort, we invite all knowledge engineering researchers and practitioners to join us.

### CCS CONCEPTS

• Software and its engineering → Software architectures; • Computing methodologies → Semantic networks; Ontology engineering.

#### KEYWORDS

knowledge engineering, knowledge graphs, quality attributes, software architectures

#### ACM Reference Format:

Bradley P. Allen, Filip Ilievski, and Saurav Joshi. 2018. Identifying and Consolidating Knowledge Engineering Requirements. In Proceedings of ACM Conference (Conference'17). ACM, New York, NY, USA, [16](#page-15-0) pages. [https:](https://doi.org/XXXXXXX.XXXXXXX) [//doi.org/XXXXXXX.XXXXXXX](https://doi.org/XXXXXXX.XXXXXXX)

Conference'17, July 2017, Washington, DC, USA

© 2018 Association for Computing Machinery.

ACM ISBN 978-1-4503-XXXX-X/18/06. . . \$15.00

<https://doi.org/XXXXXXX.XXXXXXX>

# 1 INTRODUCTION

Knowledge engineering (KE) is the discipline of building and maintaining processes that produce knowledge. Per [\[31\]](#page-8-0), knowledge can be defined as a set of beliefs that are "(i) true, (ii) certain, (iii) obtained by a reliable process". KE workflows have been popular throughout the evolution of computer science and AI under the intuitive assumption that the reliability of intelligent agents (e.g., chatbots) strongly depends on high-quality knowledge [\[1,](#page-8-1) [6,](#page-8-2) [7,](#page-8-3) [11,](#page-8-4) [12,](#page-8-5) [14,](#page-8-6) [17,](#page-8-7) [19,](#page-8-8) [26,](#page-8-9) [30](#page-8-10)[–32,](#page-8-11) [35\]](#page-8-12). And yet, KE as a discipline has changed considerably since its initial flowering during the period associated with expert systems development in the nineteen-eighties.

During the period from 1955 to today, we can identify four distinct eras in the history of knowledge engineering. We dub these the Dawn of AI, the Expert Systems Era, the Semantic Web Era, and the Language Model Era. [Figure 1,](#page-1-0) which we discuss in detail below, displays the four eras on a timeline, showing that the requirements for KE processes have been shifting in response to the perceived shortcomings of systems created in the preceding period. KE has focused on reliability during the Dawn of AI, domain-specific workflows in the Expert Systems era, accessible and interoperable knowledge production in the Semantic Web era, and curatable and affordable workflows in the latest Language Model era. The dynamic requirements have been accompanied by a shift in the target stakeholders, e.g., the Semantic Web era has focused on the needs of knowledge engineers, whereas the language modeling era aims to primarily enable data scientists. From today's perspective, the divergence of stakeholders and requirements has resulted in four key challenges. (1) Each era has pain points, i.e., requirements that have not been fully incorporated: language model engineering suffers from low interoperability [\[6\]](#page-8-2), the Semantic Web is vulnerable to the lack of high-quality schemas [\[14\]](#page-8-6), and expert systems' pain point is scalability [\[12\]](#page-8-5). (2) While many knowledge resources and tools have been developed, they rely on misaligned era-specific technologies, i.e., bespoke knowledge representation choices, domain-specific languages and systems, and siloed data-sharing practices, which has stymied progress. (3) The lack of consensus on KE best practices hinders adoption for organizations wanting to apply KE to their use cases, as the costs of setting up, maintaining, and extending these stacks for organizations are unclear. Meanwhile, large internet companies such as Google and Facebook rely on custom architectures and representations that most enterprises may struggle to reproduce. (4) The KE practices adopted by different communities do not align well with software engineering (SE) best practices, which makes adoption even more challenging.

How does one build a standard workflow for KE that simultaneously supports the requirements of diverse stakeholders in a way that will address the above four challenges (1-4)? We propose to develop a

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

## <span id="page-1-0"></span>Conference'17, July 2017, Washington, DC, USA Allen, Ilievski, and Joshi

![](_page_1_Figure_2.jpeg)
<!-- Image Description: The image displays a line graph showing the percentage of n-grams related to "expert systems," "ontology," "metadata," and "machine learning" in English books published between 1955 and 2019. This is overlaid with a timeline depicting eras in AI development (Dawn of AI, AI Winter I, Expert Systems Era, AI Winter II, Semantic Web Era, Language Model Era). Below, a network diagram connects these eras to key quality attributes (reliability, efficiency, domain-specificity, etc.) and features associated with each era. The image illustrates the evolution of AI research and its corresponding characteristics over time. -->

Figure 1: Evolving requirements over four eras of the history of knowledge engineering, as witnessed by research that has been responding to the shifting KE landscape [\[1,](#page-8-1) [6,](#page-8-2) [7,](#page-8-3) [11,](#page-8-4) [12,](#page-8-5) [14,](#page-8-6) [17,](#page-8-7) [19,](#page-8-8) [26,](#page-8-9) [30](#page-8-10)[–32,](#page-8-11) [35\]](#page-8-12). We show English language book n-gram occurrence percentages over time for n-grams indicative of each era [\[37\]](#page-8-13). We describe the figure in detail in [section 4.](#page-3-0)

reference architecture (RA), a key SE artifact that enables technology adoption within a community of practice by capturing design patterns that together provide a flexible framework to build and design software systems. RAs provide numerous advantages, such as interoperability, reduced development costs, improved communication among stakeholders, and the use of best practices, which collectively help overcome barriers such as organizational resistance and lack of expertise [\[23\]](#page-8-14). Although there may be concerns about the learning curve associated with RAs and the need for concrete examples [\[22\]](#page-8-15), the literature agrees that RAs simplify the development process, promote standardization, and facilitate collaboration. By focusing on usability and consolidation, RAs can minimize the learning curve and provide well-documented, application-oriented processes supported by easy-to-use and well-integrated tools. These benefits facilitate the adoption of data analytics in enterprises [\[8,](#page-8-16) [24,](#page-8-17) [29\]](#page-8-18), supporting workflows similar to those in KE.

By providing these benefits, an RA can be used to directly address the four key challenges described above. Through consolidation, RAs can allow stakeholders to effectively communicate and collaborate (1), analyzing their requirements, prioritizing their choice of technologies, and detecting and addressing pain points early in the KE process. Through standardization, RAs can harmonize different (era-specific) perspectives (2) into a single infrastructural view. By supporting usability and interoperability, RAs can facilitate adoption by new organizations (3) by mapping requirements to the components or patterns in the RA that address them. And, by simplifying the development process, RAs can facilitate alignment of KE with SE best practices (4).

Thus, we believe that developing an RA for KE is a critical step toward the adoption of knowledge technologies by diverse stakeholders, including the mainstream SE community. Acknowledging

that the development of an RA for KE is an ambitious endeavor and one that requires community discussions, in this paper we make the first step, by identifying and consolidating requirements that have arisen across diverse stakeholders and KE eras. We start with studying RA development methodologies and analyzing the gap between existing KE practices and mainstream RA practices in SE, in [section 2.](#page-1-1) We profile prominent KE stakeholder personas, each with their typical needs, corresponding tooling, and pain points as indicators for unsatisfied needs, in [section 3.](#page-2-0) Then, we dive deeper into the requirements of these stakeholders within the four KE eras since the dawn of AI and derive a consolidated list of requirements in [section 4.](#page-3-0) In [section 5,](#page-5-0) we show how our derived requirements can be applied to evaluate the suitability of three prominent and relatively recent KE architectures as future RAs for KE. The satisfaction of the requirements can be traced to the focus of each architecture, whereas socio-technical requirements (e.g., curatability) are missed by all architectures. We suggest that future KE architectures should be evaluated against our requirements and they should attempt to address the missing requirements. Inspired by our findings, we conclude with a discussion of the next steps toward a comprehensive RA for KE in [section 6.](#page-7-0)

### <span id="page-1-1"></span>2 BACKGROUND

In this section, we introduce prior work on RAs, describe common SE methodologies for creating RAs, and explain current practices in KE workflows. Thus, we characterize the gap between mainstream RA and KE practices and describe how this gap could be bridged. Reference architectures Per Angelov et al. [\[2\]](#page-8-19), an RA is a generic architecture for a class of information systems that are used as a foundation for the design of concrete architectures in this class. RAs provide the highest level of abstraction, they emphasize heavily architectural qualities, their stakeholders are considered but absent from the architecture, they promote adherence to common standards, and are effective for system development and communication [\[3\]](#page-8-20). Many RAs have been proposed in the past decades, some of which have gained wide adoption in their domains [\[13\]](#page-8-21). The Andreessen Horowitz reference architecture [\[8\]](#page-8-16) for emerging data infrastructure and platforms is a snapshot of the current industry stack and trends that subsumes most current uses of data within an enterprise. Typical RAs for big data usually follow a three-step lifecycle consisting of data ingestion, transformation, and serving [\[3\]](#page-8-20). Reference architectures of this type are examples of a key element in domain-specific software architectures (DSSAs) [\[34\]](#page-8-22). DSSAs combine an understanding of the problem space being addressed with business goals and technology stack solutions to provide guidance in the implementation of software architectures for a given area of application. It is noteworthy that the Andreessen Horowitz RA makes no reference at all to mainstream KE technologies, like knowledge graphs (KGs) or Semantic Web concepts or products, especially given the care it takes to address specific use cases related to machine learning.

RA methodologies A method to design a software architecture by [\[25\]](#page-8-23) consists of five steps: establishing its scope, selecting and investigating information sources, performing an architectural analysis to identify significant requirements, synthesizing the RA, and evaluating it through surveys as well as its instantiation and use. Software architecture methodologies leverage requirements as a common denominator between stakeholder needs and technical patterns, where the features and the functions are modeled in a product-independent way [\[9\]](#page-8-24). Two types of requirements are commonly used: functional requirements (FRs) and quality attributes (QAs) [\[5\]](#page-8-25). Functional requirements describe what the system components are responsible for, i.e., they state what the system must do and how it must behave or react to runtime stimuli [\[5\]](#page-8-25). FRs are satisfied by assigning an appropriate sequence of responsibilities throughout the architectural design. Bass et al. [\[5\]](#page-8-25) define a quality attribute as "a measure or testable property of a system that is used to indicate how well the system satisfies the needs of its stakeholders". QAs should be unambiguous and testable, and measure the ability of a system to satisfy the stakeholder's goals, characterized using one or more scenarios. QAs can be mapped into general categories, e.g., those that describe a runtime property of the system (e.g., availability), or those that describe a developmental property of the system (e.g., testability). Designing for a system that satisfies all QAs is a matter of making the appropriate tradeoffs [\[5\]](#page-8-25). In this work, we follow the RA methodology and terminology from Bass et al. [\[5\]](#page-8-25), and align our requirements with the categories in this book. KE workflows CommonKADS [\[32\]](#page-8-11) is a methodology for the extraction of expert knowledge into knowledge bases based on lifecycle and corresponding models. The popular Semantic Web Stack [\[16\]](#page-8-26) prescribes a layered infrastructure of technologies and languages that would constitute a Semantic Web application. Recognizing that knowledge can be extracted at scale from unstructured data, by crowdsourcing, and by neural models, recent work has developed KE workflows that are domain- or application-specific. KGs are popular within the Library and Information Studies (LIS) [\[36\]](#page-8-27) community or in e-commerce applications like the Amazon Product

KG [\[40\]](#page-8-28), resulting in specific architectures. This pluralism of architectures, which may or may not adhere to the Semantic Web Stack, as well as the importance of considering a broad set of KE stakeholders (e.g., practitioners beyond academia) have been recognized by Hogan [\[17\]](#page-8-7). Hogan notes that the broad adoption of KE practices and artifacts is hindered by the lack of understanding of users, and the mismatch between existing KE approaches and mainstream SE tools and practices. Ironically, while the field of Semantic Web puts a lot of emphasis on developing artifacts like ontologies and KGs that enable common understanding between humans and machines, it has not caught up on the idea of developing shared architectures through which different concerns can be expressed, negotiated, and resolved among stakeholders for large, complex systems [\[5\]](#page-8-25). We address this critical gap by consolidating the requirements across stakeholders and eras, with the final aim to devise an RA for KE.

#### <span id="page-2-0"></span>3 STAKEHOLDER PROFILES

Since an RA is an instrument for reaching a consensus between stakeholders, it is essential to understand their requirements [\[9\]](#page-8-24). We briefly describe the presently addressed needs and pain points (as open challenges) of four stakeholder profiles: a knowledge explorer, a software developer, a data scientist, and a knowledge engineer. While the full accounting of an evaluation of an RA would include profiles of business or organizational stakeholders; for the purpose of this discussion, we focus on these four profiles as useful abstractions of user tasks to drive our understanding of their needs independent of a specific business or organizational application.

Knowledge explorers access data to find (usually) small pieces of information from user-friendly interfaces. As such, they are not expected to be proficient with KGs, AI, or even computer science. Knowledge explorers often are domain experts, e.g., in publishing, health, or e-commerce. A knowledge explorer might visualize knowledge in intuitive ways,[1](#page-2-1) or perform semantic text search [\[18\]](#page-8-29). Pain points for knowledge explorers include interfaces with ambiguous semantics (accessibility) [\[17\]](#page-8-7), misaligned tooling (comprehensiveness) [\[19\]](#page-8-8), and lack of modular design [\[19\]](#page-8-8).

Software engineers design, develop, maintain, test, and evaluate computer software. Background knowledge may benefit software engineers in several ways, e.g., through content enrichment [\[21\]](#page-8-30) or recommendation techniques [\[40\]](#page-8-28). Software engineers face a mismatch of assumptions and common KE practices, i.e., the stack of tooling and formats used by software engineers is often different from those developed by knowledge graph engineers. Further obstacles include the lack of robustness to a missing schema [\[14\]](#page-8-6), the lack of software developer-friendly documentation and data catalogs (accessibility) [\[17\]](#page-8-7), and the entry-level costs (affordability) [\[6\]](#page-8-2). Data scientists draw insights and build predictive models from data using various techniques, typically based on machine learning. A common goal of data scientists is to minimize failure in prediction tasks according to metrics like accuracy and precision. Data scientists may use available knowledge to enhance their training process and ultimately boost or evaluate the performance of their models. Knowledge may be used to support the development of a model for a specific task (e.g., entity linking [\[4\]](#page-8-31)), a larger suite of tasks (e.g., analytical operations), or support benchmarking and

<span id="page-2-1"></span><sup>1</sup><https://gephi.org/>

evaluation [\[39\]](#page-8-32). Yet, the integration of knowledge structures in data science research may be hindered by the lack of comprehensiveness in tooling, the quality of the knowledge produced by the KE process (robustness to noise) [\[19\]](#page-8-8), and the ability to scale economically with the amount of knowledge produced [\[12\]](#page-8-5).

Knowledge engineers build, maintain, and query knowledgebased systems. Consequently, they require efficient and easy-to-use ways to access and modify information systematically. Tools for knowledge engineers support querying (e.g., by graph databases like Graph DB)[2](#page-3-1) or support contributions to knowledge artifacts (e.g., via ontology editors like Protege [\[28\]](#page-8-33)). Pain points include the lack of tools that support the creation of large-scale knowledge artifacts (scalability) [\[12\]](#page-8-5), missing support for manual curation of knowledge (curatability) [\[6\]](#page-8-2), and difficulty in terms of incorporating more sources or modalities of data (extensibility) [\[30\]](#page-8-10).

# <span id="page-3-0"></span>4 A HISTORIOGRAPHICAL APPROACH TO IDENTIFYING KE REQUIREMENTS

We argue that a useful way to understand the evolution of stakeholders' requirements for KE is to examine representative literature from its four eras that take a specific position on what those requirements should be. Following mainstream SE practices and inspired by our characterization of stakeholder profiles, in this section, we derive a list of functional requirements (Table [1\)](#page-4-0) and quality attributes (Table [2\)](#page-4-1). Before we present the set of FRs and QAs, we discuss two caveats. First, we note that the FRs and QAs are inherently biased by the authors' knowledge and perspective of prior work. Second, as we aim to be faithful to the perspective of multiple stakeholders, some of the requirements may be interdependent, i.e., they may partially overlap or even contradict each other. We allow for the subjectivity and the dependencies to co-exist at this point.

## 1 Requirements from the Dawn of AI

We start with Frank Ramsey's 1929 definition of knowledge as a set of beliefs that are "(i) true, (ii) certain, [and] (iii) obtained by a reliable process" [\[31\]](#page-8-0) as a baseline requirement that KE processes be reliable (Q01). From the late nineteen-fifties, some of the earliest work in AI additionally identified the requirement that such processes also be computationally efficient (Q02), in the sense that they complete execution in a reasonable amount of time and space [\[26\]](#page-8-9). Newell and Simon [\[26\]](#page-8-9) were optimistic about the potential of goal-directed search using heuristics as a general approach to problem-solving to be useful for practical applications. Still, by the beginning of the nineteen-seventies, it was clear that such systems were difficult to use in developing applications that were recognizably more than just toy tasks.

### 2 Requirements from the Expert Systems era

By the mid-seventies, having been deeply involved in attempting to apply Newell and Simon's model, Feigenbaum became convinced that automating knowledge production required a domainspecific (Q03) focus to succeed [\[11\]](#page-8-4). His evangelism of KE (a term he was instrumental in propagating the use of) engendered a period of intense activity in the construction of expert systems for the

purposes of decision support in business enterprise settings. By the early nineteen-nineties, however, Feigenbaum and others acknowledged that the expert systems approach resulted in systems that were brittle and hard to maintain. Without abandoning his requirement that KE is domain-specific in application focus and thus heavily dependent on subject matter expertise, he argued that future knowledge-based systems also be scalable (Q04), globally distributed (Q05), and interoperable with other knowledge bases (Q06) to address these shortcomings [\[12\]](#page-8-5). At that point in time, there was no consensus about how such requirements could be addressed, but in retrospect, one can argue that in [\[12\]](#page-8-5) Feigenbaum anticipated several aspects of what several years later would come to be known as the World Wide Web. As the nineties progressed, efforts were made to provide support for KE best practices (Q07) through the definition of structured methodologies [\[32\]](#page-8-11).

#### 3 Requirements from the Semantic Web era

With the establishment of the Web and the emergence of Web architectural principles, Berners-Lee argued for a "Web of Data" based on linked data principles, standard ontologies, and data-sharing protocols that not only provided an implementation of Feigenbaum's requirements, but with a single stroke established Web-centric open standards that use W3C semantic web standards (e.g., RDF, SPARQL) (F01) that anyone could adopt [\[7\]](#page-8-3). The subsequent twenty years witnessed the development of a globally federated open linked data "cloud", as well as the refinement of techniques for ontology engineering, i.e., the development and publishing of shared data schemas with semantics using linked data principles. Enterprises in particular found better value propositions for using such techniques toward the improvement of access and discovery of Web content and data, in contrast to the automation of decisionmaking that was the primary value proposition for knowledgebased systems during the expert systems era [\[15\]](#page-8-34). However, while progress was made in building systems based on such principles, the general adoption of specific principles advocated for by the semantic web community by the broader community of software developers and web application designers was slow, leading semantic web researchers to identify additional requirements for broader adoption, such that the core tools and standards used in semantic web application be more developer-friendly (Q08) and more directly aligned with software industry norms (Q09), and that measures be taken to make federated open data more robust to noise in data sources (Q10) [\[17\]](#page-8-7). Additional focus on support for heterogeneous query methods (F02) and support for federated query (F03), and on making access to linked data robust to data catalog incompleteness (Q11) while maintaining the practical benefits of open source and open standards (Q12) led to new requirements towards those ends [\[14,](#page-8-6) [35\]](#page-8-12).

In projects over the last ten years where commercially-useful enterprise KGs have been produced, such as the Google KG [\[27\]](#page-8-35) and Amazon's Product KG [\[10\]](#page-8-36), the attempt to address Q08 and Q09 has led to a reliance on custom architectures and approaches, which do not address the requirements of interoperability and federation identified by Feigenbaum and Berners-Lee [\[7,](#page-8-3) [12\]](#page-8-5). The impact of these projects, together with efforts to establish architectures that improve the developer experience associated with the development

<span id="page-3-1"></span><sup>2</sup><https://www.ontotext.com/products/graphdb>

#### <span id="page-4-0"></span>Table 1: Quality attributes for knowledge engineering. We map each of the requirements to the closest category from the software engineering handbook by [Bass et al.](#page-8-25) [\[5\]](#page-8-25).

| id  | requirement              | ref. | scenario                                                                                           | category         |
|-----|--------------------------|------|----------------------------------------------------------------------------------------------------|------------------|
| Q01 | reliability              | [31] | the knowledge engineering process is reliable, i.e. the knowledge produced can be trusted to       | reliability      |
|     |                          |      | be true and justified                                                                              |                  |
| Q02 | efficiency               | [26] | the knowledge produced by the KE process can be applied in a computationally tractable and         | performance      |
|     |                          |      | efficient manner                                                                                   |                  |
| Q03 | domain-specificity       | [11] | the KE process is tailored to use cases associated with a specific domain or area of expertise     | maintainability  |
| Q04 | scalability              | [12] | the knowledge engineering process scales economically with the amount of knowledge pro             | scalability      |
|     |                          |      | duced (measured in terms of, e.g. rules, triples, nodes, edges, etc.)                              |                  |
| Q05 | distributivity           | [12] | the knowledge produced by the KE process can be distributed and hosted across multiple sites       | availability     |
| Q06 | interoperability         | [12] | the knowledge produced by the KE process can be easily shared across sites and applications        | interoperability |
| Q07 | supports knowledge en    | [32] | the KE process follows methodologies for creating an ontology and elicitation of knowledge         | maintainability  |
|     | gineering best practices |      | from subject matter experts towards the creation of a knowledge model                              |                  |
| Q08 | accessibility            | [17] | the barrier to adoption by users of the KE process is low                                          | usability        |
| Q09 | supports software engi   | [17] | the knowledge engineering process conforms to software industry norms (e.g. the use of agile       | maintainability  |
|     | neering best practices   |      | methodologies, continuous integration and deployment, version control, automated testing,          |                  |
|     |                          |      | automated vulnerability scans, etc.)                                                               |                  |
| Q10 | robustness to noise      | [17] | the KE process is robust in the face of noise and/or adversarial manipulation of source data       | reliability      |
|     |                          |      | and/or knowledge                                                                                   |                  |
| Q11 | robustness to missing    | [14] | the knowledge produced by the KE process can be processed and/or accessed in the face of           | reliability      |
|     | schema                   |      | incomplete schemas and/or knowledge organization systems                                           |                  |
| Q12 | openness                 | [35] | the components of the KE process are implemented using open source software, with open             | maintainability  |
|     |                          |      | standards, and the knowledge produced by the knowledge engineering process is openly<br>accessible |                  |
| Q13 | comprehensiveness        | [19] | all components of an end-to-end KE process (e.g. data ingest/export, data transformation,          | usability        |
|     |                          |      | inference, knowledge publishing, etc.) are supported                                               |                  |
| Q14 | customizability          | [19] | the components of the KE process can be modified to support specific use cases                     | usability        |
| Q15 | modularity               | [19] | the components of the KE process can be selectively composed to suit a specific use case           | maintainability  |
| Q16 | domain-independence      | [19] | the KE process is generally applicable across a wide range of domains and areas of expertise       | usability        |
| Q17 | extensibility            | [30] | knowledge extraction from data or natural language performed in the KE process can easily          | maintainability  |
|     |                          |      | accommodate new sources and modalities of data or natural language                                 |                  |
| Q18 | curatability             | [6]  | the KE process supports human curation of automatically extracted and/or inferred knowledge        | safety           |
| Q19 | ethicality               | [6]  | the KE process supports compliance with and enforcement of policies and/or guidelines for          | safety           |
|     |                          |      | ethical use                                                                                        |                  |
| Q20 | sustainability           | [6]  | the cost of executing the KE process is economically sustainable for the given use case            | usability        |
| Q21 | affordability            | [6]  | the cost of access to the KE process is economically affordable for a given user community         | usability        |
| Q22 | editability              | [1]  | the knowledge produced by the KE process can be feasibly edited by humans                          | maintainability  |
| Q23 | explainability           | [1]  | the knowledge produced by the KE process provides accountability with respect to its prove         | testability      |
|     |                          |      | nance and the details of how it was produced (e.g. through human authoring, automated              |                  |
|     |                          |      | extraction, and/or inference, etc.)                                                                |                  |

#### Table 2: Functional requirements for knowledge engineering.

<span id="page-4-1"></span>

| id  | requirement                       | ref. | scenario                                                                                                        |
|-----|-----------------------------------|------|-----------------------------------------------------------------------------------------------------------------|
| F01 | supports semantic web standards   | [7]  | the KE process supports the use of W3C semantic web standards, including those for knowledge represen           |
|     |                                   |      | tation (RDF), serializations (e.g. Turtle, JSON-LD, etc.) and querying (SPARQL)                                 |
| F02 | supports heterogeneous query      | [14] | the knowledge produced by the KE process can be queried using multiple query languages (e.g. SQL, Cypher,       |
|     |                                   |      | SPARQL, etc.) and query execution strategies (e.g. federated query, centralized query, find-and-follow, etc.)   |
| F03 | supports federated query          | [35] | the knowledge produced by the KE process can be queried across multiple sites                                   |
| F04 | imports common data formats       | [19] | the KE process supports import of data and/or knowledge from software industry standard data sources            |
|     |                                   |      | (e.g. relational databases, RDF data dumps, REST APIs, etc.) and serializations (e.g. CSV, JSON, Parquet, etc.) |
| F05 | exports common data formats       | [19] | the knowledge engineering process supports export of produced knowledge to software industry standard           |
|     |                                   |      | data delivery mechanisms (e.g. as serialized data dumps, through publish/subscribe messaging, through           |
|     |                                   |      | REST APIs, search engine indexes, etc.)                                                                         |
| F06 | provides user-friendly interfaces | [19] | the knowledge produced by the knowledge engineering process can be accessed and applied through one             |
|     |                                   |      | or more user-friendly interfaces (e.g. command line interfaces, visual editors and browsers, REST APIs, etc.)   |
| F07 | schemaless                        | [30] | the knowledge engineering process does not need schemas to be defined to support effective knowledge            |
|     |                                   |      | extraction from data and/or natural language                                                                    |
| F08 | automates knowledge extraction    | [30] | knowledge extraction from data or natural language performed in the KE process is automatic and does            |
|     |                                   |      | not require human labor                                                                                         |

of KGs, has led to the need for a finer-grained articulation of what KE should provide to developers.

We identify the following additional requirements based on the principles explored in the development of the recent KGTK toolkit and our experience in its use [\[19\]](#page-8-8). As KE today impacts a wide range of disciplines and stakeholders, its infrastructures need to be comprehensive (Q13), i.e., encompass the wide range of typical KE operations: data collection, processing, querying, transformation, and distribution. Standardization and adoption of KE practices rely on the presence of such comprehensive tooling, as already shown by the role of infrastructures like HuggingFace for natural language processing,[3](#page-5-1) and scikit-learn for machine learning. [4](#page-5-2) To make the architecture accessible and understandable for a wide range of stakeholders, KE should be familiar and relevant to the world of each stakeholder, designed to follow their best practices as closely as possible (cf. Q07 and Q09). KE needs to be scalable (Q04): popular KGs today, like Wikidata, count their statements in the billions. While modern hardware and software can support the manipulation of data of such sizes, this is a non-trivial challenge and requires solutions that can scale with the current and future size of these resources. Given the variety of intended use cases, KE needs to be customizable (Q14): its components should allow for enough flexibility to be adaptable to diverse stakeholder needs. To facilitate Q14, the infrastructure should be designed in a modular way (Q15) where the user can pick and choose the components that are of use to them, and ignore the rest. A key aspect of modular architecture is to provide stakeholders with atomic functions for common operations, such as getting labels and aliases for an item, describing an item, querying for similar items, text searching for entities or events, fact extraction, and extracting a subset for reuse. As the adoption of KE is tightly coupled with their role in broader systems, the infrastructure for KE should natively integrate common data formats (F04) used in AI and SE, including CSV, TSV, JSON, and SQL. KE processes should support the production of common data formats (F05), analogous to those in F04. Together, F04 and F05 ensure that KE is interoperable with mainstream AI software, like HuggingFace. KE should be domain-independent (Q16), i.e., generalizable to a wide range of use cases. Ideally, a representative selection of use cases should be provided to illustrate the simplicity, effectiveness, and efficiency of including knowledge technologies in developer tasks. To support rapid prototyping and make KE easy to get started (cf. Q08), KE should provide userfriendly interfaces (F06), both in the form of command language as well as graphical interfaces like browsers and search endpoints.

## 4 Requirements from the Language Model era

The success of connectionist methods arising from the proliferation of graphical processing hardware for matrix arithmetic and concurrent innovations in neural architectures [\[38\]](#page-8-37) has led to a new set of possibilities for the production of knowledge. While at the time of this writing, it is difficult to summarize this area due to the burst of research in this direction, two perspectives on the relation between language models and knowledge bases have emerged over the last several years, both yielding new KE requirements.

One perspective is that a language model can serve directly as a knowledge base that is queryable using natural language prompts [\[30\]](#page-8-10). The argument is made that given this, new requirements for knowledge production are that it can extract knowledge from text without a data schema (F07), that is, be driven by learning directly from unstructured natural language modeling without recourse to pre-defined knowledge organization schemes, that it can extract knowledge from text and/or data with minimal human labor (F08), making it easy to incorporate new sources of data and/or knowledge (Q17). Note that these requirements are at odds with the far more human labor-intensive processes assumed in previous eras. This stance is not without controversy; though the emergent ability of large language models to generate syntactically correct and complex natural language is on the surface impressive, approaches to ensure that the natural language produced is meaningful and correct are still under development, and commercial offerings based on large language models can often be shown to confabulate, leading to the potential for them to be used in harmful ways [\[6\]](#page-8-2). This leads to new requirements that the production of knowledge using these technologies make it easy for humans to curate extracted knowledge (Q18) and that they support ethical use (Q19). In addition, the cost associated with the creation of language models, and in particular the often hidden and potentially exploitative use of manual labor to prepare data to tune them, suggests the need that KE processes be economically sustainable to run (Q20) and affordable to use (Q21).

A more conservative perspective is that a language model can be a useful component in a KE workflow that combines techniques based on the use of language models together with more traditional symbolic approaches [\[1\]](#page-8-1). Here the desired requirements are that language modeling be accessible to a broad community of developers and/or users (Q08), that it supports the manual editing of extracted knowledge (Q22) and supports the explanation of reasoning methods (Q23), again reflecting a degree of concern on the harnessing of language models in ways that address the clear shortcomings of the initial generation of such technologies.

## <span id="page-5-0"></span>5 EVALUATING EXISTING ARCHITECTURES

Per Bass et al. [\[5\]](#page-8-25), architectures should be evaluated for the QAs they support in the context of well-defined scenarios. Accordingly, we applied Q1-23 to three architectures: TG [\[33\]](#page-8-38), BioCypher [\[20\]](#page-8-39), and KGTK [\[19\]](#page-8-8). We have selected these three architectures because all of them attempt to provide a comprehensive approach to KE, albeit with a different focus: TG derives workflow patterns in a bottomup manner based on a survey of existing KE methods, BioCypher proposes a generic architecture for the domain of biomedicine, whereas KGTK is a toolkit that aims to cover the comprehensive functionality needed for KE at scale. Each of these is an example of a domain-specific[5](#page-5-3) software architecture [\[34\]](#page-8-22) in that they combine a reference architecture with a component library and guidance as to how to use components in the library to instantiate various elements of the reference architecture. Given an architecture, we

<span id="page-5-1"></span><sup>3</sup><https://huggingface.co>

<span id="page-5-2"></span><sup>4</sup><https://scikit-learn.org>

<span id="page-5-3"></span><sup>5</sup>Note that the sense in which the term "domain" is used in DSSAs is distinct from that assumed in the use of domain-specificity (Q03) and domain-independence (Q15) in the QAs. In the DSSA instance, we take "domain" to refer to the general practice of KE, while in the QAs listed above and the architecture evaluations below, we use this term to refer to the area of knowledge or expertise itself, independent of the practice of KE.

<span id="page-6-0"></span>Table 3: Evaluation of KE architectures by support for QAs. TG corresponds to Tamašauskaite and Groth [ ˙ [33\]](#page-8-38), BC is Bio-Cypher [\[20\]](#page-8-39), and KT is KGTK [\[19\]](#page-8-8). Our 23 QAs are mapped to the nine categories of [\[5\]](#page-8-25).

| category         | id  | quality attribute           | TG | BC | KT |
|------------------|-----|-----------------------------|----|----|----|
|                  | Q01 | reliability                 | ✓  | ✓  | ✓  |
| reliability      | Q10 | robustness to noise         | ✓  | ✓  | ✓  |
|                  | Q11 | robustness<br>to<br>missing | ✓  | -  | ✓  |
|                  |     | schema                      |    |    |    |
| performance      | Q02 | efficiency                  | ✓  | ✓  | ✓  |
|                  | Q03 | domain-specificity          | -  | ✓  | -  |
|                  | Q07 | supports knowledge engi     | ✓  | -  | -  |
|                  |     | neering best practices      |    |    |    |
| maintainability  | Q09 | supports SE best practices  | -  | ✓  | ✓  |
|                  | Q12 | openness                    | -  | ✓  | ✓  |
|                  | Q15 | modularity                  | -  | ✓  | ✓  |
|                  | Q17 | extensibility               | ✓  | ✓  | ✓  |
|                  | Q22 | editability                 | -  | -  | -  |
| scalability      | Q04 | scalability                 | ✓  | ✓  | ✓  |
| availability     | Q05 | distributivity              | ✓  | ✓  | ✓  |
| interoperability | Q06 | interoperability            | ✓  | ✓  | ✓  |
|                  | Q08 | accessibility               | -  | ✓  | ✓  |
|                  | Q13 | comprehensiveness           | ✓  | -  | ✓  |
|                  | Q14 | customizability             | ✓  | ✓  | ✓  |
| usability        | Q16 | domain-independence         | ✓  | -  | ✓  |
|                  | Q20 | sustainability              | -  | ✓  | ✓  |
|                  | Q21 | affordability               | -  | -  | -  |
|                  | Q18 | curatability                | -  | -  | -  |
| safety           | Q19 | ethicality                  | -  | ✓  | ✓  |
| testability      | Q23 | explainability              | -  | ✓  | ✓  |

study its design to evaluate whether it addresses a particular attribute. Table [3](#page-6-0) summarizes the results of this evaluation, where the QAs are grouped in the categories from [\[5\]](#page-8-25).

1. Reliability maps to three quality attributes. Reliability (Q01) and robustness to noise (Q10) are satisfied by all three architectures. TG's pipeline uses inference, validation, and optimization to ensure that only valid and relevant knowledge is included in the KG. BioCypher supports reliability through end-to-end testing, violation reports, and propagation of information about evidence and provenance. KGTK supports reliability and robustness to noise through operations for validation and cleaning. TG is robust to a missing schema (Q11) thanks to its development of ontology to represent domain knowledge, utilizing non-relational database systems to store KGs, and leveraging machine learning to extract, process, and complete knowledge. KGTK handles missing schemas by validating that the data meets the format specification and cleaning syntactic and semantic violations of the specification. BioCypher cannot function if a schema is missing.

2. Performance in the sense of efficiency (Q02) is emphasized in all three architectures. TG describes computationally tractable algorithms for each of its components (e.g., PageRank for the data identification component). BioCypher enables the creation of custom KGs in minutes by using its adapters for data ingestion and providing a schema configuration. KGTK focuses on large KGs and stores its intermediate results to disk for time and memory efficiency, which results in a relatively short running time.

3. Maintainability is the richest category in our evaluation, as it maps to seven QAs. Out of the three architectures, only BioCypher is domain-specific (Q03) as it focuses on the biomedical domain and illustrates its utility through a set of appropriate use cases. TG is not domain-specific because the same architecture is used both for manipulating a generic open KG and a domain-specific KG about user experience practices. KGTK illustrates its functionality on various domains, including scientific publications and finance, however, it is not tailored to use cases associated with any specific domain. TG supports KE best practices (Q07), as it involves generic components such as ontology development, knowledge acquisition, representation, completion, validation, and maintenance. BioCypher and KGTK do not support ontology development which is a key constituent of the KE process. BioCypher and KGTK address SE best practices (Q09), such as continuous integration, deployment, and testing methods. TG is more suited for initial KG development and does not support agile development, continuous integration and deployment, version control, and automated testing. Openness (Q12) is unclear in the case of TG, as the implementation of its components is underspecified. BioCypher is open because all its components are open-source and it is developed by a research community, while KGTK is based on open-source components with extensive documentation. Modularity (Q15) is supported by Bio-Cypher through its easily replaceable adapters and KGTK through its standard data format which is used by all of its components. TG is not modular: although its components can be easily excluded, the data format may vary across its components. Extensibility (Q17) is the only QA that is explicitly addressed by all three architectures: TG includes an update step, BioCypher allows extensions through the reuse of existing adapters for new knowledge sources, while KGTK allows extensions through its join operation and import/export of common data formats. The aspect of editability (Q22) is a gap in all three architectures: each of them focuses on optimizing the automatic components and their connection, but none explains how manual editing would be carried out.

4. Scalability is well-supported by existing architectures. TG achieves scalability (Q04) by using an efficient NoSQL storage and spreading the data across a cluster of machines. BioCypher achieves scalability by separating data storage and analysis, allowing each component to be scaled individually, while using distributed computing infrastructure, such as computing clusters, to perform both tasks in close proximity. KGTK uses scalable SQL storage accompanied by indexing and caching mechanisms that together ensure high scalability to large KGs like Wikidata.

5. Availability, which corresponds to our distributivity (Q05), is explicitly addressed by all three architectures. TG enables knowledge to be distributed to different sites by using clusters over a NoSQL database system, where all data is replicated for redundancy and high availability. BioCypher addresses distributivity by separating data storage from analysis and through federated machine learning. KGTK enables distributivity by providing a simple model that fits different data representations and is mapped to a database management system that supports both relational and non-relational databases.

6. Interoperability, i.e., the ability to share the produced knowledge across sites and applications, is supported by all three architectures. TG offers interoperability (Q06) via standardization of its

steps to produce, complete, and store knowledge, which is finally stored in relational databases. BioCypher's ETL pipeline incorporates several resources into a single ontological framework that follows the FAIR (findable, accessible, interoperable, reusable) principles. KGTK supports interoperability by representing its knowledge with the widely used format of tab-separated values (TSV), and by storing the data internally in a relational database, which can be easily shared across applications.

7. Usability maps to six QAs. Accessibility (Q08) for novel users is addressed by BioCypher based on its FAIR principles, and by KGTK through its extensive documentation and introductory materials. As TG has no specific framework that users can install and work with, it does not address accessibility. BioCypher does not support comprehensiveness (Q13) as it does not contain crucial components of end-to-end KE, including knowledge acquisition and quality evaluation. Conversely, TG and KGTK aim to support all the components of the KE process. Customizability (Q14) is the only usability QA that is explicitly supported by all three architectures: TG and KGTK can manipulate both large generic KGs and domain-specific KGs showing they can be adopted for multi-purpose data science operations over KGs by modifying several components in its pipeline, while BioCypher supports customization through manipulating its underlying ontology with extra information. BioCypher does not support domain-independence (Q16) as it does not focus on integrating generic KGs but rather focuses on domain-specific knowledge. TG and KGTK manipulate generic KGs and domain-specific KGs achieving domain independence. The related attribute of economic sustainability (Q20) is supported by BioCypher through its automatic ontology construction and curation techniques, and similar in KGTK through its validation, cleaning, and further transformation operations that anticipate no cost for manual curation. Ontologies in TG are constructed manually which can be expensive and labor-intensive. However, affordability (Q21) is not addressed by any of the three architectures: as TG has no specific framework, its start-up cost cannot be computed; BioCypher and KGTK are open-source projects and are freely available, which may incur costs for their infrastructural setup, training, or support.

8. Safety is translated into two QAs. While all three architectures support distilling valid and relevant knowledge automatically and aim to minimize the need for human curation, curatability (Q18) by humans is not addressed by any of them. Ethicality (Q19) is unclear in the case of TG since its external algorithms may or may not ensure legal compliance, collaborative and transparent development, community oversight, and quality control for ethical use. As an open-source project, BioCypher ensures these properties, and it does not allow the creation of entities without associated source, license, and version parameters. Similarly, KGTK's ethicality is supported by following open-source practices.

9. Testability maps to explainability (Q23). Although each component of TG uses an efficient machine learning algorithm that incorporates feedback to evaluate the KG, TG does not include different stakeholders or clear documentation steps, and some of its algorithms may be non-interpretable. BioCypher explicitly records the evidence (which experiment and publication the knowledge is derived from) and provenance (who provided which aspects of the primary data) during the KG creation, making it accountable and

explainable. KGTK's knowledge-creation process stores the intermediate outputs on disk, enabling reproducibility and inspection. Lessons learned We highlight four key lessons from this evaluation. (1) Our 23 derived quality attributes can be effectively applied for a formal assessment and comparison of KE architectures. We propose that future KE developers should keep Q1-23 in mind when devising novel architectures. (2) None of the three architectures ticks all the boxes, yet, their requirement satisfaction aligns well with the focus of the architectures, e.g., KGTK puts a strong focus on usability requirements, whereas BioCypher excels in terms of maintainability. (3) Failing to satisfy a QA comes with a measurable price - e.g., the fact that BioCypher strictly requires a schema makes it non-applicable to use cases without a high-quality schema. Meanwhile, since all architectures fare well on scalability, we expect them to be applicable to large KGs like Wikidata. (4) Certain QAs: editability, curatability, and affordability are missed by all architectures, confirming Hogan's [\[17\]](#page-8-7) argument for better socio-technical integration in KE, which should be addressed in future KE architectures.

# <span id="page-7-0"></span>6 FROM REQUIREMENTS TO A REFERENCE ARCHITECTURE FOR KE?

In our work so far, we identified the needs of various stakeholders and KE eras and consolidated them into quality attributes and functional requirements. We demonstrated the utility of these QAs to test the suitability of current (and future) RAs for KE. The evaluation of three candidate architectures revealed that none of the architectures satisfies the full set of QAs, though the KGTK architecture came the closest. We observed that the three architectures took different approaches to support a certain attribute (e.g., scalability), whereas attributes that relate to human involvement through curation or affordability were generally absent from all three architectures. We proposed that future KE architectures should be evaluated against these 23 QAs and 8 FRs, and they should provide better support for socio-technical requirements.

Encouraged by these insights, we plan to continue the development of an RA for KE by following standard software methodologies. We will survey relevant stakeholders for their prioritization of attributes to understand their relevance across applications. Next, we will use this prioritization to guide the creation of an RA that will combine the complementary strengths of different existing architectures. We will apply our QAs to assess additional KE architectures, such as the Wikidata ecosystem. We will also extend our evaluation to representative data engineering architectures, which will enable us to qualify the material differences between KE and data engineering. We anticipate that such a human-centric iterative methodology will yield a comprehensive RA for KE that supports the requirements of different stakeholders and KE eras, thus naturally facilitating adoption. As this endeavor requires a collaborative effort, we invite the entire KE community to join us.

## 7 ACKNOWLEDGMENTS

We thank Nenad Medvidovic for his invaluable review and suggestions. We thank Jay Pujara and Juan Sequeda for the inspiring discussions. Filip Ilievski and Saurav Joshi are funded by the DARPA Knowledge Management at Scale and Speed (KMASS) program.

Identifying and Consolidating Knowledge Engineering Requirements Conference'17, July 2017, Washington, DC, USA

### REFERENCES

- <span id="page-8-1"></span>[1] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. 2022. A review on language models as knowledge bases. arXiv preprint arXiv:2204.06031 (2022).
- <span id="page-8-19"></span>[2] Samuil Angelov, Paul Grefen, and Danny Greefhorst. 2009. A classification of software reference architectures: Analyzing their success and effectiveness. In 2009 Joint Working IEEE/IFIP Conference on Software Architecture & European Conference on Software Architecture. IEEE, 141–150.
- <span id="page-8-20"></span>[3] Pouya Ataei and Alan Litchfield. 2022. The state of big data reference architectures: a systematic literature review. IEEE Access (2022).
- <span id="page-8-31"></span>[4] Tom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos Christodoulopoulos, and Andrea Pierleoni. 2022. ReFinED: An Efficient Zero-shot-capable Approach to End-to-End Entity Linking. arXiv preprint arXiv:2207.04108 (2022).
- <span id="page-8-25"></span>[5] Len Bass, Paul Clements, and Rick Kazman. 2022. Software architecture in practice (fourth ed.). Addison-Wesley Professional.
- <span id="page-8-2"></span>[6] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT '21). Association for Computing Machinery, New York, NY, USA, 610–623. [https://doi.org/10.1145/3442188.](https://doi.org/10.1145/3442188.3445922) [3445922](https://doi.org/10.1145/3442188.3445922)
- <span id="page-8-3"></span>[7] Tim Berners-Lee, James Hendler, and Ora Lassila. 2001. The semantic web. Scientific american 284, 5 (2001), 34–43.
- <span id="page-8-16"></span>[8] Matt Bornstein, Jennifer Li, and Casado. Martin. 2020. Emerging Architectures for Modern Data Infrastructure. [https://future.com/emerging-architectures-modern](https://future.com/emerging-architectures-modern-data-infrastructure/)[data-infrastructure/.](https://future.com/emerging-architectures-modern-data-infrastructure/) Accessed: 2022-12-02.
- <span id="page-8-24"></span>[9] Robert Cloutier, Gerrit Muller, Dinesh Verma, Roshanak Nilchiani, Eirik Hole, and Mary Bone. 2010. The concept of reference architectures. Systems Engineering 13, 1 (2010), 14–27.
- <span id="page-8-36"></span>[10] Xin Luna Dong. 2018. Challenges and innovations in building a product knowledge graph. In Proceedings of the 24th ACM SIGKDD International conference on knowledge discovery & data mining. 2869–2869.
- <span id="page-8-4"></span>[11] Edward A Feigenbaum. 1977. The art of artificial intelligence: Themes and case studies of knowledge engineering. In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, Vol. 2. Boston.
- <span id="page-8-5"></span>[12] Edward A Feigenbaum. 1992. A personal view of expert systems: Looking back and looking ahead. Knowledge Systems Laboratory, Department of Computer Science, Stanford . . . .
- <span id="page-8-21"></span>[13] Lina Garcés, Silverio Martínez-Fernández, Lucas Oliveira, Pedro Valle, Claudia Ayala, Xavier Franch, and Elisa Yumi Nakagawa. 2021. Three decades of software reference architectures: A systematic mapping study. Journal of Systems and Software 179 (2021), 111004.
- <span id="page-8-6"></span>[14] Olaf Hartig. 2022. "Reflections on Linked Data Querying and other Related Topics". [https://olafhartig.de/slides/Slides-DKG-SWSA-Talk.pdf.](https://olafhartig.de/slides/Slides-DKG-SWSA-Talk.pdf) Accessed: 2022-03-17.
- <span id="page-8-34"></span>[15] James Hendler, Fabien Gandon, and Dean Allemang. 2020. Semantic web for the working ontologist: Effective modeling for linked data, RDFS, and OWL. Morgan & Claypool.
- <span id="page-8-26"></span>[16] James A Hendler. 2009. Tonight's Dessert: Semantic Web Layer Cakes. In European Semantic Web Conference. Springer, 1–1.
- <span id="page-8-7"></span>[17] Aidan Hogan. 2020. The semantic web: Two decades on. Semantic Web 11, 1 (2020), 169–185.
- <span id="page-8-29"></span>[18] Filip Ilievski, Wouter Beek, Marieke van Erp, Laurens Rietveld, and Stefan Schlobach. 2016. LOTUS: Adaptive text search for big linked data. In European Semantic Web Conference. Springer, 470–485.
- <span id="page-8-8"></span>[19] Filip Ilievski, Daniel Garijo, Hans Chalupsky, Naren Teja Divvala, Yixiang Yao, Craig Rogers, Ronpeng Li, Jun Liu, Amandeep Singh, Daniel Schwabe, and Pedro Szekely. 2020. KGTK: a toolkit for large knowledge graph manipulation and analysis. In International Semantic Web Conference. Springer, Cham, 278–293.
- <span id="page-8-39"></span>[20] Sebastian Lobentanzer, Patrick Aloy, Jan Baumbach, Balazs Bohar, Katharina Danhauser, Tunca Doğan, Johann Dreo, Ian Dunham, Adrià Fernandez-Torras, Benjamin M Gyori, et al. 2022. Democratising Knowledge Representation with BioCypher. arXiv preprint arXiv:2212.13543 (2022).
- <span id="page-8-30"></span>[21] Mengtao Lyu, Xinyu Li, and Chun-Hsien Chen. 2022. Achieving Knowledge-as-a-Service in IIoT-driven smart manufacturing: A crowdsourcing-based continuous enrichment method for Industrial Knowledge Graph. Advanced Engineering Informatics 51 (2022), 101494.
- <span id="page-8-15"></span>[22] Silverio Martínez-Fernández, Claudia P Ayala, Xavier Franch, and Helena Martins Marques. 2017. Benefits and drawbacks of software reference architectures: A case study. Information and software technology 88 (2017), 37–52.
- <span id="page-8-14"></span>[23] Silverio Martinez-Fernandez, Paulo Sergio Medeiros Dos Santos, Claudia P Ayala, Xavier Franch, and Guilherme H Travassos. 2015. Aggregating empirical evidence about the benefits and drawbacks of software reference architectures. In 2015 ACM/IEEE international symposium on empirical software engineering and measurement (ESEM). IEEE, 1–10.
- <span id="page-8-17"></span>[24] Sergi Nadal, Victor Herrero, Oscar Romero, Alberto Abelló, Xavier Franch, Stijn Vansummeren, and Danilo Valerio. 2017. A software reference architecture for

semantic-aware Big Data systems. Information and software technology 90 (2017), 75–92.

- <span id="page-8-23"></span>[25] Elisa Y Nakagawa, Fabiano C Ferrari, Mariela MF Sasaki, and José C Maldonado. 2011. An aspect-oriented reference architecture for software engineering environments. Journal of Systems and Software 84, 10 (2011), 1670–1684.
- <span id="page-8-9"></span>[26] Allen Newell, John Calman Shaw, and Herbert A Simon. 1958. Elements of a theory of human problem solving. Psychological review 65, 3 (1958), 151.
- <span id="page-8-35"></span>[27] Natasha Noy, Yuqing Gao, Anshu Jain, Anant Narayanan, Alan Patterson, and Jamie Taylor. 2019. Industry-scale Knowledge Graphs: Lessons and Challenges: Five diverse technology companies show how it's done. Queue 17, 2 (2019), 48–75.
- <span id="page-8-33"></span>[28] Natalya Fridman Noy, Monica Crubézy, Ray W Fergerson, Holger Knublauch, Samson W Tu, Jennifer Vendetti, and Mark A Musen. 2003. Protégé-2000: an open-source ontology-development and knowledge-acquisition environment.. In AMIA... annual symposium proceedings. AMIA Symposium. 953–953.
- <span id="page-8-18"></span>[29] Pekka Pääkkönen and Daniel Pakkala. 2020. Extending reference architecture of big data systems towards machine learning in edge computing environments. Journal of Big Data 7, 1 (2020), 1–29.
- <span id="page-8-10"></span>[30] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint arXiv:1909.01066 (2019).
- <span id="page-8-0"></span>[31] F.P. Ramsey. 1929. Knowledge. In F.P. Ramsey: Philosophical Papers. Cambridge University Press, 110–111.
- <span id="page-8-11"></span>[32] August Th Schreiber, Guus Schreiber, Hans Akkermans, Anjo Anjewierden, Nigel Shadbolt, Robert de Hoog, Walter Van de Velde, and Bob Wielinga. 2000. Knowledge engineering and management: the CommonKADS methodology. MIT press.
- <span id="page-8-38"></span>[33] Gyte Tamašauskait ˙ e and Paul Groth. 2022. Defining a Knowledge Graph Devel- ˙ opment Process Through a Systematic Review. ACM Transactions on Software Engineering and Methodology (2022).
- <span id="page-8-22"></span>[34] Richard N Taylor, Nenad Medvidović, and Eric M Dashofy. 2010. Software architecture: foundations, theory, and practice. John Wiley & Sons, Inc.
- <span id="page-8-12"></span>[35] WDQS Search Team. 2022. "WDQS Backend Alternatives: The Process, Details and Results". [https://www.wikidata.org/wiki/File:WDQS\\_Backend\\_Alternatives\\_](https://www.wikidata.org/wiki/File:WDQS_Backend_Alternatives_working_paper.pdf) [working\\_paper.pdf.](https://www.wikidata.org/wiki/File:WDQS_Backend_Alternatives_working_paper.pdf) Accessed: 2022-08-15.
- <span id="page-8-27"></span>[36] Karim Tharani. 2021. Much more than a mere technology: A systematic review of Wikidata in libraries. The Journal of Academic Librarianship 47, 2 (2021), 102326.
- <span id="page-8-13"></span>[37] Google Books Ngram Viewer. 2022. "Percentage of n-grams in books published in English between 1955 and 2019 that are 'expert systems', 'ontology', 'metadata', or 'machine learning'.". [https://books.google.com/ngrams/graph?content=](https://books.google.com/ngrams/graph?content=expert+systems%2Contology%2Cmachine+learning%2Cmetadata&year_start=1955&year_end=2019&corpus=26&smoothing=1) [expert+systems%2Contology%2Cmachine+learning%2Cmetadata&year\\_start=](https://books.google.com/ngrams/graph?content=expert+systems%2Contology%2Cmachine+learning%2Cmetadata&year_start=1955&year_end=2019&corpus=26&smoothing=1) [1955&year\\_end=2019&corpus=26&smoothing=1.](https://books.google.com/ngrams/graph?content=expert+systems%2Contology%2Cmachine+learning%2Cmetadata&year_start=1955&year_end=2019&corpus=26&smoothing=1) Accessed: 2022-08-10.
- <span id="page-8-37"></span>[38] Haohan Wang and Bhiksha Raj. 2017. On the origin of deep learning. arXiv preprint arXiv:1702.07800 (2017).
- <span id="page-8-32"></span>[39] Haris Widjaja, Kiril Gashteovski, Wiem Ben Rim, Pengfei Liu, Christopher Malon, Daniel Ruffinelli, Carolin Lawrence, and Graham Neubig. 2022. Kgxboard: Explainable and interactive leaderboard for evaluation of knowledge graph completion models. arXiv preprint arXiv:2208.11024 (2022).
- <span id="page-8-28"></span>[40] Nasser Zalmout, Chenwei Zhang, Xian Li, Yan Liang, and Xin Luna Dong. 2021. All You Need to Know to Build a Product Knowledge Graph. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 4090–4091.

## A APPENDICES

# A.1 Tamašauskaite and Groth's knowledge ˙ graph development process

Reliability (Q01) - If the desired knowledge graph is generic, then it covers multiple domains, and is publicly available. Otherwise, if it is domain-specific, then it is commonly used in organizations for their operations. In either case, the data collected from various sources may not be of good quality. After performing the Extract knowledge step from the architecture, the next step is Process Knowledge where one of the sub-steps is Complete Knowledge which focuses on enriching the knowledge in the knowledge graph as well as improving the overall quality. This includes performing reasoning and inference, validating the triples, and optimizing the knowledge graph. Basically, it makes sure that only valid and relevant knowledge is included in the knowledge graph. Additionally, removing nodes that are not relevant to the domain and eliminating conflicts and gaps in the knowledge graph. In summary, the

knowledge produced by the knowledge engineering process can be trusted to be true and justified.

Efficiency (Q02) - The pipeline aims at guiding developers to build knowledge graphs which means that developers have the ability to make decisions, and choose the desired and most efficient algorithms in each step of the KG development process. For example, in Identify Step - For data acquisition, efficient algorithms like Focused crawling, Page Rank Algorithm, and A\* Algorithm can be used. For database harvesting, data mining techniques like K-means clustering, Expectation-Maximization Algorithm, and kNN can be utilized. Extract Knowledge - In order to extract entities, techniques such as named-entity recognition (NER), sequence labeling, and word embeddings can be utilized. Similarly, in order to extract relations and attributes, methods such as neural information extraction, open information extraction, and word embeddings can be utilized. Process Knowledge - ML models (Bert, Ensemble learning, CRFs), feature vector similarity methods, and sorted neighborhoods are some efficient methods that can be used for entity resolution in order to remove duplicates and eliminate ambiguity. In summary, knowledge produced by the pipeline can be applied in a computationally tractable and efficient manner.

Domain specificity (Q03) - The Tamašauskaite and Groth ˙ knowledge graph development process is evaluated with 2 different types of KG - generic open KG (DBpedia) and domain-specific KG (User Experience Practices Knowledge Graph) to see to what extent the process is suitable and relevant to real-life scenarios. As a result, both the KG's process is similar to the proposed architecture. Hence, the knowledge engineering process is not tailored to use cases associated with a specific domain or area of expertise but rather supports both generic open and domain-specific KG.

Scalability (Q04) - The pipeline consists of processing steps such as - extracting knowledge, processing knowledge, and constructing a knowledge graph. The data collected from the data acquisition component can be stored in a NoSQL database system as they are capable of storing and processing big data. Scalability is achieved by spreading the storage of data and the work to process the data over a large cluster of machines. Hence, the knowledge engineering process scales economically with the amount of knowledge produced (measured in terms of, e.g. rules, triples, nodes, edges, etc.)

Distributivity (Q05) - The pipeline consists of processing steps such as - extracting knowledge, processing knowledge, and constructing a knowledge graph. The data collected from the data acquisition component can be stored in a NoSQL database system as it is a distributed system where several machines work together in clusters. Each piece of data is replicated over those machines to deliver redundancy and high availability. Hence, the knowledge produced by the knowledge engineering process can be distributed and hosted across multiple sites.

Interoperability (Q06) - The key to interoperability is standardization. Steps such as process knowledge, complete knowledge, and storing KG in a relational database system ensure efforts to achieve standardization. The knowledge produced by the knowledge engineering process can be easily shared across sites and applications if the data is stored in relational databases. Whereas for non-relational databases, the process of migration can be more complex as they have different data models and query languages. Hence, the knowledge produced by the knowledge engineering process can be easily shared across sites and applications.

Supports knowledge engineering best practices (Q07) - The methodology adheres to standard knowledge engineering methodology as it involves the usage of domain experts to construct ontology, is a structured approach in terms of knowledge acquisition, representation, validation, and implementation, and can be tailored to specific use cases, ensures knowledge quality, ensures maintenance of the KG, and involves usage of appropriate tools and techniques to construct KG. While mapping/constructing the ontology, the pipeline makes sure that the good practices of ontology development are followed. Hence, the knowledge engineering process adheres to the methodologies for creating an ontology and elicitation of knowledge from subject matter experts towards the creation of a knowledge model.

Accessibility (Q08) - Although the steps in the proposed architecture are provided in a very well-structured, clear format, it has no framework that users can install and work with, due to which it does not address accessibility. Hence, the barrier to adoption by users of the knowledge engineering process is high.

Supports software engineering best practices (Q09) - The proposed process is more suited for initial knowledge graph development, where it is necessary to determine the data and the structure of the knowledge graph rather than the continuous development of knowledge graphs. Additionally, the process does not support metadata management, temporal aspects, versioning, and incremental updates. Hence, the knowledge engineering process does not conform to software industry norms (e.g. the use of agile methodologies, continuous integration, and deployment, version control, automated testing, automated vulnerability scans, etc.)

Robustness to noise (Q10) - In the Identify Data step, the authors highlight the importance of identifying the data sources as it influences the overall KG development process. Additionally, they also extract knowledge, process knowledge, and finally construct the KG by integrating and completing knowledge. This includes performing reasoning and inference, validating the triples, and optimizing the knowledge graph. Hence, the knowledge production process is robust in the face of noise and/or adversarial manipulation of source data and/or knowledge.

Robustness to missing schema (Q11) - As the architecture involves the development of ontology to represent domain knowledge, utilizing non-relational database systems to store KG, and leveraging machine learning to extract and process knowledge i.e to complete knowledge, it is more robust to the missing schema. Hence, the knowledge produced by the knowledge engineering process can be processed and/or accessed in the face of incomplete schemas and/or knowledge organization systems.

Openness (Q12) - The implementation of the algorithms utilized in the various components of the architecture is not specified. It is possible that these algorithms may or may not be open-source. Hence, the components of the knowledge engineering process are mostly not implemented using open-source software, with open standards, making the knowledge produced by the knowledge engineering process not openly accessible.

Comprehensiveness (Q13) - The main sections and the subsections of the proposed architecture include Identify Data, Construct KG Ontology, Extract Knowledge (Extract entities, extract

relations, extract attributes), Process Knowledge (Integrate Knowledge, Construct or Map ontology, Complete Knowledge), Construct Knowledge Graph (Store KG, Display KG, Enable use), and Maintain KG (Evaluate KG, Update KG). Hence, all components of an end-to-end knowledge engineering process (e.g. data ingest/export, data transformation, inference, knowledge publishing, etc.) are supported.

Customizability (Q14) - While evaluating the proposed architecture against DBpedia and User Experience Practices Knowledge Graph, several components were modified to support specific use cases. Data were identified from various Wikimedia project sources for DBpedia, and using Google Forms sources for UEPKG. DBpedia extracts knowledge using DBpedia Information Extraction Framework (DIEF), whereas UEPKG utilizes ETL strategy to extract knowledge. The constructed KG in DBpedia is evaluated using community reviews, contributions, and feedback whereas UEPKG is evaluated based on user input, traffic analytics, and search analytics. Hence, the components of the knowledge engineering process can be modified to support specific use cases.

Modularity (Q15) - Although the architecture allows composing modules i.e breaking the pipeline down into smaller, independent components or modules, it does not support a simple representation format that all modules in the architecture operate on, and does not include a comprehensive set of features - import and export modules for a wide variety of KG formats, compute embeddings module, and compute graph statistics. Hence, the components of the knowledge engineering process cannot be selectively composed to suit a specific use case.

Domain-independence (Q16) - The proposed architecture is evaluated with 2 different types of KG - generic open KG (DBpedia) and domain-specific KG (User Experience Practices Knowledge Graph) to see to what extent the process is suitable and relevant to real-life scenarios. As a result, both the KG's process is similar to the proposed architecture. Hence, the knowledge engineering process is generally applicable across a wide range of domains and areas of expertise.

Extensibility (Q17) - Update the Knowledge Graph step in the Maintain the Knowledge Graph component elaborates upon updating the KG when there is a new data source relevant to the knowledge domain. But once, the new data is identified, the process is repeated from step 1 of the architecture. In summary, knowledge extraction from data or natural language performed in the knowledge engineering process can easily accommodate new sources and modalities of data or natural language.

Curatability (Q18) - After performing the Extract knowledge step from the Tamašauskaite and Groth knowledge graph develop- ˙ ment process, the next step is Process Knowledge where one of the sub-steps is Complete Knowledge which focuses on enriching the knowledge in the knowledge graph as well as improving the overall quality. This includes performing reasoning and inference, validating the triples, and optimizing the knowledge graph. Basically, it makes sure that only valid and relevant knowledge is included in the knowledge graph. Additionally, removing nodes that are not relevant to the domain and eliminating conflicts and gaps in the knowledge graph. As manual curation is not explicitly mentioned, the knowledge engineering process does not support the human curation of automatically extracted and/or inferred knowledge.

Ethicality (Q19) - The external algorithms utilized in each component of the architecture may or may not ensure legal compliance, collaborative and transparent development, community oversight, and quality control for ethical use. Hence, the knowledge engineering process does not support compliance with and enforcement of policies and/or guidelines for ethical use.

Sustainability (Q20) - The 2nd step of the process is Construct the KG Ontology that provides a top-level structure of the knowledge graph where ontologies are constructed if the domain is narrow and this can be expensive and labor intensive. The next steps in the process are Extract Knowledge, and Process Knowledge which utilize machine learning models i.e large language models to extract and process knowledge. Large language models require large amounts of data to perform well and again this would involve labor and ultimately be expensive. Hence, the cost of executing the knowledge engineering process is not economically sustainable for the given use case.

Affordability (Q21) - As the proposed architecture does not have a dedicated framework, the start-up cost cannot be computed and ultimately cannot be evaluated against various affordability factors such as infrastructural setup, training, or support. Hence the cost of access to the knowledge engineering process is not economically affordable for a given user community.

Editability (Q22) - The Process Knowledge step ensures that the knowledge extracted is of high quality. Although automated techniques are used to perform knowledge integration, eliminating redundancy, contradiction, and ambiguity, it is possible that these models might be inaccurate based on multiple factors and hence not perform well. As the manual editing feature is not explicitly mentioned, the knowledge produced by the knowledge engineering process cannot be feasibly edited by humans.

Explainability (Q23) - Although each component in the process uses machine learning algorithms that are simple and efficient, incorporating feedback to evaluate the KG, it does not incorporate different stakeholders, or clear documentation steps, and also some of the algorithms might be black-box and non-interpretable. Hence, it does not provide accountability for provenance and the details of how it was produced, it is not explainable.

# A.2 BioCypher

Reliability (Q01) - BioCypher enables the creation of task-specific KG using a modular approach where the KG depends on consistent and comprehensive annotations of major actors in the biomedical community. The automated end-to-end testing of millions of entities and relationships per KG increases trust in the consistency of the data. Additionally, the BioCypher migration is tested end-to-end, including deduplication of entities and relationships as well as verbose information on violations of the desired structure (e.g., due to inconsistencies in the input data). Lastly, during the creation of KG, evidence (which experiment and publication the knowledge is derived from) and provenance (who provided which aspects of the primary data) are always propagated. Hence, the knowledge engineering process is reliable, i.e. the knowledge produced can be trusted to be true and justified.

Efficiency (Q02) - The translation framework of BioCypher is very fast and makes it easy to create custom knowledge graphs by using adapters for data ingestion and a schema configuration for graph structure and ontology mappings. With an existing configuration, it takes only a few minutes to build a knowledge graph that is specific to a task, and even starting from scratch can be done in just a few days of work. This allows for rapid prototyping and automated machine learning (ML) pipelines that iterate the KG structure to optimize predictive performance. In order to achieve high performance, property graph database technology is implemented that provides an intuitive query interface. All in all, the knowledge produced by the knowledge engineering process can be applied in a computationally tractable and efficient manner.

Domain specificity (Q03) - BioCypher is a FAIR (findable, accessible, interoperable, reusable) framework that transparently builds biomedical KGs while preserving the provenances of the source data. The framework demonstrates usefulness through the use cases that focus mainly on the maintenance of task-specific knowledge stores, interoperability between biomedical domains, to on-demand building of task-specific knowledge graphs for federated learning. The framework does not focus on integrating generic open knowledge but rather focuses on domain-specific knowledge. Hence, the knowledge engineering process is tailored to use cases associated with a specific domain or area of expertise.

Scalability (Q04) - As the amount of biomedical data grows larger, integrated analysis pipelines become more extensive and, consequently, more expensive. To ensure the success of various systems biomedicine projects, it is important to have a flexible approach to managing and analyzing large knowledge sets. Bio-Cypher achieves scalability by separating data storage and analysis, allowing each component to be scaled individually, while using distributed computing infrastructure, such as computing clusters, to perform both tasks in close proximity. It facilitates the maintenance of Sherlock software that along with a configuration enables the project database to be upscaled to an arbitrary number of nodes on an in-house or commercial cluster just as the project requires. Hence, the knowledge engineering process scales economically with the amount of knowledge produced (measured in terms of, e.g. rules, triples, nodes, edges, etc.)

Distributivity (Q05) - BioCypher achieves distributivity by separating data storage and analysis, allowing each component to be scaled individually, while using distributed computing infrastructure, such as computing clusters, to perform both tasks in close proximity. Additionally, BioCypher facilitates federated machine learning by providing an unambiguous blueprint for the process of mapping input data to ontology. After organizers determine the schema for a particular machine learning project, the BioCypher schema setup can be distributed, guaranteeing consistent database organization across all training instances. Hence, the knowledge produced by the knowledge engineering process can be distributed and hosted across multiple sites.

Interoperability (Q06) - BioCypher is an ETL pipeline with a focus on interoperability in biomedicine. It is a FAIR (findable, accessible, interoperable, reusable) framework to transparently build biomedical knowledge graphs while mapping knowledge onto biomedical ontologies serving the purpose of achieving harmonization. BioCypher framework incorporates multiple resources such as OmniPath, CKG, CROssBAR v2, Bioteque, and Dependency Map KG. The mapping of each of these knowledge collections onto

the same ontological framework ultimately ensures interoperability across various biomedical domains. Hence, the knowledge produced by the knowledge engineering process can be easily shared across sites and applications.

Supports knowledge engineering best practices (Q07) - Bio-Cypher facilitates the harmonization of datasets using ontology mapping and also provides ways of updating the ontology but does not focus on ontology development. Secondly, the process does not mainly focus on knowledge acquisition which is a key constituent in the knowledge engineering process. Hence, the knowledge engineering process does not adhere to the methodologies for creating an ontology and elicitation of knowledge from subject matter experts towards the creation of a knowledge model.

Accessibility (Q08) - BioCypher is a framework that adheres to the FAIR principles, meaning it is designed to be findable, accessible, interoperable, and reusable. It is utilized to construct biomedical knowledge graphs in a transparent manner. By mapping the knowledge onto biomedical ontologies, BioCypher ensures harmonization, promotes human and machine readability, and facilitates access to non-specialist researchers. It increases accessibility to the community by creating user-friendly interfaces using open standards. They explicitly mention there is no framework that provides easy access to state-of-the-art KGs to the "average" biomedical researcher, a gap that BioCypher aims to fill. In summary, the barrier to adoption by users of the knowledge engineering process is low.

Supports software engineering best practices (Q09) - The sustainability of research software is closely tied to the level of adoption and contributions from the community. BioCypher is opensource software that employs modern continuous integration and deployment methods, and it has a diverse community of researchers and developers involved from the outset. This approach enables the creation of resilient workflows that are thoroughly tested end-toend, ensuring the integrity of the scientific data. Hence, the knowledge engineering process conforms to software industry norms (e.g. the use of agile methodologies, continuous integration and deployment, version control, automated testing, automated vulnerability scans, etc.)

Robustness to noise (Q10) - BioLink and BioRegistry are 2 important pieces of BioCypher. BioLink is a framework that is used to represent biomedical concepts and their relationships. BioRegistry is another resource that provides a registry of consistent vocabularies for these concepts and also offers validation of identifiers. Additionally, performing automated testing that covers all aspects of a knowledge graph, including millions of entities and relationships, boosts confidence in the consistency of the data. Hence, the knowledge production process is robust in the face of noise and/or adversarial manipulation of source data and/or knowledge.

Robustness to missing schema (Q11) - BioCypher uses a BioLink model as a comprehensive and generic biomedical ontology. In cases where it may be necessary, this ontology can be substituted with more targeted and purpose-driven ontologies or enhanced to better align with the specific needs of a particular task. It uses adapters to create task-specific KG. If for a resource, an adapter doesn't exist, it uses a schema configuration file to mediate between the structure of the input data and the resulting BioCypher KG structure. As the presence of a schema configuration and the absence of mentioning of non-relational database systems makes it

difficult to handle missing schema. Hence, the knowledge produced by the knowledge engineering process cannot be processed and/or accessed in the face of incomplete schemas and/or knowledge organization systems.

Openness (Q12) - BioCypher is open-source software that employs modern techniques of continuous integration and deployment. From the outset, it has attracted a diverse group of researchers and developers who collaborate on the development of the project. The integration approach includes a systematic and thorough biomedical ontology, known as the BioLink model, as well as a comprehensive registry and resolver for biomedical identifier resources, referred to as the BioRegistry. Both projects, like BioCypher, are open-source and community-driven. Hence, the components of the knowledge engineering process are implemented using opensource software, with open standards, and the knowledge produced by the knowledge engineering process is openly accessible.

Comprehensiveness (Q13) - BioCypher is an extract-transformload pipeline that emphasizes interoperability within the field of biomedicine. Although it incorporates processes such as knowledge representation, knowledge implementation, and knowledge maintenance, it does not explicitly include knowledge acquisition and knowledge evaluation. All in all, all components of an end-to-end knowledge engineering process (e.g. data ingest/export, data transformation, inference, knowledge publishing, etc.) are not supported.

Customizability (Q14) - The authors utilize the ontology manipulation capabilities offered by BioCypher to extend certain branches of the broad but fundamental BioLink ontology, where it is advantageous to have more detailed information about the data that is integrated into the knowledge graph. For example in the Tumour Board use case, the schema of BioLink has a single, general "sequence variant" class, which does not provide much detail. To address this, a detailed subtree from the Sequence Ontology (SO) to the BioLink ontology at this node was added, resulting in a hybrid ontology that combines the generality of BioLink with the precision of a specialized sequence variant ontology. Additionally, BioCypher enables the creation of a subset of the entire knowledge collection in a fast and straightforward manner, ensuring that sensitive, irrelevant, or unlicensed data is not included. In summary, the components of the knowledge engineering process can be modified to support specific use cases.

Modularity (Q15) - BioCypher uses a modular approach for maintaining multiple task-specific knowledge graphs (KGs) from overlapping primary resources. It involves recombining individual data "adapters" for primary resources in a reusable manner, which simplifies maintenance by allowing centralized management of each adapter, rather than requiring primary resource maintenance within each individual KG. Additionally, the main goal is to achieve KG standardization and does include a comprehensive set of features - import and export modules for a wide variety of KG formats, and compute embedding modules. Hence, the components of the knowledge engineering process can be selectively composed to suit a specific use case.

Domain-independence (Q16) - BioCypher is a FAIR (findable, accessible, interoperable, reusable) framework that transparently builds biomedical KGs while preserving the provenances of the source data. The framework demonstrates usefulness through the use cases that focus mainly on the maintenance of task-specific

knowledge stores, interoperability between biomedical domains, to on-demand building of task-specific knowledge graphs for federated learning. The framework doesn't focus on integrating generic open knowledge but rather focuses on domain-specific knowledge. Hence, the knowledge engineering process is not generally applicable across a wide range of domains and areas of expertise.

Extensibility (Q17) - The modular structure of BioCypher offers a significant benefit of reusing already existing adapters for primary or secondary knowledge sources. In the event that an adapter is not present for a specific resource, a new adapter can be created by following the pattern of an existing adapter within the framework. Additionally, the authors utilize the ontology manipulation capabilities offered by BioCypher to extend certain branches of the broad but fundamental BioLink ontology, where it is advantageous to have more detailed information about the data that is integrated into the knowledge graph. Hence, knowledge extraction from data or natural language performed in the knowledge engineering process can easily accommodate new sources and modalities of data or natural language.

Curatability (Q18) - From the Tumour Board use case, it is quite visible that the current manual workflow for identifying actionable genetic variants involves complex database queries to various established cancer genetics databases, which is a complex and timeconsuming process. After each query, the results require curation by geneticists to ensure that there are no duplicates from different databases and to evaluate their biological relevance, level of evidence, and potential for actionability. BioCypher transforms each individual primary resource into a task-specific, integrated knowledge graph (KG). During the build process, it maps the contents of each primary resource to ontological classes, which greatly reduces the need for manual curation and harmonization of the database results. As mentioned it reduces the need for manual creation and does not explicitly mention the usage of manual curation, the knowledge engineering process does not support human curation of automatically extracted and/or inferred knowledge.

Ethicality (Q19) - The "strict mode" of BioCypher does not allow the creation of entities without associated source, license, and version parameters. As a result, BioCypher can prevent the redistribution of data whose original license does not permit it, and ensure that proper credit is given to the creators of the data. In short, a requirement in BioCypher for these parameters ensures that data is used in an ethical and legal manner. BioCypher is free software under an MIT license, openly developed and available. As it is an open-source project, it ensures legal compliance, collaborative and transparent development, community oversight, and quality control. These measures provide fairness, transparency, privacy, and accountability. Hence, the knowledge engineering process supports compliance with and enforcement of policies and/or guidelines for ethical use.

Sustainability (Q20) - BioCypher facilitates the harmonization of datasets using ontology mapping and also provides ways of updating the ontology but does not focus on ontology development using a manual process or a domain expert. Additionally, during the build process, it maps the contents of each primary resource to ontological classes, which greatly reduces the need for manual curation and harmonization of the database results. Also, it does

not involve expensive labor to create huge datasets for large language models as it uses methods such as i.e BioLink, BioRegistry, etc throughout the process. Lastly, as BioCypher utilizes Sherlock, a project database can be easily scaled up to any number of nodes as needed, which not only saves computing time but also helps to reduce costs. Hence, the cost of executing the knowledge engineering process is economically sustainable for the given use case.

Affordability (Q21) - BioCypher is an open-source project and depending upon the use case, the start-up cost may vary, and it may incur costs for infrastructural setup, training, or support. Hence the cost of access to the knowledge engineering process is not economically affordable for a given user community.

Editability (Q22) - BioCypher ensures automated end-to-end testing of millions of entities and relationships per KG increasing trust in the consistency of the data. Additionally, during the build process, it maps the contents of each primary resource to ontological classes, which greatly reduces the need for manual curation and harmonization of the database results. As editing of knowledge produced is not explicitly mentioned in the paper hence the knowledge produced by the knowledge engineering process cannot be feasibly edited by humans.

Explainability (Q23) - BioCypher is an extract-transform-load pipeline with a focus on interoperability in biomedicine. It facilitates the harmonization of datasets using ontology mapping and ensures automated end-to-end testing of millions of entities and relationships per KG, i.e., no manual interference is involved throughout the pipeline. Secondly, during the creation of KG, evidence (which experiment and publication the knowledge is derived from) and provenance (who provided which aspects of the primary data) are always propagated. Lastly, it is an open-source project and community-driven. All in all, the knowledge produced by the knowledge engineering process provides accountability with respect to provenance and the details of how it was produced (e.g. through human authoring, automated extraction, and/or inference, etc.)

## A.3 KGTK

Reliability (Q01) - The validation and clean operations under the KGTK graph curation and transformation module ensure that the knowledge produced is reliable. For example, the validate operation ensures that the data meets the KGTK file format specification, detecting errors such as nodes with empty values, values of unexpected length (either too long or too short), potential errors in strings (quotation errors, incorrect use of language tags, etc.), incorrect values in dates, etc. Similarly, the clean operation fixes a substantial number of errors detected by validate operation, by correcting some common mistakes in data encoding (such as not escaping 'pipe' characters), replacing invalid dates, normalizing values for quantities, languages, and coordinates using the KGTK convention for literals. Finally, it removes rows that still do not meet the KGTK specification (e.g., rows with empty values for required columns or rows with an invalid number of columns). Hence, the knowledge engineering process is reliable, i.e. the knowledge produced can be trusted to be true and justified.

Efficiency (Q02) - In a data-centric KG pipeline, a lot of tools are required to complete the process efficiently. If performed separately, there can be potential issues such as - some tools might

not work with large KG, and interoperating between tools require data transformation scripts. KGTK is a comprehensive library of tools and methods to enable easy composition of KG operations (validation, filtering, merging, centrality, text embeddings, etc.) to build knowledge-based AI applications and as it stores intermediate results to disk for time and memory efficiency, it results in a much shorter running reported time compared to other tools. It has a specific file format, varied transformation operations, etc which helps to complete the data-science task very efficiently. To test the efficiency of KGTK, a test was performed that filters out all the Qnodes which have P31 property in Wikidata. This process took 20 hours in Apache Jena[6](#page-13-0) and RDFlib[7](#page-13-1) , 4 hours and 15 min in Graphy[8](#page-13-2) , and just 1 hour and 30 min using KGTK. Hence, the knowledge produced by the knowledge engineering process can be applied in a computationally tractable and efficient manner.

Domain specificity (Q03) - KGTK is a data science-centric approach designed to represent, create, transform, enhance, and analyze KGs. The authors have illustrated the importance of integrating, and manipulating large open generic KGs such as Wikidata, DBpedia, and ConceptNet. Additionally, they used KGTK with the CORD-19 dataset provided by the Allen Institute for AI. They enhanced the dataset with KGs such as DBpedia, and Wikidata to incorporate gene, chemical, disease, and taxonomic information, and computing network analytics on the resulting graphs. KGTK illustrates functionalities in various domains, including scientific publications and finance, however, it is not tailored to use cases associated with any specific domain.

Scalability (Q04) - KGTK has illustrated usage by working with large KGs such as Wikidata, DBpedia, and ConceptNet. The KGTK format does not differentiate between attributes or qualifiers of nodes and edges and full-fledged edges. Instead, tools working with KGTK graphs can interpret edges in different ways as needed. In the KGTK file format, any element can be considered a node, and every node can have any type of edge connecting it to any other node. This allows KGTK files to be mapped to DBMS, for example, relational or non-relational databases, and can be utilized to achieve scalability. KGTK uses scalable SQL storage accompanied by indexing and caching mechanisms that together ensure high scalability to large KGs like Wikidata. Hence, the knowledge engineering process scales economically with the amount of knowledge produced (measured in terms of, e.g. rules, triples, nodes, edges, etc.)

Distributivity (Q05) - KGTK has illustrated usage by working with large KGs such as Wikidata, DBpedia and ConceptNet. The KGTK format does not differentiate between attributes or qualifiers of nodes and edges, and full-fledged edges. Instead, tools working with KGTK graphs can interpret edges in different ways as needed. In the KGTK file format, any element can be considered a node, and every node can have any type of edge connecting it to any other node. This allows KGTK files to be mapped to DBMS, for example, relational or non-relational databases, and can be utilized to achieve distributivity. Hence, the knowledge produced by the knowledge engineering process can be distributed and hosted across multiple sites.

<span id="page-13-0"></span><sup>6</sup>https://jena.apache.org/

<span id="page-13-1"></span><sup>7</sup>https://rdflib.readthedocs.io/en/stable/

<span id="page-13-2"></span><sup>8</sup>https://graphy.link/

Identifying and Consolidating Knowledge Engineering Requirements Conference'17, July 2017, Washington, DC, USA

Interoperability (Q06) - Interoperating between tools in a data science-centric KG construction pipeline may require developing data transformation scripts as some of them may not support the same input/output representation. KGTK uses a standard KGTK file format which is tab-separated values (TSV) to represent edge lists, making it easy to process with many off-the-shelf tools. Additionally, The knowledge produced by the knowledge engineering process can be easily shared across sites and applications if the data is stored in relational databases. Whereas for non-relational databases, the process of migration can be more complex as they have different data models and query languages. Hence, the knowledge produced by the knowledge engineering process can be easily shared across sites and applications.

Supports knowledge engineering best practices (Q07) - KGTK is a framework for manipulating, validating, and analyzing large-scale KGs. The pipeline mechanism involves multiple modules such as data ingestion, data integration, data manipulation, data validation, data cleaning, querying, analytics, and export. As the pipeline does not explicitly mention the construction of ontology, the knowledge engineering process does not adhere to the methodologies for creating an ontology and elicitation of knowledge from subject matter experts towards the creation of a knowledge model.

Accessibility (Q08) - KGTK is inspired by Scikit-learn and SpaCy, two popular toolkits for machine learning and natural language processing that have had a vast impact by making these technologies accessible to data scientists and software developers. Similarly, KGTK is a framework for manipulating, validating, and analyzing large-scale KGs that can be adopted for multi-purpose data-science operations over KGs, independently of the domain. Also, it is an open-source project and has well-documented documentation that helps users quickly get started working with KGTK. Hence, the barrier to adoption by users of the knowledge engineering process is low.

Supports software engineering best practices (Q09) - KGTK is an open-source project and it is closely tied to the level of adoption and contributions from the community. It supports features such as continuous integration and deployment, version control, and testing. Hence, the knowledge engineering process does conform to software industry norms (e.g. the use of agile methodologies, continuous integration and deployment, version control, automated testing, automated vulnerability scans, etc.)

Robustness to noise (Q10) - KGTK supports validate and clean operation which ensures the information that is added to the database at the time of data integration is valid and does not contain any potential errors. For example, the validate operation ensures that the data meets the KGTK file format specification, detecting errors such as nodes with empty values, values of unexpected length (either too long or too short), potential errors in strings (quotation errors, incorrect use of language tags, etc.), incorrect values in dates, etc. Similarly, the clean operation fixes a substantial number of errors detected by validation, by correcting some common mistakes in data encoding (such as not escaping 'pipe' characters), replacing invalid dates, normalizing values for quantities, languages, and coordinates using the KGTK convention for literals. Finally, it removes rows that still do not meet the KGTK specification (e.g., rows with empty values for required columns or rows with an invalid number of columns). Hence, the knowledge production process is robust

in the face of noise and/or adversarial manipulation of source data and/or knowledge.

Robustness to missing schema (Q11) - KGTK handles missing schema using graph curations operations such as validate and clean. The validate operation ensures that the data meets the KGTK file format specification, detecting errors such as nodes with empty values. Similarly, the clean operation fixes a substantial number of errors detected by validating the operation and removes rows that still do not meet the KGTK specification (e.g., rows with empty values for required columns or rows with an invalid number of columns). Hence, the knowledge produced by the knowledge engineering process can be processed and/or accessed in the face of incomplete schemas and/or knowledge organization systems.

Openness (Q12) - KGTK is an open-source project i.e., all the modules are developed without any external dependencies and can be utilized by any developer to their fullest extent. It is being continuously updated and new features are getting added to it. It has well-documented documentation that enables users to quickly get started with the toolkit. Hence, the components of the knowledge engineering process are implemented using open-source software, with open standards, and the knowledge produced by the knowledge engineering process is openly accessible.

Comprehensiveness (Q13) - KGTK currently supports 13 operations grouped into 4 modules - importing modules, graph manipulation modules, graph analytics modules, and exporting modules. More specifically, it supports data ingestion, data integration, data transformation, inference, and data exporting. All in all, all components of an end-to-end knowledge engineering process (e.g. data ingest/export, data transformation, inference, knowledge publishing, etc.) are supported.

Customizability (Q14) - KGTK has illustrated usage by working with large open generic KGs such as Wikidata, DBpedia, and ConceptNet and domain-specific use cases such as the CORD-19 dataset. The heterogeneity of these cases shows how KGTK can be adopted for multi-purpose data-science operations over KGs, independently of the domain. In each of these use cases, the methods employed in the pipeline are modified to suit specific needs. KGTK has a pipelining architecture based on Unix pipes that allows chaining most operations in the required fashion using the input/output commands and KGTK file format. Hence, the components of the knowledge engineering process can be modified to support specific use cases.

Modularity (Q15) - The KGTK pipeline allows users to compose modules selectively depending upon the use case requirements. Additionally, it uses a simple representation file format that all modules in the toolkit operate on to enable tool integration without additional data transformations. It has the ability to integrate mature existing tools without a need for a new implementation, contains a comprehensive set of features - import/export, transformation, analytics, etc, and uses a pipeline mechanism that allows composing modules in arbitrary ways to process large KGs. Hence, the components of the knowledge engineering process can be selectively composed to suit a specific use case.

Domain-independence (Q16) - KGTK is a data science-centric approach designed to represent, create, transform, enhance, and analyze KGs. The authors have illustrated the importance of integrating and manipulating large open generic KGs such as Wikidata, <span id="page-15-0"></span>DBpedia, and ConceptNet. Additionally, they used KGTK with the CORD-19 dataset provided by the Allen Institute for AI. They enhanced the dataset with KGs such as DBpedia, and Wikidata to incorporate gene, chemical, disease, and taxonomic information, and computing network analytics on the resulting graphs. Hence, the knowledge engineering process is generally applicable across a wide range of domains and areas of expertise.

Extensibility (Q17) - KGTK utilizes an import module to transform different input data formats into KGTK file format to ensure interoperability. One of the KGTK operations under the graph curation and transformation module is join which allows to integrate and extend the existing datasets with additional information. For example, the authors used KGTK with the CORD-19 dataset provided by the Allen Institute for AI. They enhanced the dataset with KGs such as DBpedia, and Wikidata to incorporate gene, chemical, disease, and taxonomic information, and computing network analytics on the resulting graphs. Hence, knowledge extraction from data or natural language performed in the knowledge engineering process can easily accommodate new sources and modalities of data or natural language.

Curatability (Q18) - Once the data is in KGTK format, curating operations can be performed such as cleaning and validation. The validate operation ensures that the data meets the KGTK file format specification, detecting errors such as nodes with empty values, values of unexpected length (either too long or too short), potential errors in strings (quotation errors, incorrect use of language tags, etc.), incorrect values in dates, etc. Similarly, the clean operation fixes a substantial number of errors detected by validate operation, by correcting some common mistakes in data encoding (such as not escaping 'pipe' characters), replacing invalid dates, normalizing values for quantities, languages, and coordinates using the KGTK convention for literals. Finally, it removes rows that still do not meet the KGTK specification (e.g., rows with empty values for required columns or rows with an invalid number of columns). As manual curation is not explicitly mentioned, the knowledge engineering process does not support the human curation of automatically extracted and/or inferred knowledge.

Ethicality (Q19) - As KGTK is an open-source project, it ensures legal compliance, collaborative and transparent development, community oversight, and quality control. These measures provide fairness, transparency, privacy, and accountability. Additionally, all the methods in all the modules are developed by the KGTK authors hence there is no dependency that might have affected transparency otherwise. Hence, the knowledge engineering process supports compliance with and enforcement of policies and/or guidelines for ethical use.

Sustainability (Q20) - KGTK is a data science-centric toolkit designed to represent, create, transform, enhance, and analyze large KGs. KGTK currently supports 13 operations grouped into 4 modules - importing modules, graph manipulation modules, graph analytics modules, and exporting modules. It does not involve manual development of ontology during the process nor does it require curating large sets of data for large language models (except embeddings which are optional) unlike the Tamašauskaite and Groth ˙ knowledge graph development process where ML models were required in almost every step of the process. Hence, the cost of

executing the knowledge engineering process is economically sustainable for the given use case.

Affordability (Q21) - KGTK is an open-source project and depending upon the use case, the start-up cost may vary, and it may incur costs for infrastructural setup, training, or support. Hence the cost of access to the knowledge engineering process is not economically affordable for a given user community.

Editability (Q22) - KGTK helps manipulate, curate, and analyze large real-world KGs. Throughout the pipeline it follows ways of manipulating and curating the information using operations such as validate and clean, it does not explicitly mention editing information manually by humans. Hence, the knowledge produced by the knowledge engineering process cannot be feasibly edited by humans.

Explainability (Q23) - KGTK follows a pipeline mechanism that allows composing modules in arbitrary ways to process large public KGs such as Wikidata, DBpedia, or ConceptNet which makes the entire process automated. Additionally, additional features can be passed alongside node1, label, and node2 feature columns in the KGTK file format to include provenance information about the creator of the statement and the original source. Additionally, the knowledge-creation process stores the intermediate outputs on disk, making it reproducible and easy to inspect. Hence, the knowledge produced by the knowledge engineering process provides accountability with respect to provenance and the details of how it was produced (e.g. through human authoring, automated extraction, and/or inference, etc.)
