---
cite_key: hoang_2025
title: Knowledge Graph Fusion with Large Language Models for Accurate, Explainable Manufacturing Process Planning
authors: Danny Hoang, David Gorsich, Matthew P. Castanier, Farhad Imani
year: 2025
doi: 10.48550/arXiv.2506.13026
url: https://arxiv.org/abs/2506.13026
relevancy: High
downloaded: 'Yes'
tags:
- Knowledge Graphs
- LLM
- Manufacturing
- Process Planning
- CNC Machining
- Explainable AI
tldr: Combines knowledge graphs and LLMs to enhance manufacturing process planning
  accuracy and explainability
date_processed: '2025-07-02'
phase2_processed: true
original_folder: arxiv_2025_kg_manufacturing_process_planning
images_total: 6
images_kept: 6
images_removed: 0
keywords:
- 1 introduction
- 1 knowledge graph
- 1 large language models in machining
- 2 knowledge graphs in machining
- 2 models
- 3 graph retrieval and transversal
- 3 question category
- 4 large language model generation
- DeepMind
- ElKharboutly
- ForInsights
- GPT
- MapReduce
- PyTorch
- SolidWorks
- accuracy
- ai-driven
- al-dahle
- artificial intelligence
- bio-process
- brain-inspired
- cam-based
- cam-based cnc
- capp-oriented
- cause-effect
- closed-loop
- closed-source
- cloud-based
- diagnosis
- efficiency
---

# KNOWLEDGE GRAPH FUSION WITH LARGE LANGUAGE MODELS FOR ACCURATE, EXPLAINABLE MANUFACTURING PROCESS PLANNING

Danny Hoang

School of Mechanical, Aerospace, and Manufacturing Engineering University of Connecticut Storrs, Connecticut, USA

David Gorsich

The United States Army Combat Capabilities Development Command Ground Vehicle Systems Center Warren, Michigan, USA

## Matthew P. Castanier

The United States Army Combat Capabilities Development Command Ground Vehicle Systems Center Warren, Michigan, USA

Farhad Imani <sup>∗</sup>

School of Mechanical, Aerospace, and Manufacturing Engineering University of Connecticut Storrs, Connecticut, USA

### ABSTRACT

Precision process planning in Computer Numerical Control (CNC) machining demands rapid, contextaware decisions on tool selection, feed-speed pairs, and multi-axis routing, placing immense cognitive and procedural burdens on engineers from design specification through final part inspection. Conventional rule-based computer-aided process planning and knowledge-engineering shells freeze domain know-how into static tables, which become limited when dealing with unseen topologies, novel material states, shifting cost-quality-sustainability weightings, or shop-floor constraints such as tool unavailability and energy caps. Large language models (LLMs) promise flexible, instruction-driven reasoning for tasks such as G-code synthesis to spindle-load queries, but they routinely hallucinate numeric values and provide no provenance. We present Augmented Retrieval Knowledge Network Enhanced Search & Synthesis (ARKNESS), the end-to-end framework that fuses zero-shot Knowledge Graph (KG) construction with retrieval-augmented generation to deliver verifiable, numerically exact answers for CNC process planning. ARKNESS (1) automatically distills heterogeneous machining documents, handbooks, G-code annotations, and vendor datasheets into augmented triple, multi-relational graphs without manual labeling, and (2) couples any on-prem LLM with a retriever that injects the minimal, evidence-linked subgraph needed to answer a query. Benchmarked on 155 industry-curated questions spanning tool sizing, feed-speed optimization, and tolerance diagnostics, a lightweight 3B-parameter Llama-3 augmented by ARKNESS matches GPT-4o accuracy while achieving a +25 Percentage Point (pp) gain in multiple-choice accuracy, +22.4 pp in F1, and 8.1× ROUGE-L on open-ended responses. Additionally, by grounding reasoning in precise triples, ARK-

DISTRIBUTION STATEMENT A. Approved for public release; distribution is unlimited. OPSEC9768

<sup>∗</sup>Corresponding author: farhad.imani@uconn.edu

NESS enables smaller models to match or exceed the accuracy of much larger cloud models, reducing numeric hallucinations by 22 pp, and running fully on-premise for privacy-preserving, real-time inference on the shop floor.

**Keywords:** Large Language Model · Retrieval Augmented Generation · Knowledge Graph, · Manufacturing Process Planning

## 1 Introduction

Modern production processes, such as precision machining, demands accuracy margins and tolerates virtually no numerical error, imposing substantial mental and operational loads on engineers from initial design specifications to the final part evaluation [\[1,](#page-17-0) [2,](#page-17-1) [3\]](#page-17-2). In high-stakes sectors, such as aerospace and energy, dimensional tolerances routinely tighten to ± 5 µm, while surface-integrity constraints, white-layer thickness, and residual stress, must remain within narrowly defined thresholds [\[4\]](#page-17-3). To meet these criteria, machinists must simultaneously optimize a variety of intertwined process parameters, such as tool type, feed rate, spindle speed, and depth of cut, carefully trading off cycle time against surface finish, tool wear, and geometric fidelity [\[5,](#page-17-4) [6,](#page-18-0) [7\]](#page-18-1). An incorrect decimal drill size or an outdated cutting-speed chart therefore translates directly into scrap, rework or latent defects that may only become apparent after catastrophic failure [\[8\]](#page-18-2). Such unplanned downtime erodes an estimated 11% of annual revenue for Fortune Global 500 manufacturers, roughly US \$1.5 trillion lost each year [\[9\]](#page-18-3).

Computer-Aided-Process-Planning (CAPP) engines and the Computer-Aided Manufacturing (CAM) modules bundled within mainstream Computer-Aided Design (CAD) suites (e.g., Siemens NX, CATIA, and SolidWorks) form automation pipeline that ingests STEP-compliant product models (AP203/242) and emits executable Numerical Control (NC) results [\[10\]](#page-18-4). A dedicated CAPP system executes the high-level analysis loop, geometry interrogation, feature recognition, precedence, constraint resolution, raw-stock/fixture/machine assignment, and operation-graph synthesis, before compiling feeds, speeds, and tool-change cycles into ISO6983 or STEP-NC. Embedded CAM modules then refine the middle layers: selecting cutters from vendor libraries, applying parameter presets, generating and simulating toolpath with collision checks and material-removal estimates, and invoking feature-based machining or knowledge-based-engineering rules for common prismatic features. Collectively, these rule-driven frameworks can compress planning lead time by 2–10×, yet they share a structural brittleness. Hard-coded heuristics falter when faced with unseen topologies (e.g., self-intersecting free-form pockets), novel material states, shifting cost–quality–sustainability weightings, or shop-floor constraints, such as tool unavailability and energy caps. Moreover, neither tier natively ingests telemetry (e.g., cutting force, vibration, and thermal imagery) generated by machining cells, forcing human experts to intervene, reconcile documentation, and manually retune parameters, an increasingly untenable bottleneck as geometrical complexity and throughput requirements escalate.

Large Language Models (LLMs) have emerged as versatile, instruction-following agents capable of synthesizing domain knowledge, reasoning under uncertainty, and generating executable results [\[11\]](#page-18-5). Recent studies demonstrate their ability to synthesize 3-axis drilling paths, to answer contextual queries about spindle loads and feed rates, and to troubleshoot controller alarms with human-level accuracy in descriptive tasks [\[12\]](#page-18-6). Šket et al. benchmarked GPT-3.5 and GPT-4 across independent G-code generation, self-interpretation, and error simplification; GPT-4 rendered more correct toolpath yet remained limited to simple drilling operations and required extensive prompt steering [\[12\]](#page-18-6). To broaden functionality beyond code synthesis, ChatCNC was introduced, coupling multiple LLM agents with real-time machine telemetry so that an operator can, for example, ask "What was the spindle load at 09:32?"; the system attained 93.3% accuracy on complex production-tracking queries [\[13\]](#page-18-7). Nevertheless, both studies expose two systemic bottlenecks: first, dependence on precise user prompts or narrowly structured SQL retrieval, and second, fragile performance when required context is absent from the local database.

Domain-specific fine-tuning offers one mitigation strategy. Rosati et al. raised ROUGE-1 scores of Llama-3 from 0.164 to 0.314 using industrial user manuals [\[14\]](#page-18-8). CNCGPT fine-tuned GPT-3.5 Turbo on machine instructions and trouble-shooting logs, boosting factual correctness from ROUGE-L 0.296 to 0.692 [\[15\]](#page-18-9). Yet intensive fine-tuning presents new trade-offs: (1) escalating computational cost as the query-response inventory grows into the thousands; (2) rapid obsolescence that mandates frequent retraining; and (3) data-sovereignty concerns when proprietary part files or production metrics traverse external cloud APIs. Hence, a complementary mechanism is required, one that supplies rich, verifiable context to an LLM without chronically retraining the model or exposing sensitive data.

Knowledge Graphs (KGs), on the other hand, provide a structured, semantics-rich foundation for capturing machining know-how as typed entities (features, tools, operations, machines) and relations (e.g., requiresTool, precedes, causes) [\[16,](#page-18-10) [17,](#page-18-11) [18\]](#page-18-12). For CAPP, Wang et al. generated a feature-machining KG and proposed a cosine-similarity retrieval that chose industry-valid hole-machining schemes with 0.8450 similarity to expert templates [\[19\]](#page-18-13). Guo et al. organized

![](_page_2_Figure_1.jpeg)
<!-- Image Description: The image presents a comparison of knowledge representation using a knowledge graph and a large language model (LLM). A knowledge graph visually depicts relationships between entities (e.g., twist drill, tap drill, clearance height) involved in machining an impeller blade. A user query exemplifies the graph's functionality. In contrast, the LLM section shows an example of an incorrect response to a similar query, highlighting knowledge gaps in the LLM's reasoning, including hallucinations and lack of explicit knowledge traceability. The knowledge gaps are identified for both representations. -->

<span id="page-2-0"></span>**Figure 1:** Illustration contrasting a knowledge graph (top) and a large language model (bottom) handling the same tap-drill sizing query. In the knowledge graph section, technical entities (i.e., Twist Drill, Clearance Height, Tap Drill, Titanium Nitride, 5-axis CNC Milling Machine, and Impeller Blade) are connected by explicit relations but can often be missing contextual links and undiscovered facts. The LLM section shows the model's generated answer ("8 mm drill bit"), annotated to expose imprecision risks, implicit untraceable knowledge, and domain inefficiency in specialized machining.

historical process routes by feature topology and machine capability, trimming part-routing time from 50–80 min to approximately 15 min [\[20\]](#page-18-14). Beyond planning, KGs have achieved >90% accuracy in root-cause diagnostics of rotary machinery even under missing sensor data by reasoning over causal chains [\[21\]](#page-18-15). Despite these gains, KG deployment still depends on laborious, manual curation: (1) domain experts must annotate source texts, validate triples, and periodically inject new machining knowledge; (2) Simple subject–predicate–object triples also struggle to encode context such as tolerance stack-ups, fixture constraints, or process-chain rationale for nuance components. For example, a given connection between the tool used to create thin walled geometry might omit the dependency between the optimal tool geometry corresponding the the feature size and specific tolerances required for that specific part.

Figure [1](#page-2-0) showcases these limitations by contrasting (1) a stand-alone knowledge graph that offers traceable but context-starved reasoning, returning no answer when crucial relations such as material-specific clearance or over-size rules are absent, and (2) a stand-alone LLM that confidently hallucinates an incorrect 8 mm recommendation, revealing the need for a hybrid KG and LLM architecture that is both provenance-aware and context-adaptive. This research introduces Augmented Retrieval Knowledge Network Enhanced Search and Synthesis (ARKNESS), a hybrid framework that combines LLMs with semantically enriched, domain-specific KGs that provides answers that are grounded in validated information ensuring that recommendations for machining process planning queries are both reliable and contextual relevant. In addition, by grounding LLM prompts in these rich subgraphs, ARKNESS can deploy a compact parameter model on-premise with accuracy on par with, or exceeding, much larger cloud LLMs, slashing compute and deployment overhead. The main benefits of this paper are:

1. A model-agnostic, KG-augmented CAPP assistant. We present a retrieval-augmented generation pipeline that couples any large language model with a multi-relational machining knowledge graph. A graph-aware prompt compiler injects provenance-linked triples into the context window, yielding numerically precise and explainable process-planning answers.

- 2. Self-supervised KG distillation from technical corpora. A zero-shot, GPT-based entity–relation extractor converts heterogeneous machining documents (e.g., PDF manuals, specification sheets, and NC code comments) into contextualized triples, eliminating the manual curation bottleneck that has constrained previous CAPP-oriented KGs.
- 3. Lightweight, on-prem hallucination suppression. On a 155-query benchmark, ARKNESS paired with a smaller LLM model matches or surpasses much larger cloud LLMs while reducing hallucinations, demonstrating a privacy-preserving path to real-time shop-floor deployment.

The rest of the paper is as follows: Section [2](#page-3-0) provides relevant knowledge graph and large language modeling research in manufacturing planning contexts, Section [3](#page-4-0) provides the implementation of ARKNESS for automatic graph creation for retrieval and answering, Section [4](#page-8-0) describes the experimental setup, Section [5](#page-10-0) presents the experimental results, and Section [6](#page-17-5) concludes the paper.

## <span id="page-3-0"></span>2 Research Background

## 1 Large Language Models in Machining

LLMs are rapidly transitioning from research curiosities to core enablers of AI-driven production, thanks to their capacity to parse free-form instructions, fuse heterogeneous context, and return actionable, domain-specific guidance. A recent survey by Li et al. categorized their early penetration into the manufacturing stack—spanning generative CAD modeling, bio-process recipe design, robot path planning, and vision-based quality control [\[22\]](#page-18-16). Within subtractive manufacturing, the most direct application is natural-language-to-G-code translation. Šket et al. evaluated commercial ChatGPT models for 3-axis G-code generation testing GPT-3.5 and GPT-4 in three phases including independent G-code generation, interpretation of the generated G-code, and detecting and simplifying errors [\[12\]](#page-18-6). Their results showed promise of implementing LLMs for G-code generation with GPT-4 producing more correct toolpaths but is severely limited to simple operations such as drilling. Additionally, their method relied heavily on user input to align the LLMs for generation which can lead to increased downtime if deployed in manufacturing environments. Jeon et al. expanded upon just G-code generation by developing ChatCNC that integrated various LLM agents with real-time CNC machining data [\[13\]](#page-18-7). This allowed users receive context-aware answers regarding the status of their 3-axis CNC machines such as spindle load at specific instances or times; their method achieved an accuracy of 93.3% in queries requiring complex data inference such as production tracking showcasing applications in analyzing data recorded in the manufacturing pipeline. Despite the high accuracy, the authors acknowledged that the model resulted in failures when encountered with missing context or the inability to retrieve information from their database. This suggests a more in depth search beyond conventional retrieval methods such as defined SQL database structures is required to supplement existing knowledge gaps.

Beyond prompting and traditional retrieving techniques, researchers have also introduced domain-specific fine-tuning to better align a LLMs responses with domain-specific knowledge. Rosati et al. finetuned Llama 3 for industrial applications achieving uplifts in average ROUGE-1 F1-score from 0.164 to 0.314 when trained on user manuals of a 360-degree camera [\[14\]](#page-18-8). Wang et al. finetuned GPT-3.5 for air craft maintenance outperforming general GPT-3.5 and its upgraded counterpart GPT-4.0 [\[23\]](#page-18-17). In areas specific to machining, Soundararajan et al. developed CNCGPT aimed for on-site CNC operator assistance by finetuning GPT-3.5 Turbo on machine-specific data, operational instructions, and troubleshooting assertions [\[15\]](#page-18-9). Results showed an increase in factual correctness when using ROGUE-L scores from 0.296 to 0.692 before and after finetuning, respectively. Despite the validity of their method, there are some challenges that might prevent further deployment on factory floors. One such issue is the amount of resources and training time required as the authors only trained around thirty query-response examples. In real world scenarios, there might be hundreds to thousands of scenarios and responses which can severely limit scalability during training. Moreover, as new scenarios continually emerge, the system must be frequently retrained to stay current which can further strain computational resources and complicates maintenance. There is also the challenge of security regarding production processes and information contained within. The authors used commercial LLMs which operate on external, cloud-based platforms where data flows and storage may raise concerns about data privacy and intellectual property protection. This reliance on third-party systems increases the risk of unauthorized access or data leakage, making it imperative to implement solutions that explore on-premise solutions to ensure that proprietary manufacturing data remains strictly confidential.

### 2 Knowledge Graphs in Machining

The ability to reuse and implement machining knowledge in a structured way provides not only enhanced consistency across planning and decision making but also ensures existing knowledge gaps are mitigated. Xiao et al. reviewed how computer-aided process planning involving knowledge graphs can benefit from reduced labor costs, shortened production cycles, and more intelligent use of existing information [\[16\]](#page-18-10). The authors analyzed key steps of implementing knowledge graphs from process knowledge representation to process knowledge graph construction and validation, showcasing how these methods can overcome traditional CAPP by reducing excessive manual interventions, and increasing flexibility and generalization. This implementation of process knowledge was shown in Wang et al. where they constructed a process knowledge graph for feature-based machining to automate machining scheme selection [\[19\]](#page-18-13). Through use of an improved cosine-similarity formula for machining scheme selection, they achieved a similarity score of 0.8450 closely matching existing mature schemes implemented in industry for a typical shell part composed of 6 holes. By using their method, Wang et al. argued that the recommended machining steps would reduce tool load and increase both part quality and machining safety. In a similar study, Guo et al. created a knowledge graph for process route reuse by organizing historical process plans using part feature topology and machine capabilities [\[20\]](#page-18-14). When introducing a new part, the authors determine the process route that best aligns with its feature topology through a similarity check with existing process routes. Their case study on a shaft part resulted in completing the overall machining process route in about 15 minutes, down from the typical 50 to 80 minutes, significantly enhancing efficiency.

Another major application of machining knowledge graphs involves fault diagnosis and maintenance of CNC equipment. Manufacturing systems generate heterogeneous data from sources such as sensors, logs, and maintenance reports that if left in isolation or silos can be difficult for diagnosis if problems arise. Knowledge graphs offer a unified representation by connecting physical components, signals, and failure modes in a network of cause-effect and part-whole relationships. Qiu et al. tackled this "data island" problem by constructing a multi-layered knowledge graph that integrated sensor data and domain knowledge, enabling the automatic identification of health changes in the X-axis ball screw drive system on a vertical milling tool (model XHK-5140) over 201 days, while similarity-based reasoning over the graph quantified the deviation from a healthy baseline [\[24\]](#page-18-18). Building on this idea, Cai et al. introduced a multi-level fault diagnosis KG for rotating machinery combining hierarchical knowledge of subsystem states with Bayesian reasoning, offering probabilistic inference across the graph to pinpoint root causes [\[21\]](#page-18-15). Notably, even with missing sensor outputs, their method achieved 91.1% diagnostic accuracy by leveraging the relationships between related symptoms and outperformed traditional rule-based fault diagnosis.

Despite the considerable progress in implementing knowledge graphs, there remain challenges in deploying them at scale for machining and industry. These aforementioned methods continue to depend heavily on extensive manual curation. The process of extracting, validating, and continuously updating the domain knowledge requires significant human knowledge and intervention, making it difficult to scale rapidly or adapt quickly to new information. This laborintensive approach not only increases the risk of inconsistencies or omissions but also impedes real-time responsiveness in dynamic manufacturing environments. Furthermore, the conventional reliance on simple triple-based representations constrains the ability to capture the full contextual richness necessary for nuanced decision-making in machining processes. Such representations often fall short in encoding the detailed circumstances, rationale, and intricate relationships that underlie complex manufacturing operations. As a result, they may fail to fully express how various machining operations interrelate or why specific process choices are chosen. This would thus lead to a superficial understanding of the underlying mechanics and process dependencies. These limitations highlight the need for more flexible and richly contextualized approaches in knowledge representation to support the evolving demands of modern machining applications.

## <span id="page-4-0"></span>3 Research Methodology

This section details the two main components of the ARKNESS framework with 1) Knowledge graph construction and 2) Graph transversal, retrieval and LLM response. Figure [2](#page-5-0) provides the general overview of the two components.

## 1 Knowledge Graph

A knowledge graph can be considered as a set of tuples G = {(v, r, w)} where v and w are vertices (entities) from a set of vertices V and r is an edge (relationship) from a set of relationships R. The set of tuples is commonly known as triples denoted as (*head*, *relation*, *tail*) representing v, r, and w, respectively.

### <span id="page-4-1"></span>3.2 Automated Graph Construction

The knowledge graph construction for ARKNESS is mainly done using textual data from documents relevant to the users chosen domain. Due to the heterogeneous nature of document types and file formats (e.g., .docx, .pdf, .pptx), preprocessing was first done to extract the textual information from these different sources. This was achieved through

![](_page_5_Figure_1.jpeg)
<!-- Image Description: The image presents a flowchart depicting a two-stage knowledge graph-based question answering system. The upper section shows the knowledge graph creation workflow: technical documents are processed by a large language model (LLM) to generate a knowledge graph stored in a database. The lower section illustrates the query answering process: an input query triggers a semantic search within the database, retrieves relevant triples/context, and feeds this to an LLM to produce the final answer. The flowchart uses icons to represent different components and processes. -->

<span id="page-5-0"></span>**Figure 2:** Overview of the framework with incorporating knowledge graph with large language model, illustrating the process from graph creation and storage to semantic search, match, and retrieval, and finally to language model response.

the Docling [\[25\]](#page-18-19) Python package which efficiently parses through the documents and extracts the raw text subsequently exporting the content as Markdown files for further processing.

Document splitting was done through parsing out each individual paragraph. Given a document with p individual paragraphs, each paragraph was given to a LLM to extract entities and relations forming T triples. Specific instructions were given to guide the LLM to output structured information given below:

```text
-Goal-
Extract structured triples directly from the input text in the following format:
ENTITY_1, RELATIONSHIP_TYPE, ENTITY_2, "RELATIONSHIP_DESCRIPTION"
-Steps-
1. Read the input text carefully to identify:
- Key entities: Concepts, systems, technologies, or processes central to the text.
- Relationships: Clear actions or connections described in the text that link two
entities.
- Descriptions: Verbatim or paraphrased descriptions from the text that explain the
relationship.
2. For each relationship, construct a triple:
- ENTITY_1: The primary concept or entity initiating the relationship.
- RELATIONSHIP_TYPE: The action or connection type as described in the text.
- ENTITY_2: The target concept or entity affected by ENTITY_1.
- RELATIONSHIP_DESCRIPTION: A concise description of the relationship directly sourced
from the input text.
3. Each triple must be clear and in this format:
ENTITY_1, RELATIONSHIP_TYPE, ENTITY_2, "RELATIONSHIP_DESCRIPTION"
4. Use the original text verbatim where possible for the description, ensuring accuracy.
Avoid adding external interpretations or explanations.
```text

This process was repeated for each document chosen creating G<sup>D</sup> subgraphs. The purpose of including the 'Relationship Description' in the triplet information is to retain the relevant contextual information by which the triplet was created. This removes ambiguity and often missing information found in traditional knowledge graphs. All subgraphs were then stored into respective text files which were then combined together for further processing.

A hierarchical database was then created using PostgreSQL to efficiently store and retrieve entities, relations, and their associated context. Specifically, Python and the Psycopg library was used to interface with the database where during initialization three primary tables are created: (1) subjects table: stores unique entity names with a serial primary key, (2) relations table: associates each subject with its corresponding relationships where each entry references a subject's unique identifier, ensuring relationships are correctly linked, and (3) objects table: stores ending triplet entities along with corresponding contextual information. This structure can be also represented as follows. Let S be the set of subjects (entities). For each subject s ∈ S, let R(s) be the set of relations associated with s. For each relation r ∈ R(s), let O(r) be the set of ending objects linked to r, where each object is represented as a tuple (v, c) with v representing connected entities, and c representing the corresponding context. The entire data base is thus defined by the set:

$$
\mathcal{D} = \{ \langle s, r, (v, c) \rangle | s \in S, r \in R, (v, c) \in O(r) \}
$$
\n
$$
(1)
$$

with a single triple entry represented as:

⟨s, r,(v, c)⟩ (2)

### 3 Graph Retrieval and Transversal

![](_page_6_Figure_6.jpeg)
<!-- Image Description: This image from an academic paper illustrates a knowledge graph-based question-answering system for machining processes. The left side shows knowledge graph construction from documents, including extraction, relation detection, and database storage in PostgreSQL. The right depicts graph traversal and question answering. A query about drilling parameters is processed through vector embedding, similarity checks, and ranking, culminating in a final answer retrieved from the graph database. The graph includes nodes representing materials, tools, and machines with their properties and relationships. The system leverages tagged information to enhance accuracy. -->

<span id="page-6-0"></span>**Figure 3:** Overview of the knowledge graph construction and graph transversal for user queries. The left side depicts the automated graph construction process from document selection, preprocessing, entity-relationship extraction, triplet generation and context matching, and database storage. The right side depicts knowledge graph retrieval given a user query by embedding the query to get a vector representation, performing a similarity check over the triples in the graph, and ranking the most relevant triples to give to the large language model for answer generation.

Figure [3](#page-6-0) provides an indepth illustration of how triplets are compared against a query and retrieved to be given to a LLM. Once the database is created and stored, the information contained within is incorporated to provide additional context and information to anchor subsequent questions. This is achieved through retrieving the relevant triplets and context from the knowledge graph. The retrieval process begins by first encoding a given natural language query, or question, into a vector q ∈ R d . This is achieved using a chosen semantic embedding model f : Text → R d . Similarly, each triplet t<sup>i</sup> from the database is encoded into embedding e<sup>i</sup> using the same semantic embedding model where f : t<sup>i</sup> → R d . The similarity between the query and each triple is then calculated using cosine-similarity according to:

$$
\text{sim}(q, e_i) = \frac{q \cdot e_i}{\|q\| \|e_i\|} \tag{3}
$$

This similarity score provides an evaluate of how semantically similar the query given is to each of triplet in the knowledge graph allow one to ascertain its relevance.

Given the potentially massive scale of knowledge graphs, which can reach hundred of thousands or more connections, a candidate pool C is constructed by selecting indices corresponding to a chosen a top-K triples with the highest similarity scores s<sup>i</sup> = sim(q, ei). Having only the highest similarities chosen allows the most semantically relevant triples to be selected for further processing. The candidate pool is thus defined as:

$$
C = \{i \in \{1, 2, ..., K\}\} \text{ with } s_{i_1} \ge s_{i_2} \ge \dots \ge s_{i_k} \tag{4}
$$

As mentioned previously, because a knowledge graph can contain hundred of thousands or even millions of connections, relying solely on the top-K triples could overlook additional relevant triples connected to them. To explore the extended neighborhood of these triples, we transverse through the graph by performing a beam search expansion. Let E(i) denote the set of adjacent embedded triples indexed by i (i.e., those sharing a node with triple i). We conduct a beam search recursively over a chosen maximum expansion depth dmax. Here higher depths allows one to explore deeper through the graph allowing more information to be retrieved. For an initial depth d = 0, we first set the initial candidates as C<sup>0</sup> = C. For each subsequent depth d ≥ 1 we find additional candidates using:

$$
C_d = \bigcup_{i \in C_{d-1}} \text{Top-}b\Big(\{j \in E(i) \mid \text{sim}(q, e_j)\}\Big) \tag{5}
$$

where Top-b(·) selects the chosen number of b candidates, also known as beam width, with the higher similarity scores relative to the embedded query q. Here, beam width is a chosen parameter where higher values of b allows the model to explore a wider set of candidates to expand from for each step. This can potentially capture more semantically relevant triples missed in the initial top-K. Conversely, lower values of b restricts the search to the more promising candidates which reduces computational overhead at the expense of possibly missing relevant connections that lie outside the top b candidates. In order to avoid re-processing triples, a set of visited triples E<sup>v</sup> is maintained such that the recursive expansion only considers new nodes defined as:

$$
C_{d+1} = \bigcup_{i \in C_d \backslash V} \text{Top-}b\Big(\{j \in E(i)\}\Big) \tag{6}
$$

After recursively expanding through the chosen depth and beam size, the final set of triples retrieved is defined by:

$$
C^* = \bigcup_{d=0}^{d_{\text{max}}} C_d \tag{7}
$$

As a final step after the final set of candidate triples are chosen, the corresponding context c from the database is retrieved as well. This creates a set C of relevant information:

$$
\mathcal{C} = \{ \langle s_i, r_i, (o_i, c_i) \rangle : i \in I \}
$$
\n
$$
(8)
$$

### 4 Large Language Model Generation

The last step of answering the query q involves prompt construction where the information retrieved from the knowledge graph is given to the large language model to help its answering. This is done by defining a set of system instructions that designate that the LLM is design to answer questions incorporate both its own knowledge and the given retrieved knowledge. For example, we define the system instructions as:

'You are designed to help answer questions using retrieved knowledge. Not all knowledge given

Isys = need to be used but focus on the most important information. Remember this knowledge, if there is any, to help your decision making:' (9)

Then the final prompt P given to the LLM is constructed by concatenating the system instructions, query, and retrieved knowledge graph information:

$$
P(q, \mathcal{C}) = I_{\text{sys}} \oplus \mathcal{C} \oplus q \tag{10}
$$

The final answer ALLM is then generated by the LLM:

$$
A_{\text{LLM}} = \text{LLM}(P(q, \mathcal{C}))\tag{11}
$$

## <span id="page-8-0"></span>4 Experimental Design

### 1 Knowledge Graph

To test the efficacy of ARKNESS, we first created our machining knowledge graph composed of research and technical documents spanning 5-axis milling capabilities, industrial robotic integration in CNC machining, sustainability in CNC machining, CNC programming and process planning, and fundamentals of CNC machining. By exposing ARKNESS to a spectrum of complexity levels, domain-specific terminologies, and the assembled documents enables a comprehensive evaluation of the model's CNC machining knowledge. Each document was processed using the automated graph-construction pipeline detailed in Section [3.2](#page-4-1) using GPT-4o [\[26\]](#page-18-20). An example regarding the output given the text information is given in the following box [\[27\]](#page-18-21). Descriptions of each documents chosen are listed in Table [1](#page-9-0) along with the total number of entities, relations, and triples after processing. In total, the full knowledge graph contains 4329 triples, 6659 unique entities, and 1251 unique relations providing a relatively large graph to test to test ARKNESS' capacity robust inference, and accurate knowledge retrieval across diverse CNC-machining scenarios. An example of the graph-construction output given text input is given below:

Input:

A 5-axis CNC milling machine tool is a sophisticated piece of equipment used for precision machining of complex parts with high accuracy and consistency. It has five axes of motion that can move the cutting tool in various directions to create intricate shapes and contours. The machine is designed to perform a wide range of operations, including milling, drilling, cutting, and engraving, using multiple axes of movement. A 5-axis CNC milling machine tool can create highly complex geometries using five axes of motion during machining operations that which can be difficult or impossible to be implemented by using traditional 3-axis machine.

Output:

5-AXIS CNC MILLING MACHINE TOOL, is used for, PRECISION MACHINING, "A 5-axis CNC milling machine tool is a sophisticated piece of equipment used for precision machining of complex parts with high accuracy and consistency." 5-AXIS CNC MILLING MACHINE TOOL, has, FIVE AXES OF MOTION, "It has five axes of motion that can move the cutting tool in various directions to create intricate shapes and contours." 5-AXIS CNC MILLING MACHINE TOOL, performs, "MILLING, DRILLING, CUTTING, AND ENGRAVING", " The machine is designed to perform a wide range of operations, including milling, drilling, cutting, and engraving, using multiple axes of movement." 5-AXIS CNC MILLING MACHINE TOOL, creates, HIGHLY COMPLEX GEOMETRIES, "A 5-axis CNC milling machine tool can create highly complex geometries using five axes of motion during machining operations that which can be difficult or impossible to be implemented by using traditional 3-axis machine." TRADITIONAL 3-AXIS MACHINE, limits, GEOMETRY COMPLEXITY, "Machining operations that can be difficult or impossible to be implemented by using traditional 3-axis machine."

### 2 Models

To rigorously assess how model choice shapes our framework's performance, we evaluate a diverse set of both open-source and closed-source large language models across multiple parameter scales. For open sources models, Llama 3.2 3B Instruct [\[36\]](#page-19-0), LLama 3.1 8B Instruct [\[37\]](#page-19-1), and Qwen 2.5 7B Instruct [\[38\]](#page-19-2) were chosen due to their ease of access and relatively low computational requirements; here B represents the number of parameters in billions. By spanning 3B, 7B, and 8B parameter tiers, we can isolate how model capacity and design choices interact with our knowledge graph augmentation. Evaluating these models across a broad spectrum of parameter scales enables us to quantify how the integration of supplementary knowledge graph information influences overall performance. To establish an upper bound on attainable performance, we also benchmark against state-of-the-art closed sourced models including GPT-4o [\[26\]](#page-18-20), its smaller GPT-4o-mini, Gemini 2.0 Flash [\[39\]](#page-19-3), and Gemini 2.0 Flash-Lite [\[40\]](#page-19-4). These comprehensive models enables us to quantify precisely how supplemental structured knowledge closes the gap between open source baselines and leading proprietary offerings.


| Document | Description | Knowledge Graph Size |
|------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------|
| A Review in Capabilities and<br>Challenges of 5-Axis CNC<br>Milling Machine Tool Oper<br>ations [27] | This review surveys recent advances and challenges in<br>5-axis CNC milling, including error modeling and com<br>pensation, toolpath and process optimization, virtual ma<br>chining systems, tool wear and temperature prediction,<br>and sustainability considerations. | # of triples: 464<br># of unique entities: 654<br># of unique relations: 199 |
| Design and development of<br>a CNC machining process<br>knowledge base using cloud<br>technology [28] | This paper presents a cloud-based CNC machining process<br>knowledge base that maps STEP-NC to an OWL ontol<br>ogy and leverages Hadoop's HBase for scalable storage,<br>MapReduce driven querying, and SWRL-based reasoning<br>to enable intelligent, high-throughput process planning. | # of triples: 559<br># of unique entities: 828<br># of unique relations: 291 |
| Exploring the Application of<br>Industrial Robots in CNC<br>Machining [29] | This paper explores the deployment of industrial robots<br>in CNC machining. It details robot selection, workflow<br>and control program design, joint and transmission mech<br>anisms, and drive system choices to automate loading/un<br>loading and enhance operational flexibility, precision, and<br>efficiency. | # of triples: 98<br># of unique entities: 151<br># of unique relations: 57 |
| Fundamentals of CNC Ma<br>chining [30] | This guide provides a practical introduction to CNC ma<br>chining fundamentals covering shop safety, tooling, coor<br>dinate systems, programming and operation of mills and<br>lathes, 2D/3D toolpaths, workholding examples, and best<br>practices for prototype and short-run production. | # of triples: 1902<br># of unique entities: 2966<br># of unique relations: 512 |
| Approaches<br>to<br>Innovative<br>Sustainable CNC Machining:<br>A Machine Learning Per<br>spective on Energy Optimiza<br>tion [31] | This paper develops a machine-learning framework for<br>sustainable 5-axis CNC milling by combining per-axis<br>power monitoring with Taguchi experimental design and<br>tree-based regression to model and predict energy con<br>sumption. | # of triples: 278<br># of unique entities: 452<br># of unique relations: 183 |
| Integration of Taguchi and<br>for<br>CNC<br>PROMETHEE<br>Milling Machining Parame<br>ter Optimization [32] | This paper integrates Taguchi's orthogonal-array experi<br>mental design with the PROMETHEE multi-criteria rank<br>ing method to optimize spindle speed, feed rate, and depth<br>of cut for CNC milling of AA6061. | # of triples: 227<br># of unique entities: 366<br># of unique relations: 74 |
| Research on CNC program<br>ming and machining process<br>based on CAD/CAM tech<br>nology [33] | This paper introduces a CAD/CAM-based CNC program<br>ming framework that uses Bayesian process-skeleton map<br>ping to link part features with reusable macro-processes<br>and applies residual-height trajectory generation and<br>nonlinear-error compensation to optimize multiaxis tool<br>paths. | # of triples: 269<br># of unique entities: 443<br># of unique relations: 167 |
| Review on Design Research<br>in<br>CNC<br>Machine<br>Tools<br>Based on Energy Consump<br>tion [34] | This review surveys global advances in modeling, de<br>signing, and evaluating CNC machine tools from an<br>energy-consumption perspective. | # of triples: 270<br># of unique entities: 470<br># of unique relations: 140 |
| Automation<br>in<br>Robotical<br>CNC Machine Tools, A Re<br>view [35] | This review surveys robotics-driven advances in CNC ma<br>chining. Automated material handling and tool changing<br>to adaptive control, quality inspection, data analytics, and<br>collaborative robots are evaluated for their impacts on effi<br>ciency, precision, and safety. | # of triples: 262<br># of unique entities: 380<br># of unique relations: 129 |

**Table 1:** Overview of source documents and their corresponding knowledge graph sizes.

### 3 Question Category

Following knowledge graph construction, we devised two question formats namely, multiple choice and open ended to evaluate each model's capabilities. Multiple choice questions present a controlled benchmark for retrieval precision, enabling objective measurement and direct comparison across models, while open-ended questions simulate real-world case studies by challenging models to perform generative synthesis and coherently integrate the retrieved information into comprehensive answers. For each question format, two categories of questions were created namely: content

specific and machining specific. Content specific questions refer to the information written in the documents that do not require quantitative analysis. Machining specific questions refer to questions that require quantitative precision and decision-making based on numerical parameters. An example of each question in multiple choice format is given below:

### Content Specific Question

Which statement best describes a key distinction between 3-axis and 5-axis CNC milling machines?

- A. 3-axis machines are used for metals, while 5-axis machines are used exclusively for plastics.
- B. 5-axis machines include tilt and rotation of the workpiece or tool, in addition to X, Y, and Z motion.
- C. 3-axis machines are larger and require more floorspace than 5-axis machines.
- D. 5-axis machines do not allow any vertical movement while 3-axis machines do.[4pt]

### Machining Specific Question

For drilling operations on stainless steel (303), what feed (in inches per revolution) is recommended?

- A. 0.0005 in/rev
- B. 0.0010 in/rev
- C. 0.0020 in/rev
- D. 0.0030 in/rev

GPT-4o was implemented to generate each question from the documents chosen with a total of 65 content specific multiple choice, 45 machining specific multiple choice, 104 content specific open ended, and 45 machining specific open ended.

## <span id="page-10-0"></span>5 Experimental Results

![](_page_10_Figure_16.jpeg)
<!-- Image Description: The image presents eight line graphs (two rows of four) comparing the accuracy of different large language models (LLMs) across varying "Top K" values and depths (0-3). Each graph shows accuracy performance for several LLMs, (Gemini 2.0 Flash, Gemini 2.0 Flash-Lite, GPT-40, GPT-40 mini, Llama 3.1 8B Instruct, Llama 3.2 3B Instruct, and Qwen 2.5 7B Instruct). The top row displays accuracy at different depths while the bottom row displays a different accuracy metric. The purpose is to illustrate the relative performance of various LLMs under different parameter settings. -->

<span id="page-10-1"></span>**Figure 4:** Graph of large language model model accuracy as a function of graph traversal depth and Top K triple retrieval for a) content specific queries and b) machining specific queries.

ARKNESS is implemented using Python, PyTorch, and respective APIs for each of the chosen models. We initially explore how knowledge graph augmentation can enhance the performance of large language models using controlled graph traversals as defined by Eq. [5.](#page-7-0) Figure [4](#page-10-1) shows experiments that vary the depth of these traversals and measure the effect on the accuracy of the LLM answers. In these experiments, we also focus on selecting the Top K highest scoring triples along with there corresponding context. Figure [4a](#page-10-1) presents results with multiple choice questions that were designated content specific. As shown, as we increase the number of Top K triples retrieved from the graph to give to the LLMs the general trend is that performance increases. This trend holds true across most depths tested as there is an exception where GPT-4o mini at depth one experiences a decrease in performance, possibly because the additional graph-derived information is redundant or less informative, overwhelming the model's initial context. There is a notable increase for Gemini 2.0 Flash at depth 0 between no information given (Top K of 0) to Top K of 10 increasing from an accuracy of 0.867 to 0.933. It can also be observed that as the depth increases, the performance across the retrieved Top K stabilizes, indicating that for these types of questions a depth of 0 is most optimal since additional information does not yield further benefits. This stabilization implies that the initial retrieved context is already sufficiently rich, and incorporating extra graph-derived details beyond depth zero fails to enhance model accuracy and may even introduce superfluous noise. Compared to content-specific questions, we observe a significantly greater trend for quantitative machining specific questions, with performance improvements manifesting across all depths and Top K values as shown in Figure [4b](#page-10-1)). This indicates that additional graph-derived context plays a crucial role in accurately identifying the correct quantitative values for machine-specific queries, with stabilization around a depth of 2 demonstrating that these more challenging questions benefit from a deeper graph traversal to capture the necessary relevant information.

![](_page_11_Figure_2.jpeg)
<!-- Image Description: The image presents two radar charts (a and b) comparing the performance of several large language models (LLMs). Each chart displays the relative performance of different LLMs against a baseline, across five unspecified metrics represented by the pentagon's vertices. The charts visually represent the LLMs' strengths and weaknesses concerning these metrics. Different line styles and colors represent different LLMs (e.g., Gemini, GPT-40, Llama, Qwen). Percentage differences from the baseline are annotated. The charts likely aim to illustrate the comparative capabilities of these models in the paper. -->

<span id="page-11-0"></span>**Figure 5:** Radar plots of average large language model accuracy performance across knowledge graph completeness levels ranging from baseline (0%) to full graph (100%) for a) content Specific and b) machining specific multiple choice queries.

Figure [5](#page-11-0) showcases our model under conditions of incomplete knowledge graphs to emulate real-world scenarios, where the available data may be partial or sparsely connected. This approach not only tests the robustness of the graph augmentation mechanism but also helps us understand how missing nodes and relations affect the retrieval of relevant information and, ultimately, the accuracy of the LLM responses. We again tested the same aforementioned LLMs under varying graph sizes, specifically, baseline with no graph, 25%, 50%, 75%, and full graph of 100%. We randomly dropped triples to achieve the chosen graph sizes and each model was tested 10 times for each respective graph size with the average reported on the plot and the green percent increase indicating the maximum increase across all models tested when increasing the graph size. Figure [5a](#page-11-0) presents the performance metrics for content-specific questions across various LLMs. The results indicate only a modest improvement when additional graph context is provided, suggesting that these questions can largely be addressed with the baseline contextual information available to each model. Notably, the maximum performance gain, an absolute increase of approximately 3%, is observed when the graph size is augmented from the baseline to 25%, demonstrating that while supplementary retrieved data offers some benefit, its impact remains relatively limited for content-specific queries. Figure [5b](#page-11-0) illustrates how expanding the graph size affects performance on the more specialized quantitative machining specific questions. Unlike the content specific queries in Figure [5a](#page-11-0), these questions exhibit more pronounced gains as the graph grows beyond the baseline level indicated by the greater maximum increase across the models. When comparing closed source models with open sourced, GPT-4o effectively leverages the additional graph context to improve performance by 12.1% at a graph size of 100% compared to baseline. Additionally, Gemini 2.0 Flash-Lite achieves a relatively high increase of 15.1 % compared to baseline at a graph size of 100%. In contrast, the open source Llama 3.2 3B Instruct shows an even greater relative gain of 20.3% at graph size of 100% compared to baseline, suggesting that these models, which may have a lower baseline performance, can be significantly boosted by enhanced external context. Notably, the significant improvements seen in the smaller open source models imply that in manufacturing environments, such as the factory floor, these

models may be particularly effective when enhanced with additional contextual information. Their lower computational overhead and faster inference speeds make them especially suited for deployment in real-time industrial settings where cost efficiency and rapid responsiveness are critical.

<span id="page-12-0"></span>**Table 2:** Benchmark accuracy and F1-score performance between baseline and with knowledge graph for each large language model. Absolute increases (or no change) are in green, and decreases are in red.

| Model | File | Baseline | | Knowledge Graph | |
|-----------------------|----------------------------------------|-------------------|----------------|-----------------|----------------------------------------------------------------|
| | | Accuracy F1 Score | | Accuracy | F1-Score |
| Gemini 2.0 Flash | Content Specific<br>Machining Specific | 0.867<br>0.267 | 0.535<br>0.152 | | 0.933 (+0.066) 0.569 (+0.034)<br>0.517 (+0.250) 0.224 (+0.072) |
| Gemini 2.0 Flash-Lite | Content Specific<br>Machining Specific | 0.933<br>0.333 | 0.888<br>0.277 | 0.933 (0.000) | 0.688 (-0.200)<br>0.567 (+0.234) 0.338 (+0.061) |
| GPT-4o | Content Specific<br>Machining Specific | 0.933<br>0.667 | 0.716<br>0.464 | | 0.950 (+0.017) 0.719 (+0.003)<br>0.733 (+0.066) 0.573 (+0.109) |
| GPT-4o mini | Content Specific<br>Machining Specific | 0.933<br>0.567 | 0.906<br>0.391 | 0.933 (0.000) | 0.901 (-0.005)<br>0.683 (+0.116) 0.549 (+0.158) |
| Llama 3.1 8B Instruct | Content Specific<br>Machining Specific | 0.940<br>0.433 | 0.925<br>0.244 | | 0.953 (+0.013) 0.940 (+0.015)<br>0.633 (+0.200) 0.404 (+0.160) |
| Llama 3.2 3B Instruct | Content Specific<br>Machining Specific | 0.933<br>0.367 | 0.718<br>0.319 | | 0.950 (+0.017) 0.712 (-0.006)<br>0.583 (+0.216) 0.543 (+0.224) |
| Qwen 2.5 7B Instruct | Content Specific<br>Machining Specific | 0.867<br>0.567 | 0.580<br>0.371 | | 0.883 (+0.016) 0.614 (+0.034)<br>0.700 (+0.133) 0.463 (+0.092) |

Table [2](#page-12-0) provides a benchmark comparison between baseline model performance and results with a minimal viable integration of knowledge graph information (i.e., a chosen Top K of 10 and depth of 0) for both multiple choice datasets. Results for both accuracy and F1-score are reported with the absolute increases, or no increase, between baseline and knowledge graph augmented outputs highlighted in green and any declines shown in red. As shown in the table, for content specific questions where baseline performance was already high, the improvements are very modest and range from 0.013 to 0.066 for accuracy. However, even this relatively small uplift indicates that the inclusion of the knowledge graph imparts valuable domain-specific insights that further refine the responses. Other the other hand, for the quantitative machining specific questions the results are far more substantial. For example, Gemini 2.0 Flash-Lite achieves an uplift of 0.250 in accuracy and 0.072 in F1-score. Similarly, magnitudes of improvement is shown with Llama 3.2 3B Instruct and Llama 3.1 8B Instruct having an uplift of 0.216 and 0.200 in accuracy, respectively. These improvements provide evidence that the structured, domain-specific context provided by the knowledge graph effectively addresses the deficiencies each LLM's internal representations. It should be noted that there is a notable decrease in F1-score of 0.200 for Gemini 2.0 Flash-Lite when testing on content specific questions. This decline may be attributed to the additional context provided by the knowledge graph, which, while generally beneficial, can increase the complexity of the input and lead to confusion in the model's internal representations and final decision making. In summary, despite some localized declines, likely due to increased input complexity, the integration of knowledge graph information consistently enhances performance across most models, providing significant improvements in domain-specific accuracy and reliability across both content specific and quantitative machining specific queries, and thereby demonstrating its critical value for augmenting LLM outputs in technical applications.

Our next experiment examines the ability of LLMs to respond to open-ended questions versus multiple-choice questions. In this section, we seek to understand how the models perform when required to generate answers without predefined options, compared to when they can select from a limited set of potential responses. This approach allows us to assess the effect of external knowledge augmentation in more unconstrained scenarios. Figure [6](#page-13-0) provides the average results across 10 runs for each of the datasets for four different metrics: semantic similarity, ROUGE-1, ROUGE-2, and ROUGE-L. Here, semantic similarity evaluates the closeness in meaning between the generated answer and the reference response by comparing their vector embeddings, thereby capturing the overall semantic content beyond mere word matching. ROUGE-1 measures the overlap of individual words (unigrams) between the generated text and the grounded answer, serving as a baseline indicator of lexical similarity. ROUGE-2 extends this comparison to pairs of consecutive words (bigrams), offering insights into the consistency of short phrases. Finally, ROUGE-L assesses the longest common subsequence between the generated and reference texts, highlighting the structural similarity and the preservation of narrative flow. As shown in Figure [6a](#page-13-0)), for the content specific questions with the additional information from the knowledge graph, the overall scores are higher for most of the models across all metrics. This is especially true

![](_page_13_Figure_1.jpeg)
<!-- Image Description: The image presents two sets (a and b) of bar charts comparing the performance of various large language models (LLMs) on semantic similarity and ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores. Each chart displays scores (a for semantic similarity, b for ROUGE-1, ROUGE-2, and ROUGE-L) for each LLM, with two bars per model: one for a baseline and one for a KG (knowledge graph) method. The charts aim to illustrate the impact of KG integration on LLM performance across different evaluation metrics. -->

<span id="page-13-0"></span>**Figure 6:** Average performance over 10 runs for evaluated large language models, comparing baseline outputs with those augmented by knowledge graph data. a) Shows performance for content specific questions, and b) depicts results for machining specific questions across semantic similarity, ROUGE-1, ROUGE-2, and ROUGE-L metrics.

<span id="page-13-1"></span>**Table 3:** Open ended responses from open source models Llama 3.2 3B Instruct, Llama 3.1 8B Instruct, and Qwen 2.5 7B Instruct regarding drill sizes for CNC machining. Grounded quantitative answers are highlighted in yellow, baseline values in red, and knowledge graph augmented values in green.

| Model / Question | What is the decimal equivalent of<br>Drill Size 82? | What is the decimal equivalent of<br>Drill Size 84? | What is the decimal equivalent of<br>Drill Size 89? | |
|-----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--|
| Grounded Answer | The decimal equivalent of Drill Size<br>82 is 0.0125 inches. | The decimal equivalent of Drill Size<br>84 is 0.0115 inches. | The decimal equivalent of Drill Size<br>89 is 0.0091 inches. | |
| Llama 3.1 8B In<br>struct | We find that Drill Size<br>Baseline:<br>82 is actually equivalent to 0.082<br>inches. This is because the number 82<br>represents the 82nd drill size in the<br>series, and the decimal equivalent is<br>calculated using a formula that takes<br>into account the drill size and the<br>series.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 82 is<br>.0125 inches. | Baseline: The decimal equivalent of<br>drill size 84 is 0.84 inches, which<br>can also be represented as a fraction,<br>84/64.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 84 is<br>.0115 inches. | Baseline: The decimal equivalent of<br>Drill Size 89 is approximately 0.139<br>inches.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 89 is<br>.0091 inches. | |
| Llama 3.2 3B In<br>struct | Baseline: According to the standard<br>drill size chart, Drill Size 82 cor<br>responds to a diameter of 0.820 inches.<br>Knowledge Graph:<br>To find the<br>decimal equivalent of drill size 82,<br>we need to look at the information<br>provided. Drill size 82 corresponds to<br>a decimal equivalent of .0125 inches. | Baseline: Drill Size 84 refers to a<br>drill bit with a diameter of 84 mm. To<br>convert this to a decimal equivalent,<br>we simply need to express the diameter<br>as a decimal number. In this case, the<br>decimal equivalent of Drill Size 84 is<br>84.00 mm.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 84 is<br>.0115 inches. | Baseline: The decimal equivalent of<br>Drill Size 89 is 0.89 inches, or 0.0089<br>inches.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 89 is<br>.0091 inches. | |
| 2.5<br>7B<br>In<br>Qwen<br>struct | "Drill Size 82" likely<br>Baseline:<br>corresponds to a drill bit that measures<br>82 thousandths of an inch in diameter.<br>This translates to 0.82 inches when<br>expressed as a decimal.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 82 is<br>0.0125 inches. | Baseline: The decimal equivalent of<br>Drill Size 84 typically refers to a drill<br>bit size in the United States customary<br>Rounded to four decimal<br>system<br>the decimal equivalent of<br>places,<br>Drill Size 84 is approximately 0.0119<br>inches.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 84 is<br>0.0115 inches. | Baseline: The decimal equivalent of<br>Drill Size 89, which typically refers<br>to a drill bit size in the United States<br>customary system, is 1/2 inch.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 89 is<br>0.0091 inches. | |

for models such as GPT-4o, GPT-4o mini, Gemini 2.0 Flash, Gemini 2.0 Flash-Lite, Llama 3.1 8B Instruct, and Llama 3.2 3B Instruct having an increase of upwards of 3.1× for ROUGE-1, 2.8× for ROUGE-2, and 2.9× for ROGUE-L. These substantial multiplicative gains clearly indicate that the integration of graph-derived information greatly enhances the models' ability to capture and reproduce relevant ground information thereby significantly improving the quality and fidelity of generated responses in content specific questions. Figure [6b](#page-13-0)) showcases an even more pronounced improvement for open ended generation when testing on the quantitative machining specific queries. For semantic similarity, there is an improvement up to 1.2× across all models with the exception of Gemini 2.0 Flash-Lite. For ROUGE metrics, all models exhibit an increase in scores compared to baseline with up to 8.5×, 8.8×, and 8.1× for ROUGE-1, ROGUE-2, and ROUGE-L, respectively. These results provide evidence that more quantitative questions, which demand precise numerical accuracy and technical detail, benefit particularly from the enriched context. While semantic similarity indicates that the basic understanding of quantitative content is already strong, the substantial improvements in ROUGE scores reveal that the additional context helps the models generate far more accurate and detailed quantitative responses.

<span id="page-14-0"></span>**Table 4:** Open ended responses from closed source models Gemini 2.0 Flash, Gemini 2.0 Flash-Lite, GPT-4o, and GPT-4o-mini regarding drill sizes for CNC machining. Grounded quantitative answers are highlighted in yellow, baseline values in red, and knowledge graph augmented values in green.

| Model / Question | What is the decimal equivalent of<br>Drill Size 82? | What is the decimal equivalent of<br>Drill Size 84? | What is the decimal equivalent of<br>Drill Size 89? |
|--------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Grounded Answer | The decimal equivalent of Drill Size<br>82 is 0.0125 inches. | The decimal equivalent of Drill Size<br>84 is 0.0115 inches. | The decimal equivalent of Drill Size<br>89 is 0.0091 inches. |
| Gemini 2.0 Flash | Baseline: Drill sizes are often referred<br>to by a number, letter, or fractional<br>inch. Drill size 82 is a numbered drill<br>size. Numbered drill sizes range from<br>size 80, which is 0.0135 inches, to<br>size 1, which is 0.228 inches.<br>The<br>size increases as the number decreases.<br>Drill size 82 is equal to 0.0156 inches. | Baseline: The decimal equivalent of<br>Drill Size 84 is 0.0095 inches.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 84 is<br>.0115 inches. | Baseline: The decimal equivalent of<br>Drill Size 89 is 0.2010 inches.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 89 is<br>.0091 inches. |
| | Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 82 is<br>.0125 inches. | | |
| Gemini 2.0 Flash<br>Lite | Baseline: The decimal equivalent of<br>drill size 82 is approximately 0.0400<br>inches. | Baseline: The decimal equivalent of<br>Drill Size 84 is 0.040 inches. | Baseline: The decimal equivalent of<br>drill size 89 is approximately 0.040<br>inches. |
| | Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of drill size 82 is .0125<br>inches. | Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of drill size 84 is .0115<br>inches. | Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of drill size 89 is .0091<br>inches. |
| GPT-4o | Baseline: For drill size 82, which is<br>part of the numbered drill bit system,<br>the decimal equivalent is 0.0120<br>inches.<br>Graph:<br>deci<br>Knowledge<br>The | Baseline: In the standard number drill<br>size system, the smallest size is 80,<br>which corresponds to a diameter of<br>0.0135 inches. If you are referring to a<br>drill size outside of this standard range,<br>it might be a typographical error | Baseline: In the context of standard<br>numbered drill bits, Drill Size 89 is<br>not typically recognized. The standard<br>numbered drill bit sizes go from size<br>80, which has a decimal equivalent of<br>0.0135 inches. |
| | mal equivalent of Drill Size 82 is<br>.0125 inches. | or a non-standard size specific to a<br>particular manufacturer or application.<br>Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 84 is<br>.0115 inches. | Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 89 is<br>.0091 inches. |
| GPT-4o-mini | Baseline: Drill Size 82 has a diameter<br>of approximately 0.4130 inches. | Baseline: Drill size 84 corresponds to<br>a diameter of 0.0860 inches. | Baseline: Drill size 89 corresponds to<br>a diameter of approximately 0.0135<br>inches. |
| | Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 82 is<br>.0125 inches. | Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 84 is<br>.0115 inches. | Graph:<br>deci<br>Knowledge<br>The<br>mal equivalent of Drill Size 89 is<br>.0091 inches. |

In addition to using quantitative metrics such as semantic similarity and ROUGE scores to evaluate each model's responses, we also include the full answers generated for the quantitative questions. This approach not only highlights the differences between closed source and open source models but also offers deeper insight into how each model integrates the retrieved graph knowledge into its responses. Table [3](#page-13-1) and Table [4](#page-14-0) show the baseline and knowledge graph–augmented outputs for drill size questions, which are focused on accurately determining the correct drill size

values for open and closed source models, respectively. Correct quantitative answers for each question are highlighted in yellow, baseline LLM values in red, and knowledge graph–augmented values in green for each table. As shown, the use of the knowledge graph consistently improves all open ended outputs of the LLMs' answers by correcting the hallucinations present in the baseline answers. For example, in the open source group, models such as Llama 3.2 3B and Llama 3.1 8B provided baseline answers that deviated significantly from the grounded values (e.g., reporting 0.820 inches or 0.082 inches for drill size 82), whereas, when augmented with the knowledge graph, these models uniformly converged to the correct decimal equivalent of 0.0125 inches. Similarly, Qwen 2.5 7B Instruct confuses the drill size number for its equivalence in inches but this was effectively remedied with information from the graph. The closed sourced models exhibited a similar trend with Gemini 2.0-flash, Gemini 2.0-flash-lite, GPT-4o, and GPT-4o-mini all showing incorrect quantitative answers. These answers varied from nearly correct values, such as GPT-4o's output of 0.012 inches for drill size 82, to very inaccurate ones, including GPT-4o-mini's output of 0.4130 inches for the same drill size. Comparing across open and closed source models, the smaller open sourced models can tend to confuse the drill size number with its corresponding decimal value, effectively mistaking the numeric label for the measurement itself. In contrast, the closed sourced models often produce entirely incorrect values. In some cases, such as with GPT-4o, the model claims that the specified drill size does not exist. These results emphasize the value of incorporating a knowledge graph in open ended scenarios where choices are not given to correctly convert drill sizing information.

<span id="page-15-0"></span>**Table 5:** Open ended responses from open source models Llama 3.2 3B Instruct, Llama 3.1 8B Instruct, and Qwen 2.5 7B Instruct regarding parameter selection for CNC machining operation. Grounded quantitative answers are highlighted in yellow, baseline values in red, and knowledge graph augmented values in green.

| Model / Question | For drilling operations on steel<br>(4140), what cutting speed (SFM) is<br>recommended? | For tapping operations on alu<br>minum, what cutting speed (SFM)<br>is recommended? | For milling operations on brass with<br>a tool diameter between 0.5 and<br>1.0 inches, what chip load is recom<br>mended? |
|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Grounded Answer | For drilling operations on steel (4140),<br>a cutting speed of 90 SFM is recom<br>mended. | For tapping operations on aluminum,<br>a cutting speed of 100 SFM is recom<br>mended. | For milling operations on brass with<br>a tool diameter between 0.5 and 1.0<br>inches, a chip load of 0.004 inches per<br>revolution is recommended. |
| Llama 3.1 8B In<br>struct | Baseline: The recommended cutting<br>speed for drilling operations on 4140<br>steel ranges from 50 to 300 SFM,<br>depending on the drill bit type and<br>desired finish. | In general, the recom<br>Baseline:<br>mended cutting speed for tapping<br>aluminum ranges from 50 to 150 SFM,<br>depending on the specific conditions. | Baseline: A commonly cited range for<br>chip loads on brass is between 0.002<br>and 0.008 inches per tooth (ipt).<br>Knowledge Graph:<br>For milling |
| | Knowledge Graph:<br>For drilling<br>operations on steel (4140), the recom<br>mended cutting speed is 90 sfm. | Graph:<br>tap<br>Knowledge<br>For<br>ping operations on aluminum, the<br>recommended cutting speed is 100<br>sfm. | on<br>brass<br>with<br>a<br>tool<br>operations<br>diameter between 0.5 and 1.0 inches,<br>the recommended chip load is 0.004<br>ipr. |
| Llama 3.2 3B In<br>struct | Baseline: For drilling operations on<br>4140 steel, the recommended cutting<br>speeds typically range from 60 to<br>120 SFM, with the optimal speed<br>depending on the specific drilling<br>conditions. | Baseline: A cutting speed of 30 to<br>50 FPM is commonly recommended<br>for tapping operations on aluminum<br>alloys, but the optimal cutting speed<br>may vary depending on the application,<br>tooling, and machining conditions. | Baseline: For milling operations on<br>brass, a general rule of thumb is to<br>use a chip load that is between 0.002<br>and 0.005 inches for a tool diameter<br>between 0.5 and 1.0 inches. |
| | Knowledge Graph:<br>For drilling<br>operations on steel (4140), a cutting<br>speed of 90 SFM is recommended. | Knowledge Graph: For tapping oper<br>ations on aluminum, a recommended<br>cutting speed is 100 sfm. | Graph:<br>on<br>Knowledge<br>Based<br>the provided context,<br>for milling<br>on<br>brass<br>with<br>a<br>tool<br>operations<br>diameter between 0.5 and 1.0 inches,<br>the recommended chip load is 0.004<br>inches per revolution (ipr). |
| 2.5<br>7B<br>In<br>Qwen<br>struct | Baseline: The recommended SFM for<br>drilling 4140 steel generally ranges<br>between 25 and 35 feet per minute<br>(FPM). | Baseline: For tapping operations on<br>aluminum, a recommended cutting<br>speed (Surface Footage per Minute,<br>SFM) typically ranges between 100 to | Baseline: A common starting point<br>for chip loads in this range is approxi<br>mately 0.003 to 0.005 inches per tooth<br>(IPM). |
| | Knowledge Graph:<br>For drilling<br>operations on steel (4140), a recom<br>mended cutting speed (SFM) is 90<br>sfm. | 200 SFM.<br>Knowledge Graph: For tapping oper<br>ations on aluminum, a recommended<br>cutting speed (SFM) is 100 sfm. | Knowledge Graph:<br>For milling<br>operations on brass with a tool diame<br>ter between 0.5 and 1.0 inches, a chip<br>load of 0.004 inches per revolution<br>(ipr) is recommended. |

Table [5](#page-15-0) and Table [6](#page-16-0) present open and closed sourced model responses, respectively, to quantitative questions regarding selection of correct process parameters, which is especially important for ensuring optimal machining performance, precise material removal, and enhanced tool longevity across varying operational conditions. Similar to the previously mentioned tables, correct quantitative answers for each question are highlighted in yellow, baseline LLM values in red, <span id="page-16-0"></span>**Table 6:** Open ended responses from closed source models Gemini 2.0 Flash, Gemini 2.0 Flash-Lite, GPT-4o, and GPT-4o-mini regarding parameter selection for CNC machining operation. Grounded quantitative answers are highlighted in yellow, baseline values in red, and knowledge graph augmented values in green.

| Model / Question | For drilling operations on steel<br>(4140), what cutting speed (SFM) is<br>recommended? | For tapping operations on alu<br>minum, what cutting speed (SFM)<br>is recommended? | For milling operations on brass with<br>a tool diameter between 0.5 and<br>1.0 inches, what chip load is recom<br>mended? |
|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Grounded Answer | For drilling operations on steel (4140),<br>a cutting speed of 90 SFM is recom<br>mended. | For tapping operations on aluminum,<br>a cutting speed of 100 SFM is recom<br>mended. | For milling operations on brass with<br>a tool diameter between 0.5 and 1.0<br>inches, a chip load of 0.004 inches per<br>revolution is recommended. |
| Gemini 2.0-flash | Baseline: For 4140 steel, which is<br>a medium-carbon,<br>low-alloy steel<br>known for its high strength and good<br>hardenability, typical cutting speeds<br>for drilling operations generally range<br>from 70 to 120 SFM. | Baseline: For general-purpose tapping<br>of aluminum alloys like 6061-T6, a<br>common starting point is between 30<br>to 60 SFM. However, softer alloys<br>like 1100 or 3003 can often be tapped<br>at higher speeds, potentially up to 75<br>SFM or more. | Baseline: The recommended chip load<br>for milling brass with a tool diameter<br>between 0.5 and 1.0 inches typically<br>falls within the range of 0.001 to 0.004<br>inches per tooth.<br>Knowledge Graph:<br>For milling |
| | Knowledge Graph:<br>For drilling<br>operations on steel (4140), a cutting<br>speed of 90 SFM is recommended. | Knowledge Graph:<br>For tapping<br>operations on aluminum, a cutting<br>speed of 100 SFM is recommended. | operations on brass with a tool diame<br>ter between 0.5 and 1.0 inches, a chip<br>load of 0.004 inches per revolution<br>(ipr) is recommended. |
| Gemini 2.0-flash-lite | Baseline: For high-speed steel (HSS)<br>drills, a starting point for the cutting<br>speed when drilling 4140 steel is<br>typically in the range of 40 to 60<br>surface feet per minute (SFM) for the | Baseline: For most aluminum alloys,<br>a starting point for the cutting speed<br>is typically in the range of 50 to 200<br>surface feet per minute (SFM). | Baseline: For milling brass with a tool<br>diameter between 0.5 and 1.0 inches,<br>a chip load of 0.003 to 0.010 IPT is a<br>good starting point. |
| | steel in a hardened state.<br>Knowledge Graph:<br>For drilling<br>operations on steel (4140), the recom<br>mended cutting speed is 90 SFM. | Knowledge Graph:<br>For tapping<br>operations on aluminum, a cutting<br>speed of 100 SFM (Surface Feet per<br>Minute) is recommended. | Knowledge Graph:<br>For milling<br>operations on brass with a tool diame<br>ter between 0.5 and 1.0 inches, a chip<br>load of 0.004 inches per revolution<br>(ipr) is recommended. |
| GPT-4o | Baseline: For annealed AISI 4140<br>steel, which typically has a hardness<br>of around 150-200 Brinell, the rec<br>ommended cutting speed is usually in<br>the range of 70 to 100 surface feet per<br>minute (SFM) when using high-speed<br>steel (HSS) drill bits. | Baseline: The recommended cutting<br>speed for tapping aluminum typically<br>ranges from 75 to 150 surface feet per<br>minute (SFM).<br>Knowledge Graph:<br>For tapping<br>operations on aluminum, a cutting<br>speed of 100 surface feet per minute | Baseline: For general milling oper<br>ations on brass with a tool diameter<br>between 0.5 and 1.0 inches, a typical<br>chip load recommendation would<br>range from approximately 0.002 to<br>0.010 inches per tooth (IPT).<br>Knowledge Graph:<br>For milling |
| | Knowledge Graph:<br>For drilling<br>operations on steel (4140), a cutting<br>speed of 90 SFM is recommended. | (SFM) is recommended. | on<br>brass<br>with<br>a<br>tool<br>operations<br>diameter between 0.5 and 1.0 inches,<br>the recommended chip load is 0.004<br>inches per revolution (ipr). |
| GPT-4o-mini | for drilling<br>Baseline:<br>Typically,<br>operations on 4140 steel, a cutting<br>speed in the range of 60 to 100 SFM<br>is often recommended when using<br>high-speed steel (HSS) drill bits. | Baseline: For most aluminum alloys,<br>common<br>recommendation<br>for<br>a<br>tapping speeds is in the range of 100<br>to 300 SFM. | For brass,<br>which is a<br>Baseline:<br>relatively soft and ductile material,<br>the recommended chip load generally<br>falls within the range of 0.002 to 0.008<br>inches per tooth. |
| | Knowledge Graph:<br>For drilling<br>operations on steel (4140), the recom<br>mended cutting speed is 90 surface<br>feet per minute (SFM). | Graph:<br>tap<br>Knowledge<br>For<br>ping operations on aluminum, the<br>recommended cutting speed is 100<br>surface feet per minute (SFM). | Knowledge Graph:<br>For milling<br>on<br>brass<br>with<br>a<br>tool<br>operations<br>diameter between 0.5 and 1.0 inches,<br>the recommended chip load is 0.004<br>inches per revolution (ipr). |

and knowledge graph–augmented values in green for each table. For the open sourced models, the baseline outputs typically provide a broad range of values such as Llama 3.2 3B Instruct suggesting a range from 60 to 120 surface feet per minute (SFM), and Llama 3.1 8B Instruct suggesting a range of 50 to 300 SFM. These wide ranges reflect uncertainty and a reliance on generic guidelines which do not allow a user to determine the optimal process parameters specified in technical documents. However, when the LLMs are augmented with knowledge graph information, these models converge on a precise recommendation of 90 SFM for drilling 4140 steel, 100 SFM for tapping aluminum, and a chip load of 0.004 inches per revolution (IPR) for milling brass, correctly aligning with the grounded answers. This shift from broad ambiguous ranges to a single well-defined value highlights the corrective influence of domain-specific information. The closed source LLMs exhibit a similar trend, frequently providing parameter recommendations as broad ranges that mirror inherent uncertainty and a lack of industry-specific technical standards. However, when augmented with the knowledge graph the models once again converge to the correctly recommended process parameters. Comparing across open and closed models reveals that all tend to recommend broad and imprecise parameter ranges for operations such as drilling, tapping, and milling. Regardless of model size or architecture, the baseline outputs consistently display wide ranges that fail to account for material properties and precise machining requirements. This suggests that the inherent complexity and scale of the models do not necessarily translate into accurate internal representations of domain-specific machining standards. In summary, without integrating knowledge graph information the outputs remain too generic to guide optimal parameter selection demonstrating that the incorporation of structured, domain-specific data is essential for constraining outputs and calibrating process parameters to align with validated manufacturing standards.

## <span id="page-17-5"></span>6 Conclusion

This paper introduces ARKNESS (Augmented Retrieval Knowledge Network Enhanced Search and Synthesis), a hybrid LLM-agnostic pipeline that couples a semantically rich automatically constructed knowledge graph with any large language model to provide answers that are grounded and numerically precise. ARKNESS provides traceable answers validated in technical information through paragraph level entity-relation extraction that converts heterogeneous document formats into triples while retaining their original contextual information. Then, by storing the knowledge graph in a relational database, we can leverage vector-embedding search alongside beam search to retrieve the most relevant triples and contextual information, thereby conditioning the LLM to deliver precise, technically validated answers to user queries. Experiments on multiple choice questions across content specific and machining specific queries showcases uplifts up to 25% in accuracy and 22.4% in F1-score. For opened ended questions, uplifts up to 8.1× for ROGUE-L scores demonstrate that the framework improves the structural and semantic information of generated responses relative to the grounded answers. These advantages allow the elimination of hallucinations prevalent in LLMs thereby enhancing process planning and usefulness in CNC machining contexts. Future work will focus on extending ARKNESS to a heterogeneous multimodal knowledge graphs, further strengthening factual grounding to enable closed-loop decision support in advanced manufacturing enviornments.

## Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## Acknowledgments

This work was supported under Cooperative Agreement W56HZV-21-2-0001 with the US Army DEVCOM Ground Vehicle Systems Center (GVSC), through the Virtual Prototyping of Autonomy Enabled Ground Systems (VIPR-GS) program.

DISTRIBUTION STATEMENT A. Approved for public release; distribution is unlimited. OPSEC9768

## References

- <span id="page-17-0"></span>[1] M. Wiessner, P. Blaser, S. Böhl, J. Mayr, W. Knapp, K. Wegener, Thermal test piece for 5-axis machine tools, Precision Engineering 52 (2018) 407–417.
- <span id="page-17-1"></span>[2] D. Hoang, H. Errahmouni, H. Chen, S. Rachuri, N. Mannan, R. ElKharboutly, M. Imani, R. Chen, F. Imani, Hierarchical representation and interpretable learning for accelerated quality monitoring in machining process, CIRP Journal of Manufacturing Science and Technology 50 (2024) 198–212.
- <span id="page-17-2"></span>[3] Z. Chen, D. Hoang, R. Chen, F. Imani, Distributed hyperdimensional computing for real-time data aggregation and interpretable quality monitoring in manufacturing, in: ASME International Mechanical Engineering Congress and Exposition, volume 88605, American Society of Mechanical Engineers, 2024, p. V002T03A092.
- <span id="page-17-3"></span>[4] Z. Li, Y. Dai, Z. Sun, C. L. Guan, T. Lai, H. Xu, X. Zhou, A sub-micron precision machining and measurement method of long travel metal guideways, Journal of Manufacturing Processes 133 (2025) 947–956.
- <span id="page-17-4"></span>[5] ForInsights Consultancy, Computer numerical control (cnc) machine market forecast 2030, [https://www.](https://www.forinsightsconsultancy.com/reports/computer-numerical-control-cnc-machine-market/) [forinsightsconsultancy.com/reports/computer-numerical-control-cnc-machine-market/](https://www.forinsightsconsultancy.com/reports/computer-numerical-control-cnc-machine-market/), 2023. Accessed: 2025-05-12.

- <span id="page-18-0"></span>[6] D. Hoang, N. Mannan, R. ElKharboutly, R. Chen, F. Imani, Edge cognitive data fusion: From in-situ sensing to quality characterization in hybrid manufacturing process, in: International Manufacturing Science and Engineering Conference, volume 87240, American Society of Mechanical Engineers, 2023, p. V002T06A029.
- <span id="page-18-1"></span>[7] D. Hoang, H. Chen, M. Imani, R. Chen, F. Imani, Brief paper: Multi-task brain-inspired learning for interlinking machining dynamics with parts geometrical deviations, in: International Manufacturing Science and Engineering Conference, volume 88117, American Society of Mechanical Engineers, 2024, p. V002T05A012.
- <span id="page-18-2"></span>[8] K. Spanaki, D. Dennehy, T. Papadopoulos, R. Dubey, Data-driven digital transformation in operations and supply chain management, 2025.
- <span id="page-18-3"></span>[9] Siemens AG, The True Cost of Downtime 2022, Technical Report, Siemens, 2022. URL: [https://](https://assets.new.siemens.com/siemens/assets/api/uuid:3d606495-dbe0-43e4-80b1-d04e27ada920/dics-b10153-00-7600truecostofdowntime2022-144.pdf) [assets.new.siemens.com/siemens/assets/api/uuid:3d606495-dbe0-43e4-80b1-d04e27ada920/](https://assets.new.siemens.com/siemens/assets/api/uuid:3d606495-dbe0-43e4-80b1-d04e27ada920/dics-b10153-00-7600truecostofdowntime2022-144.pdf) [dics-b10153-00-7600truecostofdowntime2022-144.pdf](https://assets.new.siemens.com/siemens/assets/api/uuid:3d606495-dbe0-43e4-80b1-d04e27ada920/dics-b10153-00-7600truecostofdowntime2022-144.pdf).
- <span id="page-18-4"></span>[10] N. C. Nwasuka, U. Nwaiwu, Computer-based production planning, scheduling and control: a review, Journal of Engineering Research 12 (2024) 275–280.
- <span id="page-18-5"></span>[11] M. Raza, Z. Jahangir, M. B. Riaz, M. J. Saeed, M. A. Sattar, Industrial applications of large language models, Scientific Reports 15 (2025) 13755.
- <span id="page-18-6"></span>[12] K. Šket, D. Potocnik, M. Ficko, S. Klan ˇ cnik, Enhancing g-code programming in cnc machining using chatgpt: A ˇ comparative study of gpt-3.5 and gpt-4.0, Available at SSRN 4940034 (2024).
- <span id="page-18-7"></span>[13] J. Jeon, Y. Sim, H. Lee, C. Han, D. Yun, E. Kim, S. L. Nagendra, M. B. Jun, Y. Kim, S. W. Lee, et al., Chatcnc: Conversational machine monitoring via large language model and real-time data retrieval augmented generation, Journal of Manufacturing Systems 79 (2025) 504–514.
- <span id="page-18-8"></span>[14] R. Rosati, F. Antonini, N. Muralikrishna, F. Tonetto, A. Mancini, Improving industrial question answering chatbots with domain-specific llms fine-tuning, in: 2024 20th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications (MESA), IEEE, 2024, pp. 1–7.
- <span id="page-18-9"></span>[15] S. Kanimozhi, Y. Sriker, et al., Explorative deployment of fine-tuned large language model for on-site computerized numeric control machine operator assistance, in: 2024 IEEE Silchar Subsection Conference (SILCON 2024), IEEE, 2024, pp. 1–6.
- <span id="page-18-10"></span>[16] Y. Xiao, S. Zheng, J. Shi, X. Du, J. Hong, Knowledge graph-based manufacturing process planning: A state-ofthe-art review, Journal of Manufacturing Systems 70 (2023) 417–435.
- <span id="page-18-11"></span>[17] P. Wen, Y. Ma, R. Wang, Systematic knowledge modeling and extraction methods for manufacturing process planning based on knowledge graph, Advanced Engineering Informatics 58 (2023) 102172.
- <span id="page-18-12"></span>[18] D. Hoang, D. Gorsich, M. Castanier, F. Imani, Vector-symbolic knowledge graphs for enhanced memorization and reasoning in digital manufacturing, Available at SSRN 5097516 (2025).
- <span id="page-18-13"></span>[19] L. Wang, H. Cheng, R. Wang, X. Huang, Machining scheme selection of features based on process knowledge graph and improved cosine similarity matching, Machines 13 (2025) 188.
- <span id="page-18-14"></span>[20] J. Guo, J. Wu, J. Bian, Q. He, Knowledge graph-based machining process route generation method, in: International Conference on Human-Computer Interaction, Springer, 2023, pp. 35–48.
- <span id="page-18-15"></span>[21] C. Cai, Z. Jiang, H. Wu, J. Wang, J. Liu, L. Song, Research on knowledge graph-driven equipment fault diagnosis method for intelligent manufacturing, The International Journal of Advanced Manufacturing Technology 130 (2024) 4649–4662.
- <span id="page-18-16"></span>[22] Y. Li, H. Zhao, H. Jiang, Y. Pan, Z. Liu, Z. Wu, P. Shu, J. Tian, T. Yang, S. Xu, et al., Large language models for manufacturing, arXiv preprint arXiv:2410.21418 (2024).
- <span id="page-18-17"></span>[23] P. Wang, J. Karigiannis, R. X. Gao, Ontology-integrated tuning of large language model for intelligent maintenance, CIRP annals 73 (2024) 361–364.
- <span id="page-18-18"></span>[24] C. Qiu, B. Li, H. Liu, S. He, C. Hao, A novel method for machine tool structure condition monitoring based on knowledge graph, The International Journal of Advanced Manufacturing Technology 120 (2022) 563–582.
- <span id="page-18-19"></span>[25] C. Auer, M. Lysak, A. Nassar, M. Dolfi, N. Livathinos, P. Vagenas, C. B. Ramis, M. Omenetti, F. Lindlbauer, K. Dinkla, et al., Docling technical report, arXiv preprint arXiv:2408.09869 (2024).
- <span id="page-18-20"></span>[26] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276 (2024).
- <span id="page-18-21"></span>[27] M. Soori, F. K. G. Jough, R. Dastres, B. Arezoo, A review in capabilities and challenges of 5-axis cnc milling machine tool operations, Preprint (2024).

- <span id="page-19-5"></span>[28] Y. Ye, T. Hu, C. Zhang, W. Luo, Design and development of a cnc machining process knowledge base using cloud technology, The International Journal of Advanced Manufacturing Technology 94 (2018) 3413–3425.
- <span id="page-19-6"></span>[29] F. N. Guo, Exploring the application of industrial robots in cnc machining, Journal of Global Humanities and Social Sciences 4 (2023) 231–235. URL: [http://ojs.bonfuturepress.com/index.php/GHSS/article/](http://ojs.bonfuturepress.com/index.php/GHSS/article/view/1499) [view/1499](http://ojs.bonfuturepress.com/index.php/GHSS/article/view/1499). doi:doi[:10.61360/BoniGHSS232014990503.](https://doi.org/10.61360/BoniGHSS232014990503)
- <span id="page-19-7"></span>[30] Autodesk, Inc., Fundamentals of CNC Machining: A Practical Guide for Beginners, United States, 2014. URL: [https://academy.titansofcnc.com/files/Fundamentals\\_of\\_CNC\\_Machining.pdf](https://academy.titansofcnc.com/files/Fundamentals_of_CNC_Machining.pdf), desk Copy. Document Number: 060711.
- <span id="page-19-8"></span>[31] I. Nugrahanto, H. Gunawan, H.-y. Chen, Innovative approaches to sustainable cnc machining: A machine learning perspective on energy optimization (2023).
- <span id="page-19-9"></span>[32] M. Ihsan, Y. Sumantri, Y. Irawan, Integration of taguchi and promethee for cnc milling machining parameter optimization on aa6061, International Journal of Mechanical Engineering Technologies and Applications 5 (2024) 96–107.
- <span id="page-19-10"></span>[33] S. Zhang, J. Bai, Research on cnc programming and machining process based on cad/cam technology, Applied Mathematics and Nonlinear Sciences 9 (2024) 1–18. URL: <https://doi.org/10.2478/amns-2024-0516>. doi:doi[:10.2478/amns-2024-0516.](https://doi.org/10.2478/amns-2024-0516)
- <span id="page-19-11"></span>[34] H. Wu, X. Wang, X. Deng, H. Shen, X. Yao, Review on design research in cnc machine tools based on energy consumption, Sustainability 16 (2024) 847.
- <span id="page-19-12"></span>[35] M. Soori, F. K. G. Jough, R. Dastres, B. Arezoo, Robotical automation in cnc machine tools: a review, acta mechanica et automatica 18 (2024).
- <span id="page-19-0"></span>[36] Meta AI, Llama 3.2 model card, [https://github.com/meta-llama/llama-models/tree/main/models/](https://github.com/meta-llama/llama-models/tree/main/models/llama3_2) [llama3\\_2](https://github.com/meta-llama/llama-models/tree/main/models/llama3_2), 2024. Version 3.2, released 25 Sept 2024.
- <span id="page-19-1"></span>[37] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783 (2024).
- <span id="page-19-2"></span>[38] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, et al., Qwen2. 5 technical report, arXiv preprint arXiv:2412.15115 (2024).
- <span id="page-19-3"></span>[39] Google DeepMind, Gemini 2.0 flash, 2025. URL: [https://deepmind.google/technologies/gemini/](https://deepmind.google/technologies/gemini/flash/) [flash/](https://deepmind.google/technologies/gemini/flash/).
- <span id="page-19-4"></span>[40] Google DeepMind, Gemini flash lite, 2025. URL: [https://deepmind.google/technologies/gemini/](https://deepmind.google/technologies/gemini/flash-lite/) [flash-lite/](https://deepmind.google/technologies/gemini/flash-lite/).
