Papers,Authors,Published Year
Sensor fusion: Combines information from different sensors at various stages (early,intermediate,late) to enhance object detection.
Experimental evaluation on a modified NIST manufacturing task board: Tests the system’s performance in realistic industrial scenarios.,The research ensured reproducibility by training and testing all three model variants 10 times,confirming consistent results. However
Integrating depth with RGB data significantly improved object detection,especially for challenging objects and scenes; results were consistent across 10 runs,confirming robustness. No p-values were reported.
RGB-D outperforms RGB-only (mean mAP +13%,Mean Precision +11.8%) and Depth-only (mean mAP +78%,Mean Precision +57%).
RGB-D model shows improved detection,especially for objects missed by RGB-only,such as metallic or low-contrast items.
Future work needed for transparent,irregular,non-standard
Recommendation: Employ early fusion of RGB and depth data for efficient,accurate object detection in manufacturing environments.,Need for larger
Improvement of sensor calibration,depth processing,and model fine-tuning in industrial settings.
Exploration of detection for transparent,irregular,non-standard
Application of mathematical models (deterministic and stochastic),advanced analytics,and machine learning (including deep learning) for healthcare analytics and prediction.
Implementation of real-time analytics and simulation through continuous knowledge graph updates and online learning techniques.,No information available,The integration of knowledge graphs and closed-form continuous-time liquid neural networks (CfCs) enables real-time healthcare analytics
No quantitative results or statistical significance (p-values) are reported in the provided context.,Primary outcomes include improved accuracy and reliability in time-series modeling,with recorded values of 0.7362 and 0.7373.
Enables real-time analytics,early diagnosis,intervention
Facilitates personalized care and better patient outcomes.,,Combining knowledge graphs and closed-form continuous-time liquid neural networks (CfCs) enables real-time
This approach supports personalized medicine,early diagnosis,intervention
Advanced analytics techniques,including deep learning and simulation,require more development for dynamic healthcare process evaluation.
Developing a new holistic conceptual tool for designing human digital twins.,The research is not reproducible. No datasets were generated or analyzed,"and code availability is stated as """"""""Not applicable."""""""" No source code is provided."
The model does not offer comprehensive descriptions of ideal processes,states,or precise operator actions.
No datasets were generated or analyzed; no quantitative results or p-values are reported.,The primary outcome is the development of the IEC\_0.81 model,a small-scale operator information processing model.
No statistical values,measured effects,or empirical results are reported.
No datasets were generated or analyzed in this study.,The IEC\_081 model does not provide comprehensive descriptions of ideal processes,states
Further advancement is needed by examining operator actions in differing circumstances.,,
Semantic-driven entity and relationship extraction: BioBERT and medical ontologies (e.g.,SNOMED-CT,UMLS) are used to extract entities and relationships from the data chunks.
Hierarchical multi-agent framework: Integrates LLMs with specialized agents for automated knowledge graph construction,diagnosis,and validation.
No explicit quantitative results or statistical significance (p-values) are provided in the context.,KG4Diagnosis maintains diagnostic accuracy while preventing hallucination,representing an advancement over traditional single-agent approaches.
No explicit statistical values or quantitative results are provided in the context.,System performance depends on the quality and completeness of the knowledge graph,especially for rare or complex conditions.
The framework supports robust,adaptable clinical decision-making and will set new evaluation standards.,Limited effectiveness in handling rare diseases or unusual symptom combinations due to insufficient representation in training data.
Need for further benchmarking against state-of-the-art models (e.g.,ESM-1b,Med-PaLM
Evaluation Metrics (ROUGE and BLEU): Used to measure the quality and accuracy of generated responses by comparing n-gram overlaps with reference answers.,The research is reproducible. The source code and dataset are publicly available for evaluation of personalization approaches.,Our approach outperforms the baseline in all Llama-2-Chat models (7B
The improvements are statistically significant,indicating enhanced text generation quality and better alignment with the golden answer.,Primary outcomes: Our approach outperforms the baseline in BLEU-1
Measured effects: Improved text generation quality,increased lexical overlap with the golden answer,and faster response times.
Further research is needed to improve smaller models' performance on unseen data.,The proposed approach using knowledge graphs (KGs) outperforms the baseline in all evaluation metrics (ROUGE,BLEU
Recommendation: Use KGs for domain adaptation,improved response quality,faster execution
Further reducing hallucinations and factual errors in language model outputs through enhanced integration with knowledge graphs.,Future research should focus on improving the performance of smaller models on unseen data,developing concrete designs for on-device retrieval augmented generation (RAG) with knowledge graphs (KGs)
Incremental analysis via neighborhood detection,allowing experts to iteratively define and analyze logical code boundaries.,
The approach facilitates modernizing code with minimal external dependencies and provides clear integration points,but no p-values or statistical significance are reported.,Primary outcome: Developed a tool for incremental analysis of legacy applications using knowledge graphs and a customizable ontology.
Results: Demonstrated on GENAPP,creating increments (e.g.,seed transaction ‘SSP3’ led to an increment with 1 transaction
Demonstration limited to a mainframe application (GENAPP),though approach is claimed to be extensible.,Incremental analysis helps modernize legacy applications by focusing on relevant code portions and minimizing external dependencies.
Future work includes analyzing application data (tables) and extracting insights from operational logs.,Need to enhance understanding of application data (e.g.,tables) and extract insights from operational logs.
Expanding the knowledge graph to incorporate business functions and data domains beyond static analysis.,Future research directions include understanding the data of the application (such as tables) and extracting insights from operational logs. These areas are suggested as next steps beyond the current tool,which primarily leverages static analysis.
Development of FAS: Designed a novel method combining selective encryption,noise injection,and bitwise scrambling to enhance security and efficiency.
Performance Evaluation: Conducted comprehensive comparisons of encryption techniques using standardized metrics (MSSIM,VIFP) to assess computational overhead,scalability
FAS demonstrates superior robustness to data skew across all datasets,maintaining stable accuracy and privacy (MSSIM and VIFP scores minimally affected),while other methods degrade under skew.
Differential privacy offers lightweight protection with minimal computation impact; FHE provides highest security but incurs major computational costs,making FAS the most efficient and scalable for large-scale,privacy-sensitive applications.
FAS achieves consistent MSSIM and VIFP scores under both skewed and normal data,e.g.,Kidney dataset: FAS MSSIM 62/61
In COVID dataset with EffNetB0,FAS provides 46.15% encryption improvement and 69.77% overhead reduction compared to FEDML-HE; vs. MASKCRYPT: 36.36% faster encryption,60.61% lower overhead.
No self-reported problems,open questions,or suggestions for further research are mentioned.
FAS combines selective encryption,bitwise scrambling,and differential noise
FAS is efficient and scalable for large-scale,resource-constrained,and real-time privacy-sensitive applications.
Recommendation: Use FAS for practical,secure federated learning,especially in latency-sensitive fields like healthcare.
Developing cohesive solutions tailored for federated learning workflows,rather than relying on generic privacy mechanisms.,Future research should refine the FAS techniques and explore hybrid approaches across diverse datasets and federated environments to enhance scalability and applicability. There is a need to address gaps in optimizing privacy mechanisms tailored for federated workflows and further improve security-performance trade-offs.
Modular architecture for figure analysis,separating figure extraction and semantic understanding using NLP techniques.,DeepDive: Source code available at https://deepdive.stanford.edu.
Proposed an architecture more scalable,usable,and extensible than current approaches.
No statistical values or quantitative results reported.,Scalability: Frameworks rely on vertical scaling,which is insufficient for large workloads.
Practical requirements like maintainability,security,and system management are out of scope.
Future work needed on user interfaces and domain feature integration.,The study presents a scalable,flexible
Key implications include the need for easier addition of domain features,pipeline extensibility,user interfaces
Recommendations include developing user interfaces,integrating fairness APIs,and sharing implementation experiences to guide future improvements.
Insufficient support for transparency and fairness,with no mechanisms to filter or flag potentially discriminatory features.,Future research should focus on developing user interfaces
Patch Ranking: Evaluates and ranks generated patches using LLM-generated reproduction tests,regression tests,and patch size prioritization strategies.
KGCompass achieves 45.67% bug resolution,matching SWE-Agent 1.0 and outperforming Kodu (44.67%) and OpenHands (41.67%),with a low average cost per bug ($0.2013).
The hybrid KG+LLM approach yields 4.0% higher file-level and 9.1% higher function-level coverage than pure LLM; using entity path information increases successful patches from 102 to 108 (p-values not reported).,KGCompass achieves a repair success rate of 45.67%,matching SWE-Agent 1.0 (45.33%) and outperforming Kodu (44.67%)
Future improvements in patch validation could further close the small gap to the upper bound (47.67%).,Need for improved modeling of multi-hop relationships in knowledge graphs,as 69.7% of bug functions require multi-hop traversal.
Further optimization of patch ranking and candidate selection strategies to maximize repair precision and efficiency.,Future research should explore integrating more repository artifacts (like issues and pull requests) and leveraging indirect entity relationships for patch generation. There is also a need to investigate the generalizability of KGCompass to programming languages beyond Python and further analyze the impact of structural context in repair tasks.,
Data Filtering: Samples are filtered based on logical connection and hallucination criteria to ensure quality.,,Incorporating more synthetic data during supervised fine-tuning (SFT) consistently improves model performance
SENATOR demonstrates effective deficiency correction even when synthetic data is swapped between models,highlighting the generalizability of the approach; no explicit p-values are reported.,Primary outcome: SENATOR framework improves Llama-3-8B and Qwen2-7B average performance on four medical benchmarks by 11.98% and 9.15%
SENATOR uses less synthetic data (26k–128k samples) than previous methods (514k samples).,Reliance on high-quality,domain-specific knowledge graphs limits applicability where such resources are incomplete or unavailable.
Manual analysis found 37.92% of QA samples had errors: formulaic (16.77%),logical (19.56%),hallucination (1.59%).
Knowledge boundaries in large models are unclear,leading to unreliable or contradictory synthetic samples in specialized domains.,The SENATOR framework effectively detects and repairs knowledge deficiencies in large language models (LLMs) for the medical domain
Reliance on high-quality knowledge graphs and synthetic data quality are current limitations; future work should address these dependencies and improve data generation methods.,Reliance on high-quality,domain-specific knowledge graphs limits applicability; future work aims to relax this dependency by constructing approximate knowledge graphs or using retrieval-augmented methods.
The current synthetic data generation process can be improved; future work will explore advanced techniques for more relevant,diverse,and factual data.
Efficient data synthesis must be tightly coupled with effective detection of knowledge deficiencies to avoid redundancy and better repair model gaps.,Future research should focus on reducing reliance on high-quality,domain-specific knowledge graphs by exploring automatic KG construction or retrieval-augmented methods. Improving synthetic data generation—using advanced techniques for greater relevance and diversity—and adding entity type constraints for more precise domain exploration are also recommended.
Four-step EPC RECAST testing method: (1) Site inspection preparation,(2) On-site data collection,(3) In-office modeling and calibration
Comparative benchmarking: Energy consumption calculated by the standard national procedure is compared with the new generation EPC RECAST procedure to evaluate reliability and improvements.,The research has completed two out of four validation steps. The methodology uses standard national calculation tools and the EPC RECAST online interface. No source code for the project is provided in the context. Full reproducibility will be addressed in future publications after all steps are completed.,Two out of four steps of the EPC RECAST methodology have been applied and validated
Long-Term Monitoring data collection using IoT sensors was found to be cost-effective,reliable,and precise for different room sizes and complexities.
No quantitative results or statistical significance (p-values) are provided in the context.,Primary outcomes focus on comparing energy consumption calculated by different methods (EPC RECAST,standard national procedures
Results assess reliability,effectiveness,and impact of EPC RECAST data collection and calculation engine.
Further research is needed to apply and verify the remaining steps.,The EPC RECAST approach has been successfully applied and partially validated in real case studies,demonstrating its applicability and potential for improvement.
Future work will focus on data mining,model calibration,and the development of the Renovation Roadmap.
Assessment through the Quantitative Verification Strategy remains to be completed and published.,Future research should focus on the complete validation of the EPC RECAST methodology by applying and verifying the two remaining steps. Further studies are needed to assess the reliability and improvement potential of EPCs using the new generation data collection and calculation methods.,
LLM-augmented KG construction: Applies LLMs for entity discovery,coreference resolution,and relation extraction in building KGs.
Representative synergized methods include JointGT (2021),KEPLER (2021),DRAGON (2022)
No explicit quantitative results or statistical significance (p-values) are provided in the context.,No explicit primary outcomes,results
No numerical performance data or statistical analysis is reported for BertCR,Spanbert,CDLM
Retrieval-Augmented Knowledge Fusion methods,like RAG,outperform baseline models in open-domain question answering and generate more factual
Document-level relation extraction benefits from LLM-based and graph-based approaches for improved relation identification.,Effective knowledge injection for black-box LLMs remains an open challenge,especially due to limited access and prompt length constraints.
Developing methods for accurate encoding and alignment of entities across different modalities in knowledge graphs is a key future direction.,Future research should address effective knowledge injection for black-box LLMs,bridging multi-modal LLMs and KG structures
Quantitative metrics: Used metrics like BLEU,ROUGE-L,Exact Match
Ablation studies showed significant drops in performance when removing GRACG,instruction-tuning,or error-handling
The framework closely matches leading proprietary LLMs in effectiveness,offering a modular and adaptable solution for complex chemical and process engineering tasks. No p-values or statistical significance values are reported.,The PEOA framework (Baseline) consistently outperforms ablated variants across all evaluation metrics in task planning
Key metrics for MathComp (Baseline): Recall 78.82%,NDCG 0.69,COMP 76.79%
Key metrics for ChemProc (Baseline): Recall 77.77%,NDCG 0.68,COMP 75.55%
Exact Match (EM) for MathComp: PEOA 73.68%,Triplex 59.50%,Graph RAG 72.50%.
Exact Match (EM) for ChemProc: PEOA 74.13%,Triplex 63.20%,Graph RAG 73.80%.
Response generation (MathComp): BLEU 83.77,ROUGE-L 79.53,EM 80.88.
Response generation (ChemProc): BLEU 83.12,ROUGE-L 80.19,EM 84.95.
Ablation results: Removing GRACG,Instruction-Tuning,or Error-Handling significantly reduces performance across all metrics.
Human evaluation: User satisfaction and usability rated on a scale of 1–5; qualitative feedback categorized as High,Medium-High,or Medium.
The framework demonstrates high answer relevance,context relevance,faithfulness
Conclusion: The complete PEOA framework achieves optimal performance in specialized engineering tasks and closely matches leading proprietary LLMs.,,The PEOA framework demonstrates effective performance across task planning
Human-centric evaluation highlights strengths in user satisfaction,usability,adaptability
Integration of graphs with LLMs and RAG remains an emerging research area,requiring further exploration for optimal performance.,
Blockchain-based decentralized infrastructure: Blockchain is evaluated and proposed for secure,immutable storage of user information,memory pool records
Personalized machine learning (ML) models: The ML Trainer Engine trains personalized models to compose the user's cognitive digital twin.,,
Advancing automated machine learning (AutoML) methods for the learning layer,as current systems lack full automation.,Future research should focus on evaluating the proposed architecture in more scenarios
KG-Based Representation: Converts outputs and reasoning steps into explicit triples,reducing noise,mitigating bias
Toolset and prompt format optimizations improved accuracy,runtime,and cost efficiency; XML-based prompts were retained for consistency.
Merged tool sets improved accuracy,runtime,and cost efficiency.
Noise mitigation (irrelevance removal) improved reasoning by simplifying knowledge graphs.,No explicit limitations,shortcomings
The process emphasizes not inventing information and using only explicitly provided data.,No information available,Suggested future research directions include: developing new knowledge graph construction methods using predictive graph models
Performance was evaluated using standard retrieval (precision,recall,F1) and text generation metrics (BLEU
KERL outperformed baselines in recipe relevance,cooking step quality,and nutrient value accuracy.
KERL-Recom does not directly link health conditions to dietary restrictions; this is left for future research.,KERL-Recom significantly outperforms state-of-the-art LLMs in recipe recommendation,achieving a 56-point F1 improvement over Phi-3-mini-128K and 26 points over Llama-2-7B.
KERL modules deliver more relevant recipes,higher quality cooking steps,and more accurate nutrient values than baselines.
Future work should incorporate ingredient substitution,health information,and cultural preferences for improved personalization.
The full potential of KG and LLM integration in food science remains underexplored.,Future research should focus on integrating ingredient substitution,personal health information
Processing time analysis: Module processing times (e.g.,speech recognition,image capture
Pre-check: 10 scenarios (5 success,5 conditional success,0 failure)
Identification: 10 scenarios (10 success,0 conditional success,0 failure)
Instructional Guidance: 7 scenarios (7 success,0 conditional success,0 failure)
Recommendations/Solutions: 10 scenarios (10 success,0 conditional success,0 failure)
Emotional Support: 11 scenarios (11 success,0 conditional success,0 failure)
Assembly Verification: 6 scenarios (3 success,3 conditional success,0 failure)
No comprehensive user studies conducted yet.,The HDT system effectively delivers clear instructions,proactive problem-solving
Future work should focus on user studies,custom domain-specific models,and improved data privacy and security.
Conduct comprehensive user studies to evaluate effectiveness,usability,and trust in diverse HATs activities.
Address data privacy and security,including customization of data sharing,content filtering
Hyperparameter tuning was performed,selecting best values based on MRR (Mean Reciprocal Rank) on the validation set.,The research is reproducible. The source code for the project is available at https://github.com/liu-yushan/TLogic. A random seed of 12 is fixed for the rule learning component to ensure reproducible results. All experimental details and hyperparameters are provided.
The combination of rule confidence and exponential score components yields the best overall performance; statistical significance (p-values) is not reported.,TLogic outperforms all baseline methods in MRR,hits@3
No explicit mention of generalizability beyond tested datasets.,TLogic outperforms all baseline methods in most metrics (MRR,hits@3
Object distribution baseline adds little value; temporal rules are essential for strong predictive performance.,Existing embedding-based methods lack transparency and interpretability,making predictions difficult for humans to understand.
Manual creation of logical rules is challenging due to the complexity of events,leading to a knowledge acquisition bottleneck.,As future work
Secure Multi-Party Computation (MPC): Cryptographic protocols enabling parties to jointly compute functions over private data without revealing individual inputs.,The research provides source code at https://github.com/PatrickZH/Improved-Deep-Leakage-from-Gradients and https://github.com/JonasGeiping/invertinggradients. Experimental results are reported,supporting reproducibility. No explicit mention of code for the proposed Generative Gradient Leakage (GGL) method is found.
GGL demonstrates superior data reconstruction accuracy under additive noise,gradient clipping,gradient sparsification
No explicit p-values or statistical significance values are reported in the context.,The primary outcome is that the proposed GGL method reconstructs high-quality images from gradients under all four defense schemes: additive noise,gradient clipping
GGL achieves the lowest MSE-I (0.0780–0.0968),highest PSNR (10.1434–11.1902),and lowest LPIPS (0.1620–0.2561) across defenses
Many data reconstruction attacks assume idealized,bare-bone FL systems without real-world privacy defenses,contradicting industrial practices.
Existing attack methods are limited to shallow networks and low-resolution images,though some recent works extend to deeper networks and higher resolutions.,Cryptography-based (e.g.
No single method fully prevents privacy leakage; combining strategies is recommended.,Recovering high-resolution batch data with multiple local steps is still a major challenge,especially for batch sizes greater than 1 and large images.
Existing privacy-preserving methods often reduce model utility or are insufficient against advanced inference attacks.,Future research should address the challenge of recovering high-resolution batch data with multiple local steps,as current methods are limited to small images or specific scenarios. Further investigation is needed into privacy leakage under various defense strategies and the reconstruction of in-the-wild data.
Hybrid and low-supervision techniques: Combine rule-based and machine learning methods,including approaches like active learning.,No information available
Difficulty in addressing domain-independence,scalability,and heterogeneity simultaneously.
Schema-free approaches are novel,with unresolved conceptual and methodological questions.,Low-supervision methods can effectively detect urgency in short crisis messages.
The approach reduces the need for extensive manual annotation,making it scalable for real-world crisis response.,Advancing schema-free approaches to Entity Resolution (ER) to address data diversity and heterogeneity
Applying and enhancing transfer learning for ER to reduce training data requirements and improve scalability across domains.,Future research should focus on schema-free approaches to Entity Resolution (ER),improving property alignment and ground truth construction
Development of low-cost,scalable,and self-powered sensors using energy harvesting technology.
No quantitative results or statistical significance (p-values) are provided.,No primary outcomes,results
Security concerns in data collection,transmission,and storage.
Ethical issues: data privacy,informed consent,potential misuse/discrimination
Key challenges include data scarcity,security,cost
Recommendations include promoting collaboration,addressing ethical concerns,and leveraging technological advances for data collection and model development.
Need for a unified five-level roadmap to standardize modeling and foster interdisciplinary collaboration in human body DT research.,Future research should address the scarcity of clinician-annotated human data,enhance data-sharing initiatives
SPARQL Queries: Employed to retrieve and analyze temporal sensor observations and building context information from the knowledge graph.,The research proposes a reusable structured method to add temporal attributes to RDF triples and a temporal knowledge graph-based data integration schema. There is no explicit mention of source code availability for the project.,The study introduced time constraints into semantic building digital twin graphs and demonstrated that simple time or entity-specific queries are achievable
The proposed temporal knowledge graph-based schema enables integration of static and dynamic building data,but further research is needed for efficient complex queries and optimal graph representation.,Primary outcome: Introduction of a temporal knowledge graph-based data integration schema for linking static building data and dynamic sensor data using RDF format.
Further research is needed for complex queries (e.g.,cluster queries,event pattern matching).
Extending semantic language syntax increases statement complexity,though it adds flexibility.,Integrating time constraints into semantic building digital twin graphs is feasible
Unified temporal knowledge graphs can enhance data integration,reasoning,and automation in the AEC industry.
Future work should focus on optimizing temporal knowledge graph representation and developing advanced query capabilities for complex event patterns.,Efficient representation of temporal knowledge graphs for high-throughput,real-world AEC projects needs further investigation.
Integration of dynamic sensor data and building context data into a unified graph for comprehensive reasoning remains a significant challenge.,Future research should focus on: integrating dynamic sensor and building context data into unified graphs; improving efficiency and representation of temporal knowledge graphs for real projects; developing advanced query methods for complex,event-based queries; and enhancing reasoning capabilities to extract new insights from evolving knowledge graphs.
Natural Language Processing (NLP) techniques for parsing,cleaning,and structuring raw documents into manageable content and extracting metadata.
Embeddings indexing with pre-trained models (e.g.,SciBERT) to enable semantic search and retrieval of relevant information.,The research demonstrates reproducibility through a limited Agentic Publication demo
Metadata and content extraction enable transparent,traceable answers,with visualizations and summaries aiding user understanding.
No quantitative results,statistical significance (p-values),or primary research findings are reported in the context.
Biases in LLM training data can affect fairness and accountability.,Visual and summarization tools,paired with clear explanations
Multi-modal responses (e.g.,graphs,tables
Current AI systems struggle with representing complex knowledge types (e.g.,mathematical proofs,nuanced arguments
Need for AI systems that actively generate hypotheses,identify research gaps,and provide personalized
Evaluating model judgments on class rankings based on cohesion or coupling measures for validity.,,Current benchmarks mainly assess code correctness
No quantitative results,conclusions,or statistical significance (p-values) are provided in the context.
Token-based metrics (e.g.,BLEU,ROUGE
Benchmarks include HumanEval,MBPP,and BigCodeBench.
The evaluation assesses correctness,code quality (cohesion/coupling),and adherence to good software design practices.
Training and evaluation should integrate SE knowledge to produce maintainable,scalable,and robust software.
Instruction-based models and taxonomy-based frameworks (e.g.,Bloom’s Taxonomy) are recommended for comprehensive assessment of SE knowledge.,Current probing methods (like linear classifier probes) are limited in capturing complex and abstract software engineering (SE) concepts internalized by models.
There is a need for comprehensive assessment frameworks,such as Bloom’s Taxonomy,to evaluate higher-order cognitive skills in code generation.
Existing models overlook essential SE knowledge,including principles and best practices for maintainable,scalable
Multi-layer embedding contrastive learning: Mines temporal relationship connections using intra-layer and inter-layer comparative learning strategies.,No information available,The DNCL model achieved the highest MRR of 51.18% on ICEWS14
No statistical significance (p-values) is reported in the context.,DNCL achieved the best overall performance across four datasets (ICEWS14,ICEWS05-15
Need for further research on integrating multimodal information and optimizing for ultra-large-scale,ultra-sparse,and strongly noisy data.
Suggestion to explore advanced methods like large language models,meta-learning,and incremental learning.
Recommendation: Retain both intra-layer and inter-layer contrastive learning for optimal results.,Combining multimodal information (text,images
Optimizing the DNCL model structure and algorithm for ultra-large-scale,ultra-sparse,and strongly noisy data.
Introducing methods like large language models,meta-learning,and incremental learning to enhance adaptability.
Data fusion techniques: Includes early fusion (combining embeddings before prediction),joint fusion (updating embeddings during training),and late fusion (combining predictions from separate models).
No explicit quantitative results,primary findings,or statistical significance (p-values) are provided in the context.
Infrastructure challenges: heterogeneous computational resources,identity/access management,ease of use
Data silos due to administrative,privacy,and regulatory constraints impede data integration and sharing.
Need for additional efforts in data extraction,transformation,and loading.
Creation of cost-effective algorithms to enable adoption of federated learning by institutions with limited resources.,Future research should focus on developing methods to calculate the optimal privacy budget for biomedical multimodal datasets to prevent model inversion attacks while improving model performance. Further investigation is also needed to address the complexities of multimodal federated learning (FL).,
Data extraction with open coding: Two authors extracted data using open coding,with a third author resolving disagreements.,
Study types: 40% case studies,14% peer-reviewed literature,10% tool reviews
No explicit quantitative results or statistical significance (p-values) are reported in the provided context.,46 papers included: 36 white literature (78%),10 grey literature (22%).
Study types: 40% Case Study,14% Experiment,3% Exploratory Study
Survey correctness and completeness may affect results,AI-assisted programming offers significant short-term opportunities but requires explainability,especially for architectural decisions.
Ethical considerations and privacy: Integrating ethical AI practices,addressing biases,transparency
Human oversight and explainability: Ensuring human interaction,oversight,and explainability in AI-assisted programming is a key research gap.
User acceptance is based on concise,single-sentence summaries that clearly state the research goal,approach
User expects explicit mention of main objective,key method,and principal finding in the summary.
User values inclusion of terms such as research goal,approach,results
User requests concise,one-sentence summaries for academic papers.,
User prefers explicit identification of research goal,method,and principal finding.
User expects use of keywords like research goal,approach,results
User expects the Assistant to structure responses around main objective,method,and finding.
Evaluation employed the MedMCQA dataset and introduced heuristic metrics to assess chatbot response effectiveness.,,MARK’s refined memory system builds ~10 memory units per conversation (2.57 residual
Average response token count dropped from 415 to 149,indicating improved efficiency; statistical significance is shown by maximized AICS difference at α = 0.1.,Primary outcomes measured: Information Capture Score (ICS)
Results: With memory augmentation,AICS increased from 0.18 to 0.36 (100% improvement),and KPCS increased from 0.12 to 0.32 (166.7% improvement). ICS remained constant at 0.76.
Memory may override newer facts or ignore past user context.,User acceptance indicates that concise,accurate
The User values clear extraction of main conclusions,implications,and recommendations.
Introduce collaborative planning refined memory agents to dynamically manage and adapt the refinement process based on user feedback.,Future research should investigate long-term memory retention,strategies to prevent erroneous memory formation from incorrect user input
Single-cell-based methods,including scGWAS and integration of scRNA-seq with pharmacogenomic databases,to infer clinical traits and drug sensitivity.
Construction of multi-layer network models using machine learning to integrate diverse disease-associated variables for personalized treatment.,,The paper highlights significant social
A US National Academy of Science white paper recommends a cross-sector,integrated agenda to advance digital twin technologies,emphasizing harmonization and realistic applications; no quantitative results or p-values are reported.
Unresolved issues of data ownership,ethics,data security
Machine learning models often lack explainability,affecting trust and usability.,Digital twins (DTs) can transform healthcare by enabling predictive
Key challenges include ethical,regulatory,organizational
Need for solutions to disseminate digital twins globally for equitable and effective health in line with the 2030 agenda.,Future research should address gender differences in digital technology use,data ownership
Multi-scale (population,individual,tissue
Integration of environmental,genetic,and clinical data
Application of machine learning and computational methods,,
Adaptation of the WHO/HAI methodology,including the creation of two core sets of pediatric medicines and the novel NUNT parameter for affordability calculations.,The research is partially reproducible. Aggregated data per medicine and country are available from the HAI website
Only a modest sample of age-appropriate medicines was included,limiting the findings; further analyses with larger,prospectively collected datasets are needed.
No specific statistical values or measured effects are provided in the context.,Adapted methodology inherits limitations from the original tool,such as burden of disease weighting and use of national poverty line for affordability.
Further validation and analyses on larger,prospectively collected datasets are needed.,The adapted child-specific methodology provides proof of concept but was limited by reliance on historical data and a small sample of age-appropriate medicines.
The adapted affordability measure combining national poverty line and lowest-paid government worker wage needs further validation.,Future research should use larger,prospectively collected datasets with more age-appropriate medicines. Additional analyses are needed on weighting approaches for disease burden and on the minimum number of medicines required for reliable accessibility measures. More data on child medicines is needed for meaningful sensitivity analyses.
"goal\_analysis"""""""": """"""""The main goal is to identify and summarize up to three key research methods",methodologies,or techniques used in the study evaluating AGENTiGraph’s performance in NLP and CV domains.
"description"""""""": """"""""Summarize up to three key methods or techniques",ensuring concise and clear explanations.,
"execution\_strategy"""""""": """"""""First",examine the methodology section for explicit mentions of research methods. Next,distill and summarize the top three methods in bullet points
In the computer vision domain,14 out of 34 queries were satisfactory; 20 suggested improvements,mainly requesting more detailed technical explanations.
User satisfaction was high for efficiency and focused answers,especially for users with prior knowledge,but completeness and technical depth were noted as areas needing improvement. No statistical significance (p-values) reported.
Measured effects: High user satisfaction for efficiency and conciseness,especially for users familiar with core concepts.,The study may lack sufficient technical depth in explanations of complex concepts.
Inconsistencies in response quality across domains and question types.,AGENTiGraph provides concise,efficient responses
Recommendations include enhancing technical depth,handling abstract queries better,and adding domain-specific improvements.
Enhance domain-specific explanations with more technical detail and practical examples,particularly in Computer Vision.,What are the main subfields connected to semantic role labeling that I should explore for a deeper understanding?
How do named entity recognition and relation extraction interact,and what other concepts bridge these tasks?,
Taxonomy-based analysis: Categorizes MIA research into update-based and trend-based approaches to structure the review and comparison of methods.,,Robust defense mechanisms can prevent membership leakage in federated learning (FL)
Many defense strategies either overstate effectiveness or cause significant utility loss; DP offers strong privacy but with high utility costs,making it unsuitable for critical applications. No statistical significance (p-values) reported.,There is a lack of a standardized evaluation benchmark for assessing defense mechanisms
No specific statistical values or measured effects are provided.,Lack of a standardized evaluation benchmark,making comparison of defense mechanisms difficult.
Scarcity of defenses balancing privacy,utility,and efficiency.
Potential negative correlations between defenses against MIAs and other attacks,such as model extraction or poisoning.,There is a critical need for a unified evaluation benchmark to compare defense mechanisms against MIAs in federated learning (FL).
Future research should explore new attack vectors,refine defenses,and consider correlations among different attack types.
There is a scarcity of defense mechanisms that balance privacy,utility,and computational efficiency.
Multimodal multidimensional data integration from multiple sources,such as text,tabular data
Application of natural language processing (NLP) to detect domain concepts and generate resource description framework (RDF) graphs.,The research uses publicly available data via APIs (such as Reddit API and Open Data API) and does not include real patient data. There is no mention of source code availability for the project. Reproducibility is limited to data collection methods described; no code is provided.,The study proposes using Reddit data and sentiment analysis to identify vaccine misinformation in online communities.
No statistical significance (p-values) or quantitative results are reported; all case scenarios are illustrative,not based on real patient data.,No real patient data or human subjects were included; all scenarios were generated for demonstration.
The PHL (personal health library) is at the prototype stage; no primary outcomes,results,or measured effects (including statistical values) are reported.
No ethics committee approval due to absence of human data.,The PHL app can promote HPV vaccinations and cancer screening,and address barriers to other vaccinations and health care delivery.
The PHL is in prototype development; formal user experience assessments are planned after full implementation.,The PHL is currently a prototype; further research is needed to implement,optimize
Formal user experience assessment (qualitative and quantitative) is planned to evaluate effectiveness,usability,and popularity.
Future work includes expanding PHL applications beyond HPV to other vaccinations and chronic conditions.,Future research should assess the PHL’s effectiveness,functionality
Modeling research artifacts using established vocabularies and ontologies (e.g.,schema.org) to enhance visibility,findability
Application of deep learning methods,especially in natural language processing and image analysis,involving code
RKGs facilitate structured,machine-actionable representations of research,benefiting researchers
RKGs enable better research management,enhanced reproducibility,transparency
RKGs facilitate discoverability,integration,and interoperability of research artifacts.
No explicit statistical values or measured effects are provided.,Many research artifacts and their relationships are not properly referenced,limiting citability and credit.
Limited availability of quality-controlled ground truth datasets for scholarly information extraction tasks.,Research Knowledge Graphs (RKGs) transform how research artifacts are stored,managed
RKGs support FAIR principles,improving transparency,traceability
Well-structured RKGs facilitate efficient research management,discovery,and validation.
Challenges in integrating and interlinking diverse Research Knowledge Graphs (RKGs) to enable comprehensive knowledge discovery.,Future research should address gaps in reproducibility and transparency,explore standardized approaches for building and interlinking Research Knowledge Graphs (RKGs)
Comparative Prompt Evaluation: Assessed Responsible Prompt Engine (RPE) against standard prompting methods (zero-shot,few-shot,instruction-tuned) across four health-related datasets.
Multimodal Input Testing: Incorporated both textual and screenshot data (e.g.,wearable device summaries) to evaluate system performance in multimodal scenarios.,The research is reproducible. The official implementation repository
RPE-generated prompts led to improved semantic similarity (BERTScore),content overlap (ROUGE-L),and word-level accuracy (BLEU) compared to baseline methods.
RPE responses demonstrated higher instructional compliance and responsibility rubric scores,but no explicit p-values or statistical significance measures are reported.,Primary outcomes:
RPE outperformed zero-shot,few-shot,and instruction-tuned prompting strategies for both patient-side and provider-side prompts.
RPE led to improved structured,ethical,and personalized prompt generation
Exact numerical values for these metrics are not provided in the context.,Limited adaptability of LLMs for long-term,personalized well-being interventions beyond clinical or mental health.
Direct performance comparisons with prior systems are invalid due to differing experimental setups,data sources,and model types.
Impact on workforce sustainability remains challenging to address at this early stage.,The Responsible Prompt Engine generates structured,ethical
Slot-based prompting and safety tuning enhance transparency,ethical guardrails,and factual grounding in health-related outputs.
Multimodal data integration and predictive modeling enable personalized,actionable recommendations for diverse health scenarios.,Enhancing the adaptability of large language models (LLMs) for long-term
Developing algorithms to transform unstructured user input into structured prompts and system instructions for adaptive AI inference.,Future research should focus on enhancing LLM adaptability for long-term,personalized well-being interventions
Systematic review synthesizing over 100 studies. The study also includes experimental evaluation of the Responsible Prompt Engine using four benchmark datasets across diverse health domains. The evaluation is not randomized,controlled,or blinded; it is primarily observational and comparative.
Semantic-preserving user-item interaction publishing: Ensures privacy while maintaining semantic information by carefully perturbing and publishing user-item interactions.,,FedHGNN outperforms all FedRec models by up to 34% in HR@10 and 42% in NDCG@10
Ablation studies show the two-stage perturbation mechanism significantly improves performance; FedHGNN achieves HR@10 of 0.4185 (ACM),0.4373 (DBLP),and 0.2977 (Yelp).
Ablation study shows two-stage perturbation improves performance; FedHGNN outperforms all its variants in most metrics.,Assumes centralized data storage,which may not be realistic due to privacy concerns and regulations (e.g.
Performance depends on the number of shared HINs and privacy budgets.,FedHGNN achieves superior or competitive performance compared to baselines,especially on ACM and DBLP datasets.
First-stage perturbation is essential for privacy,and the designed EM preserves user high-order patterns with minimal performance loss.,How to rigorously define and protect privacy in HIN-based federated recommendation (FedRec) settings.
Investigating why PFedRec outperforms some GNN-based FedRec baselines,particularly the impact of personal item embeddings and parameter initialization.,Future research should address: (1) formally defining privacy in HIN-based FedRec
Cognitive Behavioral Therapy (CBT): Targets the connection between thoughts,emotions,and behaviors to address psychological challenges.
Person-Centered Therapy: Emphasizes empathy,acceptance,and the client’s autonomy in the therapeutic process.
Ablation studies show synthetic multi-turn dialogue fidelity over 60%,with performance drops when linguistic style (68.75%),therapy technique (75.0%)
No explicit p-values or statistical significance values are reported in the context.,The synthetic multi-turn dialogue achieved a fidelity of over 60% in integrating linguistic style,therapy technique
Expert evaluation scores for PsyDTCorpus: Conversation Strategy 8.39,State and Attitude 8.69,Relationship Building 8.29
Ablation study results: 68.75% fidelity without linguistic style,75.0% without therapy technique,62.5% without client personality.
Further research suggested for broader applicability.,The PsyDT multi-turn dialogue synthesis method achieves over 60% fidelity,effectively integrating linguistic style
Manual evaluation shows PsyDTCorpus outperforms baseline datasets in conversation strategy,state and attitude,relationship building
Ablation studies confirm each element (linguistic style,therapy technique,client personality) is crucial for high-quality dialogue synthesis.
Recommendation: Use the PsyDT framework for generating professional,consistent multi-turn mental health dialogues.,The framework only constructs digital twins with specific counseling styles and cannot meet all clients' psychological needs.
Further research is needed to address the complexity of psychological counseling beyond current model capabilities.,,No information available
Quantitative evaluation: Used Precision,Recall,and F1 Score to measure extraction accuracy and effectiveness.
Digital twin architecture: Implemented a three-layer knowledge graph (concept,model,decision layers) integrating physical and virtual environments for process simulation and optimization.
No p-values or statistical significance values are reported in the context.,The proposed three-layer knowledge model achieved Precision: 77.38%,Recall: 83.14%
It outperformed BERT-Bi-LSTM-CRF (Precision: 71.43%,Recall: 73.17%,F1 Score: 72.29%)
The model demonstrated superior accuracy,generalizability,and effectiveness in extracting knowledge triples from manufacturing documents.
Dependence on expert knowledge,limiting universality and flexibility; slows adaptation to new information.,The proposed three-layer knowledge model outperforms existing methods in extracting accurate and relevant knowledge triples
Integrating domain knowledge and cognitive reasoning enhances accuracy,generalizability,and adaptability across technical environments.
The model enables dynamic optimization,real-time monitoring,and improved efficiency in smart manufacturing systems.
Dependence on expert knowledge: Over-reliance on domain experts limits adaptability. Future directions include automated knowledge extraction using machine learning and natural language processing.,Future research should address data integration difficulties by developing universal data exchange standards and ontology alignment methods,improve real-time data processing with scalable frameworks and edge computing
Error-aware generation with multi-agents: Uses multiple agents to reduce hallucinations and improve factual accuracy during biography generation.,,The proposed system reduces atomic fact errors by 35% and achieves higher ROUGE scores compared to baselines.
Index construction time is 422.65s,much faster than GraphRAG (5279.65s) and NER baselines (885.68s,1037.31s)
Online retrieval is extremely fast (0.00146s),and the longer generation time (1236s) significantly lowers hallucination rates and improves factual accuracy.,Our system (AIstorian) achieves highest ROUGE-1 (83.69)
Retrieval: Precision 0.936,Recall 0.944,F1 Score 0.923.
Information fragmentation occurs due to scattered sources and chunking.,The proposed system achieves higher factual accuracy and stylistic consistency than baselines,notably reducing hallucination rates.
Lack of domain expertise leads to errors,highlighting the need for better knowledge retrieval and integration mechanisms.,
Reflexive Thematic Analysis: Used to analyze participant viewpoints from questionnaires and co-design sessions.,,Visual outputs significantly improved task efficiency and comprehension
Organization and interpretability received the highest ratings (KG: A = 4.66; LLM: A = 4.67; Interpretability: A = 4.58),while granularity was rated lower and showed statistical significance (A = 4.25,p = 0.042).
Participants strongly preferred visual and hybrid formats over text-only,citing clarity,rapid information delivery
Accuracy (mean = 4.25,SD = 0.75) and Granularity (mean = 4.25,SD = 0.62) were slightly lower.
Participants preferred hybrid visualizations for clarity and rapid information delivery.,High latency due to multiple prompts for better feedback,causing slower response times.
Users may struggle with complex concepts due to lack of domain knowledge.,Both KG and LLM outputs were highly rated for organization and interpretability,making information easy to understand and use.
Visual outputs significantly improved task efficiency,comprehension,and decision-making.
Recommendation: Use structured visualizations and progressive disclosure to enhance user experience and support preferences.,Difficulty distinguishing reliable from unreliable healthcare information,especially with LLM-generated content; need for enhanced trust and transparency.
Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Maintenance,Peng Xin,Wang Chong
Synergistic Pipeline: Integrates large language models (LLMs) and static program analysis for comprehensive knowledge extraction and representation.,,The code digital twin model integrates key knowledge elements (domain concepts
The framework emphasizes combining structured (e.g.,knowledge graphs) and unstructured (e.g.,documentation) knowledge for comprehensive software understanding.
The framework aims to integrate structured and unstructured knowledge,manage ambiguity and versioning,and link key knowledge elements (domain concepts
Maintaining knowledge completeness as software evolves.,The code digital twin framework integrates structured and unstructured software knowledge for scalable,adaptable representation.
Key challenges include knowledge integration,ambiguity management,and ensuring long-term scalability and adaptability.
Establishing efficient,accurate,and scalable automatic pipelines for extracting and updating knowledge from evolving software artifacts.
Managing ambiguity and knowledge versioning to ensure adaptability and coherence as software evolves.,Future research should focus on: developing scalable frameworks for integrating structured and unstructured software knowledge; creating efficient knowledge extraction pipelines; adapting to evolving software artifacts; leveraging human input through incentives and user-friendly interfaces; and maintaining knowledge completeness and accuracy as systems evolve.,
Implementation of fusion models using various weighting schemes (accuracy weighting,entropy weighting,linear/logistic regression
Use of provenance chains to track the origin and influence of information for interpretability and to prevent feedback loop errors.,,The Digital Twin (DT) design enables holistic patient attribute estimation and interpretability
The DT system’s modularity,feedback loops,and provenance chain support transparency and ongoing improvement
Statistical values: No specific numerical/statistical values (e.g.,ROC AUC,F1 scores) are reported in the context.
Clinical translation requires rigorous validation,regulatory compliance,robust control systems
Limitations include data availability,model modularity,and potential for increased health inequality.
Future opportunities span multiple medical fields,but require large-scale,diverse data and further development.
Robustness Analysis: Training multiple models independently on the same data to assess semantic consistency and reliability of outputs.,The research is reproducible. The source code and datasets for ReGraP-LLaVA are released at: https://github.com/xyfyyds/ReGraP.,ReGraP-LLaVA demonstrates robust
Hard-prompt method achieves highest accuracy,with less than 0.4% difference from other methods.,The model's answers can be overly detailed when a short response is sufficient.
Some evaluation aspects (e.g.,multi-object QA,relational reasoning) are not supported by all compared benchmarks.
Recommendation: Use longer,detailed CoT answers for challenging queries; shorter answers suffice for basic tasks.,Integrating images
Evaluating MLLMs’ relational reasoning and knowledge connection capabilities for personalized,context-dependent queries.,
Incorporated both manual and semi-automated methods,combining human and machine efforts for assessment.,
The framework uses quantitative scores for QDs and quality metrics (QMs),with alpha- and beta-weights to aggregate and tune results; all scores and weights are within \[0,1].
Introduction of additional QDs (e.g.,conciseness,interlinking
No explicit statistical values or measured effects reported.,Most authors,except \[13
A unified quality assessment approach is lacking and needed for broader adoption.,The study proposes 20 quality dimensions (QDs) and several metrics for assessing Knowledge Graphs (KGs),emphasizing cost-effectiveness
Future work includes developing,evaluating,and semi-automating the framework
Pursuit of a unified quality assessment approach to improve adoption and applicability of KGs globally.,Future research should focus on developing and evaluating the proposed KG assessment framework,conducting expert surveys to refine quality dimensions (QDs) and the framework
Replay-Based Rehearsal and Pruning: Uses replay of recent experiences and adaptive pruning based on usage counters and feedback to maintain memory efficiency and prevent forgetting.,,
Addresses the stability–plasticity dilemma by separating learning timescales (STM,LTM,PM).
Table 1 conceptually compares the framework to existing methods on memory efficiency,catastrophic forgetting mitigation,edge compatibility
Tradeoff between plasticity and efficiency.,The proposed architecture combines neuroscience-inspired strategies for continual learning,including synaptic pruning
Selective memory consolidation and modular expert gating enable efficient,personalized,and energy-aware on-device AGI.
Further research is needed on neuromorphic integration,theoretical guarantees,and balancing generalization with personalization.
Developing theoretical guarantees for stability–plasticity trade-offs and error bounds in online,pruned,and updated models.
Integrating the framework with neuromorphic hardware or spiking neural networks for enhanced energy efficiency.,Suggested future research directions include: implementing the architecture on neuromorphic hardware or with spiking neural networks; developing theoretical guarantees for continual learning (such as stability–plasticity trade-offs and error bounds); optimizing pruning and consolidation strategies; managing expert modules; and balancing generalization with personalization.,
STRESSnet tool: Provides ongoing,unobtrusive evaluation of individual and team communications for stress-related states.,No information available
Further research suggested for broader applicability and validation.,STRESSnet offers an unobtrusive way to assess cognitive and emotional states,such as stress and anxiety
This method supplements,but does not replace,other assessment approaches.
Addressing how lexical measures can be integrated with traditional assessment approaches for comprehensive monitoring.,,
Enhancing Temporal Knowledge Graph Representation with Curriculum Learning,Liu Yihe,Shen Yi
Ablation study: The study removes or modifies specific modules (e.g.,difficulty assessor,sphere filtering) to analyze their individual contributions.
Incremental training strategy: The model is trained incrementally,focusing first on simpler data and progressively introducing more complex data.,
All improvements were statistically significant,demonstrating enhanced efficiency and generalization without altering model structure.,Curriculum learning (CL) improved MRR by 1–2% for DE-SimplE and DistMult on ICEWS14
Ablation study: Removing CL components reduced performance; full CL achieved highest MRR (53.29 on ICEWS14).,Improvements in MRR scores are relatively modest,ranging from 1–2%.
No explicit mention of other limitations or suggestions for further research.,The proposed curriculum learning framework improves model performance and training efficiency,achieving 1–2% higher MRR and faster convergence than baseline models.
The framework is adaptable,requiring only changes in training sample order,not model structure.
Addressing challenges related to data sparsity and incomplete temporal knowledge graphs during training.,,Ablation study; experiments removing specific modules; curriculum learning framework; incremental training strategy; staged training with increasing data complexity; bucket-based grouping (3
Dynamic global information attention layer (GIA): Integrates global information for improved reasoning in temporal knowledge graphs.,The paper provides a detailed experimental setup for reproducibility,including datasets
No explicit p-values or statistical significance values are reported.,Primary outcomes measured: Mean Reciprocal Rank (MRR),Hits@1
Only reports experimental results under the original setting,which may yield incorrect higher-ranking scores in some cases.,TD-RKG outperforms baseline models on most datasets
Current models do not fully consider the varying influence of time on different entities.,Future research should address the model's limitation in handling new entities,especially in datasets like YAGO. Adopting a meta-learning strategy is suggested to improve adaptation and reasoning for new entities. Further exploration of implicit correlations and time sensitivity challenges is also recommended.
Contrastive Enhanced Learning for Multi-Label Text Classification,Wu Tianxiang,Yang Shuqun
Distance metrics (cosine,Euclidean,Manhattan): Evaluated for measuring vector distances in the label space
Attention mechanisms: Integrated to combine label and text information,enhancing semantic representation of labels.,
Incorporating comprehensive label information and optimizing multiple loss components led to superior classification performance compared to existing models; no explicit p-values or statistical significance values are reported.,On AAPD,"the proposed model (""""""""Ours"""""""") achieved the highest scores: P@1: 86.58"
Ablation tests showed removing contrastive label representation learning,positive label representation alignment,or contrastive global representation learning reduced performance.
Combining label and document information is often simplistic,limiting full utilization of label information.,Incorporating label information significantly improves multi-label text classification performance.
Positive label representation alignment,though simple,robustly enhances classification performance.
Future research should explore advanced integration strategies for label and document information to enhance multi-label text classification.,Future research should focus on improving label representation,especially balancing label textual semantics and label correlation. There is a need to develop methods that better utilize label information
Sensitivity Analysis: The study tests the impact of activated client numbers,receptive field depth,and interaction item protection (privacy mechanisms) on model performance.
Incorporating knowledge graphs and relation-aware aggregation,especially using GNNs,significantly improves recommendation accuracy.
Not all methods leveraging side information are effective; design and integration are critical.,Lack of user connections: The current framework does not utilize user connections,limiting potential improvements.
Balancing privacy and performance: Excessive privacy measures (e.g.,high flip rates) can degrade system performance,requiring careful trade-offs.
Retrieval-Augmented Generation (RAG): Chunking text,converting it to vectors,storing in a vector database
Dataset preparation: Splitting engineering documents into text chunks,generating question-answer pairs using Llama-3.3-70B-Instruct for fine-tuning and testing.,The research describes its experimental setup and evaluation metrics but does not mention the availability of source code or other materials needed for full reproducibility. No links or references to project source code are provided.
No quantitative results or statistical significance (p-values) are reported.,Primary outcomes measured: ROUGE-L (R-L),Exact Match (EM)
Some RAG methods assume each query has a single answerable chunk,do not handle multi-hop retrieval,and are only tested on small datasets.
RAG struggles with noisy data and sometimes cannot reject irrelevant queries.,SLG outperforms stand-alone Llama-3.1-8B-Instruct and Llama-3.2-1B-Instruct models,especially in resisting hallucinations (EM metric: 3x better).
There is a need for methods that combine computational efficiency,high accuracy,and effective handling of domain-specific tasks.
RAG-based techniques lack standardized evaluation metrics and are overly reliant on complex models.,The study suggests future research should include more extensive comparisons with larger models like Llama-3.3-70B-Instruct and RAG. There is also a need to develop methods that combine computational efficiency with high accuracy for domain-specific tasks,addressing current limitations in model selection and evaluation.
Federated Learning (FL): Enables privacy-preserving graph computation by avoiding raw data transmission,though model updates may still risk privacy.,
No quantitative results or statistical significance (p-values) are reported in the provided context.,The paper systematically reviews privacy-preserving techniques in graph machine learning,focusing on both data and computational aspects.
It introduces various attacker types,required background knowledge,and corresponding protection mechanisms.
Switching-based Graph Generation: Uses iterative edge switching (Monte Carlo method) to anonymize graphs while preserving features like degree distribution,eigenvalues,eigenvectors
No explicit statistical values or quantitative results are provided.,Assumption of shared model architecture across clients,despite optimal architectures differing due to graph size and over-smoothing in GNNs.
Need to avoid cross-client transmission without degrading model performance.,Current graph-level federated learning (FL) often assumes shared model architectures,but optimal architectures may differ across clients due to data and graph size differences.
Future research should address model heterogeneity and disentanglement of task-relevant information for better privacy-utility trade-offs.,Addressing model heterogeneity in graph-level federated learning (FL),as optimal model architectures may differ across clients with varying graph sizes.
Combining privacy-preserving data generation and computation to enhance privacy while maintaining model utility,despite associated challenges.,Suggested future research directions include: designing privacy-preserving graph generation methods that consider downstream machine learning tasks
Uniform Manifold Approximation and Projection (UMAP): Another nonlinear technique for uncovering intrinsic data structure.,,The paper proposes that dimensionality transformation
No quantitative results or statistical significance (p-values) are reported.,In rats,all cortical unimodal and transmodal areas project to the parahippocampal region; in humans
Neural oscillation frequencies (ripple,gamma,spindle
Suggestion for future research on representational dimensions across species.,Episodic memories are dynamic and undergo dimensionality transformation,often losing or altering details over time.
Gamma oscillations may facilitate these neural transformations,impacting memory encoding,storage
Dimensionality reduction supports efficient memory,object recognition,and goal-directed behavior.
The relative importance of oscillatory power and frequency for dimensionality transformation.,Future research should investigate how representational dimensionality changes across brain regions during encoding and retrieval,differences between unimodal and transmodal areas across species
Applied the Lambda architecture pattern to organize components by latency requirements into batch,speed,and serving layers.
Used autonomous synchronization management to replicate and synchronize data stores for balancing large data processing.,The research presents a preliminary reference software architecture developed by the Teaming.AI project. There is no explicit mention of the availability of source code for the project. Reproducibility details beyond the architectural description are not provided.,A preliminary reference software architecture was developed
No results or measured effects,including statistical values,are reported in the provided context.
Future validation will assess applicability,scalability,and ethical compliance
Identifying and implementing functionalities for orchestrated machine learning in knowledge graphs is challenging.,The reference software architecture is framework- and technology-agnostic,serving as a blueprint for various manufacturing contexts.
Validation will assess applicability,scalability,and ethical compliance in different domains.
Expert and operator interviews,plus quantitative data,will be used for evaluation.
Examining approaches for calculating graph embeddings and understanding their computational costs is a significant research gap.,Future research should address: monitoring teaming aspects in human-AI interaction,scalability for near-realtime IIoT data processing
Application of data mining,information processing,social network analysis
Utilization of a three-tier system architecture (Data Access,Business Logic,Application layers) with technologies like TITAN graph database
The system reveals both explicit and implicit relationships (e.g.,advisor-advisee) using knowledge graph techniques,supporting intelligent queries and personalized recommendations.
No statistical significance (p-values) or quantitative evaluation results are reported in the context.,Web of Scholars system enables efficient search,ranking
The system collects over 1.7 million scholars,1.5 million publications,and more than 433 million relationships (7 types).
Provides visualization,intelligent query,academic ranking
Challenges in mining complex implicit relationships among scholars.,Web of Scholars effectively extracts and visualizes both explicit and implicit scholar relationships using knowledge graphs,enabling comprehensive scholar profiling and analysis.
The system supports personalized services such as advisor/advisee recommendations and expert finding for students,scholars,and institutions.
It manages large-scale academic data efficiently,facilitating fast,intelligent semantic search and interactive analysis.
Open APIs and downloadable datasets promote further research and development.,Difficulty in efficiently mining and analyzing implicit relationships (like advisor-advisee) among scholars in large,complex academic networks.
Existing systems mainly focus on explicit relationships,limiting advanced services such as personalized recommendations and in-depth network analysis.,
Knowledge extraction: Enriching the KG by extracting relationships from large-scale free-text sources using text mining techniques.,,Knowledge Graphs (KGs) are increasingly used in life sciences for integrating complex
The paper identifies key challenges: incorporating KG semantics into machine learning,addressing long-tail label distributions,and creating efficient multi-modal KG representations.
No quantitative results or statistical significance (p-values) are reported in the provided context.,No information available,Difficulty in constructing customized Knowledge Graphs (KGs) from multiple sources and integrating different domain KGs.
Scalability and efficiency issues in knowledge retrieval,query,and reasoning systems.
Quality assurance challenges,including error detection,correction
Lack of widely accepted definitions and methods for explainability in AI models.,There are significant technical challenges in constructing,integrating
Continuous KG evolution,quality assurance,and robust evaluation methods for explainable AI (XAI) are needed.
Addressing the long-tail phenomenon in machine learning with KGs,particularly for rare diseases or infrequent labels,is an open research gap.
Creating efficient multi-modal representations of knowledge to enable discovery is still an unresolved issue.,Future research should focus on scalable and efficient knowledge retrieval,updating and quality assurance of knowledge graphs (KGs)
Entity linking using REL as the default method,with DBPedia Spotlight as an alternative.,The research provides an open-source demo of their solution
The open-source demo demonstrates the feasibility of translating natural language preferences into structured PKG statements,advancing user-centric PKG management.,The primary outcome is the development of a PKG API and Client enabling users to manage personal knowledge graphs (PKGs) using natural language statements.
No statistical values or quantitative measured effects are reported.,Most research on personal knowledge graphs (PKGs) remains conceptual; practical,user-facing implementations are lacking.
No further limitations or suggestions for future research are explicitly stated.,Personal knowledge graphs (PKGs) are promising tools for organizing personal information,especially as digital data grows.
The open-source demo demonstrates the feasibility of intuitive,user-centric PKGs and encourages further research into broader applications.,Need for more intuitive
Further research into simplifying PKG tools for non-expert users.,,
Timeseries analysis of sensor data: Studying how sensor data changes over time to identify patterns or issues.,The research is reproducible as the framework and source code are open-sourced and available at https://github.com/intel/driving-data-collection-reference-kit. No source code is provided for ROS,Uber AVS
Data integrity is verified through manual inspection for anomalies in camera,LiDAR,and OBD data; good data shows no frame loss or corruption.
Developed an open-source framework for capturing,evaluating,and maintaining multimodal sensor data for autonomous driving.
Enabled analysis and validation,visualization,and timeseries analysis of sensor-specific data.
Real-time synchronization of all sensors is challenging due to different FPS and lack of external control signals.,Limited on-board storage restricts continuous data capture due to high data rates (up to 4TB/hour,800MB/s).
Issues like incoherent data,corrupted frames,invalid metadata
Most existing works do not describe raw data evaluation and maintenance pipelines.,High data rates (up to 4TB/hour) and varying sensor FPS make continuous,synchronized multimodal data capture and analysis challenging.
The study recommends systematic,dashboard-based evaluation steps for data integrity,synchronization
There is a lack of standardized processes and tools for frame-by-frame evaluation and quality analysis of raw multimodal data.,Future research should focus on analysis and validation of multimodal sensor data,visualization of sensor-specific data
Development of a disease-specific local ontology and semantic reasoning rules,created with input from nephrology experts,to identify CKD-related abnormalities.
Collaborative reasoning approach that combines fragmented patient data from multiple hospitals to detect overlooked CKD cases.,,Multicenter collaborative reasoning identified CKD risks earlier
Risk coverage ratio was higher in multicenter data (165%) compared to single-hospital data (133%). No p-values reported.,The system identified CKD risks earlier than clinical assessment,with a median discovery lead time of 364 days (multicenter group) vs. 121 days (transferred group).
Data security is maintained through encrypted,high-level information sharing and local data isolation.,Limited explainability and generality of the current model remain unresolved.
Patient alignment is difficult when unique identifiers are missing; alternative approaches using nonunique identifiers are needed.,Future research should address communication bottlenecks and increased network/computational costs when scaling the system. Further systematic design,including the Hyperledger method
Merging heterogeneous clinical data to enable knowledge discovery,Seneviratne Martin G.,Kahn Michael G.
Incorporation of patient-reported outcomes (PROs) into electronic health records (EHRs) to promote patient-centric care.,,Integrating diverse datasets from multiple institutions and data types offers significant opportunities for biomedical knowledge discovery and precision medicine.
Technical and operational challenges remain,especially in harmonizing,structuring
Santillana et al.: Combined hospital visit data with Twitter,Google searches,and online health forum posts to predict influenza incidence.
No specific statistical values are provided in the context.,Difficulty harmonizing data from different abstraction levels (e.g.,diagnosis codes vs. proteomic data)
Need for improved de-identification,consent,and access control processes.
The main challenge now is harmonizing,structuring,and learning from multi-modal datasets while ensuring privacy and governance.
Ensuring data stewardship,privacy,and security is critical
Addressing equity and inclusion to prevent bias in large-scale biomedical datasets is needed.,Future research should focus on integrating diverse clinical data streams across institutions and modalities,developing flexible data storage and machine learning systems
Directory services are utilized for authentication and verification processes.,,By 2025
Cloud storage is highly scalable and manageable but faces significant challenges,mainly in security (confidentiality,integrity
Solutions include cryptography,digital certificates,attribute-based encryption
Identification of key cloud storage challenges: Security,Confidentiality,Data Dynamics
Review of possible solutions: Cryptography,Digital signatures,Proxy Re-encryption
Off-the-shelf products may not meet cloud data center requirements.,Cloud storage is rapidly evolving,driven by technologies like IoT
Key challenges remain in security (confidentiality,integrity,access
Solutions include cryptography,digital certificates,attribute-based encryption
Continued research and development are recommended to address ongoing security and management issues.,Security concerns remain unresolved,especially regarding confidentiality
Data management issues such as data dynamics,segregation,backup
More research is needed to encourage business and enterprise customers to store sensitive data in cloud storage.,Future research should address unresolved issues in cloud storage,especially security (confidentiality
Task Decomposition: Uses a human-in-the-loop approach and LLMs to break down research tasks into subtasks,which are structured into Python classes for further processing.,No information available
Hinton’s research from 2015-2024 shifted from neural network training (e.g.,layer normalization: 14,776 citations) to capsule networks (~5
No explicit p-values or statistical significance data are provided in the context.,KNOWCODER-V2,under the KDR framework
Figure 1 demonstrates KnowCoder-V2 outperforms counterparts in Comprehensiveness,Thoroughness,Factuality
Shallow knowledge computation: Only shallow text-based computations are possible,lacking deep logical deduction and statistical inference.,KNOWCODER-V2 with the KDR framework generates more comprehensive
The system excels at extracting,organizing,and updating structured knowledge from diverse
Recommendation: Use KNOWCODER-V2 for tasks requiring fine-grained,multi-step knowledge analysis and reasoning.,Coarse Knowledge Management: Existing frameworks manage knowledge at the web page level
Shallow Knowledge Computation: Current systems only perform basic text processing,lacking advanced reasoning and analysis abilities.,Future research should address limitations in coarse knowledge management
Statistical analysis using linear mixed models,discriminant classifier analysis (Wilks’ Lambda method),and receiver operator characteristic (ROC) analysis.
Biomarker assessment through group comparisons,diagnostic classification,and correlation analyses.
The best diagnostic model (GluR2,proBDNF,NRGN
Discriminant model using GluR2,proBDNF,NRGN
Absolute biomarker levels varied widely between cohorts,likely due to inconsistencies in preanalytical parameters (e.g.,blood draws
Variability in preanalytical conditions remains a challenge for analytical validation and generalizability.,,Technical difficulties in isolating and analyzing extracellular vesicles (EVs) hinder their clinical implementation as biomarkers.
Immunoaffinity methods like ExoSORT depend on marker specificity,which is rarely complete,potentially affecting the accuracy of neuron-derived EV enrichment.
Cleaning and Transformation: Identifying and removing errors,inconsistencies,and translating data into a uniform format for analysis.
Harmonization and Merging: Aligning,consolidating,and aggregating data from different systems into a consistent structure for integration.
The paper presents methods such as filtering,harmonization,aggregation
No statistical values or measured effects are provided.,Inaccurate,irrelevant
Lack of reliable and credible data sources may affect RIMS usability and researcher motivation.,RIMS improve targeted and faster information supply,becoming key for research success.
Data quality issues (inaccurate,outdated,incomplete data) impact researcher acceptance and system usage.
Effective integration and transformation processes (cleaning,harmonization,aggregation
Recommendation: Prioritize data quality and user needs to ensure RIMS effectiveness.,Insufficient data quality in RIMS,including issues with inaccurate
Need for improved harmonization and merging processes to ensure consistent and meaningful aggregation of research information.,,
Comparative Analysis of Memory Systems: Compared agents using different memory systems (short,episodic,semantic) to analyze their effectiveness.
Baseline Observation History Method: Used an agent that relies on a sequence of past observations to estimate the current state and answer questions.,,The HumemAI agent
Attention weights show a shift from short-term to long-term memory,and from episodic to semantic memory over time.,No comprehensive understanding of how long-term memory is managed.
Future work: jointly learn memory management and exploration,use graph neural networks,and learn question-answering functions directly.
Exploring other function approximators,such as graph neural networks (GNNs),may offer advantages over the currently used LSTM.
Transitioning from handcrafted question-answering functions to learning them directly is a promising future direction.,Suggestions for future research include jointly learning memory management and exploration policies,exploring other function approximators like graph neural networks (GNNs) instead of LSTM
Designing decentralized knowledge management systems to effectuate individual and collective generative capacities,Schmitt Ulrich,2019
Transdisciplinary Approach: The research applies transdisciplinary methods to integrate knowledge across disciplines and disrupt disciplinary silos.,The research follows design science research (DSR) practices and reports on early visions and prototype development. However,the prototype is still in progress
The study reports early visions of technology impact,feasibility,suitability
The study aims to complement,not replace,traditional approaches
The PKMS approach aims to empower individuals for lifelong learning,creativity,and teamwork
Further development and evaluation of the PKMS prototype,focusing on feasibility,suitability
Semantic technologies: Use of ontologies and annotations for data mining,knowledge representation,and pattern detection.
Linked Data querying: Utilization of LDflex language for dynamic querying and integration of external Linked Data resources.,,The PHL enables physicians to better understand the applicability and generalizability of treatment recommendations using visual aids and semantic annotations.
The PHL leverages innovative technologies,including Linked Open Data and ontologies,to support pattern discovery and knowledge capture.
No actual results or statistical values reported.,,The study highlights the need for automatic tools to help scientists discover patterns and associations in research data.
Visual aids are recommended to help physicians understand the relevance of clinical guidelines.,Need for more automatic tools to support scientists in pattern discovery,including link predictions and discovering complex annotation patterns across diseases and drug interventions.
Further implementation of an end-to-end intelligent recommender and digital librarian framework,including text summarization,knowledge mapping
Study 2: Pragmatic randomized controlled trial (RCT) with intervention (TM and IR-TM) and control (usual care) arms; comparative effectiveness; multi-arm; follow-up over 12 months.,,
Time series data summarization,such as calculating averages and modes for 6-hour intervals,and statistical tests like the Kolmogorov-Smirnov test to detect condition shifts.
Application of language models and the Medical Concept Annotation Toolkit (MedCAT) for extracting and annotating clinical concepts from free text,followed by integration into a graph database.,The research is reproducible. The code and demo for the Personal Health Knowledge Graph (PHKG) for COPD are available at https://github.com/Bluer01/COPH. The MIMIC-III dataset was used for testing
No explicit statistical results or measured effects reported.,The system was only tested on the MIMIC-III dataset,which may limit generalizability.
The framework is effective for COPD monitoring and is adaptable for future machine learning enhancements.,Integration of multimodal personal health and environmental data into PHKGs remains challenging,especially outside clinical settings.
Data harmonization and efficient storage solutions for heterogeneous and time-series health data require further research.,,
Structural and Comparative Analysis: Metrics are computed and compared across ArCo,CIDOC CRM,and EDM to assess structural evolution and quality indicators.
Use of OntoMetrics Tool: OntoMetrics,a web-based tool,is used to compute statistics and metrics about the ontologies.
Module metrics for ArCo report optimal appropriateness (=1),high encapsulation (~1),and low atomic size (4.85–6.63)
The evolving and heterogeneous community poses challenges for requirement collection and prioritization.,ArCo modules show optimal appropriateness (=1),excellent encapsulation (∼1)
Data cleansing and iterative refinement improve knowledge graph quality.,Lack of automatic methods for detecting,annotating
Challenges in reusing existing ontologies due to poor documentation,large size,and lack of explicit alignments.
Robustness evaluation: Tests models using modality-specific and multimodal imperfections,measuring relative and effective robustness.,MULTIBENCH emphasizes reproducibility and ease of use but does not explicitly mention the availability of source code for the project. No direct link or reference to the project's source code is provided in the context.
Simple fusion techniques (LF) balance high performance and low complexity,while complex models (MFAS,MULT) perform only slightly better on average.
No p-values or statistical significance values are reported in the context.,MULTIBENCH evaluates 15 datasets across 10 modalities and 6 research areas,testing over 20 prediction tasks.
Example improvements: MUSTARD (4.7%),UR-FUNNY (6.0%),STOCKS-HEALTH (2.8%)
Metrics: Accuracy (↑ higher is better),AUPRC (↑),MSE (↓ lower is better).
MULTIZOO toolkit enables standardized,reproducible multimodal model training and evaluation.,Tradeoffs between generality and specificity: MULTIBENCH may not leverage domain knowledge as well as task-specific models.
Limited coverage: Currently excludes areas like question answering,retrieval,grounding
Interpretability for non-image/text modalities remains an open question.,MULTIBENCH accelerates multimodal ML research by standardizing evaluation and enabling robust,transferable
Simple fusion methods (like LF) are competitive,especially for new datasets,but complex models excel on well-studied data.
MULTIBENCH currently lacks coverage of important research areas such as question answering,retrieval,grounding
Current benchmarks often overlook the tradeoffs between performance,robustness,and complexity in real-world deployment.
YCSB benchmarking tool: Used to generate workloads and measure database performance with different read/write mixes and thread counts.,The research reproduces performance and scalability experiments of HBase and Cassandra using Amazon EC2 infrastructure and the Yahoo Cloud Serving Benchmark (YCSB). No information about the availability of source code for the project is provided.,Both Cassandra and HBase scale nearly linearly; Cassandra has better read performance
Throughput and latency metrics (average,95th,and 99th percentiles) are reported for different workloads and node counts.
Most absolute performance measurements in E2 differ significantly from E1,though general trends are consistent.,Some results from experiment E1 could not be reproduced
Differences in Cassandra version used may affect comparability.,Both Cassandra and HBase scale nearly linearly; Cassandra has better read performance,while HBase has better write performance.
Recommendations: Test with different CPU core sizes and monitor disk,network,and CPU bottlenecks.
Trade-off between speed of scaling and performance variability during elasticity evaluation,highlighting the need for research on optimizing both objectives.,
Aggregation via FedAvg: The server aggregates encrypted model weights from users to update the global model without accessing raw data.,The research is reproducible. The pseudocode for the proposed approaches is provided (Algorithms 1–4),and the source code is available on GitHub \[29]. Experimental settings and datasets are described in detail
The proposed approaches outperformed most previous methods and,by using FL (federated learning) and encryption techniques,enhanced privacy and security in IoT systems. No p-values reported.
Proposed approaches outperformed most previous methods in all four measures.,,The proposed approaches significantly improve security
The approaches maintain high accuracy,precision,recall
Assessment of the scalability and efficiency of the proposed approaches in larger,real-world deployments.,
Temporal knowledge graph (TKG) extrapolation: Predicts missing entities in future events using time-stamped quadruples (s,p,o
Ablation study: Evaluates the impact of different model components and masking strategies on performance.,,CENET outperforms all baselines on public KGs (WIKI and YAGO)
The improvements are statistically significant,attributed to CENET's ability to handle imbalanced recurrence rates by learning both historical and non-historical dependencies.,CENET outperforms baselines on WIKI and YAGO
On WIKI: CENET achieves MRR 68.39,Hits@1 68.33,Hits@3 68.36.
On YAGO: CENET achieves MRR 84.13,Hits@1 84.03,Hits@3 84.23.
CENET achieves up to 8.25%,8.48%,and 20.80% improvements of Hits@1 on ICEWS18
Addressing the challenge of imbalanced recurrence rates in datasets for better event forecasting.,,Historical contrastive learning (CENET) uses a two-stage design: (1) learning contrastive representations with a supervised contrastive loss
Axioms and rules utilization: Applies fundamental principles and logical statements to structure,interpret,and infer new knowledge from patient data.
Qualitative assessment: Employs usability studies,content analysis,and expert panel reviews to evaluate alignment with clinical best practices and user experience.
Quantitative assessment of PCKGs uses metrics such as accuracy,recall,and precision to numerically evaluate performance; completeness and consistency are emphasized
PCKGs show promise in disease prediction,personalized treatment recommendations,and clinical trials
PCKGs enhanced accuracy,efficiency,and outcomes in clinical trial patient selection.
No explicit statistical values reported.,Difficulties in knowledge acquisition,completion
Construction of PCKGs is time-consuming and heavily reliant on source data quality (Gyrard et al.,2018; Cong et al.,2018).
Data quality and standardization remain significant challenges.,PCKGs (Personalized Clinical Knowledge Graphs) enhance personalized patient care by integrating diverse healthcare data,improving treatment accuracy and efficiency.
Innovative processing techniques (reasoning,semantic search,inference) make PCKGs more actionable for clinical decision-making.
Key challenges include data fragmentation,integration complexity,and scalability.
Future work should focus on improving data integration,scalability,and patient-centric approaches in clinical trials.
Overcoming challenges in knowledge acquisition,completion,and temporal KG development to maintain up-to-date and comprehensive patient profiles.
Confirmatory factor analysis (CFA) and Harman’s single factor test were conducted to assess validity and internal consistency.,Raw data is available in the Supplemental Files online at http://dx.doi.org/10.7717/peerj.13861#supplemental-information. There is no mention of source code for the project.,Fragmented reading (content
All three fragmentation types were negatively associated with cognitive depth: content (β = −0.56,p < 0.01),time (β = −0.35
The structural equation model showed good fit (X2/df = 2.51,CFI = 0.92,NFI = 0.91
Content fragmentation,temporal fragmentation,and attentional fragmentation each had a positive effect on cognitive breadth.
Content fragmentation,temporal fragmentation,and attentional fragmentation each had a negative effect on cognitive depth.
Study focused only on five variables; did not include factors like reading motivation,engagement,personality traits
Future research should include larger,more diverse samples and additional variables.,Limited generalizability due to the sample being only 916 Chinese participants from six universities; future research should use larger and more diverse samples.
Need to examine additional variables such as reading motivation,reading engagement,personality traits
Explore the effects of individual and cultural differences on fragmentation and cognitive development.,Future research should use larger and more diverse samples,including Western populations
Comparative evaluation of three candidate knowledge engineering architectures against 23 quality attributes and 8 functional requirements.,BioCypher is open-source software,openly developed and available under an MIT license. It uses modern software engineering practices (continuous integration
BioCypher excels in maintainability,ethicality,and explainability but is not affordable or editable by humans.
None of the architectures satisfied all QAs; each aligned with its focus (e.g.,KGTK on usability,BioCypher on maintainability).
All architectures satisfied reliability (Q01),robustness to noise (Q10),efficiency (Q02)
BioCypher enforces ethicality (Q19) by requiring source,license,and version parameters
Explainability is insufficient due to lack of stakeholder involvement,clear documentation,and use of non-interpretable algorithms.
BioCypher strictly requires a schema,limiting applicability without high-quality schemas.,The 23 quality attributes (QAs) are effective for assessing and comparing knowledge engineering (KE) architectures; future KE development should consider all QAs.
No evaluated architecture satisfies all QAs; socio-technical requirements like editability,curatability,and affordability are missing in all.
The study recommends a human-centric,iterative approach involving stakeholder input to develop a comprehensive reference architecture (RA) for KE.,Current KE architectures lack support for key socio-technical requirements
There is a need for a human-centric,iterative methodology involving stakeholder prioritization to guide comprehensive RA development for KE.,Future research should focus on developing a comprehensive reference architecture (RA) for knowledge engineering (KE) using standard methodologies
GUARANTEED IMPROVEMENT OF THE PRIVACY-UTILITY TRADEOFF IN FEDERATED LEARNING,Ye Jiayuan,Kang Anmin
Matrix Sensing Formulation: Reformulates the problem as a matrix sensing task,enabling analysis via inexact gradient descent methods.,
CENTAUR demonstrates a better utility-privacy trade-off,outperforming both prior federated learning (FL) methods and local stand-alone training across various privacy budgets (ϵ),as shown in Figure 1 and Figure 3.
Theoretical analysis (Corollary 5.1) shows that,for sufficiently large client numbers n,CENTAUR can guarantee target accuracy ϵa within a DP budget ϵdp; statistical significance is not explicitly reported.
Statistical values are reported as mean (standard deviation) across runs.,No theoretical guarantee that CENTAUR achieves target accuracy ϵa within a small DP budget ϵdp; only an upper bound is established,not a lower bound.
Utility drop is inevitable in DP FL due to gradient clipping and sensitivity bounding.,The main bottleneck in differentially private federated learning (DP FL) is the conflict between learning the representation function and classification head under gradient clipping,causing utility loss.
Theoretical analysis shows that with sufficient clients (n),CENTAUR achieves target utility within a given privacy budget,and better dependence on data dimension (d) reduces required clients.
Recommendations: Decompose models into shared representations and personalized heads for DP FL to improve utility and scalability.,Theoretical guarantees for achieving target accuracy within a strict differential privacy (DP) budget are not fully established; only upper bounds are provided,not lower bounds.
Existing approaches mainly address linear embedding or non-DP settings; extending results to more general or non-linear settings remains open.,,
Fine-tuning LLMs: Uses fine-tuned LLMs for pre-annotating data in ontology property identification to assist annotators and reduce annotation times.,,The survey highlights that integrating LLMs with KGs enhances tasks like generating descriptive text
LLMs enhance KGs in generating descriptive text,ontology generation,inconsistency detection
No explicit statistical values or measured effects are provided.,Current rule-based techniques often overlook the semantics of relations,focusing mainly on structural information.
Further research is needed for reliable knowledge incorporation and parameter reduction in LLMs.,The integration of LLMs and KGs enhances tasks like KG completion,fact-checking
New research directions include efficient training data selection,smaller LLMs,and advanced architectures for AGI.
Advancing under-explored areas such as multi-hop question answering,query generation from text,and KG-powered chatbots.
Knowledge graph analysis: Used the SPOKE knowledge graph to map relationships between biomedical entities and inform predictive modeling.,The research is reproducible with some restrictions. The Arivale dataset is available upon request. The SPOKE knowledge graph is accessible at https://spoke.ucsf.edu/. Analysis code is available at https://github.com/IlyaLab/t2d-dt-modeling and additional knowledge graph code at https://github.com/yjzhang/kg\_feature\_engineering.,Predictive models using clinical
Knowledge graph analyses identified molecular pathways linking predictive proteins to T2D,suggesting potential treatment paths; no explicit p-values reported.,Primary outcomes measured: Changes (deltas) in HbA1c
Results at 6 months (mean change): HbA1c −0.05,glucose −0.48,eGFR 1.04
Results at 1 year (mean change): HbA1c −0.11,glucose −0.18,eGFR −0.05
Need for more T2D-specific data,though this may introduce confounders.,The study successfully used multiomic and clinical data to predict changes in T2D-related clinical variables over time.
Recommendations include further investigation of significant,yet unmeasured,related features.
Need for improved dashboards and user interfaces,including natural language interfaces,to increase adoption.
Further integration of mechanistic information into models to better estimate biological parameters.,Future research should focus on collecting densely sampled longitudinal phenomic data,increasing sample size
Vector Data Management: Approaches for high-dimensional data indexing,join,geometric querying
Lack of ground truth datasets and benchmarks in emerging domains.,Knowledge Graphs (KGs) and Large Language Models (LLMs) together improve multi-modal data management,accuracy
Challenges include vector data management,efficiency,scalability
Recommendations include developing techniques for provenance,bias mitigation,and secure knowledge extraction.
Developing methods for data cleaning,integration,and augmentation using the synergy of LLMs and knowledge graphs.
Creating ground truth datasets and experimental benchmarks for evaluating LLM and knowledge graph integration in various domains.,Future research should address: integrating graph structures with multi-modal data,extracting relevant subgraphs
Comparison with baseline methods: Benchmarks StreamE against other models (e.g.,xERTE,RE-GCN) using multiple datasets and performance metrics.
Ablation studies show significant drops in Hit@1 and Hit@10 when key components are removed,confirming their importance; no explicit p-values are reported.,Primary outcomes were measured using Mean Reciprocal Rank (MRR) and Hit@k (Hit@1
StreamE achieved the best results on all four datasets,e.g.,on ICEWS 14: MRR 23.15
StreamE outperformed the second-best method by 14.1%,1.6%,9.1%
StreamE had lower training time per epoch (40s) and fewer parameters (1.09M) compared to xERTE (1096s,3.36M) and RE-GCN (118s,5.96M).
The propagation influence of new knowledge is underexplored; future studies should better model and leverage propagation paths.,,
Black box and white box modeling: Utilizes use case diagrams and internal block diagrams for black box modeling,and block definition diagrams and activity diagrams for white box modeling to analyze and allocate system requirements.,
Measures of performance (MoPs) include fuel pressure,fuel flow,and longitudinal/lateral center of gravity (CoG) constraints.
No explicit statistical values (e.g.,p-values) are provided.,
There is a need for improved model/data exchange technologies to maintain consistency between SysML models and simulation environments.,,The study design is a simulation-based
Categorization and comparison: The study categorizes state-of-the-art research and compares traditional recommender system methods (content-based and collaborative filtering-based).,,Knowledge graph embedding methods show varying link prediction performance
Knowledge graphs are widely applied in education,scientific research,healthcare
Multi-hop reasoning in knowledge graphs faces high computational costs and uncertainty in inferred knowledge,highlighting the need for multi-source reasoning and error reduction; no p-values reported.,KGNN (Lin et al. 2020): Predicted drug-drug interactions by mining relationships between drugs and their neighborhoods in medical knowledge graphs.
Incomplete representation of knowledge; many entities and relationships are missing.,Knowledge graphs offer significant opportunities for improving AI systems and applications in various fields,including healthcare and education.
Major technical challenges remain,especially in knowledge acquisition,integration from multiple sources
Integrating and acquiring knowledge from multiple sources into a unified knowledge graph faces significant technical limitations.,Future research should focus on multi-hop knowledge reasoning,verification of inferred knowledge
Knowledge graph enhancement: Includes before-training,during-training,and post-training methods to integrate structured knowledge into language models.
Incorporating external knowledge,especially via knowledge graphs,improves performance in tasks like Named Entity Recognition
No explicit quantitative results or statistical significance (p-values) are provided in the context.,KGPLMs outperform traditional PLMs in capturing factual and relational information,leading to improved language understanding and generation.
KGPLMs show better performance in Named Entity Recognition,Relation Extraction,and Sentiment Analysis
LUKE: Entity typing +4.6,Relation classification +6.7,Question answering +19.2
K-Adapter: Entity typing +4.1,Relation classification +1.9,Question answering +5.4
ERICA: Entity typing +4.4,Relation classification +2.2,Question answering +1.5
Post-training enhancement methods are low-cost,easy to implement,and effectively improve LLMs’ performance on specific tasks but may limit generation flexibility.
Inability to explicitly access,edit,or update knowledge.
Further research needed for LLMs as reliable knowledge bases.,LLMs have limitations in factual knowledge modeling,reasoning
Incorporating multimodal and temporal knowledge from knowledge graphs to enhance KGPLMs beyond single-modality and static data.,Suggested future research directions include: investigating the scaling law of KGLLMs to optimize model size and efficiency; merging different types of knowledge; incorporating multimodal and temporal knowledge; improving prompt template selection with KGs; addressing hallucination,inconsistency
Assessed explained variance using Nakagawa’s R² and computed Wald’s 95% confidence intervals and p-values for inference.,The gridExtra package (v2.3; Auguie,2017) was used for figure creation. No explicit mention of source code for the project or reproducibility details specific to gridExtra are provided in the context.
EBs density alone was a significant predictor of temporal compression (coefficient = 0.44,SE = 0.17,95% CI \[0.11
High inter-rater reliability was observed for scoring recalled experience units (ICC = 0.96,95% CI \[0.94,0.97]); results were robust to exclusion criteria.
Cannot determine if WM affects temporal compression during encoding,retrieval,or both.
Recommendation: Consider both WM capacity and EBs density when studying temporal memory.,No information available,Future research should investigate the specific role of working memory (WM) sub-processes in temporal compression of events
Design framework: The methodology included needs analysis,identification of digital twins,service mapping
Design patterns were applied in three case studies: heliostat field,water distribution system,and smart city.
Quality attributes (e.g.,performance efficiency,reliability
Smart city: Interoperability,performance efficiency,and reconfigurability patterns were effective.
No self-reported problems or suggestions for further research are mentioned.,Quality attributes are central to effective system design; multiple design patterns may be applied,requiring trade-offs.
Separation of concerns,modularity,and scalability are recommended for performance and reliability.
Need for more case studies to measure effectiveness and utility in diverse scenarios.,,
Employed one-dimensional convolution-based GCN (Graph Convolutional Network) layers with specific hyperparameters (embedding size 200,2 layers,dropout 0.2).
Optimized model parameters using the adam optimizer with a learning rate of 0.001.,,The TRCL model achieved optimal entity prediction performance on ICEWS14 and ICEWS18 when α (history weight) = 0.3 and learning rate = 0.001
TRCL outperformed existing methods in most metrics across four benchmark datasets; however,its performance on ICEWS05-15 highlighted challenges with limited training data. No p-values were reported.,The proposed TRCL model achieved the highest performance on entity prediction tasks across ICEWS14
On ICEWS14: MRR 45.07%,H@1 34.71%,H@3 50.22%
On ICEWS18: MRR 33.78%,H@1 23.26%,H@3 38.20%
Expansion to relationship prediction tasks and emerging facts is needed for broader applicability.,The TRCL model outperforms existing methods in most metrics for temporal knowledge graph (TKG) reasoning,especially in extrapolation tasks.
Future work should improve data efficiency and extend TRCL to relationship prediction tasks.,Limited research on extrapolation reasoning in temporal knowledge graphs,which is more practical and challenging than interpolation reasoning.
Lack of explainable approaches for link forecasting and reasoning in temporal knowledge graphs.,Future research should investigate the reasoning ability of models towards emerging facts. There is a need to address limitations in predicting new or previously unseen events,as current models like TRCL focus mainly on historical and repetitive facts for entity prediction.
Degree bias adjustment: Adjusts for node connectivity differences,applied in a component/layer-specific manner.,The research is reproducible. All datasets and computer code used are available: chemical and protein-chemical links from STITCH v5
B-RWR increases diffusion scores for nodes with large BRW attribute values,with the effect growing as the rate parameter approaches 0.,Over-representation analysis (ORA) identified significant Gene Ontology and Disease Ontology terms in the AMEND module from TCGA-KIRC data at a significance level of 0.01 (Benjamini-Hochberg adjusted).
In OGT-KO mouse liver samples,mitochondrial fatty acid beta-oxidation enzymes (ECI1,ECI2) showed equivalent down-regulation across omic datasets
Peroxisomal lipid metabolism genes (ACOX1,ACOX3,ALDH3A2
These results suggest O-GlcNAcylation is a novel regulator of lipid metabolism in mouse liver.,PPI networks suffer from degree bias due to technical and study biases,affecting node degree.
AMEND is broadly applicable,especially for complex,noisy data with high-quality molecular interaction networks
AMEND is not suitable for sample clustering,suggesting a gap for unsupervised multi-omic integration methods.,The study notes that AMEND may not be optimal for all research questions or experimental designs and that results depend on the choice of network. Future research could compare AMEND with non-network-based multi-omic integration methods and explore its performance in different experimental contexts.
Patient-Centric Knowledge Graphs: A Survey of Current Methods,Challenges,and Applications
Knowledge extraction: Using techniques like Named Entity Recognition (NER) and Relationship Extraction (RE) to identify key entities (e.g.,drugs,symptoms) and their relationships.
Evaluation methodologies: Employing qualitative (usability studies,expert reviews) and quantitative (accuracy,recall
Quantitative assessments of PCKGs use metrics such as accuracy,recall,and precision to evaluate completeness and consistency; specific numerical results or p-values are not provided.
PCKGs show promise in enhancing clinical decision support and clinical trials,but challenges remain in construction,robustness
Results show significant improvements in accuracy,efficiency,and outcomes for clinical trial patient selection.
Challenges in data fragmentation,noncommensurability,and semantic inference within cardiovascular data.
Difficulties in knowledge acquisition,completion,and temporal KG development.
Integrating diverse data sources into PCKGs enhances clinical trial design,patient selection,and engagement.
Key challenges include data fragmentation,complexity,and scalability.
Continued research and collaboration are recommended to maximize PCKGs’ impact on healthcare.,Improving knowledge acquisition,completion
Addressing data quality,standardization,and integration challenges due to heterogeneous data structures and varying medical standards.
Enhancing KG embedding methods to better utilize auxiliary texts and overcome structural sparsity for richer patient information representation.,Future research should address challenges in knowledge acquisition,completion
Descriptive statistics are the primary data analysis technique,used in 92% of recent studies.,Most related publications do not provide reproducible data or source code. Only four out of 14 publications offer their data; three have broken links
Reporting of threats to validity (91.3%),data provision (71.3%),and research questions/answers (23.7%) increased over time.
Case study usage decreased (22.3% for the target state),and action research is rarely used (0% for the target state); no p-values reported.,The proportion of papers reporting an empirical study increased over time
Use of three to five empirical methods per paper increased (22% for three,25.3% for four,26.7% for five).
Reporting of threats to validity,data provision,and research questions increased (91.3% for threats to validity).
There is a need for standardized terminology and theories for more consistent empirical research representation.,Only papers from one publication venue were selected,limiting generalizability.
Publication bias is present.,There is a positive trend toward more empirical studies providing their data,with reported data URLs increasing from 25.4% (pre-2010) to 71.3% (2020–2025).
Collaboration among researchers is needed to reduce redundancy and improve cumulative research.,Lack of comprehensive,up-to-date
Need for technical infrastructures (like ORKG) with additional features to enable sustainable and continuous literature reviews.,Future research should focus on developing,expanding
A novel customizing knowledge graph evaluation method for incorporating user needs,Zhang Ying,Xiao Gang
Model stacking: Uses multiple mini-models to evaluate correlations,translational invariance,and path existence between entities for comprehensive knowledge graph assessment.
EP-TWCS sampling method: Clusters entities by user needs,assigns weights based on entity popularity,and iteratively samples to efficiently estimate accuracy with minimal bias.
No explicit p-values or detailed quantitative results are provided in the context.,Primary outcome: The Entity Popularity Weighted Clustering Sampling (EP-TWCS) method was evaluated on NELL,YAGO
Compared to other methods (SRS,RCS,WCS
Annotation errors from human misunderstanding cause omissions,misjudgments,and inconsistencies.
Difficulty maintaining accuracy,completeness,and timeliness during continuous updates.
Accuracy assessment results,confidence intervals,and sample sizes stabilize at n = 8
Recommendations include flexible,user-oriented evaluation and parameter adjustment to improve efficiency and accuracy.,No information available
Analysis of publication years,venues,citation counts
Construction and visualization of keyword co-occurrence networks using VoSViewer software.,No information available,From 2013 to 2022
Breakthroughs often originate from development and industry applications rather than research projects.,No information available,Research on open knowledge graphs has grown significantly from 2013 to 2022
The community should reconsider collaboration between government,research,civil society
Addressing legal and technical barriers to achieve Findability,Accessibility,Interoperability
Developing robust cybersecurity algorithms for social network analysis and language processing using open knowledge graphs.,Future research should explore the evolution and current use of open knowledge graphs,especially their integration with advanced machine-learning techniques
Online similarity ratings: Amazon Mechanical Turk workers rated visual similarity between target and foil frames.,The data and analytic code supporting the findings will be deposited at Open Science Framework upon acceptance. No current source code is provided in the context.,Significant differences (p < 0.001 or p < 0.01) were found between visual boundary types (NB
Conclusions highlight that visual and neural reinstatement measures are statistically significant across conditions; all p-values for main effects are < 0.001 or < 0.01.,Primary outcomes measured differences in visual attributes (luminance,contrast
Significant effects found for all attributes across boundary types (all P < 0.001),except for some pairwise comparisons (e.g.,NB vs SB for luminance
ANOVA tested neural context reinstatement at boundaries and targets,covarying with confidence,boundary type
No explicit study conclusions,key implications,or recommendations are provided in the context.
No information available regarding main findings or recommendations.,,
Digital twins allow real-time monitoring,accurate diagnoses,proactive interventions
They support predictive analytics,allowing prediction of disease progression,identification of high-risk individuals
Digital twins optimize clinical operations by streamlining workflows,improving resource allocation,and increasing operational efficiency.
Necessity for cultural shifts in adoption,Digital twin technology can revolutionize healthcare by enabling personalized treatment,predictive analytics
It empowers healthcare professionals with accurate diagnoses,real-time patient monitoring,and proactive interventions.
Digital twins optimize resource allocation,streamline workflows,and improve operational efficiency.
Limited research on ethical,privacy,and regulatory challenges associated with digital twins in healthcare.
Use of hybrid symmetric and asymmetric cryptographic methods,combining authenticated key establishment and capability issuance.,
FSSA-KA’s total computational cost per protocol run (4.086 ms) is comparable to Maat-I & Maat-II combined (3.953 ms),with individual times: Server 1.227 ms,Client 1.440 ms
FSSA-KA achieves stronger security,improved usability,and simplified key management compared to Maat.
FSSA-KA offers stronger security,improved usability,and easier key management compared to Maat
Necessity to implement and evaluate the proposed approach in real distributed file systems to assess cost savings and identify further advantages or limitations.,Future research should implement the proposed approach in a real distributed file system to analyze key management cost savings and identify additional advantages or limitations. Further investigation is needed into security impacts from hardware or software failures exposing secret keys,which current models do not explicitly address.
Pearson correlations and linear regression analyses were conducted to examine relationships between variables and tasks.,The research's reproducibility is supported by a clear description of methods and analysis. The raw data can be requested from the authors. There is no mention of source code availability for the project.,The Sport Group showed significantly better precision than the Sedentary Group in faster trials of the Time Comparison Task (M = 0.62 vs. M = 0.48; p = 0.021)
Main effects were found for time (F(1,40) = 23.48; p = 0.000; ηp2 = 0.370) and type (F(1,40) = 57.77; p = 0.000; ηp2 = 0.591)
The study concludes that physical activity benefits time processing in young female adults,paralleling findings in spatial memory,with all significant results at p < 0.05.
Repeated measures ANOVA for RT showed a main effect of type (F(1,40) = 5.66; p = 0.022; ηp² = 0.124) and group (F(1,40) = 5.42; p = 0.025; ηp² = 0.120)
Significant Pearson correlations were found between several measures within and between tasks (e.g.,BOX-B1 and BOX-B2: r = 0.655,p < 0.01; RT-BOX-B1 and RT-BOX-B2: r = 0.782
Differences between open-skill and closed-skill sports,and exercise intensity,were not addressed.
Limited sample availability.,Physical activity (PA) benefits time processing in young female adults,similar to its effects on spatial memory.
Future research should examine different types of sports,age,and sex influences on time processing.
Combining behavioral and neurophysiological data (e.g.,EEG) may reveal underlying neural mechanisms.,Examine how different types of sports (sport typologies) influence the relationship between physical activity and time processing.
Investigate the roles of age,sex,gaming proficiency
Integrate behavioral data with neurophysiological measures (like EEG) to reveal neural mechanisms underlying time processing.,Future research should examine the influence of sport typology (open-skill vs. closed-skill),exercise intensity
Storage and querying of data in a graph database (Stardog),enabling knowledge inference through a reasoner.,The original contributions are included in the article/supplementary material. The prototype is intended to be released as open-source software for validation
PKGs enable semantic search and knowledge inference,supporting complex,dynamic healthcare environments and informed decision-making.
No quantitative results or statistical significance (p-values) are reported.,,Integrity and validity of collected data must be guaranteed to ensure reliability.
Key challenges include ensuring data integrity,privacy,and implementing effective access-control policies.
Future work involves selecting optimal technologies,addressing practical implementation issues,and exploring simulation capabilities.
Investigating further extensions,including integrating computational models into Digital Twins to enable simulation capabilities.,Future research should focus on selecting optimal technologies for framework implementation
Comparative Evaluation and Case Study: SSTKG is benchmarked against models like SVR,LSTM,GRU
T-test results showed no statistically significant difference between predictions before and after masking related entities (p-values: 0.9998975,0.999873,0.6717662).
SSTKG demonstrated strong interpretability and effectively modeled both spatial and temporal relationships for prediction tasks.,The SSTKG model was evaluated using the Spend-Ohio dataset,focusing on interpretability and prediction accuracy.
Masking influential entities (A,B,C) changed prediction results
t-test results (Table 8): p-values (0.9998975,0.999873,0.6717662) led to rejecting the null hypotheses
SSTKG outperformed SVR,GRU,LSTM
Future work is needed to refine the SSTKG construction algorithm and enhance model dynamism.,The proposed SSTKG framework effectively models both spatial and temporal relations,outperforming baseline models (SVR
Future work includes refining the SSTKG algorithm,enhancing its ability to capture entity mobility,and balancing model size with efficiency.
Balancing model size and efficiency.,Future research should focus on refining the SSTKG construction algorithm,enhancing the model’s ability to reflect dynamic entity mobility (such as shifting user locations)
Reasoning with historical context: Predicts missing links by analyzing relevant historical facts related to the query.,,TKGC methods are categorized into six types
Results are evaluated using Hits@k,Mean Ranking (MR),and Mean Reciprocal Ranking (MRR).
No specific statistical values or measured effects are provided in the context.,Limited prediction accuracy,especially on the GDELT dataset.
Time-aware negative sampling is rarely explored in TKGC.,TKGC methods have advanced link prediction by incorporating temporal validity,but challenges remain
Temporal logical rules enhance explainability and coverage for link prediction.,Incorporating external knowledge to improve prediction accuracy,especially by enriching structural and temporal information.
Utilizing semantics of entities and relations,such as names and types,through pre-trained language models for better link prediction.
Use of a matching policy and algorithm to map and merge disparate dimensions based on names,alternative names,and descriptions.
Application of open source libraries to convert documents into TF-IDF (Term Frequency-Inverse Document Frequency) matrices with 3-grams and compute cosine similarity for dimension matching.,The research provides a Python code fragment for the matching algorithm (Figure 2) and states that the implementation uses open source libraries for data structures and similarity computation. However,there is no explicit mention of a public source code repository or full project code.
Quantitative results show a significant similarity score (e.g.,FOKEI and OKEI: 0.600247262; MOSTATUS: 0.649865971),supporting merging of these dimensions.
The approach forms a formal foundation for automatic merging,but human affirmation is required; further research is needed to enhance attribute matching and user interface development. No explicit p-values reported.,The proposed algorithm integrates heterogeneous sources using additional textual features of dimensions (alternative names and descriptions).
A score matrix fragment is presented,showing precise similarity values for various dimensions (e.g.,FOKEI: 0.358797982
The approach demonstrates the ability to match dimensions between sources,aiding integration into the IAM system.,The technology requires a specialist to design a common storage scheme
Development of a user interface to support user queries to the Integral Analytical Model (IAM) is required.,Future research should focus on developing more advanced intelligence merging algorithms,enhancing the intellectualization of matching dimension attributes
Local-Global Query Contrast Module: Integrates global and local historical information to enhance prediction robustness through contrastive training.,,The proposed LogCL model outperforms variants and baselines on ICEWS14
No explicit p-values or statistical significance measures are reported in the context.,Primary outcomes were measured using mean reciprocal rank (MRR) and Hits@k (k=1,3
Comparative models (e.g.,DisMult,ComplEx) showed lower performance across datasets.
Sensitivity analysis and noise studies were conducted but specific statistical values are not provided in the context.,Importance of historical information related to the query is often neglected,impairing prediction accuracy.
Robustness to noise is rarely considered in TKG reasoning research.,LogCL outperforms state-of-the-art methods on four benchmark datasets,with MRR improvements up to 7.9%.
Recommendation: Combine both global and local historical information for best results in TKG extrapolation tasks.,Existing methods often focus only on either global or local historical patterns,lacking effective integration of both.
There is a need for better methods to robustly combine global and local historical information for improved future fact forecasting.,Future research should address filtering irrelevant KG snapshots based on queries to improve prediction accuracy and enhance model robustness against noise. There is also a need to better integrate both global and local historical information and to overcome current limitations in capturing important historical patterns.,
Task Allocation and Intelligent Routing: Assigning tasks and directing data intelligently between LLMs and SLMs to optimize collaboration and performance.,,The paper identifies significant progress and potential in LLM-SLM (large and small language model) collaboration
Key challenges include collaboration efficiency,inter-model consistency,lack of standardized evaluation metrics
Future research should focus on dynamic collaboration strategies,privacy-preserving protocols,and robust conflict resolution mechanisms for reliable outputs.
The context discusses challenges,open issues,and communication methods in LLM-SLM collaborative systems
Difficulty ensuring consistency in output style,knowledge scope,and factual accuracy across models.
Lack of standardized evaluation metrics and benchmarking platforms for collaborative effectiveness,efficiency,and cost.
Challenge in balancing privacy preservation with collaborative performance.,Combining Large Language Models (LLMs) and Small Language Models (SLMs) can improve AI efficiency,economy
Key collaboration modes include pipeline,hybrid/routing,auxiliary/enhancement
Main challenges: collaboration efficiency,inter-model consistency,evaluation metrics
Future research should focus on dynamic collaboration strategies and standardized interfaces.,Develop privacy-preserving data sharing protocols and formalize privacy guarantees for LLM-SLM collaboration,balancing privacy with collaborative performance.
Establish comprehensive monitoring frameworks,standardized deployment patterns,and automated fault diagnosis tools for collaborative systems.
Improving Temporal Reasoning of Large Language Models via Recounted Narratives,Zhang Xinliang Frederick,Beauchamp Nick
Narrative-aware Demonstrations: Uses concise,simple narrative examples to guide model outputs.,
Simple Report-style GPT-4 narratives achieve the highest F1 scores (up to 65.7 on ProScript),emphasizing conciseness,simplicity
Fine-tuned LLAMA3-8B achieved 71.9 F1 and 1.40 GED on ProScript,outperforming GPT-4 (63.9 F1,1.64 GED).
Average alignment between automated responses and human inspections was 72.8%.,Only three evaluation benchmarks were used,which do not cover all possible domains (e.g.
Further research is needed to test generalizability and credibility.,Small language models (LLMs) perform much worse than GPT-4 in temporal reasoning tasks,reaching only 40–60% of its F1 scores.
Simple Report-style prompts with GPT-4 give the best results due to their conciseness,simplicity,and factual accuracy.
Using NOT can help small LLMs match or surpass GPT-3.5 performance.,There is a significant gap in temporal reasoning performance between AI systems and humans,and between proprietary large language models (LLMs) and open-weights
Human baseline comparisons are incomplete,as evaluations were only conducted on one dataset; further human evaluations on additional datasets are needed.,Future research should investigate temporal graph generation (TGG) in more diverse domains
Data fusion: Combines various descriptions into a single representation to resolve value-level heterogeneity.,,No single truth inference algorithm consistently outperforms others across datasets; 17 algorithms were compared on five datasets
Main objectives in addressing data heterogeneity are completeness,conciseness,and correctness of unified data; no p-values or quantitative statistical significance reported.
The truth inference problem is unresolved; no single algorithm consistently outperforms others,indicating a need for further research.,Handling data heterogeneity and inconsistency in Big Data integration remains a significant challenge.
Integrating textual data with nontextual data (such as images) presents new research opportunities and challenges.,Future research should address the instability of truth inference algorithms across datasets,as no single method consistently outperforms others. Additional directions include handling Big Data integration
Simulated negotiation scenarios: Negotiation situations were simulated to reflect real-world contexts due to recruitment and device constraints.,,The Negotiation approach achieved the highest satisfaction rate at 83.3% overall (70% homeowners
No p-values or statistical significance data are provided in the context.,Negotiation approach achieved the highest satisfaction rate: overall 83.3% (homeowners 70%,guests 90%).
No explicit statistical values (e.g.,p-values) for comparisons between approaches provided in the context.,Simulated negotiation scenarios used due to difficulty recruiting real-world participants with compatible devices and privacy disclosure.
Participants sometimes lost track of each other's preferences,prolonging negotiations.,Negotiation using ThingPoll helps balance privacy and functionality but can be time-consuming and may lead to unequal participation.
Design improvements should address workload,equity in participation,and preference management.
Conducting in-the-wild studies to capture real-world negotiation dynamics,as current findings are based on simulated scenarios and a limited participant pool.,Suggested future research directions include: exploring the ethical balance of power between homeowners and guests
The study aims to (1) observe how humans negotiate smart home privacy configurations,(2) inform the design of the negotiation system ThingPoll,(3) evaluate ThingPoll’s practicality
Information fusion techniques: Combine multi-modal data (textual and structured) and use attention mechanisms to dynamically weight information sources,improving model accuracy and reliability.,
Models like Know-Evolve,GHNN,and EvoKG address dynamic event prediction using various neural and probabilistic frameworks
Scalability and interpretability remain key challenges; proposed solutions include distributed computing,sampling techniques,and attention mechanisms
Need for improved methods for information fusion and interpretability.,Incorporating large language models (LLMs) into TKGRL can significantly improve model accuracy and effectiveness,but challenges like computational complexity and bias must be addressed.
T2TKG introduces a novel approach to capture intra-time and inter-time latent relations,enhancing entity prediction in temporal knowledge graphs.,Addressing scalability challenges in Temporal Knowledge Graph Representation Learning (TKGRL)
Incorporating information from multiple modalities and leveraging large language models to better represent dynamic and evolving temporal knowledge graphs.,Future research should address scalability challenges,enhance interpretability
Algorithmic pre-screening to reduce human screening of patients with normal cardiac function.,The research supports reproducibility through open-source algorithms and required teams to submit code for training and running models. Code was submitted via private GitHub or Gitlab repositories and will be publicly released after the Challenge and publication of conference papers.,No primary findings or quantitative results are reported; analysis and conclusions will be shared after the Challenge concludes.
Results: No results reported; analysis pending Challenge conclusion.,,Algorithmic pre-screening using open-source algorithms can reduce human screening of patients with normal cardiac function
Novel evaluation metrics supported reproducibility,generalizability,and relevance of the research.
Cost-based scoring is controversial but important for improving access in resource-constrained environments.,,
Event calculus integration: Maintains correct temporal intervals for fluents and is combined with survival analysis to handle incomplete knowledge.,,Temporal projection methods improved Q/A performance across all query sets
The largest improvement was 141% (Query Set 3),and the smallest was 31% (Query Set 5).,Significant improvement in Q/A performance in all query sets with temporal projection methods.
Further research is suggested for these areas.,The proposed knowledge representation and reasoning methods significantly improve temporal reasoning,leading to a 49% average increase in Q/A performance.
Future work should focus on reasoning about state transitions,recurrent events,and probabilistic event effects.
Developing the capability to reason about probabilistic effects of events and estimate the likelihood of event occurrence within a time interval.,The study suggests future research should focus on: (1) reasoning about when individuals enter or transition between time-dependent states; (2) extending methods to handle recurrent and periodic events; and (3) reasoning about probabilistic effects and estimating event likelihoods within time intervals.,
Survey assessment: A 44-question survey was used to evaluate each method across the five criteria.,75% (n = 12) of the methods provided tools for reproducible workflows and installation,such as Docker containers (n = 6) and Jupyter or R Notebooks (n = 8). Source code for Graph Toolkit
Statistical significance (p-values) is not reported in the provided context.,The average coverage score across five assessment criteria was 3.93 (min = 2.79,max = 4.90).
75% (n = 12) provided tools for reproducible workflows,including Docker (n = 6) and Jupyter/R Notebooks (n = 8).,Systematic comparison was subjective
Most methods provide good usability,availability,and reproducibility
Limitations include subjective assessment,incomplete inclusion of new methods,and lack of quantitative benchmarking.
Future work should include formal benchmarking and evaluation against additional tools.,The comparison of open-source KG construction methods was subjective,limited to three researchers
Embeddings are only available for small KGs; generating embeddings for larger KGs remains a challenge.,Future research should include: (1) objective,quantitative comparisons of KG construction methods using the same data; (2) formal evaluation of data integration and ontology alignment pipelines against tools like Web Karma
A generative model of memory construction and consolidation,Spens Eleanor,Burgess Neil
Statistical analysis: Statistical parameters such as means,standard deviations,and confidence intervals were reported.
No outliers were excluded,and sample sizes were chosen to achieve statistically significant results; however,no specific quantitative results or p-values are provided.
The findings are fully replicable using code and data available on GitHub.,No primary outcomes,results
The study is a computational analysis using secondary data; details on outcomes or effects are not included.,Randomization in the usual sense does not apply,as only secondary data was used.
No modelling related to sex,gender,race
No outlier exclusions; subset restrictions are stated in Methods.,The study is fully computational,using only secondary data
No specific recommendations are provided.,The study does not use data relating directly to neurological conditions,though it discusses their relation to the model.
No modeling was performed related to sex,gender,race
Future work could involve applying the model to primary data or broader demographic variables.,,
Organ-on-a-chip Technology: This technique is used to explore biological mechanisms by perfusing patient plasma through lung tissue organoids.,The article and supplementary material include the original contributions. There is no explicit mention of source code availability. For further details,inquiries can be directed to the corresponding authors.
Using Knowlets and curated relationships,68 genes were identified as co-occurring with morbid obesity,type 2 diabetes
No explicit statistical values provided.,Machines struggle with near sameness,conceptual drift
Human bottlenecks in manual queries and algorithms limit scalability.,The main barrier to realizing FAIR Digital Twins (FDTs) is conceptual,not technological.
Developing Knowlets to support machine learning and knowledge discovery requires addressing limitations of fixed ontologies and static knowledge graphs.,Future research should focus on clarifying foundational concepts for FAIR Digital Twins (FDTs),addressing conceptual—not just technological—barriers
Implemented the model in PyTorch,using Adam optimizer,grid search for hyperparameters
LTGQ also led in Hits@1,Hits@3,and Hits@10 across these datasets
No p-values or explicit statistical significance values are reported in the context.,Primary outcomes are evaluated using Mean Reciprocal Rank (MRR) and Hits@N (N=1,3
Ablation studies show that removing DTM,QFM,or DM modules reduces performance on ICEWS14 and YAGO11k.
On YAGO11k and Wikidata12k,the model shows marked performance improvements (exact values not specified).,
The model’s aggregation and dissemination modules,using triaffine transformations and CNNs,enhance feature fusion and adaptability.
Recommendation: Apply LTGQ for improved temporal knowledge graph completion tasks.,,
Relation Extraction using Distant Supervision and a neural architecture (Aggregated Piecewise Convolutional Neural Network) to identify relationships between entities.,,Knowledge Graphs are powerful and flexible for integrating heterogeneous data
The XI Pipeline enables semi-automatic integration of various data types (publications,social content,cloud infrastructure) into Knowledge Graphs.
No quantitative results or statistical significance (p-values) are reported.,Introduced the XI Pipeline,an end-to-end process to semi-automatically map content onto a Knowledge Graph.
Deployed Guider for integrating cloud infrastructure data,enabling job auditing,compliance
Integration quality depends on the Knowledge Graph,which may contain errors,inconsistencies
Some input data mentions (e.g.,noun phrases) cannot be resolved by current methods.,Knowledge Graphs are powerful for integrating diverse data
The integration quality depends on the Knowledge Graph’s accuracy and completeness; errors or missing data must be addressed first.,The integration process for mapping input data onto a Knowledge Graph is highly complex and time-consuming,especially for unstructured and semi-structured data.
Classical NLP and entity linking techniques are not applicable to certain data types,such as log data without sentences,requiring customized approaches.
Relation extraction remains a challenging task due to the many explicit and implicit ways relationships can be expressed between entities.,Suggested future research directions include: improving automated entity linking and co-reference resolution techniques,addressing errors and missing data in Knowledge Graphs
Pattern-Based Design Applied to Cultural Heritage Knowledge Graphs,Carriero Valentina Anita,Gangemi Aldo
Test-driven design: Using test data and tools (e.g.,TESTaLOD) to validate ontology development against requirements.,The research is reproducible: all test cases (inference
Module metrics show high quality: appropriateness = 1,encapsulation = 0.96–1,coupling = 0
No statistical significance (p-values) is reported in the provided context.,Relationship Richness: ArCo v0.1 (0.43),v0.5 (0.34)
Inheritance Richness: ArCo v0.1 (1.1),v0.5 (2.9),v1.0 (2.48)
Terminological Coverage: ArCo v0.1 (0.09),v0.5 (0.37),v1.0 (0.56)
Number of axioms: ArCo v0.1 (715),v0.5 (9,564)
Number of logical axioms: ArCo v0.1 (180),v0.5 (2,210)
Number of classes: ArCo v0.1 (54),v0.5 (329),v1.0 (340)
Number of object properties: ArCo v0.1 (38),v0.5 (332),v1.0 (616)
Number of datatype properties: ArCo v0.1 (8),v0.5 (153),v1.0 (154)
Number of annotation assertions: ArCo v0.1 (429),v0.5 (6,357)
Modular ontology design,as in ArCo,enhances flexibility
Recommendation: Adopt modular,pattern-based approaches and improve tools for pattern selection and reuse.,Lack of well-documented
Absence of automatic procedures for detecting,annotating,and reusing ODPs in knowledge graphs.
Need for improved expressiveness in annotation languages like OPLa to capture more attributes and relations of patterns.,Future research should address the lack of well-documented,high-quality Ontology Design Patterns (ODPs)
IMU: Inertial Measurement Unit,a sensor for motion tracking.,The research goal is to develop a smartphone-based continuous complex positioning platform; the approach involves integrating multiple sensors and wireless signals for seamless indoor-outdoor positioning; the principal finding is the successful architectural design and implementation of a flexible platform enabling accurate real-time positioning research.
Smartphone-based continuous complex positioning platform leveraging on-device and server-based processing of diverse sensor and signal measurements.,,The paper presents a smartphone-based continuous complex positioning platform integrating on-device and server-based positioning with support server communication for seamless indoor-outdoor localization.
No quantitative results,primary findings,or statistical significance (p-values) are explicitly provided.
Results: The platform enables seamless integration of various positioning techniques (GNSS,Wi-Fi,BLE
Need for further research to develop correction data mechanisms for indoor positioning errors.,The study presents a smartphone-based continuous complex positioning platform architecture,enabling seamless integration of diverse positioning techniques.
Comparative experiment: The effectiveness of PRIVAFRAME was compared with a transformer-based deep learning approach.,The research is reproducible. The dataset and Python source code are available at https://github.com/Gaia-G/SPeDaC-corpora and https://github.com/Gaia-G/PRIVAFRAME,but access requires signing an agreement for ethical research purposes. The test set is included in the PRIVAFRAME repository.
Critical categories with low or zero identification included POLITICAL AFFILIATION,PROFESSIONAL CERTIFICATION,PROFESSIONAL EVALUATION
Excellent identification accuracy (+90%) for categories like DISABILITY,NAME,PERSONAL POSSESSION
Critical performance (−55%) for AGE,PHYSICAL TRAITS,POLITICAL AFFILIATION
True positive rates ranged from 0% (e.g.,POLITICAL AFFILIATION) to 100% (e.g.,CRIMINAL).
False positives varied widely by category (e.g.,0 for FETISH,746 for PERSONAL POSSESSION).
High false positives for categories like AGE,CREDIT & SALARY,DEMOGRAPHIC
Some categories (e.g.,DISABILITY,OFFSPRING
Suggestion to increase sample sentences and refine modeling strategies.,The knowledge graph approach is more accurate than deep learning models for identifying fine-grained sensitive personal data categories (PDCs),especially with limited labeled data.
Need for ontological extension of PRIVAFRAME to include thematic roles and sensitivity variables for more nuanced detection.,Future research should address FRED’s missed frame extraction for certain PDCs (e.g.,GENDER
Compared FedMIA methods against six baseline attack methods and tested robustness against six defense techniques,including Gradient Perturbation,Gradient Sparsification
Used AUC and TPR@FPR as evaluation metrics to assess attack effectiveness and utility loss.,,FedMIA
Conventional defenses (perturbation,sparsification,mixup) are ineffective against FedMIA
Larger hypervolume (HV) values indicate better privacy-utility trade-offs; however,statistical significance (p-values) are not reported.,Primary outcomes measured: Attack effectiveness (TPR@FPR=0.1%)
TPR@FPR=0.1% for FedMIA-II reaches up to 83.14 (IID,AlexNet),78.55 (β=10)
Hypervolume (HV): Lower values indicate more effective attacks. For AlexNet,FedMIA-II achieves HV=0.2588 (Mixup defense); for ResNet,HV=0.2969 (Perturb defense).
Generated images from embeddings visually resemble original training images,indicating privacy leakage.,Conventional federated learning privacy defenses (perturbation
There is an urgent need for new defenses specifically targeting cross-client information leakage.,FedMIA is a highly effective membership inference attack in federated learning,exploiting updates from non-target clients.
Conventional privacy defenses (perturbation,sparsification,mixup) are ineffective against FedMIA.
Further exploration required on the impact of different federated learning settings (e.g.,client numbers,communication rounds
A federated learning framework (PFGNN) using graph similarity strategy,attentive aggregation scheme,and function encryption (specifically
Comparative experiments were conducted between PFGNN,Centralized ML,and FedAvg using GraphSAGE for local model training.
PFGNN improved accuracy by 7.38% on average over FedAvg under different labels and graphs (Cora: 7.14%,Pubmed: 6.03%,Citeseer: 9.04%).
Function-Hiding MIFE provides computational privacy guarantees,protecting client weights and model parameters from adversaries; statistical significance (p-values) not reported.,PFGNN achieved the highest average accuracy on Cora (0.9213)
Under different labels and graphs,PFGNN outperformed FedAvg with improvements: Cora 7.14%,Pubmed 6.03%
Time overhead for encryption/decryption was lower in PFGNN than Hybrid,e.g.,for 3 clients: Enc (PFGNN) 1.883 vs. Enc (Hybrid) 4.145; Dec (PFGNN) 2.034 vs. Dec (Hybrid) 11.654.
Recommendation: Use PFGNN for secure,efficient,and accurate federated learning on graph data.
Reducing communication overhead while maintaining model accuracy and privacy in distributed training frameworks.,,
Hierarchical Benchmark Design: TIMEBENCH categorizes tasks into symbolic,commonsense,and event temporal reasoning to comprehensively evaluate temporal reasoning abilities.
No explicit p-values or statistical significance measures are provided in the context.,Primary outcomes are measured using the overall score S,which combines BLEU-4
For SituatedGen (Table 8),top model scores: LLaMA2† 70b: 74.92,LLaMA2† 13b: 64.81
For TimeBench (Table 9),LLaMA2-Base70b achieves Avg. 64.4,Baichuan2-Base13b Avg. 51.3
Effects of prompting strategies (CoT,FS,FS CoT) are reported for each model and task.
Statistical values include accuracy (Acc),exact match (EM),and F1 scores for each subtask.
Significant gap remains between model and human performance.,Chain-of-thought (CoT) reasoning is not consistently effective; it often leads to performance declines,with an overall decrease of 7.4% in zero-shot settings.
Recommendation: CoT prompting should be applied selectively,as its effectiveness varies by task.,There is a substantial gap between state-of-the-art large language models (LLMs) and human performance in temporal reasoning.
LLMs struggle with abstract time understanding,temporal relations modeling,and lack temporal commonsense.
Chain-of-thought prompting does not consistently improve temporal reasoning performance,indicating a need for better reasoning techniques.,Future research should address the substantial gap between state-of-the-art large language models and human performance in temporal reasoning
Explanation: These keywords refer to digital health records,wearable devices,artificial intelligence methods
Prototype evaluation using MAUQ: Healthcare providers evaluated the prototype with the mHealth App Usability Questionnaire,assessing usability,interface
Usability scores were high: ease of use averaged 4.5–5,interface and satisfaction averaged 4–4.4,with 7–10 participants rating items ≥4.
60% of participants preferred an anomaly alert feature; summary reports were favored weekly,monthly,or upon anomaly detection. No p-values reported.
Study did not directly involve patients; only healthcare professionals participated.,The prototype was rated highly for ease of use,interface
Key recommendations include expanding compatibility to all smartwatches,adding features like exercise time tracking,heart rate variability
Future work should involve larger,more diverse participant groups and integrate patient perspectives in the co-design process.,Need for further research on effective co-design approaches involving healthcare providers
Exploration of methods to enhance the usability and adoption of machine learning-based health monitoring applications.,Future research should include larger sample sizes for broader representation,involve patients and healthy individuals in co-design
Human Evaluation: Clinical psychology graduate students evaluate generated summaries for factual consistency and usefulness.,,The fine-tuned L-Phi model showed the highest factual consistency (3.83) and usefulness (general: 3.48
No significant improvement was observed for Moments of Change (MoC); p-values or statistical significance are not reported.,Fine-tuning on a single dataset (Phitemp,Phitl) worsened hallucination issues and reduced performance (FC: .141
L-Phi and P-Phi models achieved the best automatic evaluation results (e.g.,L-PhiNST\&PRT FC: .424,EA: .971; P-PhiNST\&PRT FC: .397
Fine-tuned models significantly reduced hallucinations but did not show significant improvement in Moments of Change (MoC).,"LLM-generated clinical summaries may contain factual inaccuracies (""""""""hallucinations"""""""") and biases",potentially leading to serious errors in mental health decision-making.
The study is limited to 30 sampled TalkLife timelines for evaluation,which may affect generalizability.,The fine-tuned L-Phi model significantly reduces hallucinations and improves factual consistency and usefulness in mental health timeline summarization.
Jointly training on multiple data types fails to integrate them effectively,leading to worse performance than in-context learning.,
Unified reinforcement learning method: Integrates semantic representation and connectivity,using a policy network trained with reinforcement learning to optimize recommendations.,
Quantitative results: For example,on the cloth dataset,NDCG@10 improved from 2.770% (TPRec) to 2.935% (ours)
The model consistently produces fewer invalid users than PGPR and TPRec,demonstrating better guidance for reinforcement learning agents; no p-values are reported.,The primary outcomes are NDCG@10
On the cloth dataset: NDCG 2.935,Recall 4.949,Precision 0.798.
On the phone dataset: NDCG 5.467,Recall 9.311,Precision 1.419.
On the beauty dataset: NDCG 5.883,Recall 9.025,Precision 1.881.
The model had fewer invalid users compared to PGPR and TPRec.,,The proposed temporal knowledge graph path reasoning model outperforms existing methods (PGPR
It achieves 1%-7% improvement over TPRec in NDCG,Recall,and Precision at K=10.
Recommendation: Incorporate temporal information for higher-quality inference paths and improved recommendation performance.,Most current knowledge graph reasoning (KGR) methods treat user-item interactions as static,ignoring time information
Existing methods using time information mainly enhance data representation,not reasoning,missing deeper inference from timestamps.
Embedding-based and path-based methods each use only part of the knowledge graph information; a unified approach integrating both is needed.,,
Visualization techniques,including summaries (digests of daily life) and landmarks (cues for remembering experiences),to help users explore large datasets.
Development and use of a prototype system for data organization and social use studies.,,Aggregating heterogeneous personal data using time
Studied data organization by time,location,and people.
Developed digest views (yearly,monthly,daily) and visualized summaries and landmarks.
Public summaries for groups,communities,areas
Key challenges include schema matching,access permissions,and unified information architecture.
Future work should focus on public summaries,efficient network management,and activity recommendations.
Creating public summaries for groups,communities,areas
Inclusion and exclusion criteria: Applied to select articles based on language,publication date,relevance
Data extraction form: Used to consistently extract metadata and synthesized data from each included article for analysis.,,Out of 32 included articles
Integration effort: High (most studies),Medium (some),Low (none reported).
Security and privacy issues not discussed due to lack of coverage in included articles.,Most studies focus on integrating health data for General Healthcare Information Management,followed by Health/Lifestyle Self-Management and Chronic Disease Management.
Integration and utilization approach classifications are vague and overlapping,requiring clearer definitions and frameworks.,
Case studies evaluating KG construction effectiveness using real-world healthcare data and various evaluation measures (e.g.,P,R
Many studies focus on limited data sources and lack detailed evaluation; quantitative results include KG sizes (e.g.,#n: 12,473–millions; #e: 10
Main limitations include insufficient discussion on KG construction mechanisms,limited evaluation (mainly on embedding components),and lack of real-life case studies.
Some KGs are context-specific (e.g.,Chinese context),reducing generalizability.
Systems often limited in functionality (e.g.,answering only one intention per question).,Knowledge Graph (KG) technology is effective for integrating diverse healthcare data
There is insufficient discussion on ontology design,KG statistics,and real-world validation.
Future research should address these gaps and improve evaluation methods.,Limited diversity and integration of KG data sources,often relying on a small set of biomedical publications.
Evaluation methods focus mainly on embedded components rather than the overall effectiveness and quality of the resultant KG.,Future research should address limited data sources,insufficient disclosure of KG construction mechanisms
Two-phase TKG-SRec framework: (1) entity embedding learning (pretraining on static knowledge,refinement with temporal knowledge),(2) sequential modeling with dynamic entity embeddings.
Evaluation using ranking-based metrics: Hit Ratio@k (HR@k),Normalized Discounted Cumulative Gain@k (NDCG@k),and Mean Reciprocal Rank@k (MRR@k).
TKG-SRec achieves the best NDCG@5 (0.0459,+6.9%) and NDCG@10 (0.0617,+5.0%) with statistical significance (A-value < 0.05) compared to baselines.
Static encoder (S.K.) outperforms TransE and RESCAL in KG modeling,but relying solely on static knowledge significantly decreases performance in sequential tasks.,Primary outcomes were measured using HR@5
TKG-SRec achieved the best performance on all datasets and metrics,with statistically significant improvements over the best baseline (†,𝐴-value<0.05).
Example results: LastFM NDCG@5 = 0.0459† (+6.9%),NDCG@10 = 0.0617† (+5.0%).,Overly large numbers of graph convolution layers (𝐴 and 𝐴′ = 4) degrade performance by mixing too much neighbor information.
Time-aware & dynamic KGs in sequential recommendation remain underexplored.,The proposed static encoder outperforms TransE and RESCAL,demonstrating effective graph learning and reliable entity embeddings.
TKG-SRec surpasses all baselines,confirming its superior dynamic knowledge distillation for sequential recommendation tasks.,Incorporating additional factual data (e.g.
Adopting more sophisticated time modeling methods,such as Time2Vec,to better capture periodicity and improve effectiveness.
Addressing limitations of current item-centric and user-centric knowledge filtering approaches in sequential recommendation tasks.,,
Implementation of a filtering mechanism to remove errors and redundancies based on LLM-generated scores.,The research used LLaMA-Factory to fine-tune open-source LLMs,with all training settings detailed in the code. However
Table 4 provides precise statistics: Re-DocRED (Train: 3,053 documents,59
No explicit statistical significance values are provided.,Fine-tuning LLMs requires significant GPU resources,limiting accessibility in low-resource settings.
Current LLM-based frameworks face high computational complexity and slower inference compared to non-LLM baselines.,Future research should address the lack of annotated datasets for document-level temporal relation extraction (DocTRE),develop new methods for DocTRE
Technical evaluation: System response accuracy,conciseness,and processing times were measured using statistical tests (Shapiro-Wilk
System conditions significantly improved confidence (p<.001),relevance (p<.001),and reduced difficulty (p<.001) in recalling information compared to No System.
Query Mode was rated significantly more useful than Baseline (p<.01); no significant differences in helpfulness between conditions (p=.119).,Query Mode responses were significantly shorter than Baseline (85% reduction,from 115.4 to 16.6 characters
System usability: Query Mode (80.0,𝐴𝐴=11.8),Queryless (77.1
Usefulness: Query Mode rated higher than Baseline (𝐴=5.50 vs 4.30,𝐴<.01); no significant difference between Baseline and Queryless (𝐴=.0358),or Query and Queryless (𝐴=.233).
Task load (NASA-TLX): lower in system conditions; Baseline (9.34,𝐴𝐴=7.19),Query (8.51
Further research is suggested to explore real-world scenarios and long-term use.,Query Mode produced significantly shorter and more concise responses than Baseline,with an 85% reduction in mean response length (115.4 to 16.6 characters).
Recommendations include prioritizing concise,adaptive responses and minimizing conversation disruption in memory augmentation systems.,Need for technical improvements in Memoro’s design
Exploration of alternative information presentation methods,such as providing clues or familiar voices instead of direct answers.,Future research should address issues not covered in the current Memoro design
Design science method: research approach for creating and evaluating artifacts,The research goal is to develop a knowledge graph-based solution (ESGMKG) for ESG metric management using a combination ontology development approach within the Design Science Research methodology; results show the ontology-driven ESGMKG enables unified,flexible ESG reporting and effective querying of ESG metrics information.
Prototype Implementation and Validation: The ESGMKG is implemented on the Stardog Enterprise Knowledge Graph platform,validated through usage scenarios and competency questions using SPARQL queries.,The research is reproducible through its use of the Design Science Research (DSR) methodology and implementation using semantic web standards (RDF/OWL). The ontology and queries are executed on the Stardog platform. The source code for the project is not provided in the context.
No statistical values are provided.,Limited depth and utility of the ontology; further research needed to enrich ESG model representation,data quality
Difficulty achieving consistency and comparability in ESG reporting.,The ESG Metrics Knowledge Graph (ESGMKG) effectively supports ESG metric management and assessment using a structured,ontology-based approach.
Incorporating advanced technologies like large language models is recommended to enhance analysis and reduce maintenance effort.,Enriching the ontology’s depth and utility in ESG model representation,information access
Incorporating natural language processing and large language models for automatic knowledge extraction and enhanced analysis capabilities.,Future research should focus on enriching the ontology for ESG model representation,improving information access and processing
Datasets were split chronologically into training (70%),validation (15%),and test (15%) sets.
No statistical significance (p-values) is reported; Hits@10 and MRR metrics are used,showing rankings are closely but not fully matched.,All datasets were tested and benchmarked; results are in Section 5.
Some datasets (tkgl-smallpedia,tkgl-wikidata) may contain errors from crowd-sourced sources.,The datasets are anonymized
Datasets can be used for additional tasks like user churn prediction and studying knowledge changes over time.,Scalability: Both TKG and THG methods struggle with large-scale datasets,indicating a need for more scalable approaches.
Dataset Diversity: The dataset collection lacks domains like biological and citation networks; expanding to more domains is a future direction.,Future research should explore adding discretized versions of datasets for comparing discrete and continuous methods,expanding dataset domains (e.g.
Evaluation of multiple models (e.g.,BERT,T5
The dataset leaves ample scope to improve complex Temporal KGQA.,The proposed CRONKGQA method significantly outperforms all baselines,especially on entity-type answers (Hits@1: 0.987
The dataset enables robust evaluation of temporal reasoning in QA models.,Lack of broad-coverage datasets for Temporal KGQA,limiting progress in the field.
Significant further improvements are possible in transformer-based approaches for Temporal KGQA.,CRONQUESTIONS reveals that while CRONKGQA achieves high accuracy on simple temporal reasoning,it struggles with complex reasoning. Future research should focus on improving performance on complex Temporal KGQA tasks
Proposed system architecture integrating WebAssembly,Autonomy Core,and Modular Pluggable Connectors to automate software engineering processes.
Application of neuro-symbolic fusion,combining machine learning and symbolic reasoning for autonomous decision-making and explainability.,No information available
No quantitative results,conclusions,or statistical significance (p-values) are explicitly provided.
The guiding scenario highlights areas in need of improvement in software engineering for heterogeneous,evolving platforms.,The study addresses challenges in developing multi-architecture applications
Sustaining an ecosystem of interdependent,pluggable modules through an open,shared API.
Use of a graph database to integrate knowledge and task/process management,enabling flexible modeling of documents,tasks
Layered modeling approach providing flexibility,user guidance,and traceability of communication
The TEAM model enables flexible management of documents,knowledge,tasks
After a certain point,the number of new data objects decreases,but data object relations continue to grow
Users tend to prefer general over specific task and data object types,suggesting a need for clearer guidance or support in type definition.,Lack of models to define and handle evolving mental concepts in knowledge work
Need for user interfaces that effectively manage the complexity of integrated graph models for practical user interaction.,Future research should address gaps in integrating knowledge and tasks/processes,especially supporting dynamic communication and evolving mental concepts. Further investigation is needed into flexible
Statistical Analysis: Paired t-test was used to compare idea space extension and idea quality/quantity between groups.,,The AIA design thinking framework significantly improved idea quality (Group A: 6.10 ± 0.81 vs. Group B: 4.90 ± 0.88
The approach supports deeper thinking in design and contributes to design support systems for complex sociotechnical systems.,,Participants were all students with an innovation design background
No information on long-term effects or real-world application.,Using the AIA design thinking framework with a knowledge graph significantly increased the extension of idea space (0.73 ± 0.10 vs. 0.64 ± 0.06,p = 0.001).
Applying the proposed approach to derive more effective design knowledge graphs for broader use by other designers.,,
Monitoring and evaluation: Monitoring clinical use and performance of computational tools,and evaluating their safety,efficacy
Use of metadata: Leveraging metadata for automated labeling or classification tasks in AI/ML for Precision Medicine.,The context mentions the importance of reproducibility and data sharing but does not provide specific information on the reproducibility of the research or source code for the project. Some software packages and tools are listed with URLs,but no direct source code for the main research is given.
Combining methods and drawing conclusions can be complex and non-trivial.,AI/ML approaches are highly valuable for large-scale biomedical and clinical data analysis,crucial for Personalized Medicine.
Implementing these methods in medical settings faces technical,computational,mathematical
There is a need to develop standards for data sharing,collaboration,and large-scale data analytics in biomedical and clinical ecosystems.
Graph data augmentation and external knowledge integration: Augments training data and incorporates mathematical and commonsense knowledge for better performance.,No source code for the project is mentioned in the context. The reproducibility relies on public datasets (YAGO11k,TimeQA
No statistical significance (p-values) is reported.,,TG-LLM needs adaptations for temporal commonsense reasoning.
Dataset may contain improper or harmful content.,TG-LLM,a new framework
Key enhancements include graph-based reasoning,chain-of-thought bootstrapping,data augmentation
Improving model performance on tasks requiring mathematical,logical,and commonsense knowledge integration.
Enhancing representation learning for the underlying structure and logic of temporal reasoning,which is currently underexplored.,Future research should extend TG-LLM to more complex applications like inductive and abductive reasoning
Descriptive statistics: Used mean,median,variance
Technical skills like “Creating BIM” and “Digital technologies” had lower RII values (0.617–0.622),indicating they were considered less important than interpersonal skills. Statistical significance (p-values) not reported.,
Digital skills are ranked lower than traditional skills like workflow,budgeting,and costing.
The study recommends prioritizing interpersonal skills in curriculum development for construction management.,Lack of connectivity and cohesion in research across higher education institutions,leading to fragmented knowledge.
Overemphasis on digital skills in curricula despite limited perceived importance by academics,suggesting a need to reassess curriculum priorities.,Future research should explore how practicing academics view and integrate specific interpersonal skills as foundational in construction management curricula. Action research and longitudinal case studies are recommended to validate findings and assess if applying these theories improves graduate employability in the industry.
Construction of patient-centric knowledge graphs (PHKGs) and generation of digital twins for personalized diabetes management.,,The study introduces a novel personalized diabetes management approach using digital twins and patient-centric knowledge graphs (PHKGs)
No quantitative results or statistical significance (p-values) are reported in the provided context.,,
Highlights strengths: patient-centricity,knowledge sharing,and adaptability to new data.
Recommends adopting this methodology for enhanced,individualized diabetes care.,Existing diabetes management approaches are fragmented
Future research should focus on expanding and adapting patient-centric knowledge graphs (PHKGs) as new patient data and knowledge become available.,,
Multiagent-based architecture: Implementation of agents (software assistants) to manage,monitor,and coordinate engineering tasks and data exchange across domains.
The approach was exemplified with a quadrotor UAV project,demonstrating integration of mechanical,electrical
No quantitative results or statistical significance (p-values) are provided.,,Lack of interoperability between specialized tools complicates interdisciplinary collaboration.
The approach requires further development,including architecture improvement,agent goal identification
Methodologies for integrating the approach into an overall process model are still needed.,The new approach links distributed models into a unified SysML-based data model,improving knowledge-sharing
Future work includes refining the architecture,defining agent goals,developing communication protocols
Methodologies must be established to integrate the approach into an overall process model,potentially extending the V-model with methods like SYSMOD or Pahl\&Beitz.,Future research should improve and specify the architecture for implementation
Code generation for SDKs: The ontology is designed for code generation,enabling software development kits (SDKs) in multiple programming languages for direct use of ontology concepts.,The research is highly reproducible. The source code and ontology are openly developed on GitHub (github.com/KnowOntology)
No quantitative results or statistical significance (p-values) are reported.,,Limited context window and poor scaling in LLMs.
Much remains to be done; the ontology is a first draft.,KNOW is an ontology designed to capture everyday knowledge for augmenting LLMs in generative AI,with a focus on commonsense
Current ad-hoc approaches to knowledge representation are unsustainable; adopting structured ontologies like KNOW is recommended for future-proofing AI systems.,Lack of awareness and adoption of ontologies and semantic technologies among LLM practitioners,leading to ad-hoc and non-interoperable knowledge capture.
Ongoing requirement for open-source,collaborative,and iterative development to extend the ontology’s coverage and applicability.
Multi-Agent Orchestration: Employs multiple agents communicating via message exchange,managed by the Langroid framework,to delegate and complete sub-tasks.
Dependency Graph Construction and Analysis: Constructs software dependency graphs,identifies nodes with the highest in-degree,and queries vulnerability databases for risk assessment.
No p-values or statistical significance data are provided.,,LLM responses are limited to pre-training knowledge and cannot address private or post-training information.
Updating dependencies can be cumbersome and may introduce incompatibilities.,DEPSRAG generates comprehensive Software Bill of Materials (SBOM),documenting all direct and transitive dependencies
Advanced dependency analysis addresses gaps in current tools,supporting better vulnerability management.,Lack of advanced tools for analyzing and addressing security and maintainability concerns in open-source and third-party libraries.
Need for improved orchestration mechanisms to prevent issues like infinite loops,deadlocks,and instruction deviation in multi-agent systems.
Smart Contracts: Automates and secures data access transactions within the digital twin system after authentication and policy checks.,The research is open access and the dataset can be requested by emailing 202124080119@std.uestc.edu.cn. There is no explicit mention of source code availability for the project. Reproducibility is partially supported through data access upon request.,The proposed system maintains static ciphertext size regardless of the number of receivers
Experimental results demonstrate lower signcryption and unsigncryption costs,and improved efficiency over existing frameworks; specific p-values or statistical significance are not reported.,
Some do not protect data integrity.,The proposed system provides a secure,blockchain-based digital twin model for healthcare
The scheme ensures data confidentiality,integrity,and authorship tracking.
Recommendations include adopting this model for secure,efficient digital twin operations in healthcare.,Lack of a comprehensive system model for secure health data sharing and digital twin operations.
Need for enhanced digital twin autonomy and support for complex operations like data and service contracts.,Future research should address protocol verification,develop system models for health data sharing
Data integration and merging using Karma to convert datasets into JSON-LD,link similar records,and merge documents into a unified knowledge graph.
Use of Semantic Web technologies,including common ontology,URIs
No statistical significance (p-values) or quantitative evaluation results are reported.,,No agreement on APIs or schemas among over 15 organizations
DIG enables integration of diverse data sources and extraction tools,supporting large-scale,flexible
Future work includes improving usability,richer queries,and expanding deployment.
Scalability issues in rebuilding and incrementally updating a knowledge graph with over 1.4 billion nodes.,Future research should focus on refining tools and technology for faster application development,leveraging ontological axioms for richer queries and user interface facets
Querying and analyzing the knowledge graph with SPARQL to extract trends and patterns.,The research uses the publicly available MIMIC-III dataset and tools like Protégé and Ontotext Refine for ontology development and RDF mapping. However,there is no explicit mention of source code availability for the project. Reproducibility is supported by dataset and tool access
No specific quantitative results or statistical significance (p-values) are provided in the context.,,EHRs are disorganized
Lack of standardization in data items,language,and formats complicates data integration and analysis.
Existing ontologies for MIMIC III may lack detail,coverage,or suitability for specific research questions.
Knowledge graph research faces scalability,interoperability,clinical validity
Need for more rigorous evaluation in real-world clinical settings.,Knowledge graphs enable standardized,interoperable representation and integration of EHR data
Further research is recommended to address scalability,interoperability,clinical validity
Limitations in existing ontologies for the MIMIC III dataset regarding detail,coverage,and suitability for specific research questions.
Address research gaps in scalable,interoperable,and clinically valid EHR knowledge graphs.
Automatic ontology population: Populated the knowledge base with process models,source code,and historical data to enhance tool support.
No statistical significance (p-values) was reported; further studies with more participants are planned to claim significance.,,Limited knowledge base (KB) and process/context-sensitivity in the first case study.
Only sub-ontologies for process,tools,and techniques were populated in the first case study.
Further user studies with more participants are recommended to validate results.,Integration of additional clients and resources (e.g.,mailing lists
Investigation of alternative query languages (e.g.,iSPARQLh,SPARQL-DL) and rule languages (e.g.
Conducting more user studies with larger participant groups to validate the results' significance.,Future research should focus on integrating additional clients and resources (e.g.,mailing lists
Knowledge-Enhanced Program Repair for Data Science Code,Ouyang Shuyin,Zhang Jie M.
Ablation study: Systematically removing key prompt components (API knowledge,bug knowledge) to analyze their contribution to DSrepair’s performance.,
Despite LLM non-determinism,DSrepair shows greater stability and consistently superior results across multiple trials; small standard deviations indicate statistical reliability.,
Study is focused on data science code,which may limit generalizability.,DSrepair outperforms baselines
The impact of LLM non-determinism on experimental outcomes remains a challenge for consistent evaluation.,,
Combination of deep learning techniques (e.g.,word embedding,named entity recognition
No quantitative results,statistical significance,or p-values are provided.
Ongoing challenges with data privacy,security,and secure data sharing.
Knowledge graph-based data fusion faces challenges from multi-source heterogeneity,dynamic evolution,and privacy/security issues.
Dynamic evolutionary data fusion requires further research to handle frequently updated raw data and knowledge graphs efficiently.,Future research should address fusing dynamically evolving data with knowledge graphs,especially considering high real-time requirements and high training time costs. Additional investigation is needed into fusing multi-source heterogeneous data
Coupling strategy for advection,diffusion,and production: The algorithm from reference \[14] is employed to couple these terms efficiently.
Computational and communication efficiency analysis: The study evaluates computational efficiency of local compute kernels and communication efficiency,including strategies to mitigate global synchronization overhead.,At the time of writing
Results may not generalize to other systems or problem sizes.,nekCRF achieves up to 22× speedup per compute unit over LAVp,but this advantage decreases with increased communication overhead.
Recommendations include further optimizing communication patterns and kernel memory access to enhance scalability and efficiency.,Most top kernels are limited by global memory bandwidth,with additional factors preventing ideal performance.
Optimizing the reaction rate kernel is difficult due to large working sets,irregular execution orders,and complex data access patterns.
KLONE and KGUARD algorithms: Two anonymization algorithms implementing (k,x)-isomorphism anonymization while optimizing semantic utility.,
Ensuring both privacy and utility (business semantics) in KGs is a nontrivial challenge.,The proposed (k,x)-isomorphism anonymization effectively protects against re-identification attacks
Both KLONE and KGUARD algorithms preserve high utility,with utility U close to 0,but KGUARD introduces significantly less node overhead and redundant structures.
Compared to state-of-the-art methods,these approaches provide stronger privacy guarantees,especially when derived edges are present.
Recommendation: Use KGUARD for better balance between privacy,utility,and data overhead.
There is a lack of anonymization solutions that ensure privacy while maintaining the utility of financial KGs.,Few anonymization solutions exist specifically for knowledge graphs (KGs),mainly focusing on sequential data publishing and node attributes. Future research should address privacy for stakeholders in KG financial data while maintaining utility
Quantitative analysis of system performance,including scalability tests (up to 500 KB/s telemetry,500 connected resources).
No explicit mention of other limitations or self-reported problems in the provided context.,Human-CENTRO,a reference architecture for human-machine CI (Collaborative Intelligence) interaction
The AR application outperformed traditional paper instructions: 74% felt more confident,80% found it clearer for component selection,and 90% received better assembly guidance.
Human-CENTRO is recommended for broader adoption in various fields requiring human-machine collaboration,with further validation in other industries planned.,Further assess Human-CENTRO in different industrial fields to explore its generalizability.
Guide strategic future developments for human-centric digital twin (DT)-based platforms managing human–machine collaboration.,Future research should: (i) further assess Human-CENTRO in different industrial fields to test its generalizability; (ii) adapt and evaluate Human-CENTRO in a digital factory within the AI REGIO network; (iii) extend the PT model,especially internal interactions; (iv) adopt data sovereignty mechanisms for secure data exchange.
Aggregation and integration of heterogeneous data sources (e.g.,sensors,web data) into personalized health knowledge graphs (PHKG).
Construction of PHKGs using patient queries and preferences to dynamically generate personalized summaries.,,
Challenges in defining scope,validation,ingestion
Need for effective validation mechanisms to ensure PHKGs accurately capture personal patient information.,Future research should address: dynamic creation and updating of PHKGs,accurate relationship and class identification
Experimental evaluation in Spark environment: Conducted experiments on system performance,including response times and search space reduction,using real data.
Only binary preferences are supported; more advanced preference models are not used for simplicity.,PERSEUS enables personalized,context-aware exploration of indicators in Data Lakes using user profiles and preferences.
The system supports diverse user goals and roles,enhancing usability for a broad audience.,Enhance the DL-DIVER tool for Semantic Data Lake construction by increasing interactivity and introducing a quality metric for feedback.
Advance preference-based indicators exploration by enabling preference propagation across different exploration contexts.,Future research should improve the DL-DIVER tool for better interactivity and introduce a quality metric for semantic layer management. Further work is needed on mapping metrics and preference propagation across exploration contexts. Adapting PERSEUS for new domains like MICS is also suggested.,
Scalability challenges in GNNs when handling large-scale graphs or datasets.,Developed a robust PHKG integrating movement data,clinical records
Managing scalability concerns when handling extensive datasets and expanding PD-related information coverage.,Future research should focus on sustaining the relevance of the Wear4PDmove ontology,addressing compatibility with third-party ontologies
Differences in units (e.g.,currencies,inflation adjustment) complicate integration.
There are significant business opportunities in broad data integration.,Integration of heterogeneous datasets remains challenging due to differences in formatting,terminology
Future directions include improving integration of structured and unstructured data using new language processing technologies.,Future research should focus on developing more domain-specific metadata,improving integration of heterogeneous datasets
No self-reported problems or suggestions for further research are mentioned.,Docs2KG effectively integrates heterogeneous,unstructured documents (PDF and Excel) to extract comprehensive population information from 2011 to 2021.
The dual-path strategy maximizes document type coverage and reduces time,effort,and resource requirements.
Docs2KG is adaptable and supports knowledge-grounded retrieval,reducing outdated knowledge and hallucination risks.,Extraction of multimodal data (including tables
Achieving meaningful semantic representation of data with references to the source is still unresolved.,Future research should address the challenges of extracting multimodal data from diverse formats,integrating modality-specific extraction models into a unified framework
"These are the keywords or tags for this research. A """"""""knowledge graph"""""""" is a structured representation of facts; a """"""""temporal knowledge graph"""""""" includes time information; """"""""question answering"""""""" refers to systems that answer questions using these graphs.",The paper’s main objective is to create a comprehensive temporal question categorization framework and a universal TKGQA dataset generator; using TimelineKGQA,it systematically generates diverse
CronQuestions is dominated by simple,structurally similar templates,lacking diversity in temporal relation complexity.
The framework systematically classifies question complexity and identifies four key temporal capabilities: TCR,TPR,TSO
The work enables development and evaluation of advanced TKGQA solutions and supports broad application in private domains.,Existing datasets lack comprehensive coverage of all temporal question complexity dimensions,especially aggregation and multi-dimensional temporal operations.
Effective methods for generating temporal QA pairs in private domains,especially using Large Language Models,remain largely unexplored.
This local minima issue is common in gradient descent optimization.,The study introduces a novel multimodal sensor fusion method (SFLR) that achieves superior performance in classification and reconstruction tasks,especially for human activity recognition (HAR).
SFLR is robust to subsampled,lost,or noisy data and can leverage strong modalities to recover weak ones.
Continued research in this area is recommended due to its societal benefits,particularly in healthcare and potential military applications.,No explicit research gaps or future directions are identified in the provided context.
Multi-agent collaboration requires significant computing resources and communication overhead.,The study proposes a framework for LLM-based agents in software engineering,consisting of perception
Future research should focus on improving collaboration efficiency and integrating advanced SE technologies into agents.,Limited research on LLM-based agents using tree/graph,visual
Lack of an authoritative,recognized knowledge base containing rich code-related knowledge for external retrieval in software engineering.,Future research should explore tree/graph-based
Results may not generalize to domains with much larger data,such as paired video and ASR data.,Simple
Further research is needed to refine metrics and disentangle quality from relatedness.,Need to apply proposed metrics to extremely large datasets,such as paired video and ASR data
Limitations in current methodology for further investigating relationships between metrics and downstream task performance.,Future research should apply the proposed metrics to domains with vast data,such as paired video and automatic speech recognition (ASR)
Development of a proof-of-concept prototype amid limited face-to-face interactions: A case study of an engineering two-student team,Medina Uzcátegui Luis,Mardones Fernández José
Prior PBL experience may bias results.,Shared goals,values
Further research is recommended on team size,self-regulation skills,and teamwork without PBL experience.
Future research should examine teamwork dynamics when students lack experience in a PBL (Project-Based Learning) course.,Future research should explore the impact of team size on teamwork dynamics,investigate the development of self-regulation skills in similar contexts
Additional development may be needed to improve less capable models’ understanding of temporal data.,Graphiti and Zep show strong initial results in graph-based memory,but further research is needed
There is a need for more robust,business-focused memory benchmarks and better evaluation of production scalability,including cost and latency.
Fine-tuned models and ontologies could further improve knowledge extraction and system performance.,Limited robust and complex memory benchmarks,especially for business applications; need for new benchmarks to evaluate systems like Zep.
Underexplored use of domain-specific ontologies and fine-tuned models for knowledge extraction in graph-based memory frameworks.,Future research should explore integrating other GraphRAG approaches into Zep,fine-tuning models for Graphiti prompts
Future work needed on utilizing knowledge graphs and edge information for better reasoning.,The proposed model outperforms all baselines on the Koubei dataset,achieving the highest F1 score (0.735) and RP@90% (0.561).
Recommendation: Incorporate knowledge-enhanced learning and hierarchical structures for better video scene recognition.,Effectively fusing multi-perspective information (global vs local,temporal vs non-temporal
Future work aims to better utilize knowledge graphs,especially leveraging edge information for improved reasoning in video understanding.,Future research should focus on better utilization of knowledge graphs
No generalizations of k-anonymity,l-diversity,t-closeness for KGs.
Data quality issues from source data,construction pipeline,and ontologies.
Further work is recommended to address challenges in access control and to promote adoption of Linked Enterprise Data (LED) in enterprises.,Lack of standard access control mechanisms and policy languages for federations of knowledge graphs,especially regarding security and privacy.
Persistent data quality issues throughout the knowledge graph lifecycle,including validation,traceability
Need for practical guidelines,educational programs,and standardized competencies to promote knowledge engineering in both academia and industry.
Error types include Topic Errors,Similar Entity Errors,Related Entity Errors
The heterogeneous structure across propagation steps may be too different for the same network layer to handle correctly.,HEGEL,a novel graph-based global entity linking method
Recommendation: Further research should explore additional heterogeneous information and improve error handling.,Addressing Topic Errors,which are the main challenge for current global methods
Enhancing handling of heterogeneous structures in different propagation steps within heterogeneous graph neural networks (HGNN).,Future research should address topic errors,similar entity errors
Implementation challenges: interaction with external services,access control,storage location
Key challenges include lack of large open PKG datasets,privacy concerns,and technical implementation issues.
The study aims to inspire and coordinate future research on PKGs.,Lack of large,open datasets for PKGs
Open challenges in implementing PKGs,including access control,storage location
Need for continuous integration of external knowledge sources with PKGs,potentially involving user intervention.,Future research should address the lack of large
Lack of social dimension in memory tasks; suggested for future research.,The study suggests learning-related dream reports may aid memory performance,but methodological limitations prevent firm conclusions.
The possibility that learning-related dream reports may act as reminders and induce cerebral reprocessing,which requires further investigation.,Future research should:
Address limitations of subjective dream reporting and sampling.,,The study found that dreams rarely replay complete episodic memories but may incorporate elements from recent learning
Only some QSD-based text features are considered; more attributes (e.g.,sharpness,composition
Image-text modality interaction requires further exploration.,The proposed BMQA method outperforms 25 representative BIQA methods,especially on low-light images
Text features like ‘blur’,‘noisy’,and ‘dull’ indicate poor visual experience
Insufficient research on the contribution of each training stage (self-supervised pre-training,self-supervised training,supervised training).
QuanCrypt-FL is recommended for secure,efficient,and scalable federated learning.
Further research is needed to optimize the balance between privacy protection (e.g.,pruning,quantization) and model utility in real-time
User data retention must balance privacy regulations and team objectives.,The research is in early stages,focusing on user modeling in e-learning platforms using Personal Knowledge Graphs (PKGs).
Future work involves interdisciplinary analysis,formalizing research questions,and user-centered evaluations.
Improving knowledge acquisition,entity recognition,linking
Difficulty in accurately classifying new patients into molecular subtypes when their omics data is incomplete,limiting clinical application.,IntegrAO effectively identifies clinically and biologically distinct subtypes
IntegrAO robustly integrates incomplete,heterogeneous multi-omics data,enabling accurate new patient classification.
Recommendation: Use IntegrAO for holistic patient stratification and translational applications requiring robust integration of partial multi-omics data.,Develop computational techniques that directly model heterogeneous multi-omics datasets with missing data,avoiding sample exclusion or biased imputation.
Incorporate diverse data types (e.g.,histopathology images,clinical notes
No explicit mention of other limitations,self-reported problems,or suggestions for further research found in the provided context.
Rule and ontology reasoning improve knowledge graph completeness,efficiency,and quality.
There are few research results on knowledge graphs in bridge engineering; further studies are needed to expand applications in this domain.,Future research should focus on developing automatic knowledge extraction methods to accelerate the construction of the bridge maintenance knowledge graph,as manual extraction is time-consuming. No other specific future research directions or gaps are mentioned in the provided context.
Short session sequences limit the effectiveness of large discount factors.,MB-GRL outperforms all baseline models on every metric for various behaviors,confirming its suitability for multi-behavior session-based recommendation (MBSBR) tasks.
The context focuses on experimental setup,model comparisons,and evaluation metrics.
Generalizability and robustness could be further strengthened by applying the framework to larger models and datasets.,The three-stage RL framework,especially with dynamic reward mechanisms
Enhanced temporal reasoning integration: Future work should focus on improving scalability and integrating more advanced temporal reasoning capabilities.,Future research should validate model effectiveness and generalization on more external temporal reasoning benchmarks and diverse datasets. Applying the three-stage RL framework to larger models could yield greater performance gains. Comprehensive approaches for complex,creative future-oriented reasoning and robust safeguards for generative temporal models are also needed.
Core techniques from Machine Learning (ML),Deep Learning (DL),and Natural Language Processing (NLP) are essential for processing and harmonizing industrial textual data.
Recommendations include using efficient algorithms and harmonization models to manage disparate data domains.,Data availability and domain context limitations,"especially due to the broad use of """"""""harmonization"""""""" across applications."
Need to update FHIR-based EHR models using NLP (Natural Language Processing) and DL (Deep Learning) techniques on SSU (Structured,Semi-Structured,Unstructured) data.
Further research on real-time fusion methods for emotion recognition from textual data.,Future research should address data availability and context limitations,reduce selection biases
Decision tree forests in Shapley analysis may oversimplify models without enough examples.,The Shapley analysis can identify important brain activation patterns for distinguishing events,potentially revealing patterns missed by mean signal analysis.
Trial-by-trial analysis is suggested for future research.,,Future research should use simultaneous EEG–fMRI to investigate mechanisms of late brain responses. Trial-by-trial studies using machine learning are also suggested. Further exploration of spatio-temporal processing differences and late activations in short-term memory
Limitations in sharing and expanding PKGs for collaborative domains.,PKGs serve as personalized information databases,useful for conversational agents and recommender systems.
Future work should address privacy by integrating solutions like Solid PODS.,Privacy and security issues in PKGs,including user control over sensitive data and protection from malicious access.
Integration of privacy solutions like Solid PODS into PKGs as a future direction.,Future research should address privacy and security issues in PKGs,such as user control over data access and protection of sensitive information. Integrating concepts like Solid PODS is suggested. There is also a need for more research due to sparse literature
No self-reported problems or suggestions for further research limitations are mentioned.,Software Engineering (SE) and Knowledge Engineering (KE) are increasingly interconnected,especially for developing intelligent software systems.
Future research should focus on developing intelligent software for mobile/cloud platforms and leveraging big data for predictive capabilities.,Integration of advanced technologies (like mobile cloud computing,big data analysis
Enhancing the interplay between software engineering and knowledge engineering for efficient,cost-effective intelligent system development.,Future research should focus on developing intelligent software systems for mobile devices and the cloud
No self-reported problems or suggestions for further research are mentioned.,The qualifier matcher significantly improves reasoning by effectively leveraging related qualifiers,even when queries lack explicit qualifiers.
HypeTKG’s performance improves as more qualifiers are used,indicating a positive correlation between qualifier usage and model accuracy.,Existing HKG reasoning approaches lack modules specifically for temporal reasoning.
There is a need for new benchmark datasets and models to study temporal fact reasoning over hyper-relational knowledge graphs.,Future research should address qualifier prediction and time prediction in HTKGs,develop HTKG extrapolation methods
Future work includes developing new auxiliary objectives and applying GCL4SR to other models.,Most existing methods only use local context from individual sequences,missing global context information.
There is a need to better integrate global and local context to improve sequential recommendation performance.,For future research,the study suggests developing novel auxiliary learning objectives to further improve GCL4SR’s performance. Additionally
Many missing entities,relations,and time spans in current temporal knowledge graphs.
The number of entities and relations in temporal knowledge graphs is still small,limiting downstream applications.,The proposed clustering approach effectively groups relevant contexts
Recommendation: Focus on relevant contexts and clustering to improve temporal knowledge graph applications.,Limited data quality in temporal knowledge graphs (TKGs),including missing entities
Challenges in temporal knowledge validation and evaluation,especially without golden labeled data.,Future research should address: (1) improving temporal knowledge harvesting using larger knowledge bases
Imported ontologies need refinement to retain relevant elements.,The FuS-KG is complete,concise
The new modules (“Enablers”,“Barriers”,“Arguments”) enhance AI coaching systems for monitoring users’ functional status.
Refine the Barrier ontology to improve readability,usability,and relevance by retaining only essential elements for the application domain.
Integrate the ontology with natural language understanding (NLU) and natural language generation (NLG) components,and evaluate the system in real-world coaching scenarios.,Future research should focus on: (1) expanding the knowledge base by adding missing concepts and using data mining techniques; (2) refining the Barrier ontology for better usability; (3) integrating ontology with natural language understanding/generation; and (4) evaluating the system in real-world coaching scenarios.
Limited outside graph validation due to budget,regulatory,expertise
Few studies included or suggested external validation.,Knowledge Graphs (KGs) have diverse potential in biomedicine,mainly used for medical science insights and drug repurposing.
External validation will strengthen the robustness and clinical utility of KGs.,Best practices for knowledge graph (KG) construction,especially regarding graph size
More external validation of findings from KGs is needed to ensure real-world applicability and robustness.,The study suggests future research should focus on improving data (99 counts) and algorithms (59 counts),refining graph construction best practices
The study used a mixed-method approach (design science,surveys,documentation analysis) to capture knowledge management needs.
Key challenges remain in stakeholder engagement and fully addressing all requirements.,There is a lack of user-friendly interfaces for general users,especially those without technical knowledge
More than one-third (38%) of identified requirements remain unaddressed,indicating significant gaps for future research.,Future research should address the partially or unmet requirements
Recommendation: AeonG is effective for managing temporal graph data with lower storage and query latency overheads.,Limited exploration of optimizing historical data migration strategies to further reduce query latency,especially for reclaimed data.
Insufficient study on balancing storage efficiency and temporal query performance in large-scale temporal graph systems.,,
Future research should focus on iterative improvement and community network development within PKMS.,Completion,testing
Addressing user acceptance risks,including willingness to share knowledge and ensuring privacy,confidentiality
Further exploration of “promise engineering” to build trust and sustain the PKMS-DPE approach.,Future research should focus on testing,completing
Slightly lower schema consistency for websites due to less structured content.,iText2KG enables flexible,incremental knowledge graph construction using LLMs
Integrate entity type as a parameter in the entity and relation matching process.,Future research should enhance cosine similarity metrics for entity and relation matching,remove the need for threshold hyperparameters
Malicious servers can introduce bias or leak data.,The proposed group verifiable secure aggregation (GVSA) scheme addresses privacy,efficiency
Enhancing practical deployment of GVSA in real-world environments with variable computational and communication states.,The study suggests future research should address GVSA’s challenges in practical deployment,especially handling asynchronous updates due to network latency. It recommends developing an adaptive update mechanism for users and exploring ways to maintain aggregation efficiency and security as the number of users and devices increases.
Aggregating disparate data remains challenging due to trust,privacy,ownership
Future work should focus on evaluating the quality of integrated data.,Need to address inconsistencies and redundancies when merging complementary ontologies (e.g.,BOT and BRICK) for a complete Foundation Data Model (FDM).
Challenges in integrating heterogeneous,multi-source data while maintaining autonomy and minimizing redundancy.,Future research should focus on evaluating the quality of integrated data to fully realize the benefits of data integration. This addresses the limitation that large volumes of data do not always yield high-quality analytical results.
Influence of exogenous factors (e.g.,substance abuse,negative experiences).
Strengthened connections in the salience and executive control networks improve timing abilities by integrating internal and external cues.,,Future research should address gaps in time perception studies between ages nine and adolescence
Diagnostic reasoning is not a definitive process,which may affect evaluation consistency.,The study developed and validated a human evaluation framework for assessing clinical diagnostic reasoning outputs from LLMs
Recommendations include prioritizing content-focused evaluation criteria over linguistic quality in clinical settings.,Addressing the omission of diagnoses by minimizing aleatoric uncertainty,which occurs when evidence is present but not captured by the model.
Improving DR.KNOWS by enhancing knowledge path selection,incorporating probabilistic modeling,and refining embedding quality.
Exploring graph-prompting and instruction tuning on open-source language models to better utilize relevant knowledge paths.,Future research should focus on reducing aleatoric uncertainty,especially where evidence exists but the model fails to use it. Enhancing clinical narrative embedding and improving DR.KNOWS
Study design choices (e.g.,topic selection,data filtering) may affect generalizability.
Diagnoses for COVID-19 were removed before topic modeling,possibly influencing topic composition.,Of 5
Interpretation of diffuse topics is challenging due to low specificity and coherence.,Limited coherence and interpretability of topics with the strongest increases in the PASC cohort,indicating a need for improved topic modeling methods.
Larger confidence intervals in the PASC cohort due to smaller sample size,highlighting the need for larger,more balanced datasets.
Many significant contrasts have small odds ratios,suggesting further research is needed to identify clinically meaningful differences.,Future research should assess factors like acute disease severity
Additional methods such as user trials,interviews,and heuristic evaluations are required for comprehensive assessment.
ETL operations present unique challenges,especially with multiple ETL-related complications.,Data consolidation and a continuous multimodal data supply chain can enhance clinical decision support and generate visual patient timelines.
Need for further research on tumor auto-segmentation.,Future research should address tumor auto-segmentation,evaluate multimodal versus single-modal data for outcome prediction
Study the application of federated graph neural networks to practical problems and real-world scenarios.,Future research should explore advanced embedding alignment technologies for more accurate information integration. Investigating the role and selection of shared public nodes is recommended,especially to balance privacy protection and data availability. Further studies should also examine practical applications and suitable scenarios for federated GNN frameworks.
Recognizing modality sequence as context can inform educational and rehabilitative strategies by optimizing learning and memory through multimodal integration.,,
Parameter importance varies and depends on hardware,software,and workload.
Rule-based approaches require deep system knowledge,are labor-intensive,and may not capture system dynamics.
There is a lack of experimental comparisons to validate the strengths and weaknesses of different auto-tuning approaches.,Future research should focus on: experimental comparisons of tuning approaches to validate strengths and weaknesses; abstracting applications into I/O patterns for targeted tuning; integrating tuning with other technologies like resource allocation; addressing large,sparse search spaces; and improving parameter importance identification and modeling cross-layer interactions.
MulT-EHR can potentially generalize to other graph-based domains.,Difficulty in feature representation learning due to EHR data heterogeneity,sparsity
Potential to generalize the proposed framework to other domains using graph representation learning,such as recommendation systems and molecular chemistry.,
The selection space does not include full-stack tools,so the upper-bound is higher in real-world situations.,AI-CTO effectively suggests software stack solutions by leveraging a software knowledge graph and outperforms all baseline methods in correctness (Hits@20: 0.95
Solutions recommended by AI-CTO are widely used by real companies (top20: 49,top50: 94,top100: 157).
Future work includes adding more software features (e.g.,code features) and improving the description encoder.,Include more features for software
Add attributes for relations in the software knowledge graph to enrich relational information.,Future research directions include: (1) incorporating more software features,such as code features of open-source software; (2) utilizing additional information beyond software descriptions in the description encoder; and (3) adding attributes for relations in the software knowledge graph to enrich relational information.
LLMs struggle with temporal expressions and symbolic temporal reasoning,especially multi-step tasks.,Both semantic parsing-based (SP-based) and temporal knowledge graph embedding-based (TKGE-based) methods are effective for TKGQA
Future work should expand question types and improve model robustness.,Expand question types in datasets,including more combinations
Improve LLMs’ understanding of temporal expressions and symbolic temporal reasoning,especially for complex,multi-step questions.
Address the lack of attention to the most complex temporal constraint compositions in current methods.,Future research should explore more diverse temporal question types,finer time granularity
Need for further research on disparities in PM access.,Collaborative,multidisciplinary efforts and stakeholder engagement are essential for successful personalized medicine (PM) implementation.
Sustainable investment in research,innovation,and healthcare infrastructure is crucial.
Harmonizing complex and fragmented regulations and legislation across jurisdictions to facilitate data sharing,privacy,and ethical implementation.
Enhancing interdisciplinary education,training,and collaboration among stakeholders
Use of static and dynamic graph datasets: Experiments utilize both static (e.g.,Cora,Citeseer
Application of the tdiﬀk measure: This technique determines history sizes for training by analyzing time differences in k-neighborhoods,enabling comparable temporal partitions across datasets.,
Higher detection thresholds (τ = 0.75) yield better results,while risk reduction does not improve performance; statistical significance is not explicitly reported (no p-values provided).,gDOC outperforms DOC in both MCC (correct detection of new classes) and Open F1 Macro (overall OOD detection + classification) on DBLP-easy and DBLP-hard.
Highest MCC on DBLP-hard: 0.09 (warm restarts,small history size).,Only a portion of labeled data is used for training in subsequent tasks
Incremental training and parameter reuse maintain high accuracy without full retraining.,Explore and adapt more out-of-distribution (OOD) approaches to graphs,such as using the IsoMax loss function.
Analyze why omitting old training data helps detect OOD examples and investigate alternative data removal strategies.,Suggested future research directions include: adapting more out-of-distribution (OOD) approaches to graphs (e.g.,IsoMax loss)
SOS model: Provides a high-level meta-layer to generalize and manage data models of heterogeneous NoSQL systems.,The research is reproducible. The implementation of the catalog is available at https://git.io/vxyHO. No further source code details are provided in the context.,The approach enables calculation of storage sizes and access frequencies for collections and indexes; for example
Relative access frequencies for Record,Track,and their indexes are 0.059
Relative access frequencies: Record index 0.059 (5/(5+1+5+75)),Record collection 0.87 (75/(5+1+5+75)),Track index 0.012 (1/(5+1+5+75))
The model covers RDBMS,wide-column,document
Multiple tables/collections may answer the same query,requiring a greedy selection strategy.,The proposed algorithms identify storage requirements and physical access patterns for queries
The model is applicable to RDBMS,wide-column,document
Including statistical and storage metadata allows for accurate storage size and access frequency calculations,aiding in data design decisions.,Need for extending the formalization to support cost-based schema design for NoSQL systems
Further evaluation and application of the hypergraph-based meta-representation in managing heterogeneous,semistructured data and metadata in polyglot systems.,Future research should focus on extending the formalization of data design to support data design decisions and optimization in NoSQL systems. Developing a cost estimator using storage metadata from the metamodel could enable prediction of query performance and support cost-based schema design for NoSQL systems.
Robust statistical methods: Data were analyzed using robust statistics (e.g.,robust two-way repeated measures ANOVA,robust multilevel regression) due to violations of classical assumptions.
Events involving specific actions had a higher density of experience units than spatial displacements (Q = 26.66,p < .001,ξ = 0.79).
Temporal compression operates similarly for past and future events,but compression rates vary by event type and temporal orientation.,Past events were described with a higher density of experience units than future events (Q = 11.99
Actions had a higher density of experience units than spatial displacements (Q = 26.66,p < .001,ξ = 0.79).
Density of experience units significantly predicted compression rates for memories (b = −0.06,SE = 0.009,df = 24.74
The negative relationship between density of experience units and event compression rates was stronger for future than past events (interaction: b = −0.08,SE = 0.02,df = 154.38
Estimated durations were higher for future than past events (Q = 5.06,p = .025,ξ = 0.21).
Estimated durations were higher for actions than spatial displacements (Q = 16.35,p < .001,ξ = 0.42).
Duration estimates increased with density of experience units for both past (b = 0.04,SE = 0.01,df = 17.06
No significant interaction between density of experience units and temporal orientation for duration estimates (b = 0.018,SE = 0.013,df = 240.69
The sample consisted only of thirty-two young adults,which may limit generalizability.,Temporal compression occurs similarly when remembering the past and imagining the future
Assess how future event compression impacts duration judgments.,Future research should examine whether event compression rates can be flexibly adjusted based on goals and task context. Further investigation is also needed into how the nature of events affects compression rates in both memory and future simulation,and how event compression influences duration judgments.
"Automatic classification of software bugs into """"""""what",how,"and """"""""why"""""""" categories using bug reports and program context."
Implementation of a Question Answering (QA) system that recognizes entities,generates templates,and searches the knowledge graph to assist developers.
Bug-related data resources are heterogeneous,unbalanced,and contain noise.
Information overload due to expanding and updating bug data is unavoidable.,Open source software resources contain extensive,complex bug knowledge with rich semantic associations.
The proposed framework uses knowledge graphs for bug classification,search,and recommendation
The BNER method with word embeddings enhances bug-specific entity recognition,effective even across different projects.,Need for more effective search and recommendation techniques based on knowledge graphs for bug fixing.
Development and implementation of a bug-fix knowledge QA system to assist developers in understanding,locating,and resolving bugs.
Improvement in automatic classification of bugs using bug knowledge graphs for better bug understanding and resolution.,,
Dis-Aggregated Estimation Approaches: Model-based methods involving model specification,calibration,and validation
Aggregated Estimation Approaches: Use aggregate travel demand and traffic counts to calibrate demand models and estimate origin-destination flows.,,AFFN achieved the best prediction performance (lowest MAE
Multi-task AFFN outperformed AFFN in OD and IO prediction on Nanjing,and in OD prediction on Xi’an,showing IO prediction helps OD accuracy.
Integrating graph attention,periodic flows,and external factors significantly improved prediction accuracy; self-attention on periodic flows helped
AFFN variants without graph attention,periodic flow,or external factors performed worse.
The external factor-based attention module and EMGC-GRU enhance prediction by capturing spatial,temporal,and external influences.
Aggregated estimation approaches can improve demand estimation but are limited by assignment matrix errors and data constraints.,Difficulty in accurately estimating the assignment matrix,leading to non-negligible errors in aggregated estimation approaches.
Limited data volume and model representational capability restrict the effectiveness of demand estimation methods.,Future research directions include: 1) extending from one-step to multi-step prediction models,2) predicting more detailed passenger flows by integrating local trip data (e.g.
Dis-Aggregated Estimation Approaches: model-based,includes model specification,calibration
Aggregated Estimation Approaches: uses aggregate data (traffic counts),model calibration,may assume assignment matrix independence or mutual dependence
Ontology-grounded knowledge graphs (KG) were constructed to organize,prune,and explain relationships.
The KG search identified several confounders,including genes,enzymes
The KG search successfully identified confounders,including conditions and phenotypes,some of which matched those reported in the literature (e.g.
The KG search only produced output for phenotypes as colliders and mediators; genes,enzymes,and drug exposures were also identified as potential confounders.
Future work should improve KG search for causal feature selection and standardize reporting to increase reproducibility.,The search depth threshold may have limited recall,missing relevant confounders
Future work is needed to enhance KG search for causal feature selection and extend methods to other content areas.,Future research should enhance knowledge graph (KG) search for causal feature selection,address terminology mapping issues
Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables,Gu Jian,Gall Harald C.
Design of retrieval-based neural models or inference engines paired with the constructed code taxonomy for code retrieval and generation.,No information available,The paper proposes a semantic-aware framework (SPF) using a three-layer semantic pyramid and code taxonomy to improve top-down code generation and code reusability.
The approach emphasizes graph-form representations for connecting intentions to realizations,supporting inspection,requirement elicitation
No quantitative results or statistical significance (p-values) are reported.,The primary outcome is the proposal and planned evaluation of a semantic-aware framework (Semantic Pyramid Framework,SPF) for top-down code generation.
The framework aims to improve code reusability,support inspection/manipulation,and enable multimodal representation.
The framework's applicability to real-world,complex,and diverse development scenarios remains untested.
Further evolution and targeted application areas,like WebUI and mobile development,are recommended.
Construct a code corpus of templatized,composable subprograms to promote code reusability and support code variants.,Future research should focus on: studying the framework’s behavior and impact on complex and highly modular software; building code taxonomies in limited scopes; exploring semantic associations using graph representations; constructing lexical databases with frame semantics; and developing retrieval-based neural models for broader applicability.
Comparison of hierarchical data model and KG using selected graph quality metrics relevant to TEL and VET.,,The Knowledge Graph (KG) doubled the average degree centrality (ADC) compared to the hierarchical model
No explicit p-values or statistical significance values are provided in the context.,Primary outcomes were measured using graph quality metrics: Average Degree Centrality,Clustering Coefficient (number of communities and average modularity score)
Effects: The Knowledge Graph (KG) structure improved connectedness,community formation,and the ability to represent complex learning relationships compared to the hierarchical model.
Increasing TMP robustness against textual data sparsity and enriching LO contextualization with additional domain-specific features.,Future research should focus on increasing the robustness of the text-mining pipeline (TMP) against sparse textual data and enriching learning objects' (LOs) contextualization with additional domain-specific features. Addressing multilingualism and repetitive content in knowledge graph-based recommender systems is also recommended.,Systematic literature review; conceptual framework; survey; quantitative and qualitative evaluation; comparison between hierarchical data model and knowledge graph (KG); expert validation; use of network metrics; evaluation of relation extraction; no mention of randomization
ACKG-LLM integrates multiple large language models (LLMs) and adaptive prompt generation,improving precision,recall
Knowledge Graph Normalization (KGN) Stage: Constructs and normalizes the knowledge graph based on schema constraints using extracted and enhanced triples.,,ACKG-LLM outperforms GenIE on REBEL and Wiki-NRE datasets
On the REBEL dataset (exact method): precision +1.04%,recall +1.03%,F1 score +1.01%.
On the REBEL dataset (partial method): precision +1.39%,recall +1.32%,F1 score +1.36%.
Qwen2-7B outperforms SynthIE by 1.8% (precision),1.31% (recall),1.59% (F1).
Mistral-7B-v0.3 outperforms SynthIE by 9.75% (precision),9.91% (recall),9.78% (F1).
Further research needed on hybrid strategies,efficient algorithms,and adaptive prompt tuning.
Further research is recommended on efficient algorithms and low-resource knowledge graph construction.,Integration of multimodal models (e.g.,Clip
Exploration of hybrid strategies combining small and large models to address high memory usage and improve generalization.,Suggestions for future research include: integrating models like Clip and Chatgpt-4o to improve generality; exploring fusion of latent semantic information; developing more efficient search and matching algorithms; studying hybrid strategies with small and large models; and enhancing low-resource knowledge graph construction.,
ToKEi effectively handles hierarchical time granularities,but error accumulation occurs at finer layers. Performance is strong on YAGO11K and Wikidata12K,but lower on ICEWS14 due to its time-point structure
Temporal scoping and node prediction: The study evaluates temporal scoping and temporal node prediction as part of its experimental setup.,The research is reproducible. The source code for the ToKEi project is available at https://gitlab.com/jleblay/tokei. The implementation uses Python 3 and PyTorch,and builds upon an existing project at https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding.
Statistical significance (p-values) is not reported in the context.,ToKEi achieved HITS@1 of .218 (.405),HITS@3 of .355 (.618)
For time validity prediction (Table 2),ToKEi achieved ROC scores up to .924,BA up to .822
ToKEi generally performed best on valid time model temporal KGs (YAGO11K,Wikidata12K) and lagged behind on ICEWS14,especially in HITS@10 and mean rank.
In re-ranking error (Table 6),ToKEi had PRE of .316 (.095),NRE of .286 (.622)
No explicit discussion of other limitations or generalizability.,The model achieves high performance on YAGO11K and Wikidata12K,especially at coarser time granularities
Recommendation: Address challenges in fine-grained temporal prediction and improve discrimination between true and false predictions,especially for datasets like ICEWS14.,The need to formally redefine ranking metrics for temporal knowledge graph (KG) embeddings
Difficulty in predicting fine-grained time granularities,especially for datasets like ICEWS14,where recall is low despite high precision.
Challenges in discriminating true positives and negatives across margin-based thresholds in temporal scoping tasks.,,
The model uses an embedding dimension of 200,relation-aware GCN layers (1 for YAGO,2 for others)
Performance drops with more neighbors or relations. RE-GCN can predict even without subject entity history,likely due to static graph and shared initial representations. The time gate recurrent component prevents over-smoothing and vanishing gradients in deep GCNs.,
Temporal reasoning under the extrapolation setting is applied.,The research is reproducible. The source code for the project is available at https://github.com/Lee-zix/RE-GCN.,RE-GCN outperforms all baselines on six benchmarks for temporal knowledge graph reasoning
On ICE18: MRR = 27.51,H@1 = 17.82,H@3 = 31.17
On ICE18: MRR = 30.55,H@1 = 20.00,H@3 = 34.73
Ablation studies confirm each component of RE-GCN contributes to performance gains.,Focus mainly on entity and relation of a query,neglecting structural dependencies among all facts in the knowledge graph at each timestamp.
Poor performance on datasets (e.g.,GDELT) with many abstract concepts,making temporal reasoning difficult and introducing noise.
Static information missing in some datasets (WIKI,YAGO,GDELT)
Modeling historical information is vital for most datasets.,Limited exploration of effective and efficient reasoning methods over Temporal Knowledge Graphs (TKGs),especially under the extrapolation setting.
Need for better integration of static entity properties (like entity types) into temporal reasoning models.,,
Systematic literature review: Applied exclusion,inclusion,and retention criteria to select relevant articles from major databases.
"The most frequent keywords were """"""""ontology"""""""" (14 occurrences","total link strength 33) and """"""""machine learning"""""""" (10 occurrences",total link strength 22).
Clusters represent domains such as product lifecycle management,engineering design,root cause analysis
"Top keywords include """"""""Ontology"""""""" (14 occurrences",link strength 33),"Machine learning"""""""" (10"
Initial search yielded 10,800 publications; after filtering,267 articles remained; final selection included 59 publications.
Exclusion of certain publication formats (books,technical reports,dissertations
Timeframe limited to publications from 2000 to 2024 (up to 29/02/2024).,Knowledge Graphs (KGs) play a critical role in addressing complex Smart Manufacturing (SM) challenges,enhancing resilience
Key challenges include integrating multi-modal data,incorporating KGs into existing systems,and developing suitable methods.
Recommendations highlight focusing on essential research topics and methodologies to make KGs functional in SM.,Addressing challenges in knowledge graph (KG) construction,integration
Developing innovative methods to improve the performance,scalability,and robustness of KGs in SM.
Systematic Literature Review (SLR) methodology,including defined research protocol,screening
Use of quantitative evaluation and quality criteria (citation rate,methodology relevance,logical findings) to assess included studies.
18% of methods were encryption-based,29% perturbation-based,16% blockchain-based
Quality was measured using three criteria: citation rate (QC1),methodology contribution (QC2),and clarity of findings (QC3)
No specific statistical values or measured effects are provided in the context.,Encryption techniques (HE,SMPC) increase computational load
Blockchain methods introduce scalability,computational,and communication overheads.
Construct validity may be affected by search string design.,Encryption techniques like HE (Homomorphic Encryption) and SMPC (Secure Multi-Party Computation) are widely used in FL (Federated Learning) for data confidentiality,especially in healthcare and finance.
Ongoing research should focus on balancing privacy,utility,and system efficiency in real-world FL applications.
Development of adaptive and hybrid privacy-preservation methods that balance privacy,utility,and system overhead.
Creation of benchmarks reflecting real-world trade-offs between privacy,utility,and computational/communication overhead in FL implementations.
Microservice architecture: A system design approach using independent,modular services,aligned with cloud-based practices.
The system includes a central knowledge graph,semantic data integration and exploration mechanisms,and a microservice architecture.
Further development is needed for authentication management,data security,and improved interaction workflows.
The research is ongoing,with technologies still under development.,The study demonstrates a system that integrates BIM
Future work should focus on authentication,data security,and improving user interaction workflows.
Further development is needed in authentication management,data security,and improving interaction workflows with the system.
Integration of BIM with IoT systems and other data silos,regardless of information type,requires additional research.
Multi-modal KG-based method: Integrated text,image,social graph
Comparative evaluation: Benchmarked against CNN,LSTM+Attention,SDM
Detection performance improved as data noise decreased; higher proportions of anti-real posts or few posts reduced model effectiveness. No explicit p-values reported.,Removing the top-x key properties (x=1,3
Few measurements of indicator contributions related to personal information.,The KG-based (knowledge graph-based) method achieves over 93% accuracy and F1-measure in suicidal ideation detection,outperforming other methods.
Key indicators are posts,personality,and experience
Relying solely on social media data is limited; additional data sources are recommended.,Limited incorporation of important personal factors (e.g.,parenting rearing style) due to resource constraints; reliance on social media alone is insufficient.
Need for methods to assess and filter data quality before model training.,Suggestions for future research include exploring additional data sources beyond social media to better capture personal factors like parental rearing style,addressing data noise and user identification challenges
Causal feature selection using a knowledge graph combining structured knowledge from the biomedical literature and ontologies: a use case studying depression as a risk factor for Alzheimer's disease,Malec Scott Alexander,Taneja Sanya B
Ontology-grounded knowledge graphs (KGs) were constructed and used for causal feature selection and mechanistic explanation.,The research is reproducible regarding data,as all supporting data are publicly available via the project's Zenodo repository (https://doi.org/10.5281/zenodo.6785307) with no restrictions. There is no information provided about the availability of source code for the project.
The KG search successfully identified confounders,including genes,enzymes
Possible bias from confounder misclassification,omitted variables,or overadjustment.
Future work should improve KG search and standardize reporting for better reproducibility.,The search depth threshold may have limited recall,missing some relevant confounders
Future work is needed to enhance knowledge graph (KG) search for causal feature selection.,Future research should enhance knowledge graph (KG) search for causal feature selection,improve terminology mapping
Knowledge-Based Version Incompatibility Detection for Deep Learning,Zhao Zhongkai,Kou Bonan
Evaluation of knowledge consolidation strategies (majority vote,weighted majority vote,vote by loss) for building a knowledge graph from extracted relations.
Compatible relation example: Python 3.8 and Tensorflow 2.2,confidence weight: 0.67.,Only 10 DL projects were used in the benchmark
Evaluation focused on projects with at least one version issue.,The proposed approach enables fine-grained extraction of version compatibility knowledge from Stack Overflow posts using a pre-trained QA model,achieving reasonable accuracy without finetuning.
Knowledge consolidation strategy has minimal impact on overall extraction pipeline accuracy; question template design is more influential.,Existing techniques cannot detect version issues involving drivers,OS
There is a need for improved knowledge extraction from free-form Q\&A posts to capture comprehensive and up-to-date version issues.,Future research should: (1) extend knowledge extraction beyond Stack Overflow to sources like PyPI and GitHub,redesigning consolidation to consider credibility and recency; (2) develop automatic repair strategies using the knowledge graph; (3) improve extraction accuracy by fine-tuning models or using stronger language models like ChatGPT.
Application of a constrained Louvain community detection algorithm to identify microservice candidates from the knowledge graph.,The research used two open-source projects: “E-commerce System” (by Macrozheng et al.) and “Cargo Tracking System” (available on SourceForge). No data was used for the research. Supplementary material is available online at doi:10.1016/j.infsof.2022.106992. Source code links are not provided.,The proposed GD method achieved the highest cohesion (0.60)
Performance tests (TP50,TP80,TP99
GD method demonstrated weak reliance on human experience and well-defined software,with applicability confirmed for both tested projects; statistical significance (p-values) not reported.,The proposed method yields the lowest coupling value among four strategies
Experiments show improved performance in team size reduction,cohesion,coupling
No data was used for the research described in the article.,The proposed knowledge-graph-based method (GD) achieves the highest cohesion (0.60) and lowest coupling among four extraction methods,indicating better modularity and flexibility.
Future work should optimize resource node extraction,especially for complex cloud scenarios.,Introduce more entity types
Further research and optimize the extraction method of resource nodes,considering more complex,multi-level parameters.
Personal Health Knowledge Graphs for Patients,Rastogi Nidhi,Zaki Mohammed J.
Aggregation and integration of heterogeneous data sources (e.g.,environmental sensors,web-based data) into personal health knowledge graphs (PHKG).
Construction of personal knowledge bases using text-based life logs from social media platforms.,No source code for the project is mentioned in the context. The research discusses methods and challenges for constructing personal health knowledge graphs but does not provide reproducibility details or code availability.,PHKGs (Personal Health Knowledge Graphs) are small
Major challenges include integrating heterogeneous data sources,defining relevant entities,ensuring scalability
Further research needed to define scope and validate PHKG effectiveness.,PHKGs (Personal Health Knowledge Graphs) offer personalized health recommendations but face challenges in scalability,dynamic updating
No standard model exists for PHKG representation; further research is needed.,Lack of standard models for representing and scaling PHKGs,including graph structure and dynamic creation.
Challenges in integrating heterogeneous,personal,and daily life data while ensuring validation and scalability.
Need for collaborative,hybrid approaches between patients and healthcare professionals to define PHKG content and privacy.,Future research should address: dynamic creation and updating of PHKGs
Contextual Query Suggestion w/ K𝐴: Incorporates related articles previously read by the user for more personalized suggestions.,,The K-LaMP framework significantly outperforms all baselines on Relatedness
No explicit p-values or quantitative statistical significance values are provided.,The K-LaMP framework significantly outperforms all baselines across Relatedness,Usefulness
Query Suggestion: Validity 1.769,Relatedness 0.962,Usefulness 0.948
Contextual Query Suggestion: Validity 1.966,Relatedness 1.267,Usefulness 1.245
Contextual Query Suggestion w/ K𝐴: Validity 1.822,Relatedness 1.192,Usefulness 1.166
Human evaluation may not capture all user goals.,The K-LaMP framework consistently and significantly outperforms all baselines in Relatedness,Usefulness
Recommendation: Use entity-centric knowledge stores and K-LaMP for improved contextual query suggestions.,No information available,
Evaluation on Twitter dataset: Methods are tested on a benchmark dataset of tweets to assess effectiveness.,The research uses the Moral Foundation Twitter Corpus and proposes two approaches (zero-shot and heuristic) that do not require training. Source code is referenced at https://github.com/StenDoipanni/MoralDilemmas and http://wit.istc.cnr.it/stlab-tools/fred/demo/. No further reproducibility details are provided.,The proposed unsupervised
No p-values or statistical significance measures are reported.,The frame-based approach was evaluated on 6,075 items from the MFTC test set.
Overall F1 scores: Random (.11),Zero-shot (.40),Emotion-Zero-shot (.42)
Subversion label had the highest F1 for Frame-based (.65).,Only Haidt’s Moral Foundation Theory is considered,limiting generalizability to other moral frameworks.
Further research is needed to better detect prevailing moral values and handle sentence complexity.,The study introduces two versatile,transparent
Develop methods to assign greater weight to significant sentence aspects to more accurately detect prevailing moral values.,Future research should focus on improving Zero-shot model performance to better understand complex moral values,developing methods that emphasize key sentence aspects
Connecting the Dots of Knowledge in Agile Software Development,Ouriques Raquel,Gorschek Tony
Systematic literature review: Examined knowledge management strategies and processes in agile software development.,No information available,The studies found that knowledge-based resources (KBRs) are essential for adapting to change in agile contexts
Lack of structured processes for capturing and storing knowledge leads to information overload,inefficient searches,and frustration among co-workers.
Primary outcomes: Identified challenges in producing and using PBRs (Property-Based Resources),including content overload,poor documentation
Measured effects: Lack of structured processes leads to confusion,inefficient searches,and frustration among co-workers.
Poor and isolated documentation can overload tools and confuse users.,Structured processes for creating and maintaining knowledge artifacts (PBRs and boundary artefacts) are essential to avoid information overload,confusion
Effective management of knowledge resources (KBRs) supports agile teams in adapting to market and internal changes.,Lack of structured processes for adding and maintaining content in artefacts,leading to content overload or insufficient knowledge.
Challenges in connecting knowledge sources to people’s needs,especially with varying artefact formats and terminology differences.,Future research should investigate how to effectively identify and address co-workers’ knowledge needs
Privacy-Preserving Synthetically Augmented Knowledge Graphs with Semantic Utility,Bellomarini Luigi,Catalano Costanza
KLONE and KGUARD algorithms: Two anonymization algorithms applying (k,x)-isomorphism while optimizing semantic utility.,The research is implemented in Python using NetworkX. Experiments were run on an AMD EPYC-7763v system with 64GB RAM. There is no explicit mention of the availability of the source code for the project.
No information available regarding public access to the source code.,KLONE and KGUARD achieve perfect δ-anonymity (1.0) across all tested graphs,outperforming k-Iso
No p-values or explicit statistical significance values are reported.,Both KLONE and KGUARD achieve perfect utility U (close to 0) on all real-world graphs,indicating no loss of information.
KGUARD generally outperforms KLONE in utility,fidelity,and privacy protection.
Few anonymization solutions exist specifically for KGs,especially regarding sequential publishing and node attributes.,The proposed (k
Recommendation: Use KGUARD for better efficiency and lower overhead in real-world applications.,Existing anonymization methods for knowledge graphs (KGs) often neglect derived knowledge,leading to privacy leaks.
Few anonymization solutions specifically address sequential publishing and node attribute privacy in KGs.,,
Grad-CAM was employed to explain prediction results and identify important gene pathways.,The research is reproducible. The source code is available at https://github.com/Y-Claw/Multilevel-GNN. All data are from public databases and can be accessed via public databases or the GitHub project. No further reproducibility limitations are mentioned in the context.,The proposed method achieved superior AUC values across all datasets (e.g.
Key genes and pathways identified showed statistically significant associations with survival (e.g.,MAPK signaling pathway log-rank test p-values: 1.606e-11,2.204e-07).
Survival analysis showed significant differences between groups (log-rank test values provided,e.g.,4.573e-05 for GBM
Comparative evaluation with other dimensionality reduction methods suggests further optimization and exploration are necessary.,,
Multi-channel Graph Convolution: Graph Neural Networks (GNNs) model both topological (county adjacency) and feature space relationships to improve prediction accuracy.,,The proposed TFSF-GNN model achieved the lowest errors (2015-2017 mean MAE: 2.906
Statistical values: fitting curve R²=0.80,Correlation=0.90 for TFSF-GNN predictions vs. ground truth.,Assumes independent and identically distributed (I.I.D.) samples
Simultaneous use of temporal and spatial (topological and feature) information,with multi-channel graph convolution and attention,is effective.
Improving the fusion of spatial and feature information for more accurate and generalizable yield predictions.,In future work,the feature graph will be made dynamic during training to better learn adaptive relations among nodes. Additionally
The efficacy of the method depends on the quality of the knowledge retrieval strategy; poor retrieval reduces answer accuracy. Evaluation was limited to four LLMs and two tasks due to computational constraints. MR-MKG outperforms LLaVA in most categories with fewer parameters. Adding multimodal knowledge graphs and alignment steps consistently improves performance.,,The research goal is to enhance large language models' multimodal reasoning using multimodal knowledge graphs (MMKGs); the approach
Knowledge Graph Embedding (KGE) with RGAT: Uses Relational Graph Attention Networks to encode graph structures and represent multimodal knowledge.,,The proposed MR-MKG approach achieves new state-of-the-art results on ScienceQA (up to 93.63% accuracy) and MARS (up to 41.0% Hits@1)
Incorporating multimodal knowledge graphs and cross-modal alignment provides significant performance improvements,especially on samples requiring visual reasoning (up to 3.78% and 1.41% gains,respectively); no p-values are reported.
Ablation study: Adding KG,MMKG,and cross-modal alignment improves accuracy by up to 6.70% on ScienceQA and 10.8% Hits@1 on MARS.
Using MMKG and alignment yields greater improvements on samples requiring visual knowledge (1.41% and 0.54% increases,respectively).,The effectiveness of the retrieved sub-multimodal knowledge graph depends on the success of the knowledge retrieval strategy; ineffective retrieval can lead to missing relevant knowledge and reduce LLM accuracy.
Evaluation was limited to four LLMs and two multimodal reasoning tasks due to computational constraints; larger models and broader tasks remain unexplored.,MR-MKG significantly enhances multimodal reasoning in LLMs by leveraging multimodal knowledge graphs,achieving new state-of-the-art results on ScienceQA and MARS tasks.
Tailoring retrieval strategies to the specific nature of each problem rather than relying on a single modality.,Future research should focus on improving the knowledge retrieval scheme to ensure more accurate and relevant knowledge for multimodal reasoning tasks. Additionally,scaling the method to larger language models and evaluating performance on a wider range of multimodal reasoning tasks is recommended.
Maintenance of personal health knowledge: Updating and synchronizing knowledge between personal and external graphs,addressing inconsistencies and privacy concerns.,
The primary findings highlight that Personal Health Knowledge Graphs (PHKGs) offer potential for personalized,knowledge-driven healthcare but face challenges in collection,linkage
The paper concludes that significant research is needed to develop methodologies for structuring,collecting,linking
Potential inconsistencies when updating linked knowledge.,Significant research and implementation challenges remain for Personal Health Knowledge Graphs (PHKGs),especially in collection
Key issues include structuring personal health data,ensuring privacy,and developing effective linking and update mechanisms.
Addressing these challenges is essential for leveraging PHKGs in personalized,knowledge-driven healthcare decision-making.,Developing infrastructure and methodologies for collecting
Addressing maintenance challenges,including update triggers,conflict resolution
PriPL-Tree uses a piecewise linear function to model data,improving range query accuracy under local differential privacy (LDP). It applies frequency and slope refinements to resolve inconsistencies,and adaptive grids for multi-dimensional queries. Experiments show up to 81.9% MSE reduction
Theoretical and Experimental Time Complexity Analysis: Comparison of construction and query time complexities across methods using hierarchical tree structures.,The research is reproducible. The source code,data
No explicit p-values or statistical significance measures are provided in the context.,PriPL-Tree reduces Mean Squared Error (MSE) by 10.6%–81.9% (average 56.7%) on Gaussian,MixGaussian
MSE decreases with increasing query dimension in the IPUMS dataset,unlike in highly correlated Gaussian datasets.,Existing methods assume uniform data distribution and uniform domain decomposition
Results may not generalize to datasets with different attribute correlations.,PriPL-Tree significantly reduces mean squared error (MSE) by 10.6%–81.9% (average 56.7%) on most datasets,but only by 14.9% on highly peaked (leptokurtic) distributions.
Recommendation: Use PriPL-Tree for diverse data distributions,but expect limited gains on highly leptokurtic datasets.,Need for automatic and data-aware machine learning models to further enhance estimation in LDP scenarios.
Addressing both non-uniform error and LDP noise error in arbitrary data distributions.,For future work,the study suggests exploring automatic and data-aware machine learning models to further enhance estimation in Local Differential Privacy (LDP) scenarios.
Manual annotation of posts into four suicide risk levels and 17 suicide trigger categories.,The research uses datasets derived from Reddit posts,with the original dataset including contextual information and detailed annotations. The competition dataset is more limited
No p-values or statistical significance measures were reported.,Primary outcomes measured: Classification of suicide risk categories (Indicator,Ideation
Low patient participation and ethical concerns limit feasibility of more representative datasets.,The fine-tuned decoder-only model (GPT-4o) achieved the best performance,with a wF1 score of 75.5
Simple methods and model ensembling,without advanced techniques,were highly effective
There is a need to develop datasets based on clinically confirmed suicidal tendencies,ensuring higher reliability and objectivity.,Future research should focus on developing datasets based on clinically confirmed suicide risk
BUILD-KG: Integrating Heterogeneous Data Into Analytics-Enabling Knowledge Graphs,Schatz Kara,Hou Pei-Yu
Data preprocessing steps: Obtain raw data and semantic descriptions,generate sample semantic sentences,validate/correct with scientists
Molecular dynamics simulations measured binding rate,binding strength,speed of binding
Recommendations include validating semantic sentences with domain experts and formatting data to meet specific requirements before conversion.,Lack of automated,domain-agnostic workflows for integrating heterogeneous data into unified knowledge graphs (KGs).
Need for effective involvement of domain experts as humans-in-the-loop to ensure accurate and useful KG construction.,,
The AI assistant system using a knowledge graph improved teaching outcomes,with a 35% increase in learning efficiency,21% higher exam pass rates
Intelligent matching algorithm utilizing the knowledge graph for precise teaching resource recommendations.,,The AI assistant system improved learning efficiency by 35%
The study suggests further research on multi-modal fusion for improved performance.,Multi-modal educational knowledge graphs significantly improve personalized learning efficiency and exam scores,reducing learning time by 23%.
Further optimizing personalized learning path recommendations and intelligent resource matching for improved educational outcomes.,"Future research should focus on improving the handling of the """"""""uncanny valley"""""""" phenomenon in AI assistant design",enhancing response clarity and detail
Employed personalized features,including customer embeddings and purchase history,for model training.
Conducted ablation studies to assess the impact of removing ranking,personalized features,product embedding
Ablation studies show removing ranking,personalized features,or product embedding significantly reduces performance; joint embedding of customer and product yields the best results.
Removing joint embedding: +28.1% Dev Acc@1,+20.4% Test Acc@1.,Removal of personalized features and product embeddings degrades model performance.
Errors often occur due to differences in product size,uninformative or similar titles,brand
Future work needed for better integration of personalized features and extension to cross-lingual/cross-media settings.,The proposed framework significantly improves product purchase prediction,with +32.9% (Dev) and +24.6% (Test) gains over QUARTS.
Future work includes better integration of personalized features and expansion to cross-lingual,cross-media settings.,Incorporating more informative features such as average rating
Modeling the interactions among purchase behaviors rather than using a “flat” attention-based method.,Future research should focus on integrating more informative features (like average rating,customer reviews
Experiments use PyTorch on NVIDIA A40 and AMD EPYC 7513. TANGO,TComplEx,and ComplEx are compared for TKG forecasting on ICEWS21
Data efficiency analysis by training models on varying sizes (10%,25%,50%
Fleiss’ kappa for human annotation agreement was 0.63,indicating substantial agreement; no p-values were reported.,The entity prediction model outperforms all baseline methods.
ForecastTKGQA achieves the highest results: MRR 0.339 (1-Hop: 0.216,2-Hop: 0.248),Hits@1 0.129
TKG forecasting models (BERT ext,RoBERTa ext) outperform TKGC models (BERT int,RoBERTa int).
CronKGQA and TempoQR perform poorly,suggesting TKGC representations may add noise.,Some FRQs contain erroneous examples because answers are solely determined by xERTE
No other explicit limitations or shortcomings are mentioned.,The proposed forecasting TKGQA tasks and datasets are valid and meaningful,supported by both theoretical and empirical justifications.
ForecastTKGQA outperforms all baseline models,with human performance still higher (ForecastTKGQA: 0.870 YUQ,0.769 FRQ; Human: 0.936
Recommendation: Further improve fact reasoning in QA models using insights from FRQs.,There is a large gap between model and human performance,indicating significant room for improvement in fact reasoning.
Improving QA models’ reasoning skills,especially for Fact Reasoning Questions (FRQs),is a key future direction.
Further research is needed to enhance forecasting power and answerability in TKGQA methods.,,
KGLiDS: A Platform for Semantic Abstraction,Linking,and Automation of Data Science
KGLiDS uses machine learning and knowledge graphs to abstract data science artifacts and their relationships. It combines static code analysis and documentation analysis for accurate semantic extraction,enabling efficient data profiling,discovery
Enriching static analysis with library documentation allows detection of implicit parameters and return types. The system’s library graph reveals usage patterns across data science libraries. KGLiDS supports both predefined APIs and ad-hoc queries,enabling flexible exploration and automation. Future work includes integrating large language models for more advanced use cases.,
Machine Learning-based Data Profiling: Uses machine learning to analyze and profile data items (datasets,tables,columns) for constructing a knowledge graph.
The platform enables enhanced data discovery,cleaning,transformation
KGLiDS enables new use cases in data discovery,exploration,reuse
Existing systems do not holistically interlink dataset semantics with pipeline scripts; KGLiDS aims to address this gap.,Future research should focus on incorporating large language models (LLMs) into KGLiDS for additional use cases,such as exploratory data analysis
Building Contextual Knowledge Graphs for Personalized Learning Recommendations Using Text Mining and Semantic Graph Completion,Abu-Rasheed Hasan,Dornhöfer Mareike
Use of network metrics: Metrics such as average degree centrality,clustering coefficient,weakly connected components
Text mining pipeline (TMP): A text mining pipeline was used for semantic similarity calculation of learning object titles and descriptions.,,The knowledge graph (KG) achieved an average betweenness centrality (BC) of 15.1
No explicit p-values or statistical significance are reported in the context.,The KG has an average betweenness centrality (BC) value of 15.1,about 10 times higher than the original hierarchical model.
Key metrics showed increased connectivity,higher degree centrality,and reduced weakly connected components.
Future research should focus on transforming hierarchical structures into contextualizing KGs for improved personalized learning.,,
Developed and used a Likert-scale questionnaire to measure user perceptions of Understandability,Satisfaction,and Soundness.
Compared user selections before and after explanations to indirectly assess Trust and Usefulness.,,The proposed approach using knowledge graph (KG) paths improved the performance of DITTO and HIERGAT models
No p-values or statistical significance measures are reported.,Primary outcomes were evaluated across six aspects: trust,understandability
EXKG-D achieved Precision: 81.66 (+7.04 over DITTO),Recall: 92.45 (-1.88),F1: 86.72 (+3.39)
EXKG-H achieved Precision: 68.52 (+10.83 over HIERGAT),Recall: 69.81 (-15.1),F1: 69.16 (+0.46)
Explanations generated achieved high user satisfaction (nearly 90%),soundness (88.2%),and usefulness/trustworthiness (60%).
Recommendation: Tailor model configurations when integrating paths from KGs,considering model architecture and path characteristics.,Addressing explainability
Handling incorrect predictions caused by overlapping keywords or recurring tokens in heterogeneous data needs further investigation and suitable solutions.,Suggested future research directions include addressing the challenge of balancing feature granularity (token vs. attribute level) in explanations,adapting model configurations for different architectures when integrating knowledge graph paths
The modular Digital Twin architecture uses real-time IoT data for traffic prediction,simulation,and visualization. Early stopping prevents overfitting. Increasing data sampling frequency improves prediction accuracy but raises bandwidth use
The architecture’s modularity enables flexible integration and scalability for smart city applications,supporting secure,real-time decision-making and resource optimization.
Data preprocessing and normalization: Incoming traffic data is preprocessed and normalized,then classified into traffic intensity categories before storage and analysis.,
No statistical significance (p-values) reported.,Prediction errors (MAE,MSE
1-hour sampling frequency yields lowest errors: MAE 0.154,MSE 0.049,RMSE 0.221.
2-hour frequency errors: MAE 0.226,MSE 0.084,RMSE 0.290.
Errors double at 3-hour frequency: MAE 0.471,MSE 0.393,RMSE 0.627.
1-hour and 2-hour frequencies have similar,low error rates; performance degrades significantly at lower frequencies.,Existing DT architectures are too generalized or specific to industrial applications
Lack of modularity in current architectures hinders reusability,scalability,and adaptability.
Literature gap: absence of standard modular architectures for smart city DTs.,The modular Digital Twin architecture enables flexible,scalable smart city solutions with real-time data acquisition
The Event Management Subsystem dynamically adjusts sampling frequency to optimize performance and resource consumption.,Lack of modular,standardized software architectures for smart city Digital Twins (DTs)
Insufficient support for DT functions like resource provisioning and software alignment in current architectures.,Future research should address the lack of standardized,modular software architectures for smart city Digital Twins (DTs)
The UrbanKG system outperforms the traditional NeuMF-RS solution in the site selection task for both Beijing and Shanghai,with higher N@10,H@10
UrbanKG not only improves performance but also offers more interpretable results and supports diverse urban scenarios,validating its broad applicability.,
Comparative Evaluation: Performance of UrbanKG is compared against the traditional NeuMF-RS method using metrics such as NDCG,hit ratio,and precision.
Data Fusion and Knowledge Distillation: UrbanKG fuses multi-source urban data and distills task-specific knowledge to enhance application performance.,,UrbanKG outperforms NeuMF-RS in the site selection task for both Beijing (N@10: 0.219 vs. 0.178
UrbanKG: N@10 = 0.205,H@10 = 0.671,P@10 = 0.177
UrbanKG demonstrates wide applicability and effectiveness for various urban computing research tasks.,Data fusion of urban data with different structures (tables,sequences
There is a lack of full-featured and user-friendly platforms for researchers and developers in urban computing.,,
Collaboration-oriented methods: Involve varying degrees of human participation,from full control to minimal involvement,in processes like data cleaning and labeling.
A benchmark survey identified 36 data-centric AI benchmarks (23 with open-source code),using a rigorous filtering process based on relevance,citations
Objective assessments use metrics like accuracy,timeliness,consistency
Subjective assessments use trustworthiness,understandability,and accessibility
No specific statistical values or measured effects are provided.,Cleaning and transforming data is challenging due to diverse dataset characteristics,requiring significant time and effort.
Other aspects like data access control and system maintenance add further challenges.,Data visualization leverages human visual processing to aid understanding,but selecting the best format (radial vs. linear charts) is controversial and often requires human input.
Data reduction improves model accuracy,efficiency,and interpretability by focusing on essential information and reducing complexity.
Automation is significant,but human participation remains essential in tasks like visualization selection and data cleaning.,Limited research on generating distribution-shifted data to expose model weaknesses
Research on graph data as a modality is still emerging and less studied than tabular or image data.,Future research should explore cross-task automation beyond training data development,develop unified frameworks for automating diverse data-centric AI tasks
Enabling knowledge discovery in natural hazard engineering datasets on DesignSafe,Mehta Chahak,kumar K.
The implementation uses a hybrid human-assisted and automated approach to extract and summarize metadata from scientific datasets,storing summary statistics and relationships in a knowledge graph. Large Language Models help process unstructured data. This enables complex,data-driven queries and advances scientific discovery by revealing hidden relationships.
Combining domain expertise with automation improves metadata accuracy. The knowledge graph supports advanced queries beyond traditional search,uncovering scientific phenomena otherwise buried in raw data. Integrating LLMs shows promise for automating metadata extraction from unstructured files.,
LLM-based metadata extraction: Applied Large Language Models (e.g.,GPT-4) to extract metadata from unstructured XLSX files.,All code written for the project is open source and can be found at https://github.com/chahak13/tuitus.
Used summary statistics (mean,median,count
LLMs are probabilistic and may produce inconsistent outputs without structured prompts.,Developed a hybrid,domain-specific data synthesis and metadata extraction model for natural hazard engineering datasets.
Extracting data from heterogeneous and unstructured datasets remains challenging due to diverse types and file formats.,Future research should focus on building extensive relationships across complex experimental,field
The pipeline first interpolates missing facts,then extrapolates to predict future events. TPAR,a neural-symbolic model
Dataset Splitting: Divided datasets into training,validation,and test sets
The improvements of TPAR over baselines are statistically significant,but no explicit p-values are provided in the context.,Primary outcomes are reported for link prediction tasks (Hits@1
Example interpolation results (ICEWS14): Hits@1: 57.03%,Hits@3: 69.74%,Hits@10: 80.41%
Example extrapolation results (ICEWS14): Hits@1: 36.88%,Hits@3: 52.28%,Hits@10: 65.89%
Statistical values are reported precisely in tables for all metrics and settings.,No information available,TPAR consistently outperforms all baselines in both interpolation and extrapolation link prediction tasks across four datasets.
Recommendation: Use TPAR for robust and accurate temporal knowledge graph reasoning.,Lack of unified methods: Most existing approaches address either interpolation or extrapolation,not both simultaneously.
Symbolic reasoning limitations: Symbolic methods struggle with ambiguous and noisy data,highlighting the need for more robust approaches.,Future research should focus on developing unified methods that can handle both interpolation and extrapolation reasoning
Meta-path Retrieval: Uncovers complex,multi-hop relationships between entities by traversing pre-constructed node sequences in the knowledge graph.,
Meta-path retrieval enables efficient exploration of complex,multi-hop relationships,reducing query latency and uncovering intricate connections.
No explicit quantitative results or p-values are provided in the context.,LLM-PKG outperforms other methods in accuracy,coherence
LLM-PKG provides detailed,relevant answers,clearly identifying specific biotechnologies and their impacts.
Performance metrics (selected examples from Table 1): LLM-PKG achieves 61.4,48.9,23.1
Retrieval-Augmented Generation (RAG) is limited when information is scattered,databases are large with low information density,or redundancy is high.
Vector databases lack mechanisms to ensure diversity and comprehensive fact retrieval.,The LLM-PKG approach outperforms other retrieval methods in accuracy,coherence
Integrating Regular Expression,Vector,and Meta-path Retrieval creates a more effective and sophisticated retrieval system.
LLM-PKG provides detailed,up-to-date,and well-structured insights on emerging biotechnologies in agriculture.
Recommendation: Combine multiple retrieval methods for optimal answer quality and reasoning.,Adapting PKG to support multi-turn conversational interactions for more dynamic,context-aware dialogues.
Enhancing the accuracy,coherence,and comprehensiveness of answer generation in complex reasoning tasks.
CPKT consistently outperforms six baseline models across four real-world datasets,especially on longer student sequences. Personalization and the TA-SSE (transition-aware stochastic shared embeddings) components each improve performance,with their combination yielding the largest gains. TA-SSE also helps prevent overfitting in personalized
Use of real-world datasets (MORF,ASSIST2015,EdNet
Statistical significance testing (p-value < 0.05) to assess performance improvements.,The research is reproducible. The source code for CPKT is available at https://tinyurl.com/mr9c9h5c. Four real-world datasets (MORF,ASSIST2015
CPKT achieves higher AUC and lower BCE,especially for users with longer learning trajectories; for long trajectories,p-values are 1.51e-05 (AUC) and 1.70e-19 (BCE) vs. DKVMN.
On MORF (RMSE): CPKT 0.1752 ± 0.0081\* (best),DKT 0.1990 ± 0.0087,DKVMN 0.1995 ± 0.0067.
On ASSIST2015 (AUC): CPKT 0.7274 ± 0.0032\*,DKT 0.7142 ± 0.0029,IEKT 0.7204 ± 0.0027.
On EdNet (AUC): CPKT 0.6558 ± 0.0072\*,DKT 0.6349 ± 0.0048,DKVMN 0.6291 ± 0.0070.
On Junyi (AUC): CPKT 0.8802 ± 0.0072\*,DKT 0.8709 ± 0.0072,IEKT 0.8721 ± 0.0026.
Long group AUC: CPKT 0.6475,DKT 0.6413,DKVMN 0.6315 (p=0.0745 vs DKT
Long group BCE: CPKT 0.6389,DKT 0.6562,DKVMN 0.6942 (p=4.71e-05 vs DKT
BCE: Binary Cross Entropy,a measure of prediction error for binary outcomes (lower is better).,Maximum sequence lengths for ASSIST2015
No further explicit limitations or suggestions for future research are provided.,CPKT significantly and consistently outperforms all baseline models across four real-world datasets,especially for students with longer learning trajectories.
TA-SSE helps prevent overfitting and can be applied to other sequential models.,,
Bring Privacy To The Table: Interactive Negotiation for Privacy Settings of Shared Sensing Devices,Zhou Haozhe,Goel Mayank
Simulated negotiation scenarios: Participants engaged in hypothetical smart home privacy negotiations,with social contexts (close contacts vs. acquaintances) assigned to groups.,
ThingPoll showed high usability and low workload: physical workload (mean = 24.6),frustration (mean = 26.7),temporal demand (mean = 31.2)
All users felt informed about data collection,and most agreed that ThingPoll effectively considered both their own and others’ privacy; no p-values or statistical significance were reported.,The Negotiation approach achieved the highest overall satisfaction rate at 83.3% (homeowners: 70%
ThingPoll showed high usability and low workload: physical workload (mean = 24.6),frustration (mean = 26.7),temporal demand (mean = 31.2)
Users valued the ability to share needs and concerns,contributing to high satisfaction.,Used simulated scenarios due to difficulty recruiting real-world participants with compatible devices and privacy disclosure.
General user education on device privacy features may be needed.,The negotiation approach achieved the highest satisfaction (83.3%),with both homeowners (70%) and guests (90%) reporting positive experiences.
Key design recommendations include supporting balanced participation,minimizing unnecessary explanations,and improving preference tracking.
Ensuring equitable opportunities for all users to express preferences during negotiations is still a challenge.,Future research should explore enabling anonymity for users,asynchronous negotiation mechanisms
Discerning Individual Preferences for Identifying and Flagging Misinformation on Social Media,Barman Dipto,Koidl Kevin
Use of self-report questionnaires to measure accuracy perception and trust,including attention checks and exclusion criteria for data quality.,The research used a within-subject design with 384 American participants
Numerical/statistical values are referenced in Figures 4,5,and 6
Study the impact of personalized misinformation flagging approaches versus “one size fits all” solutions.,Future research should address limitations of self-reported data,explore effects of misinformation flags on less emotional topics
Comparison with baseline methods,including DPSGD and its variants (DPSGD-IS,DPSGD-HF
Implementation of member inference attacks to evaluate the privacy-preserving effect of DPSUR.,,DPSUR consistently outperforms all competitors across all datasets and privacy budgets
DPSUR’s classification accuracy is at least 1% higher than the second best on FMNIST,CIFAR-10,and IMDb.
Moderate noise in SGD can help neural networks escape local minima,explaining DPSUR’s superior performance.,
Exploring optimal threshold selection to maximize utility gain from model updates.,,
No quantitative results or statistical significance (p-values) are reported in the provided context.,The proposed PHKG (Personal Health Knowledge Graph) enables integration and management of diverse personal health data (PHD) from sources like EHRs,wearable sensors
PHKG offers a unified,comprehensive knowledge graph,addressing challenges in data interoperability
Future research is needed to refine knowledge graph embeddings,improve natural language-driven query mechanisms,and explore additional use cases and applications.
Further development of knowledge graph embeddings and natural language-driven query mechanisms,and exploration of additional healthcare applications.,Future research should focus on refining knowledge graph embeddings
The prototype PRKG was built using Neo4j,populated with facts about the researcher using SpERT for entity and relation extraction from scholarly papers. SpERT was trained on the SciERC dataset. New insights: Extracting entities from private sources (emails,conversations) is less explored; privacy is a key concern.
Manual curation: Researcher manually curates entities and relations in the knowledge graph.,,The paper introduces the concept of a personal research knowledge graph (PRKG) for researchers
No quantitative results,statistical significance,or p-values are reported in the context.
Extracted facts included research interests,managed labs,tasks
Example extracted facts: (‘Sunita’,‘interest’,‘NLP’)
Extracting certain knowledge entities (like tasks,methods,equipment
Limited support for fine-grained access control in existing graph databases.,Personal research knowledge graphs (PRKGs) provide structured,machine-actionable knowledge about a researcher
Security and privacy must be prioritized when implementing,deploying,and sharing PRKGs.
Extraction of scholarly entities and relations from private sources like emails,conversations,or social media posts remains less explored.
Medium- or long-term studies: Lasting weeks to years,with fewer participants,allowing natural observation of effects over time.
Inquiries: Participants perform thought experiments or hypothetical scenarios and provide feedback,enabling large-scale,low-cost data collection.
No statistical significance (p-values) or quantitative results beyond participant counts were reported.,Short-term studies (20–50 participants,30–90 min exposure): Significant effects found (p < 0.001)
Multi-month user study (7 participants,up to 5 months): 249 days,46
All participants agreed cSpaces was easy to understand,use,and remember; features aligned with user needs and mental models.
Publishing full data is restricted by privacy,copyright,and non-disclosure issues.
Anonymization/obfuscation methods risk de-anonymization attacks.,A multi-lane evaluation strategy,combining short-term and long-term studies
Participants found cSpaces easy to understand,use,and more aligned with their mental models than traditional systems.
Need for improved evaluation strategies and enriched datasets to better reflect real-world Personal Information Management (PIM) scenarios.,Future research should focus on creating publicly available datasets that include user contexts and all relevant content,addressing privacy and copyright issues. Further investigation is needed into privacy-preserving data collection
Medium-/long-term studies: Several weeks to years,fewer participants,natural effects observed
No mention of randomization,blinding,or placebo controls.
iSummary uses query logs to create personalized knowledge graph summaries,focusing on maximizing coverage—how much of user queries are answered. The algorithm is scalable (linear in query log size),outperforms baselines in both quality and efficiency
Use of semantic summarization to extract minimized,useful information from large RDF knowledge graphs.,The research is reproducible. The source code and guidelines for downloading datasets and workloads are available online at https://anonymous.4open.science/r/iSummary-47F2/. The implementation uses Java
No statistical significance (p-values) is reported in the provided context.,iSummary outperforms all baselines in coverage,achieving almost double the coverage of GLIMPSE
iSummary is only 0.13 times slower than Random,14 times faster than GLIMPSE,and 40 times faster than PPR in execution time on DBpedia.
Diversity in λ/κ-Personalized summaries is not currently incorporated; summaries may lack variety.,iSummary outperforms all baselines in coverage and is significantly faster (14x faster than GLIMPSE,40x faster than PPR
Future work should explore alternative node-linking methods,study summary evolution over time,and introduce diversity in personalized summaries.
Study how personalized summaries change over time as user interests drift,considering events,disasters
Introduce diversity in λ/κ-Personalized summaries so users are not always presented with the same summary.,Suggested future research directions include: exploring alternative methods for linking κ nodes using the original data graph,studying how personalized summaries change over time with user input
Analyzed clinical features (diagnoses,procedures,prescriptions) using code frequencies and embeddings as node features in a similarity graph.
GT’s higher recall is crucial for detecting minority,HF-positive cases,while RF showed similar AUPRC (0.5132)
All models benefited from class-imbalance-adapted loss functions; statistical significance (p-values) was not reported in the context.,The combined use of medication,diagnosis
Fixed threshold for metrics is a limitation; optimizing AUROC suggested.,The GT with FL model outperformed baseline algorithms in predicting HF,achieving the highest F1 score (0.5531)
Limitations include single-center data,reliance on ICD codes,and fixed metric thresholds.
Future work should use multi-center data,alternative cohort identification,and optimize metrics across thresholds.
Future research should investigate alternative graph representations,inductive learning,and integration of additional multimodal patient data.
Ontology-Based Integration: Utilizes the ALIGNED suite of ontologies for tool integration,unified governance,and provenance tracking across software and data engineering.
Requirements Analysis: Conducts thorough analysis of software and data engineering needs to develop ontological models.,The Seshat project provides an open repository of expert-curated historical data,with provenance information recorded for traceability. However
The ALIGNED ontology suite enables integrated governance,supporting complex requirements and interoperability in large-scale,data-intensive systems.
No statistical significance (p-values) are reported in the provided context.,The ALIGNED suite of ontologies was evaluated on four large-scale,data-intensive systems engineering use cases.
Ontologies were verified for logical correctness using DL reasoners for satisfiability,incoherency,and inconsistencies.
External ontologies (e.g.,PROV-O,SKOS) were extensively reused.
Ontologies were made publicly available,well-documented,and licensed under Creative Commons Attribution License.
Sustainability was ensured via public Github repositories and long-term maintenance by ontology engineers.,No generic,domain-independent design intent capture model is available as a design pattern.
dcat vocabulary cannot further describe dataset distributions.,The ALIGNED suite of ontologies supports productivity and agility in data and software engineering by providing semantic models for design,processes
Further empirical evaluation of the ontologies is recommended.,Lack of ontologies for integrating and aligning software and data engineering tasks,processes
Insufficient granularity in metadata models to semantically describe complex datasets in a rich way.,,
Patterns for Representing Knowledge Graphs to Communicate Situational Knowledge of Service Robots,Zhang Shengchen,Wang Zixuan
Data analysis used visual grounded theory,including steps like positional coding,abstraction
Researchers iteratively coded and reviewed diagrams to identify and formalize low-level and high-level patterns.,The research is highly reproducible. Canvases created by participants,the NVivo source file for coding
Challenges included confusing or repetitive information; recommendations include user-friendly language and avoiding loops in knowledge graphs. No p-values reported.,Participants gave high ratings for interface clarity (questions 13-16,mean = 1.58
The interface was considered useful (question 1,mean = 1.90,sd = .88).
Some found information confusing (question 12,mean = .80,sd = 1.81).
Some had difficulty finding information (question 11,mean = .80,sd = 1.23).
Helpfulness in understanding the robot rated lower (question 7,mean = .70,sd = 1.77).
Multimodal interaction (voice,gesture,gaze) is recommended for future interfaces to improve situational knowledge exchange.
Developing user-friendly ontologies and multimodal interaction (e.g.,voice,gesture) to improve non-expert understanding and communication.
Crowdsourcing campaign with provenance metadata collection,profiling user contributions,and gathering opinions on controversial statements.
Basic statistical analysis of knowledge graph changes,classifying triples as ‘Added’,‘Removed’
The paper proposes metrics: Evolutionary Synchronisation (ES),Change Alignment,and Evolutionary Dependency (ED) to measure and analyze ontology evolution.
Future work includes further experimental evaluation,refining metrics,and iterative bias control methods.
Providing explanations for detected evolution phenomena to understand underlying causes.,Future research should investigate food data evolution,health perceptions
Random experiment: random changes (addition,deletion,update) are applied to knowledge graphs
No mention of randomization,blinding,control
KGT personalizes language models by optimizing an external knowledge graph (KG) instead of model parameters,improving efficiency and interpretability. KGT maintains high performance as the query set grows,unlike baselines. Limitation: KGT depends on the model’s ability to follow instructions during personalization.
KGT’s scalability supports long-term,user-specific knowledge accumulation,making it suitable for ongoing personalization needs.
Empirical Evaluation: Conducts experiments on datasets (e.g.,CounterFact) to compare KGT with baseline methods in terms of efficacy,paraphrase rate
KGT is more efficient,using less memory and time than baselines; no p-values are reported.,KGT significantly outperforms baselines in efficacy and paraphrase scores on CounterFact and CounterFactExtend datasets.
Further research is suggested to address this dependency.,KGT significantly outperforms baselines in personalization efficacy and paraphrase rates,with improvements up to 61% and 46% respectively.
KGT achieves the lowest latency (0.15s) and GPU memory usage (15,904MB),reducing memory cost by up to 77%.
Further research is needed to enhance efficiency and interpretability in model personalization using knowledge graphs.,Future research should address the limitation that KGT depends on the LLM’s ability to follow instructions when calculating probabilities and collecting feedback. Further investigation is needed to improve model personalization efficiency and interpretability,and to explore KGT’s scalability for extensive personalized knowledge.
Model thinking techniques: Compared zero-shot,few-shot,and chain of thought (CoT) methods to assess their impact on model performance.
Evaluation metrics: Used precision,recall,and F1-score to measure model effectiveness.
The proposed model thinking technique (chain of thought) achieved 95.48% precision (statistically significant improvement over zero-shot at 71.55%).,Primary outcomes were measured using precision,recall
Qwen-turbo: Precision 83.95%,Recall 84.47%,F1 84.21%
ChatGLM2-6B: Precision 51.22%,Recall 75.90%,F1 61.17%
ChatGLM3-6B: Precision 59.26%,Recall 80.00%,F1 68.09%
ChatGLM2-6B\* (fine-tuned): Precision 71.62%,Recall 81.54%,F1 76.27%
ChatGLM3-6B\* (fine-tuned): Precision 83.75%,Recall 83.75%,F1 83.75%
Proposed method (GLM4): Precision 95.48%,,The proposed LLM-based method effectively constructs educational knowledge graphs with high precision
Addressing challenges in converting fragmented knowledge into structured knowledge points for automated EduKG construction.,Future research should focus on extending applications to large-scale datasets while maintaining high accuracy,and further exploring personalized educational practices through local deployment of individual models. Addressing limitations in data quality
FedPerGNN uses decentralized user data and privacy-preserving techniques for GNN-based personalization. Key strategies include pseudo interacted item sampling,encrypted item IDs,and local differential privacy (LDP) to protect user data. Increasing pseudo items and noise improves privacy. No raw user-item data is collected.
FedPerGNN achieves K/M index privacy,and privacy can be tuned by adjusting pseudo item count and noise parameters. Collaboration between servers could risk privacy,but model update methods still protect private ratings.
Communication Cost Analysis: Assessment of model communication cost per client under different privacy and expansion settings.,The research is reproducible: all datasets used are publicly available,and the source code is provided at https://github.com/wuch15/FedPerGNN45. Experiments and implementation details are described in the Methodology section and Supplementary Information for reproducibility.
Performance differences are statistically significant,with error bars representing mean results and 95% confidence intervals (n = 5),but specific p-values are not provided.
Download bandwidth requirements are higher than upload,but acceptable for real-world use.,
Longitudinal-based models: Use temporal dependencies in a patient's medical history,often leveraging Electronic Health Records (EHRs),for personalized and safer recommendations.
Use of Electronic Health Records (EHRs): Incorporate comprehensive historical medical data for improved recommendation accuracy.,,KGDNet achieved the highest PRAUC (0.7657 ± 0.0015)
No explicit p-values or statistical significance values are reported in the context.,Primary outcomes were measured using DDI Rate,PRAUC
KGDNet outperformed all baselines in PRAUC,F1 Score,and Jaccard.
Ablation studies showed removing modules (knowledge graph,DDI graph,fusion
KGDNet outperforms existing models on the MIMIC-IV EHR dataset across PRAUC (0.7657),Jaccard (0.6765),F1 (0.5218)
The study recommends adopting knowledge graph-driven frameworks like KGDNet for safer,more effective medicine recommendations.,Limited research on integrating semantic
Insufficient exploration of hierarchical and temporal modeling of patient admission histories for medication recommendation.,,
Performance evaluation: Used F-score as the main metric,with data split into training,validation
System achieves comparable results across users,regardless of tweet volume.,No explicit limitations
Enhancing end-to-end performance in the pipelined workflow for life event extraction from tweets.,,The study design is an observational study using annotated tweets. It involves binary classification
Multimodal Reasoning with Multimodal Knowledge Graph,Lee Junlin,Wang Yequan
Knowledge graph embedding (KGE): Entities and relations are embedded and transformed to enhance reasoning,with architectures like RGAT evaluated.,
Using multimodal knowledge graphs and cross-modal alignment led to statistically significant improvements,with up to 3.78% gain in targeted ScienceQA samples and 6.6% in MARS.,MR-MKG (FLAN-T5-11B) achieves 92.78% average accuracy on ScienceQA
Ablation studies show adding KG,MMKG,and cross-modal alignment to Visual\_FLAN-T5-11B improves accuracy by 3.78%
Ambiguities in the knowledge base can result in retrieving unrelated or confusing information.,MR-MKG significantly enhances multimodal reasoning in LLMs by leveraging multimodal knowledge graphs,achieving new state-of-the-art results in ScienceQA and MARS tasks.
Tailoring retrieval strategies to specific problem characteristics,rather than relying on a single modality,is important.
Thematic classification and thorough review of 74 selected articles to extract key findings,insights,and implications.
Key themes included IoT,AI,cloud computing
DHTs enable highly personalized therapies by capturing unique genetic,metabolic,and environmental characteristics.
DHTs can improve disease modeling,risk stratification,rapid diagnosis
Ensuring data accuracy,consistency,and interoperability is challenging.
Datasets often have racial,gender,and demographic biases.
Key challenges include ensuring data quality,addressing bias,and maintaining security
Recommendations emphasize data harmonization,privacy protection,and standardized protocols for effective DHT implementation.
Create secure data-handling protocols to safeguard sensitive health information and support ethical use of Digital Human Twins (DHTs).,Future research should focus on creating robust data governance frameworks,ensuring AI transparency and fairness
Data is semantically integrated from diverse sources,published as Linked Data (LD) and FAIR data,and enables OLAP-style analysis.
The resulting BDAKG knowledge graph supports federated analytics and user-friendly exploration.,,The paper's main objective is to enable sustainable
Knowledge graphs and Semantic Web techniques: Applied for semantic integration,representation,and exploration of agriculture data using Linked Data and RDF.
Data mining and machine learning algorithms: Utilized to extract patterns and insights,such as forecasting crop yields.,The research provides reproducibility through publicly available data dumps at http://bike-csecu.com/datasets/agri/
BDAKG enables OLAP operations and answers queries in under one minute,compared to over two hours manually,demonstrating significant efficiency improvement.
No statistical significance (p-values) or quantitative experimental results beyond time comparisons are reported.,BDAKG has 9 levels,33 level attributes
Quality evaluated by completeness (schema,property,linkability)
Property completeness issue: missing scientific name for onion.,Most agriculture data sources are unsustainable,static
Datasets do not follow FAIR principles,making them hard to find,integrate
No research has addressed sustainable management and presentation of Bangladesh agricultural data.,BI tools are underutilized in agriculture,especially in Czech small farms
Most agricultural data are not FAIR-compliant,limiting discoverability,integration
The BDAKG knowledge graph enables semantic definition,integration,and analytical exploration of Bangladesh agricultural data.
Recommendation: Adopt Semantic Web and Linked Data principles for sustainable,interoperable agricultural data management.,Lack of sustainable management and presentation of Bangladesh agricultural data
Need to apply data mining techniques for forecasting and pattern extraction across broader sectors beyond agriculture.,Future research should address sustainable management and presentation of Bangladesh agricultural data,semantic heterogeneity
Further research is needed on efficient privacy techniques and bias mitigation.,,The research goal is to assess privacy preservation in federated learning (FL) under GDPR
Gradient descent variants: Using batch,stochastic,and mini-batch gradient descent methods to optimize machine learning models.
Privacy-preserving techniques: Implementing Secure Aggregation,Homomorphic Encryption,and Differential Privacy to protect data during federated learning.
Traditional anonymization techniques (like k-anonymity,l-diversity,t-closeness) are insufficient against linkage attacks; differential privacy offers stronger guarantees.
Privacy-preserving methods (Secure Aggregation,Homomorphic Encryption,Differential Privacy) mitigate some risks
Privacy-preserving techniques (e.g.,Secure Aggregation,Homomorphic Encryption
No explicit statistical values or quantitative results are provided in the context.,Data minimization is challenging in traditional ML,as it's hard to determine the minimal necessary data for training.
k-anonymity-based anonymization techniques cannot defend against linkage attacks if adversaries know sensitive attributes.,Federated Learning (FL) enhances privacy by keeping data on user devices and only sharing model parameters,but some privacy risks remain.
More research is needed on interpretable,unbiased ML models and balancing privacy,accuracy
Recommendations include thorough documentation,risk assessment,and implementing advanced privacy-preserving techniques.
In-depth research on transparency,interpretability,and algorithm fairness in federated learning (FL) systems
Thorough exploration of trade-offs between privacy utility,accuracy,interpretability
Use of process mining,data mining,and knowledge mining techniques to analyze and formalize information processes.
The system reconstructs event timelines (measurement,server receipt,“Red zone” generation) to identify process delays; only “Red zone” events were analyzed for processing speed.
The final event log included 219 patients (73 males,146 females),with event chains ranging from 5 to 1228 events.
"Time response analysis focused on """"""""Red zone"""""""" events",evaluating delays between patient measurement,server receipt
The study mainly focuses on arterial hypertension and chronic heart failure,limiting generalizability.,The intelligent integration approach enables reconstruction and evaluation of remote monitoring processes for patients with hypertension and heart failure.
Recommendation: Use integrated event logs to optimize monitoring and treatment processes.,,Future research should focus on developing effective methods and algorithms for processing unstructured and incomplete medical data
Reinforcement Learning Techniques: Methods like DeepPath and Minerva are used to infer complex queries in knowledge graph completion tasks.,The research is highly reproducible,as several open-source libraries for knowledge graph embedding (KGE) are available
Recent developments in reinforcement learning for KGC and the integration of real-world knowledge (numeric,text,images
No explicit quantitative results,primary findings,or statistical significance (p-values) are provided in the context.
Evaluation metrics used: Mean Rank (lower is better),Mean Reciprocal Rank (higher is better),and Hits@K (higher is better).
Only static knowledge graphs considered; dynamic,heterogeneous,and bipartite graphs not addressed.
Knowledge protection in cybersecurity KGs needs further research,especially for rapid updates and incident isolation.,The study provides a comprehensive overview of traditional and modern knowledge graph embedding (KGE) techniques for link prediction
Key research challenges identified include robustness,scalability,and knowledge transfer.
The study recommends further exploration of dynamic,heterogeneous,and bipartite knowledge graphs.
More research is needed on areas beyond link prediction,such as triple classification,entity classification
Open challenges include robustness,scalability,few-shot learning
Docs2KG: Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models,Sun Qiang,Luo Yuanyi
Semantic and structural proximity-based information retrieval using embedding models and similarity search within the KG.,The research uses open-source libraries such as markdownify (source code: https://github.com/matthewwithanm/python-markdownify),BeautifulSoup
No quantitative results or statistical significance (p-values) are reported.,Primary outcome: Integration of population data from 2011 to 2021 using Docs2KG,combining PDF and Excel sources.
The system reduces time,effort,and risk of outdated or hallucinated knowledge in retrieval tasks.
Achieving meaningful data representation with references to the original source for improved knowledge retrieval and reduced hallucination.,,
To extract multimodal data (tables,texts,images
The paper developed two privacy-preserving pipelines for AML detection using Fully Homomorphic Encryption (FHE): a GNN pipeline (using GIN with TFHE,optimized by quantization and pruning) and a graph-based XGBoost pipeline (using a Graph Feature Preprocessor). XGBoost achieved over 99% accuracy,F1-score
Implementation of graph-based XGBoost and Graph Neural Network (GNN) pipelines,incorporating graph feature preprocessing.,
On the imbalanced dataset,F1-score improved by 8% with graph-based features,but overall F1-score and recall were lower (F1: 0.3056–0.3867; Recall: 0.1897–0.2500).
FHE-encrypted inference incurred significant computational overhead (up to 88,822.54x slower),but increasing graph feature complexity reduced inference time; no p-values were reported.
FHE encryption caused inference time to increase by over 100,000 times compared to unencrypted inference.,Use of synthetic and modified datasets due to lack of access to real-world financial crime data.
XGBoost achieved over 99% accuracy,F1-score,precision
Investigating alternative privacy-preserving methods,such as differential privacy or multi-party computation,to enhance privacy-preserving capabilities.
Assessing scalability of solutions to accommodate larger datasets.,Future research should focus on improving FHE compatibility and model performance,exploring alternative privacy-preserving methods like differential privacy or multi-party computation
Implementation of Symbolic Semantic Parsing,FAQ Semantic Matching,and Neural Semantic Parsing for question answering.
No explicit p-values or quantitative accuracy metrics are provided.,The PKGQA system demonstrates accurate and efficient processing,providing fast and precise responses for both simple (milliseconds) and complex (2-3 seconds) queries.
All three modules (Symbolic Semantic Parsing,FAQ Semantic Matching,Neural Semantic Parsing) are important for optimal performance.
The user-friendly GUI enhances usability by clearly presenting structured knowledge and additional information.,,
Data cleaning and preprocessing,including outlier removal,normalization
Integration of optimization algorithms and Transformer networks within the CPDT framework for adaptive urban traffic management.,,The proposed model reduced average vehicle waiting time from 120 seconds (traditional method) to 75 seconds.
Reductions: fuel consumption by 30%,travel time by 25%,waiting time by 37.5%.
Superior prediction accuracy,lower Mean Absolute Error (MAE),and faster computational time compared to other models.
The approach improved traffic prediction reliability,managed flow,and decreased vehicle delays.
Recommendation: Adopt optimization and machine learning for urban traffic management.,Limited scalability of current AI-driven traffic management algorithms,making it difficult to expand their use in large urban areas.
Difficulty managing unexpected traffic surges during emergencies with decentralized adaptive systems.,Future research should address scalability issues,integration of diverse data sources
CNN-based methods: Apply convolutional neural networks (CNN) to extract features in temporal knowledge graph completion tasks.,No information available,The highest Hits@1 under the time-wise filtered setting is 61.2% (HyGNet)
Large language models (Llama,Vicuna) perform worse than most deep learning methods,with Hits@1 values of 38.6% and 39.2%
Large language models (Llama,Vicuna) have lower results than most deep learning-based methods (Hits@1: Llama-2-7b-CoH 38.6%,Vicuna-7b-CoH 39.2%).
Evaluation metrics: Mean Rank (MR),Mean Reciprocal Ranking (MRR),and Hits@N (percentage of test quadruples ranked ≤ N).
Large language models in TKGC are still exploratory and underperform compared to other methods,requiring further research and new modeling strategies.,Few-shot TKGC: Addressing the challenge of relations with few examples in temporal knowledge graphs remains in its early stages and needs further research.
Interpretability: Improving the interpretability of deep learning-based temporal knowledge graph completion methods is a promising research area.,Future research should focus on few-shot temporal knowledge graph completion (TKGC),developing unified methods for various types of knowledge graphs
Multimodal data fusion: Combining diverse data types (e.g.,text,sensor readings
The methodology enables integration of heterogeneous data sources,enhances data privacy,and demonstrates feasibility through a proof-of-concept implementation with synthetic data.
No quantitative results or statistical significance (p-values) are reported; future work will focus on rigorous quantitative evaluation and parameter sensitivity analysis.,Approximately 10,000 synthetic data points per domain were generated
Data included text and sensor modalities from smart healthcare,transportation,grid
Limited robustness to noisy,inconsistent,or erroneous real-world data.
Ethical implications and potential biases of LLMs and synthetic data remain unresolved.,The study presents a novel method using LLMs to build comprehensive,interconnected multi-modal knowledge graphs (MMKGs) for smart city (SC) cognitive digital twins (CDTs)
Recommendations include further quantitative evaluation,expanding data modalities,improving explainability
The approach aims to empower data-driven decisions and support sustainable,efficient,and citizen-centric urban environments.
Enhancing robustness to noisy,inconsistent,and erroneous real-world data in smart city contexts.
Developing optimal strategies for combining real-world and synthetic datasets while mitigating biases.,Future research should focus on parameter optimization and sensitivity analysis,robustness to noisy data
Audio preprocessing: Raw audio was processed using band-pass filtering,full-wave rectification,downsampling
The multivariate Occ-STPN model achieved the highest accuracy (93.09%) and lowest FMMSE (0.003375) using six predictors (Dishwasher,Entertainment,Fridge
FMMSE ranged from 0.003156 to 0.00397 for multivariate Occ-STPN.,Data collected only from six homes in Boulder,Colorado
Sensor hubs placed only in common areas,not in bedrooms or bathrooms,due to privacy concerns.
Heavy computation load for multimodal deep neural networks on embedded systems.,A multimodal sensor fusion framework for residential occupancy detection was proposed,unifying camera
Two Occ-STPN model variants with feature-level and decision-level fusion showed high prediction accuracy,scalability,and transferability.
Recommendation: Implement low-cost,low-power sensor systems integrated with HVAC and lighting controls for energy efficiency.,Implementation and deployment of the proposed occupancy detection system in real buildings
Integration of the occupancy detection system with existing HVAC and lighting control systems for smart,occupancy-informed operation.,Future research should focus on implementing and developing a practical occupancy detection system for energy-saving in buildings. This includes using low-powered
Evaluation using inter-annotator agreement measured by Cohen’s kappa,with scores of 0.80 (triplet extraction),0.78 (entity alignment)
Comparative analysis against state-of-the-art baselines (EXTRACTOR and LADDER),adapting outputs and formats for fair evaluation.,
Annotation quality was high,with an average Cohen’s kappa of 0.73,indicating substantial agreement.
Inter-annotator agreement (Cohen’s kappa): 0.80 (triplet extraction),0.78 (entity alignment),0.61 (relation prediction); average: 0.73.
CTINEXUS triplet extraction (GPT-4): F1-Score 87.65,Precision 93.69,Recall 82.34.
GPT-4 outperformed GPT-3.5 in entity grouping and relation prediction across all demonstration numbers.,Requires carefully chosen,high-quality demonstration examples with correct answers and prompt format.
Hallucination detection and mitigation are left for future work.,CTINEXUS outperforms existing methods (EXTRACTOR,LADDER) in cybersecurity triplet extraction
Integration of visual analytics into CTINEXUS to enhance interpretability and support timely cybersecurity decision-making.,Future research should explore downstream applications of CTINEXUS,such as integrating visual analytics to aid analysts in identifying behavior patterns and relationships. Additionally
Explore-Construct-Filter: An Automated Framework for Rich and Reliable API Knowledge Graphs,Sun Yanbang,Huang Qing
Baseline comparison: Evaluation against seven baseline methods,including MKC,APIRI
Annotation and agreement: Manual annotation of instance triples with inter-annotator agreement measured by Cohen’s Kappa (0.78,0.82).,The research provides detailed prompt designs
Cohen’s Kappa coefficients for annotation agreement are 0.78 and 0.82,indicating almost perfect agreement; statistical significance (p-values) is not reported.,The optimal threshold in the KG Filtering module is chosen from Case 3 to balance reliability and richness.
Cohen’s Kappa coefficients for annotation agreement: 0.78 and 0.82 (almost perfect agreement).,Manual annotation may introduce subjective bias,though mitigated by dual annotators and high kappa values (>0.75).
The full combination strategy generates more reliable type triples,enhancing comprehensiveness.,Adapting the method to different fields: The method’s adaptability may be limited by varying data structures and semantic characteristics across domains.
Integration with KG retrieval tools: Future work includes integrating with technologies like GraphRAG for comprehensive knowledge extraction,analysis,and utilization.
Enhancing model adaptability: Adjusting the method for specific data structures and semantic requirements in diverse fields.,Future research should focus on integrating the proposed automated KG construction method with KG retrieval tools like GraphRAG to develop a comprehensive knowledge extraction,analysis
Time-series analysis was performed using LSTM,GPT3,and BERT models to predict resource usage and detect anomalies.
Knowledge graphs were constructed to integrate,represent,and analyze operational data
BERT achieved 97.85% accuracy in predicting resource usage,excelling at complex,long-range patterns.
Successful fault prevention: 5 incidents.,,The study developed an automated monitoring method for enterprise microservices using a database knowledge graph
Recommendation: Employ knowledge graphs and automated scaling for efficient,resilient microservices operations.,Limited research on integrating real-time and historical data in knowledge graphs for proactive failure prediction and resource optimization in microservices.
Need for improved entity resolution and knowledge integration techniques to construct high-quality,reliable knowledge bases.,Future research should explore improving the integration of knowledge graphs with advanced NLP and LSTM techniques
Mining literature and pathway data to explore the relations of ketamine with neurotransmitters and gut microbiota using a knowledge-graph,Liu Ting,Feenstra K Anton
Automatic relation extraction: Applied BioKetBERT to automatically extract relationships between identified entities.,The research is reproducible. The datasets are available at the KetPath repository on GitHub,with a README file explaining integration steps. Users can download datasets
KetPath identifies novel pathway relations,including 16 literature-only connections,showing ketamine’s effects on neurotransmitters and indirect impact on BDNF.
No statistical significance (p-values) or quantitative results beyond entity/relation counts are reported.,Developed KetPath,a knowledge graph integrating ketamine pathway data from publications and databases.
Combined manual fact extraction,automatic named entity recognition (CI-er),and relation extraction (BioKetBERT).
Inferring new knowledge from the knowledge graph could be enhanced by advanced graph analysis techniques.,KetPath effectively integrates and retrieves ketamine pathway knowledge from literature and databases,supporting diverse biological queries.
The approach is feasible,biologically useful,and adaptable to other domains or drugs.
New algorithmic graph analysis techniques,such as novel weighting schemes,are needed to better infer knowledge from knowledge graphs.
Enabling building digital twin: Ontology-based information management framework for multi-source data integration,Xie X,Moretti N
Employing ontology-based processes to extract,transform,and integrate various data through a common data model and reference data libraries.
Challenges remain in aggregating disparate data due to trust,privacy,ownership
Effective data integration enables better discovery,fetching,and utilization of heterogeneous building data.
Developing strategies for integrating heterogeneous,multi-source data while maintaining autonomy and minimizing redundancy.,Future research should focus on evaluating the quality of integrated data to fully realize the benefits of data integration. There is also a need to address challenges in merging complementary ontologies
Knowledge graph construction and application in geosciences: A review,Ma Xiaogang,2022
3C guideline (Correct,Consistent,Complete): Practitioners verify that entities and relationships in the KG meet these criteria to determine when to stop use case analysis.
The study concludes that best practices,such as providing up-to-date data and complete documentation,support a robust data ecosystem.
Explanation: No quantitative data or p-values are available in the context.,The primary outcomes focus on best practices for publishing and using data on the Web,their benefits to the data ecosystem and FAIR data principles
Explanation: The context only lists best practices,benefits,and principles
Adopting best practices from FAIR data principles and Web standards enhances data interoperability,usability,and ecosystem benefits.
KG evolution and versioning: Addressing how KGs can adapt to new scientific discoveries and changing knowledge remains a significant challenge.,Future research should address KG entity disambiguation,quality measurement
Personal Health Knowledge Graphs for Patients,Rastogi Nidhi,Zaki Mohammed J.
Aggregation and integration of heterogeneous data sources (e.g.,environmental sensors,web-based data) into personal health knowledge graphs (PHKGs).
Construction of personal knowledge bases using text-based life logs from social media platforms.,No source code for the project is mentioned in the context. The research discusses methods and challenges for constructing personal health knowledge graphs but does not provide details or links to reproducible code or datasets.,PHKGs (Personal Health Knowledge Graphs) offer personalized health recommendations but face challenges like static summaries
No standard PHKG representation exists; models vary by use-case,and issues like device memory,data ingestion
No quantitative results,statistical significance,or p-values are reported.
The context is a literature review and discussion of challenges,not a report of experimental or quantitative results.,Brute force approaches do not address creating relationships
Further research needed on memory/computation requirements and hybrid approaches.,PHKGs (Personal Health Knowledge Graphs) offer personalized health recommendations but face challenges in scalability,entity updating
Further research is needed to address validation,scalability,and standardization of PHKGs.
Challenges in dynamically creating,updating,and scaling PHKGs
Need for effective validation mechanisms to ensure PHKGs accurately capture personal patient information and support personalized recommendations.,Future research should address: dynamic creation and updating of PHKGs,accurate class and relationship identification
The algorithm detects and qualifies changes in historical territories using cardinality-based rules for splits,merges,and redistributions. The HHT-SHACL FDD extension identifies flawed or incomplete data
Change detection algorithm: Implements SHACLRules and SPARQL to detect,qualify,and aggregate territorial changes in knowledge graphs.
HHT-SHACL FDD approach: Used to identify and resolve inconsistencies in data conversion and ensure knowledge graph consistency.,The research is reproducible. Source code,datasets
In the French Third Republic dataset,211 composite changes were detected,with 181 geometrically qualified; 241 feature changes occurred in 1926
The algorithm accurately aggregates composite changes and highlights data inconsistencies,but requires all building blocks to be described across the entire timespan; no p-values or statistical significance are reported.,The algorithm detected 241 feature changes in 1926
In the France: Region reform dataset,82 geometry changes were detected,all denoted as Incomplete by HHT-SHACL FDD.
All region fusions were detected and qualified as merges.,The algorithm relies on a time-exhaustive description of geometric building blocks,which may not always be available.
Further work is needed to enable more generic use via graph annotation mechanisms.,The HHT approach effectively represents historical territories and their evolution,supporting multiple hierarchies and intuitive understanding for historians.
Address limitations in the change detection algorithm,especially handling disappearing/appearing building blocks and inconsistencies in detected changes.,Future research should explore using time stamping properties instead of creating new objects to reduce knowledge graph size
PoC Design: A Methodology for Proof-of-Concept (PoC) Development on Internet of Things Connected Dynamic Environments,Prasanna K.,Ramana Kadiyala
Essence framework exercises: Stakeholder and opportunity exercises involving stakeholder engagement and assessment of project challenges,strategies,and values.
Results highlighted the importance of defining problems and solutions based on business value,calculating IoT initiative potential,involving stakeholders early
Difficulty in comparing results with other comparable results.,The PoC Design methodology is effective for developing IoT-Proof of Concepts (IoT-PoC),securing business benefits
There was difficulty comparing the PoC Design methodology results with other comparable results; future work should address this comparison.,Future research should examine the cultural perspective in PoC Design methodology,explore the impact of strict versus flexible time frames on projects
Analyzing the resulting knowledge graph to understand user workflows,tool usage,and insights.
VAKG enabled understanding of tool usage,identification of shortcomings,and comparison of user processes.
Feedback highlighted layout issues,T-SNE overlap,poor abstract reading experience
No specific statistical values were reported.,VAKG is a conceptual framework,so direct comparison with practical works is limited.
Future work is needed to explore integration with other ontologies and develop new provenance techniques.,VAKG offers a formalized,theoretically grounded process to model
Future work includes improving user-tracking and automating provenance techniques for VAKG.,Existing theoretical models are insufficient for modeling,storing
VAKG’s conceptual framework requires practical validation and extension to bridge VA theory and real-world application.,Future research should investigate user-tracking,behavior/knowledge provenance
The implementation focused on classifying primary software engineering (SE) research papers using defined inclusion/exclusion criteria,emphasizing validated empirical or non-empirical methods. Limitations include exclusion of meta-research,possible selection bias from only ICSE 2020 papers
Classification and data extraction were based on analyzing research objects,statement types,and evidence using a structured data extraction form.
Future work should extend the literature review to cover a longer time span and broader selection of conferences and journals.,Future research should extend the literature review to cover a longer time span and include a broader selection of venues,such as additional top-tier conferences and journals. There is also an open question about adapting the classification for meta-research and refining classification dimensions.
Embedding design: Techniques like item embedding,session embedding,and w-item2vec to represent items and sessions for better learning.
Attention mechanisms: Applied alone or combined with other DL methods to enhance model performance by focusing on important parts of the sequence.,The research is reproducible. The source codes and datasets used in the experiments are shared on Github: https://github.com/sttich/dl-recommendation. Datasets include RSC15,RSC19
Larger negative sample sizes notably improve model performance,especially from 0 to 32 samples,with diminishing returns as size increases further.
The optimal sampling strategy and loss function combination varies by dataset and evaluation metric,highlighting the need for validation-based tuning.,Increasing negative sample size improves model performance on all evaluation metrics; performance rises sharply from 0 to 32
Statistical significance: Δ for p-value ≤ 0.01,� for p-value ≤ 0.05,\* for p-value ≤ 0.1 (Table 6).
Incorporate side information,advanced loss functions (TOP1-max,BPR-max
Carefully balance model complexity,computational cost,and data augmentation strategies.
Need for benchmarking studies to enable fair and effective comparison among various models and techniques.,Future research should focus on rigorous and comprehensive evaluations across models,better embedding methods
Sustainable Development Goal indicator for measuring availability and affordability of medicines for children: a proof-of-concept study,Joosse Iris R,Mantel-Teeuwisse Aukje K
Introduction of the NUNT (novel parameter) to enable affordability calculations across different child age groups.,The research uses an adapted,child-specific methodology based on the WHO/HAI methodology. Individual facility data are not publicly available
No statistical values or measured effects were reported.,The methodology inherits limitations from the original tool,such as burden of disease weighting and use of the national poverty line for affordability.
The adapted tool was tested only with historical data,limiting relevance to current situations; prospective data collection is needed.,Future research should analyze larger
The paper implemented a knowledge graph in teaching by extracting 239 entities and 521 relationships using algorithms like LDA,TextRank,and TF-IDF. In experiments with two classes
Manual refinement of entities and attributes improved accuracy. Using SmartKG for storage and visualization facilitated practical application. The approach promoted teaching quality and student interest.,,The research goal is to improve teaching quality by constructing a computer network knowledge graph using data crawling
Experimental design with control and experimental groups to evaluate the impact of knowledge graph application in teaching.,,239 entities and 521 relationships were extracted and visualized using SmartKG; attributes were manually completed for each entity.
Three keyword extraction algorithms (LDA,TextRank,TF-IDF) were compared; top keywords and their weights are presented in Table 2.
The application of the knowledge graph improved students' academic performance and teaching quality.,Attributes of each entity were completed manually,which may introduce human error or bias.
No mention of large-scale or automated validation,which may affect result robustness.,The constructed knowledge graph improved students' academic performance and teaching quality compared to conventional methods.
Future work should focus on improving practical operation ability and integrating more diverse data sources for knowledge graph construction.,,The study design involved selecting two classes and assigning them as a control group and an experimental group. The experimental group used the constructed knowledge graph in teaching
GAE-Log integrates event graphs (modeling log sequences) and knowledge graphs (system info) for anomaly detection. Experiments show GAE-Log outperforms baselines,especially with both weighted event graphs and knowledge graphs. Removing the knowledge graph causes the largest performance drop,highlighting its importance. New insight: Combining both graphs is crucial for best results.
Graph-based methods: Construct graphs (e.g.,control flow graphs,time-weighted graphs
Machine learning-based methods: Apply algorithms such as support vector machine (SVM),linear regression (LR),decision tree (DT)
Statistical significance (p-values) is not reported in the provided context.,GAE-Log achieves the best F1-Scores across datasets: 0.948 (WordCount),0.952 (PageRank)
Baselines: LogRobust (0.901,WordCount),DT (0.947
Weighted event graphs further improve results across applications.,Log data are typically unlabeled,making supervised learning difficult.
Manually labeling log data is time-consuming,costly,and often impractical.
Dependencies among components can propagate failures,increasing detection difficulty.,GAE-Log consistently outperforms baseline algorithms in log anomaly detection across multiple datasets
The context focuses on experimental results,performance comparisons,and ablation studies.
The paper highlights that knowledge graphs significantly improve audit efficiency,accuracy,and depth in intelligent auditing. Key implementation insights include the need for more automated construction tools
Manual record analysis: Auditors manually inspect and analyze ledger records and financial statements.,,Knowledge graphs significantly improve audit efficiency
No quantitative results or statistical significance (p-values) are provided in the context.,Primary outcomes focus on evaluating knowledge graphs using six indicators: Accuracy,Consistency
Suggestions for future research on privacy protection and cloud optimization,Knowledge graphs greatly enhance audit efficiency,accuracy
Future research should focus on automating knowledge graph construction,improving knowledge representation and reasoning,integrating heterogeneous data
Research better integration and utilization of heterogeneous data from multiple sources,including cloud-stored data,to enrich knowledge graphs.
Lifelong machine-human alignment loop: Maintains alignment through ongoing,bidirectional interaction.,
Key findings highlight challenges in representing context generality,identity across contexts,and the need for computer vision (CV) and natural language processing (NLP) for effective AI-human alignment.
No explicit quantitative results or statistical significance (p-values) are provided in the context.,No explicit primary outcomes,results
The context describes research directions,challenges,and the use of datasets (e.g.
Complexity in label meaning across different contexts and over time.,Context representation must be general and preserve identity across contexts,enabling modeling of changing
Embedding machine learning fully in time and enabling bi-directional,cognitively cheap,and computationally affordable human-AI interaction for lifelong alignment.
Developing systematic methodologies and ethics-aware datasets for real-world,interdisciplinary experiments in human-machine symbiosis.,Future research should address: (1) developing general methods for representing context and identity across situations; (2) fully embedding machine learning in time and enabling bidirectional human-AI interaction; (3) providing AI with Computer Vision and Natural Language Processing; (4) designing systematic real-world experiments and ethics-aware datasets.
Three adaptive policies were compared: Fixed threshold policy,Demarcation policy,and Dynamic policy
Statistical data collection involved incrementing counters per update and using normal distribution approximations for performance.,,The Fixed threshold policy with T = 12 is the cheapest for uniformly distributed updates but costly for skewed updates; the Dynamic policy outperforms both Fixed threshold and Demarcation policies in cost.
Future work needed on better statistical methods,automatic optimizations,budget restrictions
Dividing data into three categories (A: strong,B: mixed,C: session consistency) and using adaptive policies significantly reduces operational costs and improves performance.
Future work should enhance statistical methods,consider energy and budget constraints,and explore relaxing other ACID properties.
Incorporate budget restrictions and explore relaxing other ACID principles,like durability,in the cost function.
Stimuli consisted of 25 videos of continuous actions,edited to create five sets with durations of 3,6
No specific quantitative results,primary findings,or p-values are explicitly stated in the context.
Sample: 2212 observations from 90 participants.,No information available,Memory for continuous events is segmented into discrete units
Recommendations include considering working memory constraints when studying episodic memory for extended events.,,Future studies should independently measure individual differences in working memory (WM) capacity and examine how these relate to the decrease in temporal ratios as event duration increases. This could clarify whether variations in WM forgetting rate explain individual differences in temporal compression.
Observational study with 90 participants and 2212 observations,,
Personal Knowledge Graphs,Balog Krisztian,Kenter Tom
Comparative analysis of PKGs and general knowledge graphs to identify unique challenges and opportunities.,,The paper defines personal knowledge graphs (PKGs)
Key challenges include lack of large open PKG datasets,complex implementation issues (privacy,access control)
No quantitative results or statistical significance (p-values) are reported.,No primary outcomes,results
Implementation challenges: integration with external services,access control,storage location
Key challenges include lack of large open datasets,implementation complexities (privacy,storage
Recommendation: Further research and coordinated efforts are needed to address open problems and advance PKGs as a distinct subfield.,Lack of large,open datasets for PKGs
Open challenges in implementing PKGs,including access control,storage location
Need for continuous integration of external knowledge sources with PKGs,potentially involving user intervention to resolve conflicts.,Future research should address the lack of large
Comparative Analysis: DGSR is compared with several baseline models,including BPR-MF,FPMC
DGSR-1 consistently outperforms DGSR-0 and most baselines,highlighting the effectiveness of the message propagation mechanism in modeling dynamic user preferences; no explicit p-values reported.,DGSR achieved the highest NDCG@10 and Hit@10 across all datasets:
Hyper-parameter tuning (layer number,sub-graph order,sequence length
GRU4Rec+ underperforms FPMC in Beauty and Games,possibly due to FPMC's focus on dynamic item transitions,which works better on sparse datasets.
Investigating the effects of different hyper-parameter settings (such as DGRN layer number,sub-graph sampling size,sequence length
Extending the DGSR framework to fuse various single-sequence models by modifying the message propagation mechanism.,,
Lifelong learning on evolving graphs under the constraints of imbalanced classes and new classes,Galke Lukas,Vagliano Iacopo
tdiffk measure: A method to determine history size for training by analyzing time differences in k-hop neighborhoods,ensuring comparability across datasets.,
Pre-trained models on static datasets (Cora,Citeseer,Pubmed) have higher accuracy and less variance than non-pre-trained models.
GAT learns fastest without pre-training; GCN lags on Cora-B,GAT lags on Pubmed.,Only a portion of labeled data is used for training in subsequent tasks
Out-of-distribution (OOD) detection is limited to crisp decision methods,not OOD scores,due to lack of validation data for threshold tuning.
gDOC with weighted loss is recommended for handling class imbalance.,Explore and adapt more out-of-distribution (OOD) approaches to graphs,such as using the IsoMax loss function.
Analyze why omitting old training data helps detect OOD examples and investigate alternative data removal strategies.,Suggested future research directions include: exploring and adapting more out-of-distribution (OOD) approaches to graphs (e.g.,using the IsoMax loss function)
Exploratory learning via form-based interface: Allowing users to browse concepts and ask questions,with the system inferring properties and valid actions.,
KB-PHaSE supports interactive simulation,fault recovery training,and exploratory learning using a reusable knowledge-base.
No quantitative results or statistical significance (p-values) are reported.,No explicit primary outcomes,results
The prototype system (KB-PHaSE) was a small demonstrator,not intended for operational use.,Knowledge patterns enable better modularization and reuse of general theories in knowledge-based systems.
Insufficient integration between knowledge patterns and existing formal specification methods in software engineering.,,No information available
Workflow/process mapping: The study mapped and compared different workflows,processes,and methodologies described in the reviewed articles.
Quantitative results: 620 process steps were initially extracted (519 unique),reduced to 414 unique steps after synonym adjustment (182 Level I,196 Level II
No statistical significance (p-values) reported; conclusions highlight the need for further empirical validation and broader evaluation of the proposed process.,Primary outcome: Identification and synthesis of main steps in knowledge graph development—(i) identify data,(ii) construct ontology
Results are based on systematic review and conceptual analysis.,Most reviewed articles lacked a solid framework,relying on project workflows.
Practical implementation of the proposed process not yet tested.,The study proposes an evidence-based,structured framework for knowledge graph development
The main steps are: identify data,construct ontology,extract knowledge
Examining how existing software development and ontology methodologies can be applied or compared to knowledge graph development.,Suggested future research directions include: studying additional industry cases; evaluating the proposed process with experts and organizations; comparing the process to existing methodologies; developing a knowledge graph using the proposed process; and researching tools and techniques for each development step. Limitations include lack of empirical validation and generalizability.,Study design characteristics: Systematic review
New insight: Combining memory with recent neighbor sampling reduces the number of neighbors needed for optimal performance.,,The research goal is to develop TGN
Neighbor sampling strategies: Test different neighbor sampling methods (last vs. uniform) and vary the number of sampled neighbors per layer.,The research is reproducible: all experiments and timings are described in detail,and the code will be made available for all experiments to be reproduced. No explicit source code link is provided in the context.
TGN-attn achieved state-of-the-art results in future edge prediction and dynamic node classification on Wikipedia,Reddit,and Twitter datasets.
TGN-attn achieved 98.46% (Wikipedia),98.70% (Reddit),and 94.52% (Twitter) average precision (transductive).
Evaluation on datasets with more event types is left for future work.,TGN achieves state-of-the-art results on multiple tasks and datasets,outperforming all baselines
TGN is recommended for applications in social sciences,recommender systems,and biological networks; future work should explore advanced model settings and domain-specific adaptations.
Investigating applications of TGN in social sciences,recommender systems,and biological interaction networks.
: A Unified Method to Access Heterogeneous Data Sources on the Web,Asprino Luigi,Daga Enrico
The paper introduces Facade-X,a unified method for accessing diverse data sources and constructing knowledge graphs using SPARQL Anything. The approach supports many file formats,uses triple-filtering to improve memory efficiency
Empirical evaluation through an online survey targeting Semantic Web practitioners and SPARQL users to assess usability and gather feedback.,The research is reproducible: the implementation,SPARQL Anything
Cognitive complexity analysis showed SPARQL Anything required fewer tokens per query than SPARQL Generate,RML,and ShExML; no p-values reported.
Cognitive complexity was measured by counting the number of tokens needed per query across frameworks.,Evaluation focused mainly on theoretical aspects,not domain-specific models.
Limited support for novel structural design patterns and specific data formats.,The proposed Facade-X approach enables robust,lossless transformation of heterogeneous data sources into RDF
Future work includes implementing connectors to relational and no-SQL databases like MongoDB.,Future research directions include: extending the meta-model to cover new structural design patterns (e.g.,AMR
Structure-Enhanced and Relation Fusion Units: Extract implicit temporal features and integrate them into relational patterns for deeper temporal understanding.,,HSTQA achieved the highest Hits@1 (0.401) and Hits@10 (0.744) on the MultiTQ dataset
The improvements are statistically significant,"with HSTQA excelling particularly on """"""""equal"""""""" and """"""""before/after"""""""" temporal question types due to its multi-granularity fusion and structure-enhanced modules.",HSTQA achieves Hits@1 of 0.401 and Hits@10 of 0.744 overall on the MultiTQ dataset.
CronKGQA improves over EmbedKGQA by 7.3% (Hits@1) and 14.9% (Hits@10).,Existing methods often rely on pre-defined rules or extensive subgraph processing,limiting adaptability to complex situations.
Accurately modeling and applying temporal constraints,especially in complex scenarios,remains challenging.
Most models assume a single temporal granularity,overlooking diverse real-world granularities (e.g.,daily
Fuzzy matching for entity identification can overlook semantic nuances,causing errors in reasoning.,HSTQA significantly outperforms baseline models in complex temporal reasoning for TKGQA
Recommendation: Use HSTQA for superior temporal question answering.,Most existing approaches overlook structural information in time-constrained questions,limiting complex temporal reasoning.
Reliance solely on relational representations can cause loss of important implicit temporal features.,,
Privacy-preserving mechanisms (differential privacy,homomorphic encryption,secure multi-party computation) protect model updates while maintaining model utility
The review does not address privacy breaches from hacking or theft.,Increasing client-side computation between communication rounds greatly reduces communication rounds needed for convergence,especially with specific mini-batch sizes and epochs.
Leveraging fog computing to reduce latency and distribute computational burdens in federated learning.,Future research should focus on improving how models learn over time in federated learning,especially as data distributions change. Methods like meta-learning
Experimental evaluation comparing performance,storage block size choices,and data page clustering methods (columnar vs PAX).
Trickle-feed optimization increased insert rate by 50%,reduced WAL syncs by 73%,and bytes written to WAL by 68%.
Columnar clustering achieved 15.8% higher overall QPH than PAX,with simple queries showing an 84.7% improvement. No p-values reported.,Columnar clustering QPH is 7x and 5x higher than PAX clustering for 690 GB and 138 GB caches
Further improvements are needed for scalability and adaptive clustering based on access patterns.,The new LSM tree-based storage architecture for Db2 Warehouse improves scalability,availability
Future work will focus on generalizing optimizations and improving scalability and adaptive clustering.,Generalizing optimizations for other database objects,such as indexes and row-organized tables.
Enhancing clustering to adapt dynamically to changing data access patterns over time.,Future research should focus on generalizing optimizations for other database objects like indexes and row-organized tables,improving scalability for efficient resource use in tiered storage
Sub-graph completion: Expands the knowledge graph by finding hidden associations between entities in a sub-graph.,,The Graphusion pipeline outperformed baselines on TutorQA Tasks 1-5
Expert evaluation on Task 6 showed Graphusion had the highest scores in Relevancy (4.85),Coverage (4.91),Convincity (4.72)
The study concludes that Graphusion’s core fusion step and well-defined seed concept generation are critical for superior performance,especially in complex QA tasks; statistical significance is not explicitly reported.,On Tasks 1-5
"Expert evaluation (Task 6): """"""""Ours"""""""" scored highest in Relevancy (4.85)",Coverage (4.91),Convincity (4.72)
GPT-4o generated more,but often less relevant,entities per response (Task 2: 11.04 vs. 2.84; Task 3: 11.54 vs. 2.87).
Using neighboring entities for improvement relies on training data,which is incompatible with zero-shot settings.,The Graphusion pipeline outperforms GPT-4o and GraphRAG across all tasks
Recommendation: Future work should include comprehensive evaluation on larger datasets,as current expert evaluation is limited by human effort.,Need for improved methods to aggregate and fuse knowledge from multiple sources for Knowledge Graph Construction (KGC).
Necessity for large-scale,high-quality,domain-specific corpora to enhance scientific KGC performance.
Privacy-Preserving Graph Machine Learning from Data to Computation: A Survey,Fu Dongqi,Bao Wenxuan
Differential Privacy Schema: Applies privacy-preserving mechanisms not tailored to specific attackers but for general scenarios.,,The paper reviews privacy-preserving techniques in graph machine learning
The paper highlights current limitations,discusses challenges like non-IIDness in federated learning,and proposes future research directions; no quantitative results or p-values are reported.
Switching-based Graph Generation: Uses iterative Monte Carlo switching of edges to anonymize the graph while preserving key features (e.g.,eigenvalues,eigenvectors
No explicit statistical values or quantitative results are provided.,Model heterogeneity: Assumes shared model architecture across clients,which may not be optimal due to different graph sizes and over-smoothing in GNNs.
Need to combine privacy-preserving data generation and computation introduces new challenges.,The study reviews privacy-preserving techniques for graph machine learning,focusing on both data protection and computation.
Current limitations are identified,and promising future research directions are proposed.,Combining privacy-preserving data generation and computation poses challenges
Avoiding cross-client transmission in subgraph-level federated learning without degrading model performance remains an open problem.,Suggested future research directions include: (1) optimizing the distribution of the privacy budget between data generation and computation to improve the privacy-utility trade-off,and (2) disentangling task-relevant from task-irrelevant information to better allocate privacy protection and enhance model performance.
PKG API: A Tool for Personal Knowledge Graph Management,Bernard Nolwenn,Kostric Ivica
Demonstrated the approach with an open-source demo focusing on understanding and representing user preferences.,The research is reproducible. The complete solution,including source code and a video demonstration
The system supports operations like ADD,GET,DELETE
No quantitative results,statistical significance,or p-values are reported in the provided context.
Compatibility issues between Pod providers and Solid apps lead to inconsistent user experiences.,Personal knowledge graphs (PKGs) can effectively organize and provide personal information,addressing the growing need for user-centric management tools.
The study introduces a robust internal data representation,an API,and a user-friendly PKG Client
The open-source demo demonstrates the feasibility of intuitive,user-centric PKGs and encourages further research into broader applications.,Lack of practical implementations of personal knowledge graphs (PKGs) that directly interface with users; most efforts remain conceptual.
Existing tools are too complex for non-expert users,highlighting the need for more intuitive,user-friendly interfaces.
Future research should explore intuitive user-centric interaction methods and broader applications for PKGs.,,
Case studies: Use of real-world scenarios to clarify problem settings and evaluate knowledge management system applications.,,The paper proposes a unified bidirectional knowledge management system (BKMS) using relational databases to log hierarchical and interconnected information
No quantitative results or statistical significance (p-values) are reported in the context provided.,No explicit primary outcomes,results
Traditional databases are often disjoint from logging systems,limiting concise,collated overviews.
Additional future research challenges exist for integrating relational databases and AI/symbolic techniques in knowledge management systems.,Automatic text summarization and topic modeling enhance knowledge management by generating concise,interpretable summaries and uncovering hidden topics.
Interconnected knowledge management systems,especially with IoT,foster open
Future research should address security,collaboration,integration
Enhancing system interoperability,including import/export with other knowledge management systems,and supporting collaborative use by multiple stakeholders
Database Search: Comprehensive searches in databases like Elsevier,ACM Digital Library,MDPI
The review provides a balanced assessment of KG construction methodologies,highlighting strengths,weaknesses
Results include an in-depth analysis of KG construction methodologies,their strengths,weaknesses
Measured effects involve evaluation criteria such as precision,recall,F1-score
No explicit statistical values are provided.,Lack of transparency in technique documentation,hindering replication and progress.
It offers an in-depth,balanced analysis of state-of-the-art KG construction methods,highlighting their strengths and weaknesses.
The study identifies current limitations and gaps,recommending further research to address these inadequacies.,Limited discussion and documentation of knowledge extraction techniques
Need for context-aware and transparent knowledge extraction methods tailored to diverse educational scenarios.,Future research should explore personalized learning paths using artificial intelligence,integration of large language models into knowledge graphs (KGs)
Deploying MATLAB analysis programs to compute and update CVSS scores via the Specified Model Interface.,The research demonstrates reproducibility by using a REST API endpoint and the Specified Model Interface,allowing access via multiple tools (MATLAB
Automated reasoning inferred 36,674 statements from 19,720 explicit ones (expansion ratio: 2.86); no p-values or statistical significance reported.
All three framework success criteria were met: data mapping,flexible tool-agnostic access,and semantic transformation.
Functional analysis of the three success criteria is still required to fully validate the DEFII framework.,The DEFII framework enables mapping engineering data to an ontology-aligned data store and supports flexible,tool-agnostic access.
Tool interoperability is promoted,allowing various tools (e.g.,MATLAB
Future research should address recursive analysis patterns and further simplify interface instantiation.,Determining how to account for recursive,roll-up analysis patterns (e.g.
Extending the DEFII framework to support additional interface instantiations and data formats beyond REST APIs,such as CSV files.,Future research should determine how to account for recursive analysis patterns (like roll-up patterns) in the interface specification. There is also a need to explore integrating multiple MISDs for broader Assessment Flow Diagrams and to investigate more complex applications
The study reveals a trade-off: enhancing privacy can reduce fairness,highlighting the challenge of optimizing both simultaneously in FL. The proposed private fair algorithms address this by integrating both constraints into the optimization process.,
Experimental comparison with benchmark FedAvg algorithms on the Adult dataset to assess performance.,,The proposed privacy-protection fairness Federated Learning (FL) method increases model accuracy but reduces fairness when privacy is added.
Experiments on the Adult dataset show a trade-off between privacy,fairness (Demographic Parity,Equalized Odds)
Client accuracy examples (with privacy): Black: 66.63%–73.75%,White: 85.70%–88.39%.,Adding privacy (e.g.
Trade-offs exist between privacy,fairness,and accuracy; improving one may harm the others.
The intrinsic connection and trade-offs between fairness,privacy,and utility in FL need further exploration.
Installation and integration of hardware and software (central server with Apache HTTP Server,Apache SOLR,MySQL database
Use of Fisher’s LSD (Least Significant Difference) statistical analysis to assess production waste rates before and after application use.,,Statistically significant reductions in batch cycle times
Estimated annual cost savings from automated MWP generation are €5,081.77 per machine.,Significant reduction in setup times: e.g.
Estimated annual cost savings: 5,081.77 €/year.,Lack of clear implementation guidelines and structured information in literature.
Implementation required strong collaborative effort and was not instantaneous.,The Service-oriented Digital Twin prototype significantly reduced setup times,cycle times
Improvements were statistically significant,confirming enhanced performance in time,cost
Recommendation: Promote human-centric,information-rich Industry 4.0 implementations.,Lack of clear implementation guidelines and structured information for transforming to a human-centric Smart Factory.
Limited evidence of real business model transformation in small and medium enterprises adopting Industry 4.0.,Future research should extend the intelligent vocal assistant's capabilities,investigate the statistical link between knowledge management and enterprise performance
The paper shows that fusing weak signals via graph convolution strengthens core and target interests,improving performance. Dynamic graph pooling with assignment regularization and weighted readout filters noise and compresses user interests. The framework is flexible for different prediction layers. New insight: even advanced sequential models face short-term memory limits,justifying the graph-based approach.
Interest Extraction via Graph Pooling: Graph pooling with assignment regularization and weighted readout filters noise and compresses user interests for improved recommendation performance.,The research is reproducible. The source code is implemented using the Microsoft Recommenders framework (https://github.com/microsoft/recommenders) based on TensorFlow (https://www.tensorflow.org). Hyper-parameter settings and dataset details are provided,supporting reproducibility. No specific project code repository is mentioned beyond the framework link.
Modeling on compressed sequences benefits all sequential models,especially with AUGRU,confirming the effectiveness and efficiency of the pooling strategy.
Interest extraction (with pooling,regularization,and readout) increases AUC from 0.8513 to 0.8906 and MRR from 0.3605 to 0.4228 (Taobao).
SURGE outperforms all baselines in AUC,GAUC,MRR
Existing methods focus more on recent behaviors,often ignoring long-term behaviors.,The SURGE model outperforms existing sequential recommendation methods on two real-world datasets.
Exploring different interest evolution layer designs for better adaptability and performance in sequential recommendation.,,Study design characteristics:
No mention of randomization,blinding,placebo
PRISMA-based scoping review: Applied for systematic selection,screening,and classification of relevant papers.
VOS viewer software: Utilized for network data mapping,visualizations,and exploring interconnections among key research elements.
A co-occurrence map of 189 keywords was created,prioritizing the top 109 most frequently used keywords,grouped into nine clusters.
Overlay visualization identified 34 key items (e.g.,digital triplet,deep learning
No statistical values reported.,Current industrial AI systems are in preliminary stages,limiting predictive abilities and understanding of digital twins.
Need for interdisciplinary collaboration and further research on human–machine integration and spatial cognition.,"The review highlights the emerging importance of the """"""""digital triplet"""""""" concept",especially its role in integrating digital twins
Key research hotspots include digital triplet,deep learning,artificial intelligence
The study recommends further research to sustain the symbiosis between physical,digital,and cyberspace through frameworks like industry 5.0.
Advancing human-centered integration in digital twins,focusing on real-time,bidirectional interaction and cognitive-based machine learning for knowledge systemizing.
Developing BCI-enabled digital triplets,including semantic reasoning of brain signals and real-time synchronization between humans and digital avatars.,Future research should focus on advancing AI and digital twins beyond preliminary stages
The paper used the scientific research method,applying Representation Theory to design an architecture (HKPoly) for querying heterogeneous,unconnected business process data. HKPoly simplifies user queries
Representation Theory: Used to design and detail the architecture,its components,and user interactions.
Experimental evaluation: Ran experiments with varying data volumes to measure processing time and compared query complexity between HKPoly and SQL.,,HKPoly reduces user query complexity: HyQL queries had 7 components versus 14 in equivalent SQL queries
HKPoly enables integrated querying of heterogeneous data stores using a knowledge graph-centric approach,improving query semantics and provenance handling.,
The architecture is flexible for implementation with various technologies.,Lack of solutions for querying heterogeneous data generated by unconnected business processes,especially regarding data linkage across diverse data stores.
Need for broader approaches that encompass heterogeneous data models,including NoSQL databases,and address data linkage
Absence of implementation and experimental evaluation in existing frameworks,highlighting the need for practical validation and performance analysis.,
Comparative analysis: The study compared technical system Digital Twins and Human Digital Twins to highlight fundamental differences.,,The paper proposes a generic
No quantitative results or statistical significance (p-values) are reported.,No primary outcomes,results
Behavior and cognitive mechanisms: Difficulty in modeling human factors,including emotions.,The study proposes a generic
Key implications include the need for regulation,ethics,transparency
Recommendations: Ensure unique identification,GDPR compliance,explainability
Need for systematic reviews and further research to formalize and define HDT,especially regarding its conceptual differences from technical Digital Twins.,Future research should include a systematic review to complete and formalize the Human Digital Twin (HDT) concept. Further investigation is needed on regulation
Semantic modeling with tools like Karma: Uses techniques such as conditional random fields (CRF) and Steiner tree algorithms to suggest and learn semantic labels and relationships.,,Creating semantic models for heterogeneous data sets involves significant initial overhead and requires substantial human input
There is a current gap between data lake platforms,OBDA,and semantic technologies
Further research is needed to customize and optimize AI (LLMs) for specific data integration tasks.,There is a significant gap between current data lake platforms,OBDA (Ontology-Based Data Access)
Limited technical interoperability,especially for NoSQL data models; current methods mainly support tabular data and need to ensure compatibility with W3C Semantic Web standards.,Future research should address: reducing initial overhead and improving usability in semantic modeling; evaluating the accuracy of automated semantic labeling; achieving technical interoperability across diverse NoSQL data models; enhancing technical abstraction for non-experts; improving applicability and maturity of OBDA solutions for Big Data; and leveraging AI/LLMs for data integration tasks.
Backbone encoder (SWIN Transformer): Extracts time-frequency representations from spectrogram patches using local attention within shifted windows.,The research is reproducible. The source code for the project is available at https://github.com/tomoyoshki/InfoMAE.,InfoMAE outperforms baselines (GMC
InfoMAE achieves real-time inference (<1 second) on Raspberry Pi 4; no p-values or statistical significance values are reported.,InfoMAE outperforms baselines (CMC,GMC
InfoMAE maintains high accuracy and F1 across increasing multimodal pair ratios (e.g.,at 50%: Acc 0.9377,F1 0.9367).
IoT sensor data can be sparse,noisy,and incomplete due to deployment heterogeneity
Key components (shared,private,temporal) are critical for cross-modal alignment.
Investigating concurrent unimodal pretraining could improve efficiency and performance.,Future research should explore concurrent unimodal pretraining,optimized attention mechanisms like FlashAttention
Standard and bounded-parameter experiments: Evaluation of BoxTE and competing models on temporal knowledge graph completion benchmarks (ICEWS14,ICEWS5-15,GDELT) in both standard and parameter-bounded settings.
Comparative analysis: Performance comparison with existing models (e.g.,TTransE,DE-SimplE
The optimal value of parameter k varies by dataset: k ≥ 2 is best for ICEWS14,k = 3 for ICEWS5-15,and k = 1 for GDELT
BoxTE is fully expressive,captures a wide range of temporal inference patterns,and provides a robust baseline for temporal knowledge base completion under computational constraints. No p-values or statistical significance are reported.
Optimal k varies: k ≥ 2 is best for ICEWS14,k = 3 for ICEWS5-15,and k = 1 for GDELT.
In the parameter-bounded setting,reducing dimensionality causes significant loss in representation capacity,especially on dense datasets like GDELT.
Further details about parameter counts are only provided in the appendix.,BoxTE achieves state-of-the-art performance on multiple temporal knowledge graph completion (TKGC) benchmarks,even under parameter constraints.
Future work should introduce benchmarks for higher-arity temporal knowledge base completion.,Lack of established benchmarks for higher-arity temporal knowledge graph completion,limiting evaluation of models like BoxTE.
Further research required to develop expressive,inductively rich temporal knowledge graph embedding (TKGE) models.,One suggested future research direction is introducing new benchmarks for temporal knowledge base completion involving higher-arity facts
The system uses graph-based analysis with Neo4j and integrates advanced NLP modules (e.g.,EVITA,Heideltime
Entity linking and disambiguation: Tools like DBpedia-spotlight or Entity-Fishing assign unique identifiers to entity mentions,resolving references and creating entity instances.,
No statistical significance (p-values) was reported in the context.,Entity mentions: Precision 0.629,Recall 0.773
Event mentions: Precision 0.782,Recall 0.663,F1 score 0.718
Entity instances: Precision 0.434,Recall 0.750,F1 score 0.550
Numeric values: Precision 0.893,Recall 0.993,F1 score 0.940
Temporal expression detection: Precision 0.965,Recall 0.921,F1 score 0.943
Temporal expression normalization: Precision 0.982,Recall 0.737,F1 score 0.842
Event participants: Precision 0.619,Recall 0.649,F1 score 0.634
Temporal links: Precision 0.494,Recall 0.257,F1 score 0.338
Limited coverage of temporal links (tlinks),including MEASURE,simultaneous
Existing methods focus mainly on sentence-level analysis,requiring additional post-processing for broader context.,The proposed system offers a promising approach to text representation in NLP using LPG-based unified context modeling.
Key limitations include incomplete detection of nested entities,limited event participant identification,and insufficient temporal link coverage.
Future work should focus on improving entity detection,event participant identification,and temporal relation coverage for more robust NLP applications.
Improve temporal links (tlinks) coverage,especially for MEASURE relations,simultaneous tlinks
Address limitations in entity mention detection,entity linking/disambiguation,event participants detection
The paper introduces ExplainTemp,a 26k-entry dataset for explainable temporal reasoning,constructed using a novel Temporal Knowledge Graph-Instructed Generation (TKGIG) approach. Fine-tuned TimeLlama models
Baseline experiments involved finetuning and evaluating models like LSTM,BERT,Flan T5
LSTM and BERT baselines achieved overall F1 scores of 10.7 and 21.4,respectively,on the ExplainTemp dataset.
The testing dataset is high-quality and standardized,with low-scoring samples excluded; statistical significance (p-values) not reported.,Two annotators rated 1
Cohen’s kappa scores: correctness (0.74),completeness (0.66),fluency (0.98) indicate high inter-rater agreement.
TimeLlama models (7b/13b) outperformed baselines in BLEU,ROUGE,and BertScore metrics.
TimeLlama-7b achieved BLEU avg 59.9,ROUGE1 46.3,BertScore F1 90.2.
ChatTimeLlama-7b achieved BLEU avg 61.9,ROUGE1 48.2,BertScore F1 88.8.
Finetuning led to improvements of 44.0 (positive),32.5 (negative),56.3 (neutral)
Large language models risk generating harmful,biased,or logically incoherent content (hallucination).
Evaluation prioritizes accuracy over fluency,which may affect assessment balance.,The study presents a high-quality
Recommendation: Use instruction tuning and high-quality datasets to enhance LLMs’ temporal reasoning and explanation capabilities.,Limited exploration of LLMs' abilities in complex event forecasting,which requires multi-step temporal reasoning and future prediction.
Need to assess the impact of instruction tuning and new datasets on LLMs' temporal prediction and explanation capabilities.,Future research should address the unexplored potential of LLMs in complex event forecasting requiring multi-step temporal reasoning,and the underexplored area of explainable temporal reasoning. Expanding benchmark datasets to include more diverse temporal reasoning tasks is also recommended to further assess and improve LLM capabilities.
Node classification on citation networks: Experiments use node classification tasks on Cora and Citeseer datasets,with backbone GNNs (GCN and GAT) to evaluate utility-privacy trade-offs.,
The framework achieves greater utility improvements under stronger privacy (lower 𝜋),e.g.,7.53% and 7.64% on Citeseer at 𝜋 = 3 and 1.
Integrating dynamic denoising with private GNN learning yields a better utility-privacy trade-off; all improvements are statistically significant (standard deviations ≤ 0.35).,The primary outcome measured is model utility,quantified by accuracy.
The proposed framework outperforms original LDPGen,DPRR,L-DPGCN
Gains are higher under stronger privacy (lower 𝜋).,,The proposed framework dynamically integrates topology denoising with private GNN learning
Need for improved integration of permuted topology denoising with GNN private learning for better privacy-utility trade-off.,,
Mixed-method approach: quantitative measurement of adaptation steps and energy consumption,and qualitative analysis of agent knowledge.,The research provides detailed descriptions of the experiment
All models achieved at least 40 coins after 30,000 generations; temporal heterogeneity showed faster evolution,especially with long-term memory.
The peek-end rule model with temporal heterogeneity demonstrated resilience to noise and capitalized on long-term memory,but no explicit p-values or statistical significance were reported.,Primary outcome: Fitness measured as the number of coins collected by agents.
The agent’s interoception (e.g.,hunger sensing) is not yet human-like; improvements are ongoing.,Temporal heterogeneity in cognitive architectures can enable faster convergence
Enhancing agent interoception (e.g.,hunger sensing) is a recommended direction for future research.,The study does not address the influence of temporal heterogeneity in different cognitive architecture topologies.
Current buffers require the same number of inputs before output; ongoing work aims to develop biologically inspired dynamic regulation.,Future research should investigate the influence of temporal heterogeneity in different cognitive architecture topologies,determine ideal values for temporal heterogeneity
Collaborative Knowledge Graph Fusion: Alternates between extracting triples from text and enriching a prior knowledge graph using relation alignment and benchmark-based supervision.,The research provides a prototype system implemented with PyTorch. Source code links are included: https://github.com/hkharryking/labeled NYT CoNLL. Additional resources used are Stanford CoreNLP and Wikidata. No further details on full code or reproducibility procedures are given.,The proposed system outperforms state-of-the-art baselines in both joint event extraction and knowledge graph embedding tasks
Statistical significance (p-values) is not reported in the provided context.,BJEEwn18 and BJEEfb15k models achieved highest F1 scores across datasets: up to 98.2 (WebNLG),99.0 (NYT)
Supervisor model achieved Hit@10,Hit@20,Hit@30 = 100.0 and MRR = 0.0294 on FB15K (KGF task).
BJEE models significantly outperformed pure BERT and other baselines in Precision,Recall,and F1.
Performance increases with more iterative rounds and optimized model parameters (e.g.,hidden dimensions,CNN kernels).
Recommendation: Further improve explorer processes for additional gains; the framework supports fully automatic knowledge graph fusion in future work.,Difficulty in aligning RDF triples,especially relation alignment
Difficulty in sharing knowledge between sub-tasks,causing error propagation and degraded sub-task performance.,Future research should focus on: (1) improving alignment of RDF triples
Class-Based Sensor-Independent Indices (CBSI),specifically CBSI-NDVI,were used to reduce spectral dimensionality.
For dry biomass: Precision 1.00,Recall 0.91,F-Score 0.95
For burnt fields: Precision 0.92,Recall 1.00,F-Score 0.96
Largest burnt area detected was approximately 80.27 sq. km on 6th Nov 2019.,Classification techniques used only one data dimension,while temporal multispectral data have both spectral and temporal dimensions
It is not possible to state which classifier is best for all tasks due to varying image characteristics and study purposes.,The proposed approach achieved high accuracy in identifying burnt paddy fields,with an overall accuracy of 98% and an F-Score of 0.96.
Users still face challenges in identifying appropriate spectral bands (red and NIR) in large multispectral datasets.,,No information available
Offline evaluation: Used statistical measures (Precision@k,Mean Average Precision,Mean Reciprocal Rank) to assess recommendation quality.
ResQue evaluation framework: Assessed perceived system qualities,usefulness,satisfaction
Keyphrase extraction improved average similarity scores (up to 0.92) and enhanced perceived system qualities,including accuracy,novelty
Participants reported higher satisfaction,perceived usefulness,and novelty with PKG-based variants
Statistical measures used: Precision@k,Mean Reciprocal Rank (MRR),Mean Average Precision (MAP).
User evaluation confirmed benefits in perceived accuracy,novelty,diversity
Further research suggested on explanation,transparency,confidence
Most users preferred recommendations considering all previously misunderstood concepts,not just current slide content.,Imprecise extraction of DNU (Did Not Understand) concepts led to irrelevant or missing key concepts
Future work should improve DNU concept selection by considering their similarity to the current slide for increased relevance.,Future research should investigate benefits on user-centric aspects such as explanation,transparency
Privacy-Preserving User-Item Graph Expansion: Encrypted item IDs and user embeddings are used to expand local user-item graphs without exposing private information.,,FedGNN achieves the best performance among privacy-preserving methods
FedGNN protects both ratings and user-item interaction histories,while other privacy-preserving methods only protect ratings; no explicit p-values reported.,FedGNN achieves the best performance among privacy-preserving methods
FedGNN protects both ratings and user-item interaction histories,unlike FCF and FedMF,which only protect ratings.
Sparse rating matrices require more pseudo items,further increasing communication cost.,FedGNN achieves competitive recommendation performance while protecting user privacy
Privacy is preserved through local gradient sharing,encrypted item IDs,and pseudo interacted item sampling.
Setting the number of pseudo interacted items (𝐴) to 1,000 balances privacy,performance
Effectiveness of fixed versus fully trainable neighbor user embeddings requires deeper investigation.,,
Two-pass annotation approach: first automated,then manual review,to reduce both system and individual biases.
Application of computer vision and speech processing techniques for automatic multimodal feature extraction.,The research uses multiple existing multimodal datasets (e.g.,IEMOCAP
Technical challenges in multimodal feature extraction (e.g.,bit rate,file encoding
Limitations include reliance on experience with multimodal narrative systems,limited knowledge of other systems' development,and assumptions about current technological capabilities.
Recommendations emphasize considering user needs,ethical implications,and system resilience.
Guidelines assume advanced technological capabilities (e.g.,computer vision,3-D modeling)
Need for balancing technical versus layman annotations to ensure both precision and user accessibility.,Future research should address gaps such as limited knowledge of how other systems are built,evolving technological capabilities in computer vision and 3-D modeling
Categorization framework: Research is organized and evaluated using a PKG ecosystem framework with aspects like population,representation/management,and utilization.
Key challenges include lack of holistic approaches,integration difficulties,and the need for PKG standards and adoption by service providers.
Insufficient focus on representation,management,and synchronization of knowledge in PKGs.
Integrating existing building blocks into practical PKG solutions is challenging,especially regarding synchronization with private,unstructured data.
Recommendations include considering the broader ecosystem,improving synchronization methods,and clarifying user responsibilities in sensitive domains.
Need for establishing PKG standards and ensuring their adoption by service providers.,The study suggests future research should focus on establishing PKG standards,integrating PKG components holistically
Link prediction evaluation: The model was evaluated using the link prediction task,generating candidate quadruples and comparing results.,The research is partially reproducible. The authors re-implemented the codes and attached their implementation in the supplementary material’s code/LCGE new folder. However
No explicit p-values or statistical significance values are reported.,Primary outcomes focus on link prediction performance (MRR,Hits@1/3/10) across datasets (ICEWS14
Adding HGE to backbones (TeRo,TComplEx,TNTComplEx
HSAE baseline not included because the author did not publish the codes.,HGE consistently improves backbone model performance on all datasets,especially on dense datasets like GDELT.
Recommendation: Use HGE to enhance temporal knowledge graph models,particularly for dense datasets.,The need for more consistent and reliable evaluation metrics and implementations
Limited availability of complete and reproducible code for some baseline models (e.g.,DyERNIE,HSAE)
Further exploration of HGE’s representational approach beyond parameter increase for improved efficiency and effectiveness.,,
Skefl achieves over 80% accuracy within 10 rounds,matching FedAvg and FedHE. Skefl Dist() adds negligible overhead,while Skefl Aggr() adds 24–38% more computation time than FedAvg. ATSS.Split() is the slowest ATSS step
Asymmetric Threshold Secret Sharing (ATSS): The study evaluates execution times of ATSS primitives (ATSS.Split(),ATSS.Combine(),ATSS.Aggregate()) across different datasets.
No p-values or statistical significance measures are reported.,Skefl,FedAvg
ATSS.Split() had the longest execution time among ATSS primitives,especially for complex datasets (CIFAR-10,SVHN: tens of seconds).
Skefl balances accuracy,privacy,and computational cost effectively.
Insufficient evaluation of protocols like Skefl in diverse,real-world federated learning environments.,
Model Performance Evaluation: The performance of selected models was reported on 30 different image types using quantitative accuracy metrics.,The research describes a rigorous,multi-stage data collection and validation process
No statistical significance (p-values) or explicit conclusions are provided in the context.,The primary outcome measured was the incidence density (/1000 person years) of hand,foot
Textual Understanding Error (6%),Rejection to Answer (3%),Annotation Error (2%)
Data contamination risk addressed but remains a concern.,The study provides detailed examples and protocols to guide annotators in handling various question types,including those with multiple images.
No explicit recommendations beyond annotation guidance are provided.,There is a significant performance gap between open-source and closed-source models,especially in complex reasoning tasks.
Models perform worse in disciplines requiring intricate perception and complex reasoning,indicating a need for improvement in these areas.,Future research should address gaps in visual perception
Abstractive Methods: Generate new summary sentences not present in the original text,using structured models (tree/graph-based) or generative models (seq2seq,RNNs
Hybrid Methods: Combine extractive and abstractive approaches,first extracting key sentences and then refining or rewriting them with an abstractive model.,The research uses an automated
No quantitative results or statistical significance (p-values) are reported in the provided context.,"Primary outcomes: The paper identifies and categorizes ATS (Automatic Text Summarization) datasets into """"""""core"""""""" (over 100 citations) and """"""""supplementary"""""""" (fewer than 100 citations) groups",summarized in Tables 2 and 3.
Need for improved evaluation methods for consistency,factual accuracy,and interpretability.
LLM-based methods offer flexible,accurate,and scalable summarization but raise concerns about bias and fairness.
Hybrid approaches can provide more comprehensive summaries by combining extractive and abstractive techniques.,Optimizing the efficiency of large language models (LLMs) for summarization,including model distillation
Developing more objective,comprehensive,and accurate evaluation metrics for ATS.
Expanding research on LLM-based summarization techniques,including in-context learning,prompt engineering
Dual reviewer assessment: Two reviewers independently evaluated papers for eligibility,implementation,and quality
Data-driven selection: Only high-quality papers (final score ≥ 3) were included in the survey.,,Human Digital Twins (HDTs) are a rapidly growing research area
HDTs demonstrate potential to improve decision-making,optimize processes,and enhance safety and efficiency across healthcare
The Simulia Living Heart project aims to develop highly accurate personalized digital human heart models for education,training,device design
No specific statistical values or measured effects are provided.,Legal concerns regarding data privacy,ownership
Technology maturity is still required for broader adoption.,HDTs (Human Digital Twins) are a rapidly growing research area with strong potential to transform healthcare,manufacturing
Key legal concerns include data privacy,ownership,and potential bias or discrimination in decision-making.
Future research should focus on improving HDT accuracy,realism,and modeling techniques.
Addressing cybersecurity,data privacy,and ethical/legal concerns to ensure responsible
Graph Learning (GNN-based methods): Utilizes graph structures to obtain structural information and high-order correlations for improved recommendations.,Reproducibility in recommendation research is challenging due to varying datasets,experimental settings
It highlights the need for transparent experimental settings,reproducibility,and standardized benchmarking datasets for fair and accurate evaluation.
No quantitative results or statistical significance (p-values) are reported in the provided context.,No explicit primary outcomes,results
The context discusses challenges in benchmarking,evaluation,and reproducibility in recommender systems but does not present specific experimental results or statistical findings.
Multi-stakeholder concerns (explainability,fairness,balance) are not fully addressed.
Future research should focus on explainability,fairness,and multi-objective goals in recommendation models.
Encouraging theoretical studies and reproducibility analysis is recommended.,The need for a large,up-to-date benchmarking recommendation dataset to track state-of-the-art models and enable fair comparisons.
Improving reproducibility through transparent experimental settings,code and dataset release,and unified evaluation frameworks.
Few-shot tuning: Experiments used varying numbers of training samples (K = 16,512,1024) to assess the impact on model performance.
Baseline comparisons: GenTKG was compared against embedding-based,rule-based,and LLM-based in-context learning methods.
Few-shot experiments show GenTKG consistently improves as training samples increase and outperforms naive ICL with as few as 16 shots; no p-values or statistical significance reported.,GenTKG achieves state-of-the-art performance,surpassing all conventional methods on four datasets in Hits@1 and Hits@3
GenTKG outperforms xERTE (embedding-based) on ICEWS14,ICEWS18,GDELT
Compared to TLogic (rule-based),GenTKG performs better on Hits@1 and Hits@3,with similar Hits@10. Slight Hits@10 drops on ICEWS14/ICEWS18 are attributed to TLogic’s dataset-specific design.
Statistical values: Relative difference for GDELT Hits@1 is 58.59% (ICEWS18 vs. original),and for YAGO Hits@1 is -10.77% (ICEWS14 vs. original).,GenTKG is limited by the input context window of LLMs (e.g.
Recommendation: Use GenTKG for efficient and accurate temporal relational forecasting.,GenTKG is limited by the input context window of LLMs,restricting the number of historical facts and thus performance.
Future work includes exploring GenTKG's generalization in inductive,zero-shot,or few-shot temporal knowledge graph tasks.
Most important comparison weights for matching: date of birth (2.4),family name (2.3),municipality of birth (2.0)
Ambiguous or false matches near the threshold complicate confident recall assessment.,Date of birth is the most important metadata field for disambiguating person records,followed by family name and municipality of birth.
Applicability and adaptation of the approach to other studies integrating historical person registers.,Future research should develop a perspective for aggregated person instances to enable prosopographical analysis over all persons. For larger person registers,adopting a blocking strategy based on metadata values is recommended to reduce comparisons. The approach can also be applied to integrate additional historical person registers.
Experiments conducted on synthetic and real-world dynamic graph data sets,with hyperparameter tuning via validation sets.,
For node classification (Elliptic),EvolveGCN-O surpasses static GCN but not GCN-GRU; dynamic models perform better,especially for the minority (illicit) class.
No explicit p-values or statistical significance values are reported in the context.,For edge classification (BC-OTC,BC-Alpha
The choice between -H and -O versions depends on the informativeness of node features versus graph structure.,EvolveGCN outperforms GCN and GCN-GRU in edge classification across all tested datasets,confirming its effectiveness.
For node classification on highly imbalanced data,dynamic models (including EvolveGCN-O) are more effective than static GCN,but GCN-GRU performs best.
Recommendation: Use dynamic models like EvolveGCN for evolving graph tasks,and select the version based on node feature informativeness.,Dynamic models struggle to adapt to sudden
Further research is needed to enhance model reliability during unexpected real-world events.,,
Organization and representation of data as sequences of personal situational contexts for real-time analysis.,The research is reproducible regarding data,as the Smart University (SU) dataset and its detailed description are available for download at https://livepeople.datascientia.eu/dataset/smartunitn2. There is no information provided about the availability of source code for the project.
The Smart University dataset includes 158 students,139,239 annotations
The model was validated using the Smart University (SU) dataset: 158 students,4 weeks,139
No statistical values or measured effects are reported.,Data are highly heterogeneous (categorical,numerical
It supports person-centric services,such as predicting habits and improving human-machine interaction.,Lack of methods for representing and managing heterogeneous
Need for knowledge-level models,like Knowledge Graphs,to make personal situational context understandable and usable.
Knowledge Graphs: The Future of Data Integration and Insightful Discovery,Mohamed Saher,Farah Kirollos
The paper highlights that knowledge graphs (KGs) and knowledge graph embeddings (KGEs) are crucial for data integration,explainable AI,and autonomous driving. Detailed KGs improve semantic capture. Dynamic Knowledge Graphs (DKG) enable real-time conceptual change. New insight: High informational detail in KGs enhances both type and relational understanding.
Relationship extraction using large language models: Unstructured data is prepared and formatted,then a prompt instructs the model (e.g.,Llama 7b) to extract subject-predicate-object triples
Explainability techniques in AI: Knowledge graphs are used for pre-modeling,in-modeling,and post-modeling explainability to enhance understanding and transparency in AI models.
In autonomous driving,knowledge graphs with the highest informational detail achieved over 95% accuracy in classifying driving situations,outperforming traditional feature vectors.
Main results include improved data gathering and classification on climate change,standardization of database formats,and promotion of data interoperability.
Further research is suggested to improve generalization and robustness,especially for safety-critical applications like autonomous vehicles.,Knowledge graphs enable structured extraction and explainable clustering of entities
Knowledge graphs support explainable AI at pre-,in-,and post-modeling stages
A global Climate Action Knowledge Graph is recommended to connect diverse climate data,fostering collaboration and informed decision-making.,Achieving accurate
Mining Comments and Repositories: Mines code comments and analyzes software repositories to extract project management details,bug history,version changes
Surveys and Interviews: Conducts surveys and personal interviews with developers to analyze comprehension challenges and framework requirements.,,SMARTKT is a search framework for C/C++ and Python codebases
SMARTKT extracts and associates knowledge from multiple sources,representing it as a semantic graph to provide direct and intelligent responses,including additional alerts (e.g.
Surveys and interviews revealed existing tools use limited sources and lack an integrated,easy-to-use framework; SMARTKT addresses these gaps. No quantitative results or p-values are provided.,Primary outcomes:
Identification of knowledge types and their sources relevant to program comprehension (e.g.,software development,application-oriented
It supports various query types,providing direct responses and additional insights,such as data race alerts.
Existing tools are limited; SMARTKT addresses the need for a comprehensive,easy-to-use framework.,Existing tools consider only limited sources of information for program comprehension.
Future research should focus on developing comprehensive frameworks that integrate multiple knowledge sources to aid program comprehension.,Future research should address the limitation of current assistance tools that consider only limited sources. There is a need for an easy-to-use integrated framework that combines diverse knowledge sources to better support program comprehension and project management in evolving,fragmented software teams.
User study: 12 novice Java students were divided into two groups to compare the effectiveness of the proposed tool versus baseline tools (API-KG and Task-KG) using controlled tasks and time limits.,The research provides experimental data for replication and describes detailed sampling and annotation procedures. However,there is no mention of source code availability for the project. Thus
Four new implicit semantic relations were found in 78.3% of Q1 answer points: Function Similarity (23.5%),Function Opposite (9.8%),Behavior Difference (21.6%)
Seven implicit semantic relations were identified for RQ3: Function Collaboration (14.3%),Type Conversion (12.5%),Implement Constraint (10.7%)
API semantic relation inference accuracies: Function Opposite 1.000,Function Similarity 0.965,Behavior Difference 0.975
The constructed API-Task KG contains 8,672 API entities,7
Heuristic extraction of API semantic relations lacks generality.,The proposed approach identifies and utilizes nine types of implicit API semantic relations,improving the interpretability and precision of code search results.
The fused API-Task knowledge graph (KG) benefits code search,debugging,and optimization by providing richer semantic connections.
Recommendations include leveraging large language models to enhance entity and relation extraction for broader applicability.,Current API-KG and Task-KG lack inherent connections,limiting their ability to solve programming issues.
Future work should focus on fusing and mutually enriching know-what (API) and know-how (task) knowledge for more effective knowledge discovery.,,The study design is an empirical study involving: selection of 100 Stack Overflow Java API usage questions across 10 topics; independent extraction of answer points by two experienced Master’s students (Cohen’s Kappa 0.842 and 0.880); conflict resolution by a PhD student; and a user study with randomized
Faceted search and similarity-based ranking for object properties,supporting complex and unstructured data types.,The research is supported by open source software development
No quantitative results or statistical significance (p-values) are reported.,The Knowledge Net software prototype was tested by creating a knowledge graph fragment for an electric vehicle manufacturer,processing about 50 objects and identifying about 200 aspects.
No statistical values or quantitative effect measures are reported.,Prototype needs improvement,including user experience and performance issues.
Existing developments are insufficient for effective industry implementation.,The Knowledge Net model enables organizations to independently or collaboratively create knowledge graphs for a shared information space,essential for intelligent enterprises in Industry 4.0.
Key advantages include multidimensionality,expandability,and dynamic aspect/property structuring.
Future releases aim to enhance subject/property descriptions,introduce template-hints,and implement fuzzy search.
Lack of information on technologies used for ontology creation,unique identifier support,and dynamic ontology scheme changes.
Insufficient applied software development and industry implementation for proposed intelligent system models.,Future research should address the extent of expert intervention required in xIRBIS-ML,clarify technologies for ontology creation and processing
CTGNN model: Combines Transformer encoder layers for atom and neighbor features,followed by CGCNN (Crystal Graph Convolutional Neural Network) convolution layers,pooling
Incorporation of topological features: Includes angular and distance (RBF kernel) information in the model.,,CTGNN outperforms existing models (CGCNN
Quantitative results: For perovskite formation energy,CTGNN MAE = 0.013,R² = 0.996; for perovskite bandgap
CTGNN outperformed CGCNN and MEGNet by 51.85–59.38% (Ef) and 45.26–47.30% (Eg) on perovskite dataset.,,CTGNN significantly advances material computing
Ablation studies recommend retaining both angular encoding and dual-Transformer structures for optimal model performance.,,
Comparative analysis: Summarized and compared privacy-preserving techniques (e.g.,differential privacy,homomorphic encryption
The study analyzes privacy attacks and protection methods,including differential privacy,homomorphic encryption
Privacy-preserving methods (HE,SMPC) are computationally intensive,inefficient for large/complex data
Need for further study on algorithm convergence,optimal worker numbers,and update frequency.
It provides a systematic overview of BCFL architectures,privacy threats,and state-of-the-art privacy-preserving solutions.
The survey identifies promising future research directions and aims to guide practitioners and researchers.,Integration of zero-knowledge proofs for enhanced privacy in Blockchain-enabled Federated Learning (BCFL),allowing validation of updates without revealing raw data.
Addressing vulnerabilities in smart contracts to enhance security and robustness in BCFL models.,Future research should focus on integrating zero-knowledge proofs for enhanced privacy,optimizing gas consumption in smart contracts
Compression Mechanism: Sends only selected model parameters (based on a probability),reducing data exposure and communication cost.,
For the compression mechanism,lower bounds for privacy leakage,utility loss
Statistical significance (p-values) is not reported in the context.,The primary outcome is a No-Free-Lunch theorem (Theorem 4.6): the weighted sum of privacy leakage (ϵp),utility loss (ϵu)
Further research is needed on optimal protection hyperparameter determination at each communication round.,The unified federated learning (FL) framework quantifies the trade-off between privacy leakage,utility loss
Developing algorithms that can adaptively learn the hyperparameter for protection mechanisms.,The study suggests future research on: (1) using a generalized JS-divergence with a hyperparameter α to analyze privacy leakage and trade-offs; (2) designing meta-algorithms to find or adaptively learn optimal protection hyperparameters at each communication round; and (3) further exploration of the optimization problem.,
Training of basic skills for various agents (e.g.,Humanoid,Ant
Knowledge fusion process to combine trained skills and knowledge,enabling skill retrieval,Q\&A
Extensive experiments show KSG’s effectiveness in tasks like QA,knowledge retrieval,and skill learning; however
The KSG construction uses entity extraction,attribute extraction,and relation extraction
Experiments demonstrate KSG’s effectiveness in tasks such as question answering (QA),knowledge retrieval,and skill learning.
No specific statistical values or quantitative results are provided.,,KSG (Knowledge and Skill Graph) enables simultaneous processing of static and dynamic knowledge
Future work includes expanding KSG for real applications,providing more basic skills,offline data
KSG should provide more basic skills and offline data for reinforcement learning,meta learning,and imitation learning.
KSG needs to offer more complex relations between skills,agents,and environments for skill learning and reasoning.
The deployment showed that multimodal data (from sensors,microphones,physiological data) can distinguish team performance and support student reflection. Teachers valued the data but needed better data literacy and clearer visualizations. Trust and understanding were challenged by data incompleteness and unclear data processing. Alternative analysis models were suggested.
Human-centered approach: Collaboration with teachers,researchers,and students to co-create and evaluate the MMLA innovation.
Surveys: Researchers and students completed surveys to gather perspectives on logistics,ethics,privacy
No statistical significance or p-values were reported; findings are qualitative and not generalizable.,47 students (40 females,avg. age: 23.81
Senior teachers reported the debriefing tool reinforced discussions,validated teamwork aspects,and provided “objective feedback.”
Some students wore sensors incorrectly,affecting data quality.,Participation was limited by complex explanations and technical terms; simplifying consent forms and explanations is recommended.
Future studies should improve communication,logistics,and sustainability for broader and more reliable deployment.
Insufficient research on ethical practices,trust,and data transparency in MMLA systems
Lack of human-centered design approaches and strong partnerships with teachers and students in MMLA development.,Future research should address the limited participation of less motivated or non-consenting students to better understand their perspectives. Studies should also improve consistency in evidence collection across iterations and simplify consent forms and explanations to enhance student understanding and participation.,Study design characteristics:
In-the-wild study (conducted in real-world,everyday settings,not in a lab)
Human-centered design (collaboration with teachers,researchers,and students)
Observational (no mention of randomization,blinding,or control groups)
Structural quality metrics: Introduced to numerically represent the internal quality of knowledge graphs' structure (ontology).,,The study introduces a new metric focusing on ontology structure to evaluate knowledge graph quality
Raftel has the highest number of classes (59,662),properties (23
Structural quality metrics (normalized,combining Class Metrics (CM) and Property Metrics (PM)) show Raftel scores highest (8.71 with 1.0×CM+0.0×PM),followed by Wikidata (2.63)
Number of classes: Raftel 59,662; Wikidata 60,000; DBpedia 53
Number of properties: Raftel 23,446; Wikidata 21,607; DBpedia 7
Number of RDF triples: Raftel 348,094,663; Wikidata 253
Number of instances: Raftel 33,535,913; Wikidata 19
No explicit statistical significance values reported.,Structure-based metrics mainly focus on size and distribution,making it hard to judge overall quality.
Further research needed for structural quality metrics.,Six new structural quality metrics were proposed to evaluate knowledge graph quality,emphasizing ontology structure and utilization.
Applying these metrics to six knowledge graphs,including Raftel,provided deeper insights than size/distribution-based evaluations.
Future evaluations should use multi-dimensional approaches to assess strengths and weaknesses.,Lack of quality indicators based on knowledge graph structure (ontology),not just data size or distribution.
Need for specialized structural quality metrics that numerically represent internal knowledge graph quality.,Future research should address the lack of quality indicators based on knowledge graph structure and ontology. Current metrics mainly focus on size and basic structure,making it difficult to evaluate quality. Developing specialized structural quality metrics is recommended to better assess internal knowledge graph quality.
WaterCircles: Utilizes breadth-first search to iteratively expand from query-mapped entities,extracting relevant pathways in the knowledge graph.,The research uses the PersonalAI library for implementation but does not provide a source code link. Reproducibility details
Restrictions on node types significantly affected JudgeScore values,with best configurations showing higher scores (e.g.,0.44 for 7B models
Evaluation metrics like BERTScore lack sufficient differentiability for nuanced answer distinctions.,The study introduces an external memory architecture combining knowledge graphs and text fragments for question answering,enabling efficient relevant information extraction.
Further development to elevate personalization and contextual awareness in language model interactions.,"Future research should focus on enhancing temporal dynamics in the knowledge graph by adding a """"""""memory time"""""""" parameter and using advanced algorithms like BFS and A\*. Optimizing triplet extraction techniques is also suggested to improve efficiency",accuracy
The paper reviews privacy-preserving methods in federated learning (FL),highlighting techniques like Homomorphic Encryption (HE),secret sharing
The interplay between privacy and fairness in FL is underexplored. Combining privacy techniques with personalized models may improve the balance between privacy,fairness,and utility. The compatibility of fairness and DP also remains an open research direction.
Client selection strategies: Algorithms that actively select clients or reweight their contributions to address data distribution and fairness in federated learning.,,This is the first survey to comprehensively review privacy
The paper details privacy attacks and defenses in FL,outlines sources of bias,and summarizes fairness-aware FL approaches.
The survey highlights the need to study tradeoffs and compatibility between privacy and fairness in FL. No p-values reported.,The survey provides a comprehensive overview of privacy,fairness
Most research does not address the combined challenges of privacy and fairness in FL.,This is the first survey to comprehensively review privacy,fairness
It details privacy attacks,defenses,sources of bias
Addressing fairness issues caused by non-i.i.d. (non-independent and identically distributed) data while respecting privacy constraints remains challenging.,Future research should examine the tradeoffs between privacy and fairness in Federated Learning (FL),address fairness issues caused by non-i.i.d. data
Private Set Intersection (PSI): A cryptographic protocol allowing clients to find common data without revealing other information,implemented using a programmable pseudo random function (OPPRF).,The research implements both FedRec and DP-FedRec using Python-based code of FedGraphNN. No explicit source code link or repository is provided in the context. Therefore
In the Epinions dataset with 12 clients,DP-FedRec outperforms FedRec,balancing data privacy and availability.
The time to add noise is positively correlated with the number of points in the graph; Epinions requires more time than MovieLens1M. No explicit p-values are provided.,Primary outcomes measured: mean absolute error (MAE),mean square error (MSE)
DP-FedRec(K=10): MAE 0.8585,MSE 1.3258,RMSE 1.1493
DP-FedRec(K=5): MAE 0.8813,MSE 1.1783,RMSE 1.0875
DP-FedRec balances privacy protection and data availability without significant accuracy loss.,Only 12 categories from the Epinions dataset were selected due to memory limitations,which may affect generalizability.
No further explicit limitations or suggestions for future research are provided.,DP-FedRec achieves better performance than FedRec with 12 clients in the Epinions dataset,balancing data privacy and availability.
Need to address the Non-IID problem and privacy preservation simultaneously in federated GNNs for sub-graph-level settings.,The study suggests future research should investigate a universal differential privacy (DP) mechanism that protects both weights and edges in graph data to achieve better performance. This addresses current limitations where DP is not universally applied to all aspects of the graph data.,The study design includes: experiments on two datasets (Epinions and MovieLens)
Pairwise t-test: Statistical significance of results is assessed using the pairwise t-test at a 95% confidence level.,,DHyper achieves the best performance across all benchmarks (ICEWS18
Case studies and ablation experiments show DHyper’s hypergraph modeling captures high-order correlations,leading to better entity and relation representations than prior approaches.,DHyper achieves the highest performance across all datasets (ICEWS14
DHyper's design choices (entity/relation/prior hypergraph mappers,low-rank factorization,sparse threshold strategies) contribute to its superior results.
On ICEWS14: DHyper achieves MRR 56.15±0.28,Hits@1 43.76±0.22,Hits@3 65.46±0.16
On ICEWS18: DHyper achieves MRR 54.22±0.05,Hits@1 42.16±0.21,Hits@3 63.26±0.21
On GDELT18: DHyper achieves MRR 51.15±0.05,Hits@1 40.22±0.25,Hits@3 57.29±0.24
Future work is needed to address more complex and diverse scenarios.,DHyper achieves the best performance across all datasets,with significant improvements (up to 30.25%) over state-of-the-art methods.
Recommendation: Use 2 DHMP layers for best results.,Limited ability of some models (e.g.,TITer) to handle long-range dependencies between entities
Need for better modeling of latent pairwise correlations,such as entity groups and communities,to improve prediction accuracy.
Further exploration of derived structures to enhance representation learning in temporal knowledge graphs.,Future research should address DHyper’s current limitation of only evaluating event prediction in temporal knowledge graphs (TKGs). Suggested directions include enhancing DHyper for recommendation tasks and exploring a joint-learning framework to optimize both event prediction and recommendation for better generalization.,
The model uses intensity gain control by integrating sound input over time with an exponentially decaying window,then applying adaptive gain via nonlinear normalization. Two channels process the adapted input: an onset-sensitive channel (fast,thresholded) and an offset-sensitive channel (inverted
Gap-detection deficits can arise specifically from reduced activity in the offset-sensitive channel,not from general temporal processing loss. This dissociates onset and offset mechanisms in auditory temporal processing.,auditory; gap detection; hearing; mouse; temporal processing; thalamus
Auditory Brainstem Response (ABR) measurements: Subdermal electrodes recorded responses to auditory stimuli,with signals amplified,digitized
Stimulus presentation and analysis: Various auditory stimuli (clicks,tones,gap-in-noise
The model predicted and experiments confirmed stimulus-specific deficits in ventral MGB activity in ectopic mice,particularly for clicks following noise.,No significant differences between ectopic and nonectopic mice in:
Spontaneous firing rates,click response latencies,tone intensity thresholds
Susceptibility to anesthesia or analgesia (p ≥ 0.4).,,No significant differences were found between ectopic and nonectopic mice in basic auditory thalamic response properties or ABRs to clicks.
The necessity to explore whether similar sound-offset-specific deficits exist in humans with auditory processing disorders.,,Observational study
Non-controlled (no mention of randomization,placebo,or control group)
Retrospective analysis of physiological and histological data,,
Implementation of the “scalable innovation” heuristic to assess and mitigate scaling risks in system design.,The research describes the use of a populated prototype repository and processed datasets,but there is no explicit mention of the availability or location of the source code for the project. Therefore
PKMS interventions reduced information overload and unproductive rework,and improved innovation and holistic understanding; no p-values or statistical significance reported.,The PKMS prototype repository was populated with diverse classification datasets (e.g.
PKMS interventions reduced information overload and unproductive rework,and improved rapid iterative improvement,innovation
No explicit statistical values provided.,Scaling complexity is rarely addressed and may be underestimated,affecting broader applicability.
"The problem is described as """"""""wicked",meaning it has incomplete,contradictory
Not every captured or created meme/functionality may be immediately useful; relevance can change over time.,The PKMS project demonstrates a scalable,meme-based approach for eLearning and knowledge management
Creating tools and frameworks that align knowledge creation with skill development to better meet stakeholder needs and reduce opportunity divides.,Future research should explore decontextualizing the PKMS meta-framework for broader use,outline a sustainability vision for PKMS
Benchmarking with unified storage clients and synthetic workloads to assess system behavior under various conditions.,,En4S consistently showed lower cumulative latency than Jiffy and S3
For compute-intensive ML analytics,IO differences had minimal impact; no explicit p-values or statistical significance were reported.,En4S consistently demonstrated lower cumulative latency than Jiffy and S3
Goodput and throughput benchmarks showed En4S outperformed ReFlex and FCFS in diverse operational scenarios.,Existing ephemeral storage systems provide limited performance,scalability
Resource contention among tenants leads to performance variability,making it difficult to meet all SLOs simultaneously.,En4S consistently delivers better IO predictability and cost efficiency than S3 and Jiffy
Rigorous paper selection process based on PRISMA guidelines,including duplicate removal,screening
In-depth analysis and review of 50 selected papers focusing on semantic modeling in building operations.,,The survey reviewed 50 papers
There is a gap between theoretical knowledge and real-world application; practical integration and user-friendly platforms are needed. No p-values reported.,The methodology identified 50 papers for in-depth review after screening 27,816 (IEEE)
The methodology’s results closely matched actual building energy consumption,indicating adaptability for city-wide assessments.,Absence of a clear philosophical framework for modeling building entities
Need for technical advancements and user-friendly platforms to integrate models into Building Energy Management (BEM).,The study highlights the growing use and importance of semantic modeling,especially ontologies
The ontology landscape is fragmented,with many small,overlapping ontologies instead of unified
There is a need for systematic approaches to manage ontology extensions and maintain compatibility,minimizing duplication and fragmentation.,Future research should focus on developing unified methodologies for ontology creation
No mention of randomization,blinding,control groups
Observational and descriptive in nature,,The objectives of the study are to standardize and automate the collection and representation of building data for KPI calculation and energy performance assessment
Application analysis: Reviewed HDT applications in healthcare,industry,and daily life.
The study highlights rapid growth in HDT research since 2020,identifies trends and challenges,but does not present a practical generic HDT modeling approach; no p-values or statistical significance are reported.
Does not address building HDT models suitable for anyone regardless of location,age,gender
Further research needed to design generic models.,The study reviews the state-of-the-art in human digital twin (HDT) technologies,frameworks
HDT is a promising field with expanding applications and ongoing open issues.,Lack of a practical approach for generic HDT modeling technologies suitable for anyone regardless of location,age
Expansion of HDT applications into more innovative domains and addressing open issues and challenges in technology,social,and cognitive aspects.
Propose a comprehensive HDT framework using multi-modal and multi-source data to model human organs,body,and behavior.
The pipeline integrating a constructed knowledge graph (KG) outperforms LLaMA and GPT-4o,generating more relevant,persuasive
Comparative Analysis: Performance is compared between baseline and the proposed pipeline using both quantitative scores and expert ratings.,,The proposed pipeline outperforms the zero-shot baseline in all criteria
Incorporating neighboring concepts or Wikipedia content improves model performance,while adding lengthy lecture slides reduces it.,Primary outcomes were measured using expert evaluation on four criteria (1-5 scale): Concept Relevancy
"Ours"""""""" pipeline outperformed """"""""Zero-shot"""""""" in all criteria: Relevancy (4.845 vs. 4.750)",Coverage (4.905 vs. 4.840),Convincity (4.720 vs. 4.380)
The pipeline generates more relevant,comprehensive,and targeted content than alternatives.
Need for improved evaluation methods due to the complex,non-binary output (triplet lists) in zero-shot knowledge graph construction.,
Five-layered networking architecture analysis: The study investigates data acquisition,communication,computation
Feature selection and data imputation techniques: The study discusses methods for selecting relevant data features and handling missing data in HDT systems.,,The survey provides a comprehensive overview of Human Digital Twin (HDT) in personalized healthcare (PH)
It uniquely analyzes HDT design requirements and challenges from a networking perspective,emphasizing ubiquitous,timely
No quantitative results or statistical significance (p-values) are reported in the context provided.,The survey provides an overview of Human Digital Twin (HDT) in Personalized Healthcare (PH),highlighting differences from conventional Digital Twin (DT)
Reported results: In one HDT framework for head and neck cancer,mean and median accuracies reached 87.09% and 90.85%,respectively
Security and privacy: Vulnerable to cyber-attacks; challenges in confidentiality,access control,integrity
Key challenges include data scarcity,interoperability,privacy
The study recommends unified data management frameworks and outlines future research directions.,Data scarcity: There is an urgent need for unified and secure data management frameworks to address fragmented,heterogeneous
Explainable and efficient AI: Achieving full explainability and reliability of AI in healthcare is challenging,and there is a need for low-complexity AI models that maintain accuracy while reducing computing costs.,Future research directions include: achieving full explainability and reliability of AI in healthcare; developing low-complexity AI models for HDT without sacrificing accuracy; addressing data scarcity; enabling mobile and federated HDT; improving subsystem interoperability; designing efficient interfaces; integrating intelligent blockchain; and exploring ethical considerations.
Structure-based history augmentation improves both forward and backward inference,as shown by increased Hits@1 scores. Introducing reverse quadruples during fine-tuning generally boosts performance,especially in backward inference
Comparative Evaluation: Benchmarks against embedding-based,GNN-based,and LLM-based models
Incorporating structure-based history augmentation and reciprocal quadruples in fine-tuning yields consistent improvements in Hits@1,with gains up to 4.8% (ICEWS14,forward direction).
Among prompt strategies,ordinary and text-aware prompts outperform position-aware,and model size has minimal impact on Hits@1 performance. No p-values or explicit statistical significance are reported.
Structure-based history augmentation yields comprehensive improvement in bi-directional forecasting,with Hits@1 increases up to 4.8% (ICEWS14,forward).
Incorporating reciprocal quadruples during fine-tuning generally improves Hits@1,with increases up to 8.5% (ICEWS18,backward).
All numerical results for Hits@1,Hits@3,and Hits@10 are reported in Tables 3
Reporting of metrics is limited to Hits@1,Hits@3,and Hits@10 due to ranking constraints.
Results may not generalize to datasets with different characteristics or reasoning requirements.,LLMs are effective for temporal knowledge graph completion (TKGC),with models like Llama-2-7b-CoH and Vicuna-7b-CoH achieving or surpassing state-of-the-art results.
Further investigation is needed into key factors influencing temporal structural information reasoning using LLMs,such as historical chain length and model size.,
Curation: Researchers harmonize and resolve ambiguities in transcribed data,ensuring consistent identification of entities and concepts.,The research emphasizes reproducibility by maintaining full provenance of each data element and supporting revision of all workflow steps. The implementation is provenance-aware and allows researchers to inspect original data. The project source code is referenced at https://sealitproject.eu/.
No statistical significance or p-values are reported in the context.,Primary outcomes focus on data quality issues: missing information,data entry errors
Measured effects include impacts on accuracy of quantitative analysis,user experience,and difficulty in data comparison.
Data entry errors: Mistakes during data entry,such as filling the wrong column,can spoil user experience and analysis.
Data consistency and conciseness issues.,Data quality issues such as missing information,data entry errors
Ensuring completeness,consistency,and conciseness of data is essential for reliable quantitative analysis.
Automated or manual steps are recommended to harmonize comparative values for effective analysis.,Addressing missing information and data completeness,as missing values can impact quantitative analysis and decision-making.
Handling non-consistent comparative values (e.g.,dates,units)
Use of blockchain technology to ensure secure and efficient management of personal health data.,,The proposed personal health knowledge graph (PHKG) system maintains 100% data availability when the node drop rate is below 30% in a 500-node network.
Evaluations show the system is secure,scalable,and enables effective data sharing and integration; no specific p-values are reported.
Results showed the system is secure,scalable,and enables effective data sharing and integration.
The ontology was extended to include lifestyle factors and linked to existing medical vocabularies for improved interoperability.,Further research is needed to explore the scalability,privacy
Security aspects of using knowledge graphs in healthcare require additional research.,There is a need for further research to explore the scalability,privacy
