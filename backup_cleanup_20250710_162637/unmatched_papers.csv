title,authors,year
Sensor fusion: Combines information from different sensors at various stages (early,intermediate,late) to enhance object detection.
Experimental evaluation on a modified NIST manufacturing task board: Tests the system’s performance in realistic industrial scenarios.,The research ensured reproducibility by training and testing all three model variants 10 times,confirming consistent results. However
The RGB-D model’s mean mAP was 13% higher than RGB-only and 78% higher than Depth-only; mean precision was 11.8% and 57% higher,respectively.,
Integrating depth with RGB data significantly improved object detection,especially for challenging objects and scenes; results were consistent across 10 runs,confirming robustness. No p-values were reported.
Depth-only: mean mAP 0.269,Mean Precision 0.301,
RGB-only: mean mAP 0.425,Mean Precision 0.424,
RGB-D: mean mAP 0.480,Mean Precision 0.474,
RGB-D outperforms RGB-only (mean mAP +13%,Mean Precision +11.8%) and Depth-only (mean mAP +78%,Mean Precision +57%).
RGB-D model shows improved detection,especially for objects missed by RGB-only,such as metallic or low-contrast items.
Results are consistent across 10 runs (error bars indicate standard deviation).,Experiments did not include varying lighting conditions.,
Depth-only model struggles with short and small objects,and may hallucinate detections.,
Future work needed for transparent,irregular,non-standard
Robotic manipulation integration and further multimodal model designs are suggested for future research.,The RGB-D model outperforms both RGB-only and Depth-only models in object detection accuracy and precision.,
Early sensor fusion of RGB and depth data enables better feature extraction and generalization,improving detection in complex scenes.,
Recommendation: Employ early fusion of RGB and depth data for efficient,accurate object detection in manufacturing environments.,Need for larger
Improvement of sensor calibration,depth processing,and model fine-tuning in industrial settings.
Exploration of detection for transparent,irregular,non-standard
Application of mathematical models (deterministic and stochastic),advanced analytics,and machine learning (including deep learning) for healthcare analytics and prediction.
Implementation of real-time analytics and simulation through continuous knowledge graph updates and online learning techniques.,No information available,The integration of knowledge graphs and closed-form continuous-time liquid neural networks (CfCs) enables real-time healthcare analytics
This approach allows for continuous model updates with new patient data,supporting more accurate and informed clinical decisions.,
No quantitative results or statistical significance (p-values) are reported in the provided context.,Primary outcomes include improved accuracy and reliability in time-series modeling,with recorded values of 0.7362 and 0.7373.
Enables real-time analytics,early diagnosis,intervention
Facilitates personalized care and better patient outcomes.,,Combining knowledge graphs and closed-form continuous-time liquid neural networks (CfCs) enables real-time
This approach supports personalized medicine,early diagnosis,intervention
Recommendation: Implement this digital twin system to enhance patient outcomes and healthcare efficiency.,Integration of real-time analytics and simulation with continuously updated digital twin systems for healthcare remains underexplored.,
Advanced analytics techniques,including deep learning and simulation,require more development for dynamic healthcare process evaluation.
Developing a new holistic conceptual tool for designing human digital twins.,The research is not reproducible. No datasets were generated or analyzed,"and code availability is stated as """"""""Not applicable."""""""" No source code is provided."
The model does not offer comprehensive descriptions of ideal processes,states,or precise operator actions.
No datasets were generated or analyzed; no quantitative results or p-values are reported.,The primary outcome is the development of the IEC\_0.81 model,a small-scale operator information processing model.
No statistical values,measured effects,or empirical results are reported.
No datasets were generated or analyzed in this study.,The IEC\_081 model does not provide comprehensive descriptions of ideal processes,states
Further advancement is needed by examining operator actions in differing circumstances.,,
Semantic-driven entity and relationship extraction: BioBERT and medical ontologies (e.g.,SNOMED-CT,UMLS) are used to extract entities and relationships from the data chunks.
Hierarchical multi-agent framework: Integrates LLMs with specialized agents for automated knowledge graph construction,diagnosis,and validation.
No explicit quantitative results or statistical significance (p-values) are provided in the context.,KG4Diagnosis maintains diagnostic accuracy while preventing hallucination,representing an advancement over traditional single-agent approaches.
The framework effectively simulates real-world clinical consultations,especially for obesity and related medications (Ozempic and Wegovy).,
No explicit statistical values or quantitative results are provided in the context.,System performance depends on the quality and completeness of the knowledge graph,especially for rare or complex conditions.
Effectiveness for rare diseases or unusual symptom combinations needs further investigation.,KG4Diagnosis introduces a hierarchical multi-agent framework integrating LLMs and knowledge graphs for accurate medical diagnosis and treatment across 362 diseases.,
Expert validation ensures reliable,clinically relevant knowledge graph expansion.,
The framework supports robust,adaptable clinical decision-making and will set new evaluation standards.,Limited effectiveness in handling rare diseases or unusual symptom combinations due to insufficient representation in training data.
Heavy reliance on high-quality medical data,posing challenges for deployment in regions with limited data resources.,
Need for further benchmarking against state-of-the-art models (e.g.,ESM-1b,Med-PaLM
Evaluation Metrics (ROUGE and BLEU): Used to measure the quality and accuracy of generated responses by comparing n-gram overlaps with reference answers.,The research is reproducible. The source code and dataset are publicly available for evaluation of personalization approaches.,Our approach outperforms the baseline in all Llama-2-Chat models (7B
The improvements are statistically significant,indicating enhanced text generation quality and better alignment with the golden answer.,Primary outcomes: Our approach outperforms the baseline in BLEU-1
Measured effects: Improved text generation quality,increased lexical overlap with the golden answer,and faster response times.
The approach focuses on smaller models,which may limit performance on unseen data.,
Further research is needed to improve smaller models' performance on unseen data.,The proposed approach using knowledge graphs (KGs) outperforms the baseline in all evaluation metrics (ROUGE,BLEU
Average ROUGE-1,ROUGE-2,ROUGE-L
Recommendation: Use KGs for domain adaptation,improved response quality,faster execution
Further reducing hallucinations and factual errors in language model outputs through enhanced integration with knowledge graphs.,Future research should focus on improving the performance of smaller models on unseen data,developing concrete designs for on-device retrieval augmented generation (RAG) with knowledge graphs (KGs)
Construction of a knowledge graph using a custom ontology,representing code and dependencies in a Neo4j graph database.,
Incremental analysis via neighborhood detection,allowing experts to iteratively define and analyze logical code boundaries.,
In a demo scenario,starting with one transaction (‘SSP3’) as a seed,the increment expanded to include 13 programs and 6 tables.
The approach facilitates modernizing code with minimal external dependencies and provides clear integration points,but no p-values or statistical significance are reported.,Primary outcome: Developed a tool for incremental analysis of legacy applications using knowledge graphs and a customizable ontology.
Results: Demonstrated on GENAPP,creating increments (e.g.,seed transaction ‘SSP3’ led to an increment with 1 transaction
No explicit statistical values provided.,Current tool leverages only static analysis.,
Future work needed to understand application data (e.g.,tables) and extract insights from operational logs.,
Demonstration limited to a mainframe application (GENAPP),though approach is claimed to be extensible.,Incremental analysis helps modernize legacy applications by focusing on relevant code portions and minimizing external dependencies.
The system enables SMEs to iteratively refine increments,improving modernization efficiency.,
Future work includes analyzing application data (tables) and extracting insights from operational logs.,Need to enhance understanding of application data (e.g.,tables) and extract insights from operational logs.
Expanding the knowledge graph to incorporate business functions and data domains beyond static analysis.,Future research directions include understanding the data of the application (such as tables) and extracting insights from operational logs. These areas are suggested as next steps beyond the current tool,which primarily leverages static analysis.
Development of FAS: Designed a novel method combining selective encryption,noise injection,and bitwise scrambling to enhance security and efficiency.
Performance Evaluation: Conducted comprehensive comparisons of encryption techniques using standardized metrics (MSSIM,VIFP) to assess computational overhead,scalability
FAS demonstrates superior robustness to data skew across all datasets,maintaining stable accuracy and privacy (MSSIM and VIFP scores minimally affected),while other methods degrade under skew.
Differential privacy offers lightweight protection with minimal computation impact; FHE provides highest security but incurs major computational costs,making FAS the most efficient and scalable for large-scale,privacy-sensitive applications.
FAS achieves consistent MSSIM and VIFP scores under both skewed and normal data,e.g.,Kidney dataset: FAS MSSIM 62/61
In COVID dataset with MobileNetV2,FAS achieves 42.50% faster encryption time and 73.91% lower overhead than FEDML-HE.,
In COVID dataset with EffNetB0,FAS provides 46.15% encryption improvement and 69.77% overhead reduction compared to FEDML-HE; vs. MASKCRYPT: 36.36% faster encryption,60.61% lower overhead.
At 10% encryption,FAS achieves MSSIM score of 58%,higher than FedML-HE’s 52%
FAS consistently achieves the lowest training times and computational overhead across all datasets compared to other methods.,No discussion of limitations or shortcomings is provided in the context.,
No self-reported problems,open questions,or suggestions for further research are mentioned.
FAS combines selective encryption,bitwise scrambling,and differential noise
FAS is efficient and scalable for large-scale,resource-constrained,and real-time privacy-sensitive applications.
Recommendation: Use FAS for practical,secure federated learning,especially in latency-sensitive fields like healthcare.
Balancing strong privacy guarantees with minimal impact on model utility,especially for precision-sensitive applications like medical imaging.,
Developing cohesive solutions tailored for federated learning workflows,rather than relying on generic privacy mechanisms.,Future research should refine the FAS techniques and explore hybrid approaches across diverse datasets and federated environments to enhance scalability and applicability. There is a need to address gaps in optimizing privacy mechanisms tailored for federated workflows and further improve security-performance trade-offs.
Modular architecture for figure analysis,separating figure extraction and semantic understanding using NLP techniques.,DeepDive: Source code available at https://deepdive.stanford.edu.
Initial implementation includes distributed weak supervision using Apache Spark and Snorkel,and integration with a fairness API to flag potentially discriminatory features.,
No quantitative results or statistical significance (p-values) are reported.,Presented a flexible architecture for automating knowledge base development.,
Proposed an architecture more scalable,usable,and extensible than current approaches.
No statistical values or quantitative results reported.,Scalability: Frameworks rely on vertical scaling,which is insufficient for large workloads.
Usability: Users must write complex rules and feature extractors,often requiring scripting knowledge.,
Practical requirements like maintainability,security,and system management are out of scope.
Future work needed on user interfaces and domain feature integration.,The study presents a scalable,flexible
Key implications include the need for easier addition of domain features,pipeline extensibility,user interfaces
Recommendations include developing user interfaces,integrating fairness APIs,and sharing implementation experiences to guide future improvements.
Limited extensibility and absence of user interfaces,making pipelines hard to extend and less user-friendly.,
Insufficient support for transparency and fairness,with no mechanisms to filter or flag potentially discriminatory features.,Future research should focus on developing user interfaces
Patch Ranking: Evaluates and ranks generated patches using LLM-generated reproduction tests,regression tests,and patch size prioritization strategies.
KGCompass achieves 45.67% bug resolution,matching SWE-Agent 1.0 and outperforming Kodu (44.67%) and OpenHands (41.67%),with a low average cost per bug ($0.2013).
The hybrid KG+LLM approach yields 4.0% higher file-level and 9.1% higher function-level coverage than pure LLM; using entity path information increases successful patches from 102 to 108 (p-values not reported).,KGCompass achieves a repair success rate of 45.67%,matching SWE-Agent 1.0 (45.33%) and outperforming Kodu (44.67%)
Average cost per repair for KGCompass is $0.20,compared to SWE-Agent 1.0’s $2.18.,
Using ground-truth tests,KGCompass resolves 143 instances (47.67%).,
With Deepseek V3,KGCompass achieves a repair rate of 36.67%; with Qwen2.5 Max,33.33%.
Study focused only on Python repositories,possibly limiting generalizability.,
Further research needed for other programming languages.,KGCompass achieves state-of-the-art repair success (45.67%) and superior function-level localization (51.33%) at low cost ($0.20/repair).,
The knowledge graph-based approach generalizes well across LLMs,outperforming closed-source methods even with open-source models.,
Future improvements in patch validation could further close the small gap to the upper bound (47.67%).,Need for improved modeling of multi-hop relationships in knowledge graphs,as 69.7% of bug functions require multi-hop traversal.
Further optimization of patch ranking and candidate selection strategies to maximize repair precision and efficiency.,Future research should explore integrating more repository artifacts (like issues and pull requests) and leveraging indirect entity relationships for patch generation. There is also a need to investigate the generalizability of KGCompass to programming languages beyond Python and further analyze the impact of structural context in repair tasks.,
Data Filtering: Samples are filtered based on logical connection and hallucination criteria to ensure quality.,,Incorporating more synthetic data during supervised fine-tuning (SFT) consistently improves model performance
Synthetic data expands the coverage of pretraining data,effectively addressing both shared and model-specific knowledge deficiencies in medical LLMs.,
SENATOR demonstrates effective deficiency correction even when synthetic data is swapped between models,highlighting the generalizability of the approach; no explicit p-values are reported.,Primary outcome: SENATOR framework improves Llama-3-8B and Qwen2-7B average performance on four medical benchmarks by 11.98% and 9.15%
SENATOR uses less synthetic data (26k–128k samples) than previous methods (514k samples).,Reliance on high-quality,domain-specific knowledge graphs limits applicability where such resources are incomplete or unavailable.
Manual analysis found 37.92% of QA samples had errors: formulaic (16.77%),logical (19.56%),hallucination (1.59%).
Base model struggles with multi-hop reasoning,causing logical errors.,
Knowledge boundaries in large models are unclear,leading to unreliable or contradictory synthetic samples in specialized domains.,The SENATOR framework effectively detects and repairs knowledge deficiencies in large language models (LLMs) for the medical domain
Targeted synthetic data generation efficiently addresses specific knowledge gaps,even with less data than previous methods.,
Expanding synthetic data further enhances model performance,highlighting the value of targeted data supplementation.,
Reliance on high-quality knowledge graphs and synthetic data quality are current limitations; future work should address these dependencies and improve data generation methods.,Reliance on high-quality,domain-specific knowledge graphs limits applicability; future work aims to relax this dependency by constructing approximate knowledge graphs or using retrieval-augmented methods.
The current synthetic data generation process can be improved; future work will explore advanced techniques for more relevant,diverse,and factual data.
Efficient data synthesis must be tightly coupled with effective detection of knowledge deficiencies to avoid redundancy and better repair model gaps.,Future research should focus on reducing reliance on high-quality,domain-specific knowledge graphs by exploring automatic KG construction or retrieval-augmented methods. Improving synthetic data generation—using advanced techniques for greater relevance and diversity—and adding entity type constraints for more precise domain exploration are also recommended.
Four-step EPC RECAST testing method: (1) Site inspection preparation,(2) On-site data collection,(3) In-office modeling and calibration
Comparative benchmarking: Energy consumption calculated by the standard national procedure is compared with the new generation EPC RECAST procedure to evaluate reliability and improvements.,The research has completed two out of four validation steps. The methodology uses standard national calculation tools and the EPC RECAST online interface. No source code for the project is provided in the context. Full reproducibility will be addressed in future publications after all steps are completed.,Two out of four steps of the EPC RECAST methodology have been applied and validated
Long-Term Monitoring data collection using IoT sensors was found to be cost-effective,reliable,and precise for different room sizes and complexities.
No quantitative results or statistical significance (p-values) are provided in the context.,Primary outcomes focus on comparing energy consumption calculated by different methods (EPC RECAST,standard national procedures
Results assess reliability,effectiveness,and impact of EPC RECAST data collection and calculation engine.
No explicit statistical values or measured effects are provided in the context.,Only two out of four steps of the developed methodology have been applied and validated.,
Further research is needed to apply and verify the remaining steps.,The EPC RECAST approach has been successfully applied and partially validated in real case studies,demonstrating its applicability and potential for improvement.
Long-Term Monitoring using IoT sensors is cost-effective and provides reliable,high-quality data.,
Full methodology validation is pending further application of the remaining steps.,Full validation of the EPC RECAST methodology is pending; only two out of four steps have been completed.,
Future work will focus on data mining,model calibration,and the development of the Renovation Roadmap.
Assessment through the Quantitative Verification Strategy remains to be completed and published.,Future research should focus on the complete validation of the EPC RECAST methodology by applying and verifying the two remaining steps. Further studies are needed to assess the reliability and improvement potential of EPCs using the new generation data collection and calculation methods.,
KG-enhanced LLM inference: Utilizes KGs during the inference stage,allowing LLMs to access up-to-date knowledge without retraining.,
LLM-augmented KG construction: Applies LLMs for entity discovery,coreference resolution,and relation extraction in building KGs.
Representative synergized methods include JointGT (2021),KEPLER (2021),DRAGON (2022)
No explicit quantitative results or statistical significance (p-values) are provided in the context.,No explicit primary outcomes,results
No numerical performance data or statistical analysis is reported for BertCR,Spanbert,CDLM
LLMs may hallucinate or provide incorrect information,especially in high-stakes scenarios.,
KGs are often incomplete and hard to construct,limiting comprehensive knowledge.,
KGs typically ignore textual information,focusing mainly on structure.,
KGs lack language understanding compared to LLMs.,Integrating knowledge graphs (KGs) with large language models (LLMs) enhances performance and interpretability in downstream tasks.,
Retrieval-Augmented Knowledge Fusion methods,like RAG,outperform baseline models in open-domain question answering and generate more factual
Document-level relation extraction benefits from LLM-based and graph-based approaches for improved relation identification.,Effective knowledge injection for black-box LLMs remains an open challenge,especially due to limited access and prompt length constraints.
Developing methods for accurate encoding and alignment of entities across different modalities in knowledge graphs is a key future direction.,Future research should address effective knowledge injection for black-box LLMs,bridging multi-modal LLMs and KG structures
Custom benchmark datasets: MathComp and ChemProc datasets developed for training and evaluation,covering mathematical and chemical engineering problems.,
Quantitative metrics: Used metrics like BLEU,ROUGE-L,Exact Match
Ablation studies showed significant drops in performance when removing GRACG,instruction-tuning,or error-handling
The framework closely matches leading proprietary LLMs in effectiveness,offering a modular and adaptable solution for complex chemical and process engineering tasks. No p-values or statistical significance values are reported.,The PEOA framework (Baseline) consistently outperforms ablated variants across all evaluation metrics in task planning
Key metrics for MathComp (Baseline): Recall 78.82%,NDCG 0.69,COMP 76.79%
Key metrics for ChemProc (Baseline): Recall 77.77%,NDCG 0.68,COMP 75.55%
Exact Match (EM) for MathComp: PEOA 73.68%,Triplex 59.50%,Graph RAG 72.50%.
Exact Match (EM) for ChemProc: PEOA 74.13%,Triplex 63.20%,Graph RAG 73.80%.
Response generation (MathComp): BLEU 83.77,ROUGE-L 79.53,EM 80.88.
Response generation (ChemProc): BLEU 83.12,ROUGE-L 80.19,EM 84.95.
Ablation results: Removing GRACG,Instruction-Tuning,or Error-Handling significantly reduces performance across all metrics.
Human evaluation: User satisfaction and usability rated on a scale of 1–5; qualitative feedback categorized as High,Medium-High,or Medium.
The framework demonstrates high answer relevance,context relevance,faithfulness
Conclusion: The complete PEOA framework achieves optimal performance in specialized engineering tasks and closely matches leading proprietary LLMs.,,The PEOA framework demonstrates effective performance across task planning
Human-centric evaluation highlights strengths in user satisfaction,usability,adaptability
Qualitative feedback rates PEOA as Medium to Medium-High,suggesting room for improvement.,
Recommendation: Further enhance adaptability and user experience.,Traditional RAG techniques struggle with global questions that require a holistic understanding of knowledge bases.,
Instruction-tuning small-scale language models (SLMs) is needed for effective adaptation to complex,domain-specific tasks.,
Integration of graphs with LLMs and RAG remains an emerging research area,requiring further exploration for optimal performance.,
Blockchain-based decentralized infrastructure: Blockchain is evaluated and proposed for secure,immutable storage of user information,memory pool records
Personalized machine learning (ML) models: The ML Trainer Engine trains personalized models to compose the user's cognitive digital twin.,,
Further research needed on shell design and development.,The proposed modular cognitive architecture enables users to own and manage their digital minds securely using blockchain-based cognitive ledgers.,
Blockchain ensures the security and immutability of user data,addressing privacy concerns.,
Future work should focus on designing secure shell applications and further developing the cognitive blockchain infrastructure.,Evaluating and selecting suitable blockchain technologies for storing memory pool records and NFTs.,
Advancing automated machine learning (AutoML) methods for the learning layer,as current systems lack full automation.,Future research should focus on evaluating the proposed architecture in more scenarios
Graph Queries and Scripts: Employed for structured reasoning,allowing more complex and logical task-solving.,
KG-Based Representation: Converts outputs and reasoning steps into explicit triples,reducing noise,mitigating bias
Performance was statistically significant,with KGoT improving upon all mini-reasoning models by at least 3.5× (p-value not explicitly stated).,
Toolset and prompt format optimizations improved accuracy,runtime,and cost efficiency; XML-based prompts were retained for consistency.
Merged tool sets improved accuracy,runtime,and cost efficiency.
Noise mitigation (irrelevance removal) improved reasoning by simplifying knowledge graphs.,No explicit limitations,shortcomings
"It recommends returning """"""""True"""""""" if more calculations are required and """"""""False"""""""" otherwise",focusing strictly on the necessity for further computation.,
The process emphasizes not inventing information and using only explicitly provided data.,No information available,Suggested future research directions include: developing new knowledge graph construction methods using predictive graph models
Performance was evaluated using standard retrieval (precision,recall,F1) and text generation metrics (BLEU
Statistical significance (p-values) is not reported in the provided context.,KERL-Recom achieved a 56-point F1 score improvement over Phi-3-mini-128K and 26-point improvement over Llama-2-7B.,
KERL-Nutri showed lower mean absolute error (MAE) per micro-nutrient compared to LLaVA-Chef (e.g.,sugar MAE: 1.84 vs. 5.43).,
KERL outperformed baselines in recipe relevance,cooking step quality,and nutrient value accuracy.
KERL-Recom does not directly link health conditions to dietary restrictions; this is left for future research.,KERL-Recom significantly outperforms state-of-the-art LLMs in recipe recommendation,achieving a 56-point F1 improvement over Phi-3-mini-128K and 26 points over Llama-2-7B.
KERL modules deliver more relevant recipes,higher quality cooking steps,and more accurate nutrient values than baselines.
Future work should incorporate ingredient substitution,health information,and cultural preferences for improved personalization.
Limited research on integrating food-related Knowledge Graphs (KGs) with Large Language Models (LLMs),especially considering both health constraints and user preferences.,
The full potential of KG and LLM integration in food science remains underexplored.,Future research should focus on integrating ingredient substitution,personal health information
Processing time analysis: Module processing times (e.g.,speech recognition,image capture
In the pre-check phase,the HDT succeeded in 5/10 scenarios without feedback and required feedback for the remaining 5; no failures occurred.,
No statistical significance or p-values were reported in the context.,Primary outcomes measured: HDT system performance during a gun assembly task across five phases.,
Pre-check: 10 scenarios (5 success,5 conditional success,0 failure)
Identification: 10 scenarios (10 success,0 conditional success,0 failure)
Instructional Guidance: 7 scenarios (7 success,0 conditional success,0 failure)
Recommendations/Solutions: 10 scenarios (10 success,0 conditional success,0 failure)
Emotional Support: 11 scenarios (11 success,0 conditional success,0 failure)
Assembly Verification: 6 scenarios (3 success,3 conditional success,0 failure)
