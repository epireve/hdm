#!/usr/bin/env python3
"""
Fix CSV structure and merge processed data
"""

import csv
from pathlib import Path

# Paths
project_root = Path(__file__).parent.parent.parent
original_csv = project_root / "research_papers_complete.csv"
test_output = project_root / "research_papers_test_output.csv" 
partial_output = project_root / "research_papers_complete_updated.csv"
final_output = project_root / "research_papers_complete_final.csv"

print("Reading and fixing CSV structure...")

# Read with basic CSV reader to handle structure issues
rows = []
with open(original_csv, 'r', encoding='utf-8') as f:
    reader = csv.reader(f)
    header = next(reader)
    
    # Clean header - remove empty columns
    header = [h for h in header if h]
    print(f"Header columns: {len(header)}")
    
    # Read all rows
    for row_num, row in enumerate(reader, 2):
        # Ensure row has same length as header
        if len(row) < len(header):
            row.extend([''] * (len(header) - len(row)))
        elif len(row) > len(header):
            row = row[:len(header)]
        
        # Convert to dict
        row_dict = dict(zip(header, row))
        rows.append(row_dict)

print(f"Read {len(rows)} papers")

# Now get processed data from output files
processed_data = {}

if test_output.exists():
    print("Reading test output...")
    with open(test_output, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            cite_key = row.get('cite_key')
            if cite_key:
                processed_data[cite_key] = {
                    'Relevancy': row.get('Relevancy', ''),
                    'Relevancy Justification': row.get('Relevancy Justification', '')
                }

if partial_output.exists():
    print("Reading partial output...")
    with open(partial_output, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            cite_key = row.get('cite_key')
            if cite_key:
                processed_data[cite_key] = {
                    'Relevancy': row.get('Relevancy', ''),
                    'Relevancy Justification': row.get('Relevancy Justification', '')
                }

print(f"Found processed data for {len(processed_data)} papers")

# Update rows with processed data
updated = 0
for row in rows:
    cite_key = row.get('cite_key')
    if cite_key in processed_data:
        row['Relevancy'] = processed_data[cite_key]['Relevancy']
        row['Relevancy Justification'] = processed_data[cite_key]['Relevancy Justification']
        updated += 1

print(f"Updated {updated} papers with relevancy data")

# Write clean output
with open(final_output, 'w', encoding='utf-8', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=header)
    writer.writeheader()
    writer.writerows(rows)

print(f"\nFinal dataset written to: {final_output}")

# Statistics
stats = {'SUPER': 0, 'HIGH': 0, 'MEDIUM': 0, 'LOW': 0, 'Missing': 0}
has_justification = 0

for row in rows:
    rel = row.get('Relevancy', '').strip().upper()
    if rel in ['SUPER', 'HIGH', 'MEDIUM', 'LOW']:
        stats[rel] += 1
    else:
        stats['Missing'] += 1
    
    just = row.get('Relevancy Justification', '').strip()
    if just and just.lower() not in ['', 'none', 'null', 'not available']:
        has_justification += 1

print("\n=== Final Statistics ===")
print(f"Total papers: {len(rows)}")
print("\nRelevancy distribution:")
for level, count in stats.items():
    print(f"  {level}: {count}")
print(f"\nPapers with justification: {has_justification}")
print(f"Papers missing justification: {len(rows) - has_justification}")

print("\nIMPORTANT: The processing script completed 225 papers but only saved")
print("partial output data. The full relevancy data would need to be")
print("regenerated by running the corrected script.")