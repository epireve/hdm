---
cite_key: kejriwal_2023
title: Named Entity Resolution in Personal Knowledge Graphs
authors: Mayank Kejriwal
year: 2023
doi: arXiv:2307.12173
url: https://arxiv.org/abs/2307.12173
relevancy: High
downloaded: No
tags: 
tldr: Comprehensive analysis of entity resolution challenges specific to personal
date_processed: 2025-07-02
phase2_processed: true
original_folder: arxiv_2307.12173_arXiv_reCAPTCHA
images_total: 4
images_kept: 4
images_removed: 0
keywords: 
---

# Named Entity Resolution in Personal Knowledge Graphs

## MAYANK KEJRIWAL^∗^ , University of Southern California, USA

Entity Resolution (ER) is the problem of determining when two entities refer to the same underlying entity. The problem has been studied for over 50 years, and most recently, has taken on new importance in an era of large, heterogeneous 'knowledge graphs' published on the Web and used widely in domains as wide ranging as social media, e-commerce and search. This chapter will discuss the specific problem of named ER in the context of personal knowledge graphs (PKGs). We begin with a formal definition of the problem, and the components necessary for doing high-quality and efficient ER. We also discuss some challenges that are expected to arise for Web-scale data. Next, we provide a brief literature review, with a special focus on how existing techniques can potentially apply to PKGs. We conclude the chapter by covering some applications, as well as promising directions for future research.

Additional Key Words and Phrases: Entity resolution, instance matching, knowledge graphs, personal data, record linkage, databases, semantic web

### ACM Reference Format:

Mayank Kejriwal. 2023. Named Entity Resolution in Personal Knowledge Graphs. 1, 1 (July 2023), [[22]](#ref-120) pages. [https://doi.org/10.1145/](https://doi.org/10.1145/nnnnnnn.nnnnnnn) [nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)

## 1 INTRODUCTION

Due to the growth of large amounts of heterogeneous data (especially involving people) on the Web and in enterprise, the problem often arises as to when two pieces of information describing two entities are, in fact, describing the same underlying entity. For instance, a company may make an acquisition and attempt to merge the acquired company's database with their own database. It is probable that there is overlap between the two, and that the acquiring company shares some customers with the acquired company. Similarly, a data aggregator may be attempting to merge together profiles from different public websites, or even social media platforms. Such platforms have heavy overlap, leading naturally to the problem of 'resolving' entities.

This problem, called Entity Resolution (ER), has been researched for at least half a century in text, database, and more recently, machine learning, communities, using several methodologies (e.g. rule-based vs. statistical approaches) [[[27]](#ref-27), [102]](#ref-102). Figure [[1]](#ref-1) illustrates a prototypical example of such 'duplicate' entities that arise in two personal knowledge graphs (PKGs). An exhaustive treatment of this research is beyond the scope of this chapter, although a book level treatment has been provided by various authors, including [[[34]](#ref-34)], [[124]](#ref-124)], [[45]](#ref-45)], [[108]](#ref-108)]. Instead, we consider two goals in summarizing the literature on ER. First, we aim to synthesize common trends that have emerged over the last half-century. Impressively, despite much independent research across various fields and applications, there is robust consensus on several issues, including the abstract workflow of an ER implementation. Second, and in contrast with the first goal, we aim to discuss the key differences that have also emerged from this body of research. As will be

^∗^This preprint will appear as a book chapter by the same name in an upcoming (Oct. 2023) book 'Personal Knowledge Graphs (PKGs): Methodology, tools and applications' edited by Tiwari et al. Research underlying any aspects of this work was originally conducted by the author when he was a PhD student at the University of Texas at Austin, and has been further elaborated in his dissertation [[[32]](#ref-32)].

Author's address: Mayank Kejriwal, kejriwal@isi.edu, University of Southern California, Information Sciences Institute, Marina del Rey, California, USA, 90292.

^©^ 2023 Copyright held by the owner/author(s). Corresponding Author: Mayank Kejriwal

![This image is a graph illustrating a knowledge representation. Nodes represent entities (e.g., "John_Adams," "Microsoft") and relationships (e.g., "employs," "date_of_birth"). Edges connect nodes, showing relationships between entities. The dotted red line depicts a "sameAs" relationship, linking two representations of the same individual. The graph demonstrates how different information about individuals and organizations can be interconnected within a knowledge base.](_page_1_Figure_1.jpeg)

Figure 1. An illustration of the named entity resolution problem between fragments of two personal knowledge graphs (PKGs).

subsequently discussed, many of these differences tend to be algorithmic, rather than conceptual, and are a consequence of the natural evolution of the field over time.

Prior to discussing ER itself, an important prerequisite is deciding the data model for representing the PKG. Although several options exist, we assume the primary data model to be the structured Resource Description Framework (RDF) model [[[88]](#ref-88)], which is prominent in the Semantic Web. An alternative model, for datasets that are highly structured, regular and tabular in nature (which are unlikely for KG applications and domains) is the Relational Database (RDB) model. This model is important for historical reasons, given that much of the ER literature has traditionally been within the database community [[[27]](#ref-27)]. Indeed, there are also cases in the literature where research in the RDB community has been leveraged to solve a compatible problem (e.g. query optimization) on RDF knowledge graphs [[[3]](#ref-3), [111]](#ref-111), [113]](#ref-113)]. Hence, there is good synergy between the two models, allowing us to limit much of our treatment to the RDF model (and others similar to it) without necessarily losing generality. For the sake of completeness, we formally define an RDF graph by first defining an RDF triple:

Definition 1.1 (RDF Triple). Given three disjoint sets of I,B and L, of Internationalized Resource Identifiers (IRIs), abstract identifiers and literals respectively, a triple in the Resource Description Framework (RDF) data model is a 3-element tuple (subject, property, object), where subject ∈ I ∪ B, property ∈ I and object ∈ I ∪ B ∪ L. The triple is referred to as an RDF triple.

Given this definition, an RDF graph can be defined as a set of triples. Visually, the literals and IRIs represent nodes in the KG (which is a directed, labeled graph by definition), while the triple itself symbolizes an edge. Note that literals cannot have outgoing edges in this model. In practice, due to Semantic Web norms, such as the four Linked Data principles [[[8]](#ref-8)], IRIs in RDF KGs are just Uniform Resource Identifiers (URIs), a strict subset of IRIs. This is explicitly required by the first Linked Data principle. Furthermore, abstract identifiers ("blank nodes") are not used in KGs that are intended to be published as Linked Data. Since PKGs, especially those acquired or constructed at large scales, and meant to be linked to other sources, are likely to obey the Linked Data principles (at least approximately), these assumptions almost always hold in practical settings.

While RDF is the dominant data model used for representing KGs in the Semantic Web, it also has other important uses. For example, it is the basis in the full Semantic Web technology stack for representing RDF Schema (RDFS), and Manuscript submitted to ACM

the Web Ontology Language (OWL) [[[2]](#ref-2)]. These semantic markup languages are important for publishing detailed data schemas and ontologies [[[96]](#ref-96)], which serve as the representational metadata for the underlying KGs.

With the assumptions about the data model in place, Named Entity Resolution, henceforth called ER, can be formally defined below:

Definition 1.2 (Named Entity Resolution). Given an RDF knowledge graph, Named Entity Resolution is defined as the algorithmic problem of determining the pairs of instances (subjects and URI objects) in the graph that refer to the same underlying entity.

Entities that need to be resolved but that are not 'named' tend to arise most often in the natural language setting, rather than in KG applications. In the Natural Language Processing (NLP) community, for example, anaphora and coreference resolution are the related problems of resolving pronouns and non-named entities to their named equivalents [[[18]](#ref-18)], [[119]](#ref-119)]. Hence, such non-named mentions are not retained in the actual KG that is constructed over the raw text. Therefore, in practice, named ER is virtually identical to ER in the literature.

A critical point to note here is the notion of an instance in an RDF PKG. For instance, would we consider both literals and URIs to be instances? In general, the approaches that we consider for practical ER assume that an instance must have a representation using a URI, although many URIs may be disregarded from serving as valid inputs to an ER system. For example, based on the specifics of our domain-specific application, we may decide that we only want to resolve instances of 'customers', rather than 'contractors.' If this is the case, then URLs corresponding to contractor-entities would not be in the named entity sets input to the ER system.

A pragmatic reason for not considering literals explicitly is that, if an entity is represented only as a literal (such as a string or a number) and has no other information or attributes associated with it, domain-specific 'matching' functions would be more appropriate rather than an advanced ER solution. Several such functions are available and widely used for common attributes, such as dates, names, and addresses [[[91]](#ref-91)], [[22]](#ref-22)], [[1]](#ref-1)]. For example, if the problem was merely restricted to matching people's names, without any other surrounding context or attributes, a string-matching algorithm, some of which rely on phonetics [[[107]](#ref-107)], could be used. We subsequently provide more details on such matching functions because, aside from being useful for matching simple literals, they are also useful for converting pairs of entity representations into numeric feature vectors.

## 2 TWO-STEP FRAMEWORK FOR EFFICIENT NAMED ENTITY RESOLUTION

Even in early research, the quadratic complexity of pairwise ER was becoming well recognized [[[102]](#ref-102)]. Given two RDF graphs ^1^ and ^2^ that we represent equivalently, with slight abuse of terminology, as sets of named entities (i.e., ignoring edges, literals, and entity-types that are not of interest), a naïve ER must evaluate all possible entity pairs, namely the cross-product ^1^ × ^2^. Even assuming constant cost per evaluation, which may not always be the case, the run-time complexity is (| ^1^ || ^2^ |).

In the remainder of this section, for two input-sets ^1^ and ^2^, a named entity pair (^1^, ^2^) is denoted as bilateral iff ^1^ ∈ ^1^ and ^2^ ∈ ^2^. Given a collection of entities from ^1^ ∪ ^2^, two entities ^1^ and ^2^ are bilaterally paired iff (^1^, ^2^) is bilateral.

To alleviate the quadratic complexity of evaluating all possible bilateral pairs, a two-step framework is typically adopted in much of the ER literature [[[12]](#ref-12)]. This two-step framework is illustrated in Figure [[2]](#ref-2). The first step, blocking, uses a function called a blocking key to cluster approximately similar entities into (potentially overlapping) blocks [[[11]](#ref-11)]. A blocking method then considers which entities sharing a block to bilaterally pair, with the result that those Manuscript submitted to ACM

![The flowchart depicts a knowledge graph (KG) matching process. Two RDF KGs are input, processed via a blocking method (using a blocking key) to create a candidate set of pairs. A trained classifier then assesses the similarity of these pairs, generating `:sameAs` links as output. The diagram illustrates the system's workflow, highlighting the key steps involved in matching entities across different KGs.](_page_3_Figure_1.jpeg)

Figure 2. A typical two-step ER workflow that is often implemented in practice for solving the problem efficiently and effectively. Although the figure assumes two RDF KGs, a similar workflow would also apply to non-KG datasets, as well as to the deduplication problem (where a single dataset is input).

entity pairs become candidates for further evaluation by a matching or similarity function in the second step [[[126]](#ref-126)]. This function, which is also called a link specification function in the literature, may be Boolean or probabilistic, and makes a prediction about whether an input entity-pair represents the same underlying entity. Prediction quality metrics, such as precision and recall, can then be used to evaluate the performance of the entire ER system by comparing the predicted matches to those present in an external ground-truth of 'gold-standard' matches.

In most ER systems, ^1^ and ^2^ are assumed to be structurally homogeneous [[[12]](#ref-12), [27]](#ref-27)], a term introduced in an influential survey [[[27]](#ref-27)]. That is, ^1^ and ^2^ are assumed to contain entities of the same type (e.g., Person), and are described by the same property schema. The latter implies that the same sets of attribute-types are associated with entities in both data sources. An important special application of structural homogeneity is deduplication, whereby matching entities in a single data source must be found. In the rest of this chapter, we assume structural homogeneity as well, except where explicitly indicated. In practice, this assumption is not problematic because, even in the rare case of matching entities between PKGs with drastically different ontologies, an ontology matching solution can be applied as a first step to homogenize the data sources [[[72]](#ref-72), [104]](#ref-104), [117]](#ref-117), [123]](#ref-123)].

## 1 Blocking Step

Following the intuitions described earlier, a blocking key can be defined as follows.

Definition 2.1 (Blocking Key). Given a set of entities, a blocking key is function that takes an entity from as input and returns a non-empty set of literals, referred to as the blocking key values (BKVs) of the entity.

Let () denote the set of BKVs assigned to the entity ∈ by the blocking key . Given two data sources ^1^ and ^2^, two blocking keys ^1^ and ^2^ can be defined using the definition above. Multiple definitions are typically used only when ^1^ and ^2^ are heterogeneous. Since we are assuming structural homogeneity in this chapter, a single key (namely, ^1^ = ^2^ = ), applicable to both ^1^ and ^2^, is assumed. Without loss of generality, the literals defined above are assumed to be strings although, in principle, any data type could be used.

Example 2.2 (Example of Blocking Key). Earlier, Figure [[1]](#ref-1) had illustrated two RDF PKG fragments describing people. Although the graphs are structurally heterogeneous, let us assume that an ontology or schema matching step had Manuscript submitted to ACM

been applied such that the schema of the second graph is appropriately aligned with that of the first (e.g., DOB is mapped to date_of_birth, and so on). An example of a good blocking key K applicable to this schema might now be = (: ) ∪ (: \_ \_ℎ). We assume that there is a mechanistic function that can extract the year from the date of birth literal, and that ':instance' represents the mnemonic representation (typically a label in the RDF graph) of the instance. Applied to an entity from either dataset, K would return a set of BKVs that contains the tokens in an entity's label, as well as a single number for the year of birth, converted to a string. For example, when applied to the instance John_Adams from the first KG in Figure [[1]](#ref-1), the output (set of BKVs) returned by the blocking key would be {"John", "Adams", "1998"}. Similarly, when applied to the instance J. K. Adams from the second KG, the output returned would be the set {"J", "K", "Adams", "1998"}. Since the two BKV sets have at least one common BKV, the two instances would share at least one block.

Given the single blocking key , a candidate set of bilateral entity pairs can be generated by a blocking method using the BKVs of the entities. We briefly describe some prominent blocking methods below.

Example 2.3 (Sorted Neighborhood). Figure [[3]](#ref-3) illustrates the Sorted Neighborhood blocking method on a small relational database describing people. A single BKV is first generated (in this case, we concatenate the initials of first and last-name tokens, and the first two digits of the zipcode) for each instance in the table. Next, the BKVs are used as 'sorting keys'. Finally, a sliding window is slid over the table and all records within the window are paired and added to the candidate set of pairs that is input to the similarity step. For example, assuming a sliding window of 4, record pairs (1,2),(1,3), (1,4), (2,4), (3,4) and (2,3) are added to the candidate set C in the first sliding iteration, because the records with IDs 1, 2, 3 and 4 fall within the first window. The window slides forward by one record, and in the second iteration, new record pairs (2,5), (3,5) and (4,5) are added to C. The method terminates when the window cannot slide any further.

Besides Sorted Neighborhood and simpler blocking methods, such as simple indexing (called 'traditional blocking') clustering methods, such as Canopies, have also been successfully used for blocking [[[6]](#ref-6), [95]](#ref-95)]. The basic algorithm takes a distance function and two threshold parameters tight ≥ 0 and ≥ ℎ. It operates in the following way for deduplication (where duplicate entities must be detected in a single data source):

- (1) A 'seed' entity is chosen randomly from the set of all entities in the PKG. Let us denote this entity as .
- (2) A linear-time distance-based search is conducted in the feature-representation space and all other entities in the PKG that have distance less than to are placed into a 'canopy' represented by .
- (3) Using the results from the above search, entities with distance less than ℎ to are removed from further consideration.
- (4) The three steps above are repeated until each entity in the PKG has been assigned to at least one canopy.

In the worse case, there could be entities that represent a singleton-canopy i.e., no other entities have been assigned to them. An important point to note in the above steps is that entities that have distance less than ℎ to also have distance less than by definition. Thus, before being removed from further consideration, they are guaranteed to have been assigned to at least one canopy (represented by ). Indeed, not including the extreme (and usually, rare) case of singleton canopies, the standard behavior for most entities is that they will be assigned to at least one canopy.

Furthermore, it is easy to extend the method to the two-PKG (or even multi-PKG) case by using the entities from the smallest PKG (or some other such well-defined selection rule) exclusively as seed entities. This extension has the added advantage of rendering the algorithm deterministic, since the randomization inherent in the original version above (which determines both the set and order of seed entities ultimately considered) is no longer present. We note Manuscript submitted to ACM

## 6 Kejriwal

![The image illustrates a data processing algorithm. Two tables show personal data (First Name, Last Name, Zipcode, Blocking Key Value). A "slide window" technique processes the data in stages, creating a "candidate set C" which is a set of pairs of data. The algorithm adds pairs from the sliding window to the candidate set iteratively, as shown in the flow chart. The figure demonstrates the step-wise construction of the candidate set, highlighting the algorithm's operation.](_page_5_Figure_1.jpeg)

Figure 3. An illustration of the Sorted Neighborhood blocking method.

that the theoretical or empirical properties of the Canopies algorithm, similar to other blocking algorithms, has not been well explored in the multi-PKG case where more than two PKGs have to be resolved. Indeed, proper algorithmic architectures for multi-PKG ER remains an under-addressed problem in the AI and database literature.

Unlike an algorithm such as Sorted Neighborhood, the Canopies method is not actually dependent on an 'explicit' blocking key or scheme. However, that does not mean it is assumption-free. The choice of the feature space, and the distance function used, are both important decisions that serve as proxies for the blocking key required by more traditional approaches preceding Canopies. Hence, at least one paper in the literature [[[94]](#ref-94)] has referred to methods like Canopies as instance-based blocking as opposed to feature-based blocking, of which both Sorted Neighborhood and traditional blocking are paradigmatic examples.

However, this is not to imply that Canopies and Sorted Neighborhood do not share complementary features, both in their usage and in their developmental history. Both have been enormously popular in the ER community, and variants and versions of both, each professing to be beneficial for different use cases and datasets, have been described over the decades. That being said, the original version continues to be heavily used [[[11]](#ref-11)], and is a popular choice when the distance function is well defined (which is often the case for text data, since functions such as Jaccard or Cosine can be efficiently used [[[6]](#ref-6)]) or when it is very efficient to execute (such as certain distance functions in Euclidean space).

One example of a variant is the use of a nearest-neighbors approach, rather than a thresholding approach, for determining which entities to assign to a canopy and to remove from further consideration. Another variant, which shares chaarcteristics of a feature-based blocking method, is to use an explicit blocking key to first generate BKVs for each entity. Canopies is then applied, not to the entities themselves, but to the BKVs of the entities [[[11]](#ref-11)]. If more than one BKV is possible per entity, a set-based function is required. Thus, this method is considerably more complex than the original method, but may have advantages in specific use cases.

What is the relationship between canopies and blocks? As might be evident from the terminology itself, a canopy itself can be thought of as a block. Because canopy 'blocks' can be overlapping, the blocks are different from those found in traditional blocking where it is common to assume that blocks are not overlapping (i.e., that nodes can be assigned to at most one block, or that BKV sets always have at most size one).

Some techniques have been proposed in both the ER literature (but also beyond) that could potentially be considered as 'alternatives' to blocking. A method that is especially noteworthy in this regard is Locality Sensitive Hashing (LSH). In recent times, LSH has become popular in the Big Data community for presenting an efficient (albeit, approximate) solution to the important problem of nearest-neighbors search in spaces that have high dimensionality [[[15]](#ref-15)]. Specifically, given a an LSH family of hash functions may be defined using five parameters: (i) a distance measure , (ii) two 'radii' denoted as and , with < , (iii) two probabilities (, with > ) associated with and as suggested by the terminology.

An LSH family is considered to be (, , , )-sensitive if it is the case that any point falling inside a sphere with radius centered at point (note that the computation of this obviously depends on the feature space and ) necessarily has the same hash as with a minimum probability of , dependent on a probability distribution defined on the hash family. Furthermore, it is also required that if does not fall within a similarly defined sphere, but with radius , the probability that and have the same hash is at most [[[15]](#ref-15)]. Such (, , , )-sensitive families are widely used in practice, and a number of them have been defined in recent papers, because highly efficient algorithms can be designed to hash big datasets and determine (with some probability) when two points are very close to (or far from) each other.

One can see the application of this algorithm to blocking under the right conditions, as the goal of blocking is very similar to that of LSH. Indeed, an LSH-like algorithm is directly applicable to instance-based blocking methods like Canopies that rely explicitly on distance functions rather than blocking keys. However, it must be borne in mind that the requirements of LSH are stricter, whereas Canopies can be used with any well-defined distance measure. A good example of a distance function which is amenable to LSH is the Jaccard function, which can be approximated through the MinHash function (which has the requisite properties and sensitivity defined earlier). Certainly, despite its rigid assumptions, LSH should be considered as a viable baseline for more complex blocking methods. Beyond ER, it has also been applied to problems such as ontology alignment [[[17]](#ref-17)], and on occasion, it has also been applied to the similarity step of ER.

## 2 Similarity / Matching Step

Although the expectation is that the candidate set generated by blocking contains most (and in the ideal situation, all) of the true positives (duplicate entity pairs) present in the PKGs being resolved, its approximate and efficient nature also leads to many true negatives being present^1^. Additionally, a few false negatives may get 'excluded' from the candidate set. In a subsequent section, we discuss the evaluation of blocking to measure the extent to which this is taking place. Note that, once excluded, there is no hope of recovering 'false negatives' (except to re-execute a different blocking algorithm or two-step workflow on the entire dataset) in the similarity step. However, a 'good' similarity function can distinguish between true positives and true negatives effectively, leading to better results in the overall two-step workflow. Indeed, it is easy to see that the similarity function must be finer-grained, compared to blocking, as it needs to make that distinction, which blocking proved incapable of doing due to its focus on efficiency rather than effectiveness [[[27]](#ref-27)].

While the similarity function is referred to differently in different communities, within the Semantic Web, it is often referred to as a link specification function [[[126]](#ref-126)]. We adopt similar terminology here due to the chapter's focus on PKGs rather than relational databases or other similar data models.

Definition 2.4 (Link Specification Function). Given two data sources ^1^ and ^2^, a link specification function is a Boolean function that takes as input a bilateral pair of entities, and returns True iff the input entity pair refers to the same underlying entity (i.e., is a duplicate pair) and returns False otherwise.

Because the underlying real-world link specification function, if it even exists analytically, is not known or discoverable given small sets of duplicate examples, it has to be inferred or learned. In some cases, domain knowledge is used to devise sets of rules as a viable link specification function. In all but the most trivial cases, the function is approximate and unlikely to be perfect. In the treatment below, we assume the 'link specification function' to mean the approximated version of the function rather than the unknown underlying function. Note that the function can be real-valued i.e., given a pair of entities, it may return a value in [0, 1], which can be properly interpreted as a score or the model's belief that the pair represents a duplicate. It is also not uncommon to have 'hardened' link specification functions that would just return 0 or 1.

With the preliminaries above, the similarity step in the two-step workflow can be thought of as the application of to each pair in the candidate set [[[12]](#ref-12), [27]](#ref-27)]. If is real-valued, further assumptions are required to make the distinction between duplicates and non-duplicates. A simple approach is to use a threshold (determined heuristically, or through a set of 'development set' duplicates and non-duplicates set aside for exactly this purpose): pairs in the candidate set for which returns a score greater than the threshold are classified as duplicates and similarly for non-duplicates. In theory, one can even have two thresholds (denoted as and ), not dissimilar to blocking methods like Canopies. Pairs with scores that fall between the thresholds are usually considered to be indeterminate and flagged for manual review, or (in more complex systems) undergo processing by a more expensive, and separately trained, link specification function.

Many link specification functions have been explored by researchers through the decades, including in the expert systems community, machine learning, natural language processing, Semantic Web, and databases. For a good survey

^1^ Interestingly, the issue of false positives does not arise, because the similarity step has to make the final decision on what constitutes a positive. Hence, it does not make sense to determine which pairs are 'false' positives since one similarity function may falsely declare a pair to be a duplicate (hence, false positive) while another may not. True positives are a subset of the pairs in the ground-truth set of duplicates and must be included in blocking for any similarity function to make that determination.

Manuscript submitted to ACM

| Bob's pizza place | 1223 W. 290 St. | 4.97 / 5 stars | Parking available |
|---|---|---|---|
| Bob's pizza | 290 ^th^ & Normandie | $4.96/5$ stars | Parking available but<br>limited |
| StringSim<br>0. | StringSim ~4~<br>0.9 | **StringSim~1~** | StringSim |

Figure 4. Conversion of an entity pair to a feature vector that could then be used in ordinary classification systems (e.g., deep neural networks).

of such techniques in databases (primarily), we refer the reader to [[[27]](#ref-27)], as well as a book on data matching [[[12]](#ref-12)]. Below, we provide more details on a machine learning-centric approach to the problem.

If using machine learning to derive the function , the most important step is to determine how to extract a feature vector (whether real-valued or discrete) given a pair of entities. The procedure is illustrated schematically in Figure [[4]](#ref-4), for the case where both entities (represented as records) share the same schema or ontology. Recall that we referred to this common case as one exhibiting structural homogeneity. As the figure illustrates, a library of feature functions is assumed. The feature function is like a 'primitive': it takes a pair of primitive data types (such as strings or dates) as input and returns a numeric feature. Without loss of generality, let us assume that the output is always real-valued. Given such a library of feature functions, a pair in the candidate set can be converted to a well-defined and fixed-length feature functions. Indeed, if we assume feature functions, and such 'fields' representing the schema of the entity, the feature vector would have dimensions^2^.

Fortunately, many feature functions are available to practitioners. Some have been known for almost a century, while others are more recent. Primitive data types processed by these functions include strings, tokens, and even numbers. In some cases, the semantics of the field can play a role. For instance, phonetic feature functions are an excellent fit for strings that represent names. Good descriptions may be found in a variety of sources, including [[[27]](#ref-27)] and [[[12]](#ref-12)]. Domain-specific feature functions have also been proposed in the literature, but may be more difficult to locate or implement. Interestingly, neural networks have been used frequently in recent years to bypass the application of hand-picked feature functions. For example, the work in [[[71]](#ref-71)] considers the use of such representation learning for extracting features from geographically situated entities represented using latitudes and longitudes, rather than strings or other descriptive information.

An alternative way of extracting features is by generating hashes using several well-known LSH families [[[58]](#ref-58)]. According to this model of feature generation, the underlying link specification function can be modeled through a functional combination of various distance measures for which LSH-sensitive families exist. A validation of this model

^2^Note that this could also be the case when an entity is missing a value for a given field, or if the feature function fails for some reason. Such cases could be encoded by using a special value (such as -1) in its position in the feature vector. Such preprocessing is important, and largely a function of robust engineering, and ensure that the (ordered) feature vector always has well-defined values. Variable length feature vectors are typically very difficult for the majority of machine learning algorithms to handle.

would be consequential as it significantly eases the burden of scalability, both in the blocking and similarity steps. In the most general case, the hashes could be used as features, and an appropriate learner would be used for discovering an explicit functional combination (or rules) for class separation. The process and evaluation are described in more detail in Chapter 7 of Kejriwal (2016).

A machine learning classifier is trained on positively and negatively labeled training samples, and is used to classify vectors in the candidate set. Several classifiers have been explored in the literature, with random forest, multilayer perceptron and Support Vector Machine classifiers all found to perform reasonably well [[[58]](#ref-58), [110]](#ref-110), [118]](#ref-118)]. In more recent years, neural networks have also been applied [[[20]](#ref-20)].

## 3 Independence of the Blocking and Similarity Steps

The two-step workflow, and our description of blocking and similarity, seems to suggest that the two steps are largely independent. Historically, and even in actual practice, this has been the case, and it is even possible to 'swap' out different blocking algorithms while keeping the similarity step constant (and similarly for swapping the similarity modules while keeping blocking constant). However, it behooves us to mention that nothing prevents blocking and similarity from being interlocked. Namely, the two steps can be set up such that they 'interact' in a real system i.e., we do not need the candidate set to be completely generated prior to executing the similarity step. A simple example of this is a system that tries to conserve space by not storing the candidate set explicitly, but piping pairs to the similarity step as they are generated by blocking. The system still maintains the assumption, however, that decisions made in the similarity step have no impact on blocking (i.e., there is no backward feedback). Furthermore, depending on the blocking method used, there may be a loss in overall efficiency, since the same pair may be classified several times by the similarity step (as we are not maintaining a set-based data structure). Even in this simple example, the tradeoffs between time and space efficiency must be carefully negotiated.

A small, but growing, number of applications in recent years have also been challenging the backward-feedback assumption we mentioned above, example references being [[[106]](#ref-106), [127]](#ref-127)]. An example of a blocking method that takes backward feedback into account is comparisons propagation, which tries to use decisions made in the similarity step to estimate, in real time, the utility of a block [[[105]](#ref-105)]. Intuitively, if a block is yielding too many non-duplicates (according to the similarity function), then it may be best to cut losses and stop processing the block rather than continue processing it and generating more 'useless' pairs for the similarity step to classify as non-duplicates. In a rational decision-making setting, the expected gain from processing the block any further is outweighted by the loss in efficiency, making it appropriate to discard it.

While such techniques are promising, their implementations have mostly been limited to serial architectures, owing to the need for continuous data-sharing between the similarity and block generating components [[[105]](#ref-105), [127]](#ref-127)]. Experimentally, the benefits of such techniques over independent techniques like Sorted Neighborhood or traditional blocking (with skew-eliminating measures such as block purging) have not been established extensively enough to warrant widespread adoption. Therefore, the two-step workflow, with both steps relatively independent, continues to be predominant in much of the ER research [[[89]](#ref-89)].

## 3 EVALUATING NAMED ENTITY RESOLUTION

The independence of blocking and similarity suggests that the performance of each can be controlled for the other in experiments [[[27]](#ref-27)]. In the last decade both blocking and similarity have become increasingly sophisticated. It is now the rule, rather than the exception, to publish either on blocking or on similarity within an individual publication [[[11]](#ref-11)]. Manuscript submitted to ACM

Despite some potential disadvantages, this methodology has yielded the adoption of well-defined evaluation metrics and standards for both steps.

## 1 Evaluating Blocking

The primary aim of blocking is to scale the naïve similarity-only ER system that bilaterally pairs all entities with one another. Blocking serves this goal by generating a smaller candidate set than this 'exhaustive set'. However, if time-complexity reduction were the only goal, optimal blocking would simply yield the empty set as the candidate set. Such a system would ultimately be without utility because it would generate a candidate set with zero duplicates coverage.

In other words, duplicates coverage and candidate set reduction are two competing goals that every blocking technique (blocking key and blocking method) aims to trade off between [[[24]](#ref-24)]. This tradeoff can also be formalized. As a first step, let us denote Ω as the set ^1^ × ^2^; in other words, the exhaustive set of all bilateral pairs. Let Ω denote the subset of Ω that contains all (and only) matching entity pairs. Ω is designated as the ground-truth (equivalently, gold standard). Finally, we use C to denote the candidate set generated by applying the blocking method. Using this notation, Reduction Ratio (RR) can be defined using the formula below:

$$
RR = 1 - \frac{|C|}{|\Omega|} \tag{1}
$$

A higher RR implies greater time-complexity reduction (and depending on the infrastructure, space-complexity reduction) achieved by the blocking method, compared to generating the exhaustive set [[[11]](#ref-11)]. Although less commonly used for this purpose, RR can also be evaluated relative to the candidate set of a baseline blocking method [[[106]](#ref-106)]. The only change required in the formula above is to replace Ω with the candidate set of the baseline method. In its relative usage, such an RR would have positive value if the blocking method resulted in greater savings compared to the baseline method; otherwise, the relative RR would be negative.

Importantly, even minor differences in RR can have an enormous impact in terms of run-time because of its quadratic nature. For instance, consider the case where Ω contains 10 million pairs (a not unreasonable number that could be achieved if the two data sources only had around 1,000-2,000 instances each). An improvement of even 0.1% on the RR metric would imply savings of thousands of pairs. The same percentage could represent millions of pairs on even larger datasets.

While RR is a good way of measuring the complexity reduction goal of blocking, the Pairs Completeness (PC) metric, defined below, quantifies the method's duplicates coverage:

$$
PC = \frac{|C \cap \Omega_M|}{|\Omega_M|} \tag{2}
$$

Interestingly, the PC serves as an upper bound on the recall metric that is used for evaluating overall duplicates coverage in the ER system (i.e., after the similarity step has been applied on the candidate set). For example, if PC is only 70%, meaning that 30% of the duplicate pairs did not get included in the candidate set, then the ER system's overall recall can never exceed 70%.

While not theoretically necessary, in practice, there is a tradeoff between achieving both high PC and RR. In most blocking architectures, the tradeoff is negotiated by tuning one or more relevant parameters. For example, the sliding window parameter w in Sorted Neighborhood (illustrated earlier through an example) can be increased to achieve higher PC, at the cost of lower RR [[[24]](#ref-24)].

Although we can always plot PC-RR curves to visually demonstrate the tradeoff in a blocking system, a single number is desired in practical settings. In the literature, this number is usually just the F-Measure, or harmonic mean, between the PC and RR:

$$
F-Measure = \frac{2.PC.RR}{PC+RR}
$$
(3)

A second tradeoff metric, Pairs Quality (PQ), is less commonly known than the F-Measure of PC and RR in the wider community, but can be illuminating for comparing different blocking architectures:

$$
PQ = \frac{|C \cap \Omega_M|}{|C|} \tag{4}
$$

Theoretically, PQ may be a better measure of the tradeoff between PC and RR than the F-Measure estimate, which weighs RR and PC equally, despite the quadratic dependence of the former. For this reason, PQ has sometimes been described as a precision metric for blocking [[[11]](#ref-11)], although the terminology is superficial at best. The intuitive interpretation of a high PQ is that the generated blocks (and by virtue, the candidate set C) are dense in duplicate pairs. A blocking method with high PQ is therefore expected to have greater utility, although it may not necessarily lead to high precision or recall from the overall ER architecture.

PQ can sometimes give estimates that are difficult to interpret. For example, suppose there were 1,000 duplicates in the ground-truth, and the candidate set C only contained ten pairs, of which eight represent duplicates. In this example, PQ would be 80%. Assuming that the exhaustive set is large enough, such that RR is near-perfect, the F-Measure would still be less than 2% (since PC is less than 1%). The F-Measure result would be correctly interpreted as an indication that, for practical purposes, the blocking process has failed. The result indicated by PQ alone is clearly misleading, suggesting that, as a tradeoff measure, PQ should not be substituted for the F-Measure of PC and RR. An alternative, proposed by at least one author but not used widely, is to compute and report the F-Measure of PQ and PC, instead of PC and RR [[[11]](#ref-11)].

## 2 Evaluating Similarity

As similarity is most similar to a machine learning classification problem (such as sentiment analysis), the best way to evaluate it given the real-world requirements of ER is to compute precision and recall. Specifically, once a candidate set is output by blocking and processed by similarity, the final expected output is a partition of into sets and of pairs of resolved ('duplicated') and non-resolved ('non-duplicates') entities.

Given a ground-truth set Ω (using the terminology of the previous section) of actual duplicate entity-pairs, the true positives (TPs), false negatives (FNs), and false positives (FPs) can be computed in the usual way. Precision is just the ratio of TPs to the sum of TPs and FPs, while recall is the ratio of TPs to the ratio of TPs and FNs. Like most machine learning problems, optimizing precision in an ER application can come at the cost of optimizing recall. One way to measure the tradeoff is by plotting a Receiver Operating Characteristic (ROC), which plots true positives against false positives [[[23]](#ref-23)]. However, it is simpler to obtain a single-point estimate of this tradeoff by computing the harmonic mean (F-Measure) of precision and recall. Note that if F-Measure is used to evaluate similarity, as well as blocking (along the lines discussed in the previous section), a clear distinction must be noted between the two when reporting the results, as they are measuring tradeoffs between different quantities. An alternative to a single-point estimate is a precision-recall curve that, although related in a complex way to the ROC curve, expresses the tradeoff much more directly at different (precision, recall) points. Historically, and currently, precision-recall curves dominate ROC curves in the ER community [[[89]](#ref-89), [90]](#ref-90), [99]](#ref-99)].

![This timeline chart displays the chronological development of entity matching systems from 2007 onwards. It shows various systems categorized by their year of introduction. "Linked Open Data" is the earliest entry, followed by systems like "Silk," "Limes," and others, progressing to more recent approaches such as "DeepER" and "LIME". The chart illustrates the evolution of techniques in the field.](_page_12_Figure_1.jpeg)

Figure 5. A timeline of approaches (primarily for the similarity step) that have evolved to address ER.

We emphasize that computing a measure such as accuracy is usually not a good idea. The reason is that the vast majority of pairs are expected to be non-duplicates, meaning that if a system were to predict every pair in the candidate set to be a non-duplicate pair, the resulting accuracy would be very high! Such a measure is clearly without utility. Precision and recall preempt this problem because neither takes true negatives into account in the computation.

## 4 EVOLUTION OF RESEARCH IN NAMED ENTITY RESOLUTION

Owing to its 50-year history, many systems and research methods have been proposed for ER. Figure [[5]](#ref-5) shows how different streams of thought have emerged over the decades. While necessarily simplified (e.g., many of these classes of approaches have overlapped, and some are still widely used), the figure largely tracks what has happened in the broader AI community and its many applications. For the more technical reader, we recommend books on both ER and knowledge graphs (which tend to contain chapters on ER) [[[13]](#ref-13), [34]](#ref-34), [45]](#ref-45), [124]](#ref-124)].

In the Semantic Web community, rule-based systems were considered state-of-the-art [[[126]](#ref-126)], and are still widely used, in the immediate aftermath of Linked Open Data (which opened up ER as an important problem in that community). However, due to their many advantages, machine learning approaches have gained in popularity over the last decade. Such approaches, primarily supervised in nature, take training pairs of duplicates and non-duplicates and learn optimize parameters in a given hypothesis space to achieve high performance on an unseen test set. However, we note that rule-based and machine learning approaches are not necessarily exclusive or independent of one another. Indeed, a number of interesting hybrid approaches have been proposed, some of which are named in Figure [[5]](#ref-5). Also of interest are 'low supervision' approaches that rely on techniques such as active learning [[[53]](#ref-53), [55]](#ref-55), [103]](#ref-103)]. Deep learning has also been applied with some success to ER [[[20]](#ref-20)].

Beyond the similarity step, where machine learning has been applied and studied much more extensively, methods have been proposed in the last two decades to learn blocking keys given training data of duplicates and non-duplicates [[[31]](#ref-31), [37]](#ref-37), [48]](#ref-48), [50]](#ref-50)]. More recently, such approaches have also been tried for heterogeneous data collections [[[52]](#ref-52)]. This was quite a departure from the traditional approach, which was to manually specify blocking keys that seemed 'intuitive'. As multiple approaches have now showed, a systematic approach to learning blocking keys can lead to non-trivial savings in complexity reduction, or in improved PC and PQ. The technique for learning blocking keys is less straightforward than training classifiers for similarity, but theoretically similar to problems such as set covering [[[10]](#ref-10)]. Along with blocking scheme learning, research also continues on developing new blocking methods (or variants of existing methods, like Sorted Neighborhood) for novel ecosystems such as Linked Open Data [[[56]](#ref-56)].

## 5 CHALLENGES AND OPPORTUNITIES FOR NAMED ENTITY RESOLUTION

Despite being a 50-year old problem, and improvements in Artificial Intelligence and deep learning technology, ER remains a challenging problem due to the rapid growth of large and heterogeneous datasets published on the Web. In the previous section, a critical challenge that was described is the non-obvious ability of existing systems to simultaneously address challenges such as domain-independence, scalability and heterogeneity. At the same time, in the existing AI literature, there are theoretical and applied mechanisms to address some of these challenges. A machine learning paradigm such as transfer learning, for example, could potentially be used to handle the domain-independence requirement by first bootstrapping an ER system in a few 'anchor' domains, and then using transfer learning to adapt the system to other domains. In theory, such an approach seems feasible, but in practice, it is very difficult to execute. Some other challenges, which were also expounded upon in [[[32]](#ref-32)] are:

- (1) Schema-free approaches to ER: The increased diversity and heterogeneity of PKGs published on the Web indicates that the present time is a good one for further investigating so-called schema-free approaches to ER. We note that the traditional approaches have been primarily inspired by the Relational Database community, which tends to assume some form of schema or ontology matching in order to 'align' the types of instances before processing them further (e.g., through the two-step workflow). Conventionally, many algorithms considered such a homogeneous type-structure as a given, for both qualitative and computation reasons. We already saw in the previous sections that many algorithms only seem to apply if the structural homogeneity condition is met. However, more recently, there has been much literature calling this assumption into question and seeking to extend methods to work without it. In some of our own papers, we suggested a schema-free implementation of the classic Sorted Neighborhood algorithm that is specifically designed for RDF KGs, and empirical results showed that the method compares favorably to the more established baseline [[[54]](#ref-54), [57]](#ref-57)]. However, schema-free approaches to ER remain relatively novel in the community, and many conceptual and methodological questions remain. For example, how can we tell which schema-free features are of 'good' quality, and construct such feature functions effectively? Can deep learning play an important role to automate such construction? Finally, could such approaches allow us to bypass ontology matching altogether prior to ER? And how do we apply such approaches to domains that are not completely structured, but contain a mix of structured data and free text (of which social media applications, augmented with user meta-data, are good examples [[[25]](#ref-25), [41]](#ref-41), [84]](#ref-84), [87]](#ref-87)])?
- (2) Improving Linked Data Quality: We discussed earlier that an important prior step, often treated as an assumption in core ER research, to conducting Named Entity Resolution is to first determine which types of nodes between two KGs should be aligned. In the deduplication scenario (as opposed to resolving entities in two or more PKGs), this problem can also arise. For example, if an individual is both an author and professor, and these are two types in the KG, then it is plausible that two instances of the individual are present in the PKG. Only executing an ER algorithm on the set of professor instances, or on the set of author instances (independently) is unlikely to yield high-quality results. Hence, proper type alignment is generally necessary to find the balance between running the two-step ER workflow on the full set(s) of nodes [[[59]](#ref-59), [75]](#ref-75)], even with blocking, and being too restrictive in which pairs of types should be considered as 'aligned' for the purposes of being eligible for such processing.

On the Web, the problem of type alignment is exacerbated because of the quality of Linked Data. Many different types of entities are present, and it is not always evident whether one type should be aligned with another. A related problem is property alignment. Because this problem particularly arises with 'cross-domain' or

Manuscript submitted to ACM

encyclopedic KGs, such as DBpedia (which contains over 400 types), improving the quality of Linked Data published online, as well as re-using existing ontological types, is an important future direction. One reason why this continues to be challenging is that there is also ambiguity surrounding the construction of good ground truths for highly complex types. In a preliminary experimental work that we published [[[59]](#ref-59)], we found that that there may be at least three different ways (both inductive and deductive) of constructing reasonable ground truths, which are not always consistent. This can cause an extra layer of noise when evaluating type alignment (and following that, ER) systems. There is reason to believe that such noise is not merely hypothetical but actually exists in real Linked Open Datasets currently on the Web [[[32]](#ref-32)]. On the positive front, because there are far fewer types than entities and properties, improving their quality and enforcing better publishing standards can have lead to outsize progress in addressing these issues. In turn, adoption of Linked Data improves and more ambitious data integration applications at Web scale become feasible.

- (3) Transfer Learning: Transfer learning, surveyed in [[[9]](#ref-9)] (among many other papers), is a valuable avenue to pursue for resolving entities in large Linked Open datasets without requiring enormous amounts of training data. Although transfer learning (even for ER) is not a completely novel line of research [[[5]](#ref-5)], it has not found widespread utility yet in ER due to several technical challenges, and relatively low quality compared to fully supervised approaches. Even in other machine learning research, its progress is not completely evident, although it bears saying that it has continued to be researched actively in several mainstream applications [[[100]](#ref-100)]. However, recent advent of large language models like BERT and ChatGPT suggest that feasible transfer learning solutions may be around the corner. 'Pre-trained' versions of such models, which can be downloaded off-the-shelf and fine-tuned on specific datasets at relatively low cost [[[16]](#ref-16)], contain much background knowledge that could boost performance on ER datasets. At the same time, these models also have some problems (such as lack of explainability but also others [[[67]](#ref-67), [114]](#ref-114), [115]](#ref-115), [122]](#ref-122)]) that may pose problems in domains where a high degree of confidence is needed. Beyond classic transfer learning, other novel learning approaches, such as zero-shot and few-shot learning, also continue to be investigated for difficult machine learning problems [[[109]](#ref-109), [128]](#ref-128)]. Such techniques could prove to be essential for ER, especially with the advent of neural language models [[[16]](#ref-16)].
- (4) ER in PKGs versus Personal Knowledge Networks (PKNs): Although KGs have witnessed increased adoption in multiple domains over the last ten years, there has been a similar (and largely independent) rise in the field of network science. Network science has proven to be particularly powerful in understanding complex systems [[[68]](#ref-68)], especially those where relational structure plays an important role. A classic example is social networks, which bears close resemblance to PKGs. In recent years, it has also been applied to understanding other interesting and high-impact social domains, such as illicit finance [[[40]](#ref-40)], economics [[[47]](#ref-47)], international geopolitics and humanitarian applications [[[35]](#ref-35), [60]](#ref-60)], e-commerce [[[63]](#ref-63), [69]](#ref-69), [70]](#ref-70)], social media analytics [[[92]](#ref-92), [93]](#ref-93), [97]](#ref-97), [98]](#ref-98)], crisis informatics [[[85]](#ref-85)], misinformation and sensationalism detection [[[129]](#ref-129)], and human trafficking [[[26]](#ref-26), [43]](#ref-43), [44]](#ref-44)]. ER in the network domain is far less studied, as has its connections to ER in KG-centric communities like Semantic Web. Visualization of outputs, designing of better interfaces, and efficient human-in-the-loop tooling, both in network science and Semantic Web, remains under-studied as well [[[42]](#ref-42), [66]](#ref-66), [76]](#ref-76), [77]](#ref-77), [86]](#ref-86)]. At the same time, there is clearly a vital connection, and not just because PKGs and networks can both be represented as graphs. To take the human trafficking domain as an example, KGs have also been applied to the problem with significant success [[[36]](#ref-36), [79]](#ref-79)–[81]](#ref-81)]. Another example is e-commerce [[[4]](#ref-4), [19]](#ref-19)].
- (5) Theoretical Progress in ER as an 'AI-Complete' Problem: Named Entity Resolution is an inherently practical problem, but a theoretical understanding of the problem could open up new algorithmic directions. An example Manuscript submitted to ACM

of such a treatment is the Swoosh family of algorithms [[[7]](#ref-7)]. More recently, with an increased focus on Artificial General Intelligence (AGI), there is an open question as to whether ER can be viewed as an 'AI-complete' problem i.e., solving ER with sufficient accuracy and robustness may provide concrete evidence that we have made definitive progress on AGI. Examples of AI-complete problems include commonsense reasoning [[[61]](#ref-61)] and knowledge representation [[[116]](#ref-116)], open-world learning (including reinforcement learning techniques for open-world learning [[[21]](#ref-21), [83]](#ref-83)]). These problems, if solved, could revolutionize the applications of AI in a variety of domain, and become the underlying basis for industries of the future [[[39]](#ref-39)]. However, much theoretical work needs to be done before any of these claims can be validated with certainty.

Beyond developing better theoretical foundations, one must also bear in mind that ER does not exist in isolation, but that noise in real-world ER systems may have close connections to noise in other steps of the KG construction and refinement pipeline, such as information extraction, data acquisition [[[38]](#ref-38), [65]](#ref-65), [125]](#ref-125)] and general knowledge capture [[[82]](#ref-82)]. Scalable ER, especially in massive distributed ecosystems like schema.org [[[64]](#ref-64), [101]](#ref-101), [112]](#ref-112)] or other Web-scale and 'Big Data' applications [[[28]](#ref-28), [29]](#ref-29), [33]](#ref-33)], also remains an important under-addressed problem in the research community. This problem is only likely to get worse as advanced extensions to Linked Open Data [[[78]](#ref-78)] and other such ecosystems are proposed in the years to come (in part, due to the advent of large language models). In the network science community, there has also been increased focus on algorithmic scalability [[[121]](#ref-121)].

(6) Other Applied Directions: As PKGs are used in different ways in different domains and use-cases, the issue of properly building domain-specific PKGs (of which ER is an important component) remains critical and is an important application of the general research direction of domain-specific KG construction [[[120]](#ref-120)]. Semantic search, especially in domain-specific applications [[[46]](#ref-46)], is another important direction as it will likely require advances in domain-specific search [[[62]](#ref-62), [73]](#ref-73)]. Because of the rise of large language models, and deep learning more generally, we believe that an ER approach that melds the best of traditional approaches to the problem with deep learning could lead to significant advances in upcoming years [[[74]](#ref-74)]. This is true for KG research, more generally [[[14]](#ref-14)]. An example of such an application is hypothesis generation and geopolitical forecasting using (for example) multi-modal KGs rather than directly using raw text or video sources [[[130]](#ref-130)]. Finally, many applications require processing and conducting ER on datasets that are structurally heterogeneous. Even today, structural homogeneity remains a strong assumption in the ER community. Removing this assumption from future approaches is a promising direction [[[30]](#ref-30)], but is not without challenges [[[31]](#ref-31), [49]](#ref-49)].

## 6 CONCLUSION

Named Entity Resolution is an important application that has been researcher for almost half a century. While the early applications of ER were limited to patient records and census data, they have proliferated in an era of Big Data and open knowledge. It is likely to continue playing an important role in Artificial Intelligence efforts in both government and industry in the near- and long-term future, especially in data-intensive text applications. As a recent book discussed, industry is now starting to invest intensively in AI systems to grow or maintain a competitive advantage, and open assets such as Linked Data and off-the-shelf large language models are important drivers in such implementations. Because of the open nature of these assets, there is a more level playing field than was historically the case. Properly implementing, scaling, and evaluating advanced algorithms, and ensuring that the data is 'clean', may well provide the edge in many cases. For most datasets, achieving such quality requires concerted effort in ER. 'Properly' in this context also implies gaining a better understanding of bias in training or evaluating an ER model [[[51]](#ref-51)], and other weak spots of the model.

Named Entity Resolution in Personal Knowledge Graphs 17

This chapter provided a background on ER essentials. Particularly important is the two-step workflow, comprising blocking and similarity, and their respective evaluation methodologies and metrics, which has emerged as a standard for ER. However, many important research questions and opportunities still remain, several of which we covered in the previous section. Another important research area that is particularly important to ER but that we did not mention in the previous section is the efficient generation of unbiased training sets. While this may not prove to be a problem if zero-shot learning and few-shot learning techniques are adapted to address ER, at present, only supervised learning techniques currently have the quality necessary for high-stakes applications. Hence, generation of high-quality training sets remains an important problem. Unlike regular machine learning applications, random sampling and labeling does not work very well because most pairs of nodes in the KG are not duplicates.

## REFERENCES

- <a id="ref-1"></a>[1] Alfred V Aho and Margaret J Corasick. 1975. Efficient string matching: an aid to bibliographic search. Commun. ACM 18, 6 (1975), 333–340.
- <a id="ref-2"></a>[2] Dean Allemang and James Hendler. 2011. Semantic web for the working ontologist: effective modeling in RDFS and OWL. Elsevier.
- <a id="ref-3"></a>[3] Renzo Angles and Claudio Gutierrez. 2005. Querying RDF data from a graph database perspective. In European semantic web conference. Springer, 346–360.
- <a id="ref-4"></a>[4] Janani Balaji, Faizan Javed, Mayank Kejriwal, Chris Min, Sam Sander, and Ozgur Ozturk. 2016. An Ensemble Blocking Scheme for Entity Resolution of Large and Sparse Datasets. CoRR abs/1609.06265 (2016). arXiv:[1609.06265](https://arxiv.org/abs/1609.06265) <http://arxiv.org/abs/1609.06265>
- <a id="ref-5"></a>[5] Jonathan Baxter. 1998. Theoretical models of learning to learn. In Learning to learn. Springer, 71–94.
- <a id="ref-6"></a>[6] R Baxter, P Christen, and T Churches. 2003. A Comparison of Fast Blocking Methods for Record Linkage; erschienen in: Proceedings of the Workshop on Data Cleaning, Record Linkage and Object Consolidation at the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Washington DC (2003).
- <a id="ref-7"></a>[7] Omar Benjelloun, Hector Garcia-Molina, David Menestrina, Qi Su, Steven Euijong Whang, and Jennifer Widom. 2009. Swoosh: a generic approach to entity resolution. The VLDB Journal 18 (2009), 255–276.
- <a id="ref-8"></a>[8] C Bizer, T Heath, and T Berners-Lee. 2009. Linked data-The story so far. University of Southampton, ePrints Soton.
- <a id="ref-9"></a>[9] Bin Cao, Sinno Jialin Pan, Yu Zhang, Dit-Yan Yeung, and Qiang Yang. 2010. Adaptive transfer learning. In proceedings of the AAAI Conference on Artificial Intelligence, Vol. 24. 407–412.
- <a id="ref-10"></a>[10] Alberto Caprara, Paolo Toth, and Matteo Fischetti. 2000. Algorithms for the set covering problem. Annals of Operations Research 98, 1 (2000), 353–371.
- <a id="ref-11"></a>[11] Peter Christen. 2011. A survey of indexing techniques for scalable record linkage and deduplication. IEEE transactions on knowledge and data engineering 24, 9 (2011), 1537–1555.
- <a id="ref-12"></a>[12] Peter Christen. 2012. The data matching process. In Data matching. Springer, 23–35.
- <a id="ref-13"></a>[13] Vassilis Christophides, Vasilis Efthymiou, and Kostas Stefanidis. 2015. Entity resolution in the web of data. Synthesis Lectures on the Semantic Web 5, 3 (2015), 1–122.
- <a id="ref-14"></a>[14] Michael Cochez, Thierry Declerck, Gerard de Melo, Luis Espinosa Anke, Besnik Fetahu, Dagmar Gromann, Mayank Kejriwal, Maria Koutraki, Freddy Lécué, Enrico Palumbo, and Harald Sack (Eds.). 2018. Proceedings of the First Workshop on Deep Learning for Knowledge Graphs and Semantic Technologies (DL4KGS) co-located with the 15th Extended Semantic Web Conerence (ESWC 2018), Heraklion, Crete, Greece, June 4, 2018. CEUR Workshop Proceedings, Vol. 2106. CEUR-WS.org. <http://ceur-ws.org/Vol-2106>
- <a id="ref-15"></a>[15] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. 2004. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry. 253–262.
- <a id="ref-16"></a>[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
- <a id="ref-17"></a>[17] Songyun Duan, Achille Fokoue, Oktie Hassanzadeh, Anastasios Kementsietsidis, Kavitha Srinivas, and Michael J Ward. 2012. Instance-based matching of large ontologies using locality-sensitive hashing. In International Semantic Web Conference. Springer, 49–64.
- <a id="ref-18"></a>[18] Pradheep Elango. 2005. Coreference resolution: A survey. University of Wisconsin, Madison, WI (2005), 12.
- <a id="ref-19"></a>[19] Mozhdeh Gheini and Mayank Kejriwal. 2019. Unsupervised Product Entity Resolution using Graph Representation Learning. In Proceedings of the SIGIR 2019 Workshop on eCommerce, co-located with the 42st International ACM SIGIR Conference on Research and Development in Information Retrieval, eCom@SIGIR 2019, Paris, France, July 25, 2019 (CEUR Workshop Proceedings, Vol. 2410), Jon Degenhardt, Surya Kallumadi, Utkarsh Porwal, and Andrew Trotman (Eds.). CEUR-WS.org. <http://ceur-ws.org/Vol-2410/paper26.pdf>
- <a id="ref-20"></a>[20] Ram Deepak Gottapu, Cihan Dagli, and Bharami Ali. 2016. Entity resolution using convolutional neural network. Procedia Computer Science 95 (2016), 153–158.
- <a id="ref-21"></a>[21] Marina Haliem, Trevor Bonjour, Aala Oqab Alsalem, Shilpa Thomas, Hongyu Li, Vaneet Aggarwal, Bharat K. Bhargava, and Mayank Kejriwal. 2021. Learning Monopoly Gameplay: A Hybrid Model-Free Deep Reinforcement Learning and Imitation Learning Approach. CoRR abs/2103.00683 (2021).

arXiv:[2103.00683](https://arxiv.org/abs/2103.00683) <https://arxiv.org/abs/2103.00683>

- <a id="ref-22"></a>[22] Patrick AV Hall and Geoff R Dowling. 1980. Approximate string matching. ACM computing surveys (CSUR) 12, 4 (1980), 381–402.
- <a id="ref-23"></a>[23] James A Hanley and Barbara J McNeil. 1982. The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology 143, 1 (1982), 29–36.
- <a id="ref-24"></a>[24] Stolfo Hernández. 1998. Hernández MA, Stolfo SJ. Real-world data is dirty: Data cleansing and the merge/purge problem, Data Mining and Knowledge Discovery 2, 1 (1998), 9–37.
- <a id="ref-25"></a>[25] Minda Hu, Ashwin Rao, Mayank Kejriwal, and Kristina Lerman. 2021. Socioeconomic Correlates of Anti-Science Attitudes in the US. Future Internet 13, 6 (2021), 160. <https://doi.org/10.3390/fi13060160>
- <a id="ref-26"></a>[26] Kyle Hundman, Thamme Gowda, Mayank Kejriwal, and Benedikt Boecking. 2018. Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2018, New Orleans, LA, USA, February 02-03, 2018, Jason Furman, Gary E. Marchant, Huw Price, and Francesca Rossi (Eds.). ACM, 137–143. <https://doi.org/10.1145/3278721.3278782>
- <a id="ref-27"></a>[27] Panagiotis G Ipeirotis, Vassilios S Verykios, and Ahmed K Elmagarmid. 2007. Duplicate record detection: A survey. IEEE Transactions on Knowledge and Data Engineering 19, 1 (2007), 1–16.
- <a id="ref-28"></a>[28] Mayank Kejriwal. 2014. Populating Entity Name Systems for Big Data Integration. In The Semantic Web - ISWC 2014 - 13th International Semantic Web Conference, Riva del Garda, Italy, October 19-23, 2014. Proceedings, Part II (Lecture Notes in Computer Science, Vol. 8797), Peter Mika, Tania Tudorache, Abraham Bernstein, Chris Welty, Craig A. Knoblock, Denny Vrandecic, Paul Groth, Natasha F. Noy, Krzysztof Janowicz, and Carole A. Goble (Eds.). Springer, 521–528. [https://doi.org/10.1007/978-3-319-11915-1\_34](https://doi.org/10.1007/978-3-319-11915-1_34)
- <a id="ref-29"></a>[29] Mayank Kejriwal. 2015. Entity Resolution in a Big Data Framework. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA, Blai Bonet and Sven Koenig (Eds.). AAAI Press, 4243–4244. [http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9294](http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9294)
- <a id="ref-30"></a>[30] Mayank Kejriwal. 2015. Sorted Neighborhood for the Semantic Web. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA, Blai Bonet and Sven Koenig (Eds.). AAAI Press, 4174–4175. [http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9295](http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9295)
- <a id="ref-31"></a>[31] Mayank Kejriwal. 2016. Disjunctive Normal Form Schemes for Heterogeneous Attributed Graphs. CoRR abs/1605.00686 (2016). arXiv:[1605.00686](https://arxiv.org/abs/1605.00686) <http://arxiv.org/abs/1605.00686>
- <a id="ref-32"></a>[32] Mayank Kejriwal. 2016. Populating a linked data entity name system: A big data solution to unsupervised instance matching. Vol. 27. IOS Press.
- <a id="ref-33"></a>[33] Mayank Kejriwal. 2017. Populating a linked data entity name system. AI Matters 3, 2 (2017), 22–23. <https://doi.org/10.1145/3098888.3098897>
- <a id="ref-34"></a>[34] Mayank Kejriwal. 2019. Domain-specific knowledge graph construction. Springer.
- <a id="ref-35"></a>[35] Mayank Kejriwal. 2021. Link Prediction Between Structured Geopolitical Events: Models and Experiments. Frontiers Big Data 4 (2021), 779792. <https://doi.org/10.3389/fdata.2021.779792>
- <a id="ref-36"></a>[36] Mayank Kejriwal. 2021. A meta-engine for building domain-specific search engines. Softw. Impacts 7 (2021), 100052. [https://doi.org/10.1016/j.simpa.2020.100052](https://doi.org/10.1016/j.simpa.2020.100052)
- <a id="ref-37"></a>[37] Mayank Kejriwal. 2021. Unsupervised DNF Blocking for Efficient Linking of Knowledge Graphs and Tables. Information 12, 3 (2021), 134.
- <a id="ref-38"></a>[38] Mayank Kejriwal. 2022. Knowledge Graphs: A Practical Review of the Research Landscape. Inf. 13, 4 (2022), 161. <https://doi.org/10.3390/info13040161>
- <a id="ref-39"></a>[39] Mayank Kejriwal. 2023. Artificial Intelligence for Industries of the Future - Beyond Facebook, Amazon, Microsoft and Google. Springer. [https://doi.org/10.1007/978-3-031-19039-1](https://doi.org/10.1007/978-3-031-19039-1)
- <a id="ref-40"></a>[40] Mayank Kejriwal and Akarsh Dang. 2020. Structural studies of the global networks exposed in the Panama papers. Appl. Netw. Sci. 5, 1 (2020), 63. <https://doi.org/10.1007/s41109-020-00313-y>
- <a id="ref-41"></a>[41] Mayank Kejriwal, Ge Fang, and Ying Zhou. 2021. A Feasibility Study of Open-Source Sentiment Analysis and Text Classification Systems on Disaster-Specific Social Media Data. In IEEE Symposium Series on Computational Intelligence, SSCI 2021, Orlando, FL, USA, December 5-7, 2021. IEEE, 1–8. <https://doi.org/10.1109/SSCI50451.2021.9660089>
- <a id="ref-42"></a>[42] Mayank Kejriwal, Daniel Gilley, Pedro A. Szekely, and Jill Crisman. 2018. THOR: Text-enabled Analytics for Humanitarian Operations. In Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018, Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and Panagiotis G. Ipeirotis (Eds.). ACM, 147–150. <https://doi.org/10.1145/3184558.3186965>
- <a id="ref-43"></a>[43] Mayank Kejriwal and Yao Gu. 2020. Network-theoretic modeling of complex activity using UK online sex advertisements. Appl. Netw. Sci. 5, 1 (2020), 30. <https://doi.org/10.1007/s41109-020-00275-1>
- <a id="ref-44"></a>[44] Mayank Kejriwal and Rahul Kapoor. 2019. Network-theoretic information extraction quality assessment in the human trafficking domain. Appl. Netw. Sci. 4, 1 (2019), 44:1–44:26. <https://doi.org/10.1007/s41109-019-0154-z>
- <a id="ref-45"></a>[45] Mayank Kejriwal, Craig A Knoblock, and Pedro Szekely. 2021. Knowledge graphs: Fundamentals, techniques, and applications. MIT Press.
- <a id="ref-46"></a>[46] Mayank Kejriwal, Qiaoling Liu, Ferosh Jacob, and Faizan Javed. 2015. A pipeline for extracting and deduplicating domain-specific knowledge bases. In 2015 IEEE International Conference on Big Data (IEEE BigData 2015), Santa Clara, CA, USA, October 29 - November 1, 2015. IEEE Computer Society, 1144–1153. <https://doi.org/10.1109/BigData.2015.7363868>
- <a id="ref-47"></a>[47] Mayank Kejriwal and Yuesheng Luo. 2022. On the Empirical Association between Trade Network Complexity and Global Gross Domestic Product. CoRR abs/2211.13117 (2022). <https://doi.org/10.48550/arXiv.2211.13117> arXiv:[2211.13117](https://arxiv.org/abs/2211.13117)
- <a id="ref-48"></a>[48] Mayank Kejriwal and Daniel P Miranker. 2013. An unsupervised algorithm for learning blocking schemes. In 2013 IEEE 13th International Conference on Data Mining. IEEE, 340–349.

## Named Entity Resolution in Personal Knowledge Graphs 19

- <a id="ref-49"></a>[49] Mayank Kejriwal and Daniel P. Miranker. 2014. On Linking Heterogeneous Dataset Collections. In Proceedings of the ISWC 2014 Posters & Demonstrations Track a track within the 13th International Semantic Web Conference, ISWC 2014, Riva del Garda, Italy, October 21, 2014 (CEUR Workshop Proceedings, Vol. 1272), Matthew Horridge, Marco Rospocher, and Jacco van Ossenbruggen (Eds.). CEUR-WS.org, 217–220. [http://ceur-ws.org/Vol-1272/paper\_17.pdf](http://ceur-ws.org/Vol-1272/paper_17.pdf)
- <a id="ref-50"></a>[50] Mayank Kejriwal and Daniel P Miranker. 2014. A two-step blocking scheme learner for scalable link discovery. OM 14 (2014), 49–60.
- <a id="ref-51"></a>[51] Mayank Kejriwal and Daniel P. Miranker. 2015. Decision-Making Bias in Instance Matching Model Selection. In The Semantic Web ISWC 2015 14th International Semantic Web Conference, Bethlehem, PA, USA, October 11-15, 2015, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 9366), Marcelo Arenas, Óscar Corcho, Elena Simperl, Markus Strohmaier, Mathieu d'Aquin, Kavitha Srinivas, Paul Groth, Michel Dumontier, Jeff Heflin, Krishnaprasad Thirunarayan, and Steffen Staab (Eds.). Springer, 392–407. [https://doi.org/10.1007/978-3-319-25007-6\_23](https://doi.org/10.1007/978-3-319-25007-6_23)
- <a id="ref-52"></a>[52] Mayank Kejriwal and Daniel P. Miranker. 2015. A DNF Blocking Scheme Learner for Heterogeneous Datasets. CoRR abs/1501.01694 (2015). arXiv:[1501.01694](https://arxiv.org/abs/1501.01694) <http://arxiv.org/abs/1501.01694>
- <a id="ref-53"></a>[53] Mayank Kejriwal and Daniel P. Miranker. 2015. Minimally Supervised Instance Matching: An Alternate Approach. In The Semantic Web: ESWC 2015 Satellite Events - ESWC 2015 Satellite Events Portorož, Slovenia, May 31 - June 4, 2015, Revised Selected Papers (Lecture Notes in Computer Science, Vol. 9341), Fabien Gandon, Christophe Guéret, Serena Villata, John G. Breslin, Catherine Faron-Zucker, and Antoine Zimmermann (Eds.). Springer, 72–76. [https://doi.org/10.1007/978-3-319-25639-9\_14](https://doi.org/10.1007/978-3-319-25639-9_14)
- <a id="ref-54"></a>[54] Mayank Kejriwal and Daniel P Miranker. 2015. On the complexity of sorted neighborhood. arXiv preprint arXiv:1501.01696 (2015).
- <a id="ref-55"></a>[55] Mayank Kejriwal and Daniel P. Miranker. 2015. Semi-supervised Instance Matching Using Boosted Classifiers. In The Semantic Web. Latest Advances and New Domains - 12th European Semantic Web Conference, ESWC 2015, Portoroz, Slovenia, May 31 - June 4, 2015. Proceedings (Lecture Notes in Computer Science, Vol. 9088), Fabien Gandon, Marta Sabou, Harald Sack, Claudia d'Amato, Philippe Cudré-Mauroux, and Antoine Zimmermann (Eds.). Springer, 388–402. [https://doi.org/10.1007/978-3-319-18818-8\_24](https://doi.org/10.1007/978-3-319-18818-8_24)
- <a id="ref-56"></a>[56] Mayank Kejriwal and Daniel P. Miranker. 2015. Sorted Neighborhood for Schema-Free RDF Data. In The Semantic Web: ESWC 2015 Satellite Events - ESWC 2015 Satellite Events Portorož, Slovenia, May 31 - June 4, 2015, Revised Selected Papers (Lecture Notes in Computer Science, Vol. 9341), Fabien Gandon, Christophe Guéret, Serena Villata, John G. Breslin, Catherine Faron-Zucker, and Antoine Zimmermann (Eds.). Springer, 217–229. [https://doi.org/10.1007/978-3-319-25639-9\_38](https://doi.org/10.1007/978-3-319-25639-9_38)
- <a id="ref-57"></a>[57] Mayank Kejriwal and Daniel P Miranker. 2015. Sorted neighborhood for schema-free RDF data. In European Semantic Web Conference. Springer, 217–229.
- <a id="ref-58"></a>[58] Mayank Kejriwal and Daniel P Miranker. 2015. An unsupervised instance matcher for schema-free RDF data. Journal of Web Semantics 35 (2015), 102–123.
- <a id="ref-59"></a>[59] Mayank Kejriwal and Daniel P Miranker. 2016. Experience: Type alignment on DBpedia and Freebase. arXiv preprint arXiv:1608.04442 (2016).
- <a id="ref-60"></a>[60] Mayank Kejriwal, Jing Peng, Haotian Zhang, and Pedro A. Szekely. 2018. Structured Event Entity Resolution in Humanitarian Domains. In The Semantic Web - ISWC 2018 - 17th International Semantic Web Conference, Monterey, CA, USA, October 8-12, 2018, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 11136), Denny Vrandecic, Kalina Bontcheva, Mari Carmen Suárez-Figueroa, Valentina Presutti, Irene Celino, Marta Sabou, Lucie-Aimée Kaffee, and Elena Simperl (Eds.). Springer, 233–249. [https://doi.org/10.1007/978-3-030-00671-6\_14](https://doi.org/10.1007/978-3-030-00671-6_14)
- <a id="ref-61"></a>[61] Mayank Kejriwal, Henrique Santos, Alice M. Mulvehill, and Deborah L. McGuinness. 2022. Designing a strong test for measuring true common-sense reasoning. Nat. Mach. Intell. 4, 4 (2022), 318–322. <https://doi.org/10.1038/s42256-022-00478-4>
- <a id="ref-62"></a>[62] Mayank Kejriwal, Thomas Schellenberg, and Pedro A. Szekely. 2017. A Semantic Search Engine For Investigating Human Trafficking. In Proceedings of the ISWC 2017 Posters & Demonstrations and Industry Tracks co-located with 16th International Semantic Web Conference (ISWC 2017), Vienna, Austria, October 23rd - to - 25th, 2017 (CEUR Workshop Proceedings, Vol. 1963), Nadeschda Nikitina, Dezhao Song, Achille Fokoue, and Peter Haase (Eds.). CEUR-WS.org. <http://ceur-ws.org/Vol-1963/paper613.pdf>
- <a id="ref-63"></a>[63] Mayank Kejriwal, Ravi Kiran Selvam, Chien-Chun Ni, and Nicolas Torzec. 2020. Locally Constructing Product Taxonomies from Scratch Using Representation Learning. In IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2020, The Hague, Netherlands, December 7-10, 2020, Martin Atzmüller, Michele Coscia, and Rokia Missaoui (Eds.). IEEE, 507–514. [https://doi.org/10.1109/ASONAM49781.2020.9381320](https://doi.org/10.1109/ASONAM49781.2020.9381320)
- <a id="ref-64"></a>[64] Mayank Kejriwal, Ravi Kiran Selvam, Chien-Chun Ni, and Nicolas Torzec. 2021. Empirical Best Practices On Using Product-Specific Schema.org. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 15452–15457. <https://ojs.aaai.org/index.php/AAAI/article/view/17816>
- <a id="ref-65"></a>[65] Mayank Kejriwal, Juan F. Sequeda, and Vanessa Lopez. 2019. Knowledge graphs: Construction, management and querying. Semantic Web 10, 6 (2019), 961–962. <https://doi.org/10.3233/SW-190370>
- <a id="ref-66"></a>[66] Mayank Kejriwal, Runqi Shao, and Pedro A. Szekely. 2019. Expert-Guided Entity Extraction using Expressive Rules. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2019, Paris, France, July 21-25, 2019, Benjamin Piwowarski, Max Chevalier, Éric Gaussier, Yoelle Maarek, Jian-Yun Nie, and Falk Scholer (Eds.). ACM, 1353–1356. [https://doi.org/10.1145/3331184.3331392](https://doi.org/10.1145/3331184.3331392)
- <a id="ref-67"></a>[67] Mayank Kejriwal and Ke Shen. 2020. Do Fine-tuned Commonsense Language Models Really Generalize? CoRR abs/2011.09159 (2020). arXiv:[2011.09159](https://arxiv.org/abs/2011.09159) <https://arxiv.org/abs/2011.09159>

- <a id="ref-68"></a>[68] Mayank Kejriwal and Ke Shen. 2022. Can Scale-free Network Growth with Triad Formation Capture Simplicial Complex Distributions in Real Communication Networks? CoRR abs/2203.06491 (2022). <https://doi.org/10.48550/arXiv.2203.06491> arXiv:[2203.06491](https://arxiv.org/abs/2203.06491)
- <a id="ref-69"></a>[69] Mayank Kejriwal, Ke Shen, Chien-Chun Ni, and Nicolas Torzec. 2021. An evaluation and annotation methodology for product category matching in e-commerce. Comput. Ind. 131 (2021), 103497. <https://doi.org/10.1016/j.compind.2021.103497>
- <a id="ref-70"></a>[70] Mayank Kejriwal, Ke Shen, Chien-Chun Ni, and Nicolas Torzec. 2022. Transfer-based taxonomy induction over concept labels. Eng. Appl. Artif. Intell. 108 (2022), 104548. <https://doi.org/10.1016/j.engappai.2021.104548>
- <a id="ref-71"></a>[71] Mayank Kejriwal and Pedro Szekely. 2017. Neural embeddings for populated geonames locations. In International Semantic Web Conference. Springer, 139–146.
- <a id="ref-72"></a>[72] Mayank Kejriwal and Pedro Szekely. 2017. Supervised typing of big graphs using semantic embeddings. In Proceedings of The International Workshop on Semantic Big Data. 1–6.
- <a id="ref-73"></a>[73] Mayank Kejriwal and Pedro A. Szekely. 2017. An Investigative Search Engine for the Human Trafficking Domain. In The Semantic Web - ISWC 2017 - 16th International Semantic Web Conference, Vienna, Austria, October 21-25, 2017, Proceedings, Part II (Lecture Notes in Computer Science, Vol. 10588), Claudia d'Amato, Miriam Fernández, Valentina A. M. Tamma, Freddy Lécué, Philippe Cudré-Mauroux, Juan F. Sequeda, Christoph Lange, and Jeff Heflin (Eds.). Springer, 247–262. [https://doi.org/10.1007/978-3-319-68204-4\_25](https://doi.org/10.1007/978-3-319-68204-4_25)
- <a id="ref-74"></a>[74] Mayank Kejriwal and Pedro A. Szekely. 2017. Scalable Generation of Type Embeddings Using the ABox. Open J. Semantic Web 4, 1 (2017), 20–34. [https://www.ronpub.com/ojsw/OJSW\_2017v4i1n02\_Kejriwal.html](https://www.ronpub.com/ojsw/OJSW_2017v4i1n02_Kejriwal.html)
- <a id="ref-75"></a>[75] Mayank Kejriwal and Pedro A. Szekely. 2017. Supervised Typing of Big Graphs using Semantic Embeddings. CoRR abs/1703.07805 (2017). arXiv:[1703.07805](https://arxiv.org/abs/1703.07805) <http://arxiv.org/abs/1703.07805>
- <a id="ref-76"></a>[76] Mayank Kejriwal and Pedro A. Szekely. 2018. Constructing Domain-Specific Search Engines With No Programming. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, Sheila A. McIlraith and Kilian Q. Weinberger (Eds.). AAAI Press, 8204–8205. <https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16990>
- <a id="ref-77"></a>[77] Mayank Kejriwal and Pedro A. Szekely. 2018. Technology-assisted Investigative Search: A Case Study from an Illicit Domain. In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems, CHI 2018, Montreal, QC, Canada, April 21-26, 2018, Regan L. Mandryk, Mark Hancock, Mark Perry, and Anna L. Cox (Eds.). ACM. <https://doi.org/10.1145/3170427.3174364>
- <a id="ref-78"></a>[78] Mayank Kejriwal and Pedro A. Szekely. 2019. Co-LOD: Continuous Space Linked Open Data. In Proceedings of the ISWC 2019 Satellite Tracks (Posters & Demonstrations, Industry, and Outrageous Ideas) co-located with 18th International Semantic Web Conference (ISWC 2019), Auckland, New Zealand, October 26-30, 2019 (CEUR Workshop Proceedings, Vol. 2456), Mari Carmen Suárez-Figueroa, Gong Cheng, Anna Lisa Gentile, Christophe Guéret, C. Maria Keet, and Abraham Bernstein (Eds.). CEUR-WS.org, 333–337. <http://ceur-ws.org/Vol-2456/paper94.pdf>
- <a id="ref-79"></a>[79] Mayank Kejriwal and Pedro A. Szekely. 2019. myDIG: Personalized Illicit Domain-Specific Knowledge Discovery with No Programming. Future Internet 11, 3 (2019), 59. <https://doi.org/10.3390/fi11030059>
- <a id="ref-80"></a>[80] Mayank Kejriwal and Pedro A. Szekely. 2022. Knowledge Graphs for Social Good: An Entity-Centric Search Engine for the Human Trafficking Domain. IEEE Trans. Big Data 8, 3 (2022), 592–606. <https://doi.org/10.1109/TBDATA.2017.2763164>
- <a id="ref-81"></a>[81] Mayank Kejriwal, Pedro A. Szekely, and Craig A. Knoblock. 2018. Investigative Knowledge Discovery for Combating Illicit Activities. IEEE Intell. Syst. 33, 1 (2018), 53–63. <https://doi.org/10.1109/MIS.2018.111144556>
- <a id="ref-82"></a>[82] Mayank Kejriwal, Pedro A. Szekely, and Raphaël Troncy (Eds.). 2019. Proceedings of the 10th International Conference on Knowledge Capture, K-CAP 2019, Marina Del Rey, CA, USA, November 19-21, 2019. ACM. <https://doi.org/10.1145/3360901>
- <a id="ref-83"></a>[83] Mayank Kejriwal and Shilpa Thomas. 2021. A multi-agent simulator for generating novelty in monopoly. Simul. Model. Pract. Theory 112 (2021), 102364. <https://doi.org/10.1016/j.simpat.2021.102364>
- <a id="ref-84"></a>[84] Mayank Kejriwal, Qile Wang, Hongyu Li, and Lu Wang. 2021. An empirical study of emoji usage on Twitter in linguistic and national contexts. Online Soc. Networks Media 24 (2021), 100149. <https://doi.org/10.1016/j.osnem.2021.100149>
- <a id="ref-85"></a>[85] Mayank Kejriwal and Peilin Zhou. 2019. Low-supervision urgency detection and transfer in short crisis messages. In ASONAM '19: International Conference on Advances in Social Networks Analysis and Mining, Vancouver, British Columbia, Canada, 27-30 August, 2019, Francesca Spezzano, Wei Chen, and Xiaokui Xiao (Eds.). ACM, 353–356. <https://doi.org/10.1145/3341161.3342936>
- <a id="ref-86"></a>[86] Mayank Kejriwal and Peilin Zhou. 2019. SAVIZ: interactive exploration and visualization of situation labeling classifiers over crisis social media data. In ASONAM '19: International Conference on Advances in Social Networks Analysis and Mining, Vancouver, British Columbia, Canada, 27-30 August, 2019, Francesca Spezzano, Wei Chen, and Xiaokui Xiao (Eds.). ACM, 705–708. <https://doi.org/10.1145/3341161.3343703>
- <a id="ref-87"></a>[87] Mayank Kejriwal and Peilin Zhou. 2020. On detecting urgency in short crisis messages using minimal supervision and transfer learning. Soc. Netw. Anal. Min. 10, 1 (2020), 58. <https://doi.org/10.1007/s13278-020-00670-7>
- <a id="ref-88"></a>[88] Graham Klyne and Jeremy Carroll. 2006. Resource Description Framework (RDF): Concepts and Abstract Syntax-W3C Recommendation. h ttp.
- <a id="ref-89"></a>[89] Hanna Köpcke and Erhard Rahm. 2010. Frameworks for entity matching: A comparison. Data & Knowledge Engineering 69, 2 (2010), 197–210.
- <a id="ref-90"></a>[90] Hanna Köpcke, Andreas Thor, and Erhard Rahm. 2010. Evaluation of entity resolution approaches on real-world match problems. Proceedings of the VLDB Endowment 3, 1-2 (2010), 484–493.
- <a id="ref-91"></a>[91] Nick Koudas, Amit Marathe, and Divesh Srivastava. 2004. Flexible string matching against large databases in practice. In Proceedings of the Thirtieth international conference on Very large data bases-Volume 30. 1078–1086.

## Named Entity Resolution in Personal Knowledge Graphs 21

- <a id="ref-92"></a>[92] Yuesheng Luo and Mayank Kejriwal. 2021. Understanding COVID-19 Vaccine Reaction through Comparative Analysis on Twitter. CoRR abs/2111.05823 (2021). arXiv:[2111.05823](https://arxiv.org/abs/2111.05823) <https://arxiv.org/abs/2111.05823>
- <a id="ref-93"></a>[93] Yuesheng Luo and Mayank Kejriwal. 2022. Understanding COVID-19 Vaccine Reaction Through Comparative Analysis on Twitter. In Intelligent Computing - Proceedings of the 2022 Computing Conference, Volume 1, SAI 2022, Virtual Event, 14-15 July 2022 (Lecture Notes in Networks and Systems, Vol. 506), Kohei Arai (Ed.). Springer, 846–864. [https://doi.org/10.1007/978-3-031-10461-9\_58](https://doi.org/10.1007/978-3-031-10461-9_58)
- <a id="ref-94"></a>[94] Yongtao Ma and Thanh Tran. 2013. Typimatch: type-specific unsupervised learning of keys and key values for heterogeneous web data integration. In Proceedings of the sixth ACM international conference on Web search and data mining. 325–334.
- <a id="ref-95"></a>[95] Andrew McCallum, Kamal Nigam, and Lyle H Ungar. 2000. Efficient clustering of high-dimensional data sets with application to reference matching. In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining. 169–178.
- <a id="ref-96"></a>[96] Deborah L McGuinness, Frank Van Harmelen, et al. 2004. OWL web ontology language overview. W3C recommendation 10, 10 (2004), 2004.
- <a id="ref-97"></a>[97] Sara Melotte and Mayank Kejriwal. 2021. A Geo-Tagged COVID-19 Twitter Dataset for 10 North American Metropolitan Areas over a 255-Day Period. Data 6, 6 (2021), 64. <https://doi.org/10.3390/data6060064>
- <a id="ref-98"></a>[98] Sara Melotte and Mayank Kejriwal. 2021. Predicting Zip Code-Level Vaccine Hesitancy in US Metropolitan Areas Using Machine Learning Models on Public Tweets. CoRR abs/2108.01699 (2021). arXiv:[2108.01699](https://arxiv.org/abs/2108.01699) <https://arxiv.org/abs/2108.01699>
- <a id="ref-99"></a>[99] David Menestrina, Steven Euijong Whang, and Hector Garcia-Molina. 2010. Evaluating entity resolution results. Proceedings of the VLDB Endowment 3, 1-2 (2010), 208–219.
- <a id="ref-100"></a>[100] Grégoire Mesnil, Yann Dauphin, Xavier Glorot, Salah Rifai, Yoshua Bengio, Ian Goodfellow, Erick Lavoie, Xavier Muller, Guillaume Desjardins, David Warde-Farley, et al. 2012. Unsupervised and transfer learning challenge: a deep learning approach. In Proceedings of ICML Workshop on Unsupervised and Transfer Learning. JMLR Workshop and Conference Proceedings, 97–110.
- <a id="ref-101"></a>[101] Daye Nam and Mayank Kejriwal. 2018. How Do Organizations Publish Semantic Markup? Three Case Studies Using Public Schema.org Crawls. Computer 51, 6 (2018), 42–51. <https://doi.org/10.1109/MC.2018.2701635>
- <a id="ref-102"></a>[102] Howard B Newcombe, James M Kennedy, SJ Axford, and Allison P James. 1959. Automatic Linkage of Vital Records: Computers can be used to extract" follow-up" statistics of families from files of routine records. Science 130, 3381 (1959), 954–959.
- <a id="ref-103"></a>[103] Axel-Cyrille Ngonga Ngomo, Klaus Lyko, and Victor Christen. 2013. Coala–correlation-aware active learning of link specifications. In Extended Semantic Web Conference. Springer, 442–456.
- <a id="ref-104"></a>[104] Lorena Otero-Cerdeira, Francisco J Rodríguez-Martínez, and Alma Gómez-Rodríguez. 2015. Ontology matching: A literature review. Expert Systems with Applications 42, 2 (2015), 949–971.
- <a id="ref-105"></a>[105] George Papadakis, Ekaterini Ioannou, Themis Palpanas, Claudia Niederée, and Wolfgang Nejdl. 2012. A blocking framework for entity resolution in highly heterogeneous information spaces. IEEE Transactions on Knowledge and Data Engineering 25, 12 (2012), 2665–2682.
- <a id="ref-106"></a>[106] George Papadakis, Georgia Koutrika, Themis Palpanas, and Wolfgang Nejdl. 2013. Meta-blocking: Taking entity resolution to the next level. IEEE Transactions on Knowledge and Data Engineering 26, 8 (2013), 1946–1960.
- <a id="ref-107"></a>[107] David Pinto, Darnes Vilariño, Yuridiana Alemán, Helena Gómez, Nahun Loya, and Héctor Jiménez-Salazar. 2012. The Soundex phonetic algorithm revisited for SMS text representation. In International Conference on Text, Speech and Dialogue. Springer, 47–55.
- <a id="ref-108"></a>[108] Guilin Qi, Huajun Chen, Kang Liu, Haofen Wang, Qiu Ji, and Tianxing Wu. 2020. Knowledge graph. To appear (2020).
- <a id="ref-109"></a>[109] Sachin Ravi and Hugo Larochelle. 2016. Optimization as a model for few-shot learning.
- <a id="ref-110"></a>[110] Shu Rong, Xing Niu, Evan Wei Xiang, Haofen Wang, Qiang Yang, and Yong Yu. 2012. A machine learning approach for instance matching based on similarity metrics. In International Semantic Web Conference. Springer, 460–475.
- <a id="ref-111"></a>[111] Satya S Sahoo, Wolfgang Halb, Sebastian Hellmann, Kingsley Idehen, Ted Thibodeau Jr, Sören Auer, Juan Sequeda, and Ahmed Ezzat. 2009. A survey of current approaches for mapping of relational databases to RDF. W3C RDB2RDF Incubator Group Report 1 (2009), 113–130.
- <a id="ref-112"></a>[112] Ravi Kiran Selvam and Mayank Kejriwal. 2020. On using Product-Specific Schema.org from Web Data Commons: An Empirical Set of Best Practices. CoRR abs/2007.13829 (2020). arXiv:[2007.13829](https://arxiv.org/abs/2007.13829) <https://arxiv.org/abs/2007.13829>
- <a id="ref-113"></a>[113] Juan F Sequeda and Daniel P Miranker. 2013. Ultrawrap: SPARQL execution on relational data. Journal of Web Semantics 22 (2013), 19–39.
- <a id="ref-114"></a>[114] Ke Shen and Mayank Kejriwal. 2021. On the Generalization Abilities of Fine-Tuned Commonsense Language Representation Models. In Artificial Intelligence XXXVIII - 41st SGAI International Conference on Artificial Intelligence, AI 2021, Cambridge, UK, December 14-16, 2021, Proceedings (Lecture Notes in Computer Science, Vol. 13101), Max Bramer and Richard Ellis (Eds.). Springer, 3–16. [https://doi.org/10.1007/978-3-030-91100-3\_1](https://doi.org/10.1007/978-3-030-91100-3_1)
- <a id="ref-115"></a>[115] Ke Shen and Mayank Kejriwal. 2022. Understanding Prior Bias and Choice Paralysis in Transformer-based Language Representation Models through Four Experimental Probes. CoRR abs/2210.01258 (2022). <https://doi.org/10.48550/arXiv.2210.01258> arXiv:[2210.01258](https://arxiv.org/abs/2210.01258)
- <a id="ref-116"></a>[116] Ke Shen and Mayank Kejriwal. 2022. Understanding Substructures in Commonsense Relations in ConceptNet. CoRR abs/2210.01263 (2022). <https://doi.org/10.48550/arXiv.2210.01263> arXiv:[2210.01263](https://arxiv.org/abs/2210.01263)
- <a id="ref-117"></a>[117] Pavel Shvaiko and Jérôme Euzenat. 2011. Ontology matching: state of the art and future challenges. IEEE Transactions on knowledge and data engineering 25, 1 (2011), 158–176.
- <a id="ref-118"></a>[118] Tommaso Soru and Axel-Cyrille Ngonga Ngomo. 2014. A comparison of supervised learning classifiers for link discovery. In Proceedings of the 10th international conference on semantic systems. 41–44.
- <a id="ref-119"></a>[119] Rhea Sukthanker, Soujanya Poria, Erik Cambria, and Ramkumar Thirunavukarasu. 2020. Anaphora and coreference resolution: A review. Information Fusion 59 (2020), 139–162.

- <a id="ref-120"></a>[120] Pedro A. Szekely and Mayank Kejriwal. 2018. Domain-specific Insight Graphs (DIG). In Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018, Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and Panagiotis G. Ipeirotis (Eds.). ACM, 433–434. <https://doi.org/10.1145/3184558.3186203>
- <a id="ref-121"></a>[121] Jie Tang, Michalis Vazirgiannis, Yuxiao Dong, Fragkiskos D. Malliaros, Michael Cochez, Mayank Kejriwal, and Achim Rettinger. 2018. BigNet 2018 Chairs' Welcome & Organization. In Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018, Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and Panagiotis G. Ipeirotis (Eds.). ACM, 943–944. [https://doi.org/10.1145/3184558.3192293](https://doi.org/10.1145/3184558.3192293)
- <a id="ref-122"></a>[122] Zhisheng Tang and Mayank Kejriwal. 2022. Can Language Representation Models Think in Bets? CoRR abs/2210.07519 (2022). [https://doi.org/10.48550/arXiv.2210.07519](https://doi.org/10.48550/arXiv.2210.07519) arXiv:[2210.07519](https://arxiv.org/abs/2210.07519)
- <a id="ref-123"></a>[123] Aibo Tian, Mayank Kejriwal, and Daniel P Miranker. 2014. Schema matching over relations, attributes, and data values. In Proceedings of the 26th International Conference on Scientific and Statistical Database Management. 1–12.
- <a id="ref-124"></a>[124] Ilaria Tiddi, Freddy Lécué, and Pascal Hitzler. 2020. Knowledge Graphs for Explainable Artificial Intelligence: Foundations, Applications and Challenges. IOS Press.
- <a id="ref-125"></a>[125] Sanju Tiwari, Nandana Mihindukulasooriya, Francesco Osborne, Dimitris Kontokostas, Jennifer D'Souza, Mayank Kejriwal, Loris Bozzato, Valentina Anita Carriero, Torsten Hahmann, and Antoine Zimmermann (Eds.). 2022. Proceedings of the 1st International Workshop on Knowledge Graph Generation From Text and the 1st International Workshop on Modular Knowledge co-located with 19th Extended Semantic Conference (ESWC 2022), Hersonissos, Greece, May 30th, 2022. CEUR Workshop Proceedings, Vol. 3184. CEUR-WS.org. <http://ceur-ws.org/Vol-3184>
- <a id="ref-126"></a>[126] Julius Volz, Christian Bizer, Martin Gaedke, and Georgi Kobilarov. 2009. Discovering and maintaining links on the web of data. In International Semantic Web Conference. Springer, 650–665.
- <a id="ref-127"></a>[127] Steven Euijong Whang, David Menestrina, Georgia Koutrika, Martin Theobald, and Hector Garcia-Molina. 2009. Entity resolution with iterative blocking. In Proceedings of the 2009 ACM SIGMOD International Conference on Management of data. 219–232.
- <a id="ref-128"></a>[128] Yongqin Xian, Bernt Schiele, and Zeynep Akata. 2017. Zero-shot learning-the good, the bad and the ugly. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4582–4591.
- <a id="ref-129"></a>[129] Shuo Zhang and Mayank Kejriwal. 2019. Concept drift in bias and sensationalism detection: an experimental study. In ASONAM '19: International Conference on Advances in Social Networks Analysis and Mining, Vancouver, British Columbia, Canada, 27-30 August, 2019, Francesca Spezzano, Wei Chen, and Xiaokui Xiao (Eds.). ACM, 601–604. <https://doi.org/10.1145/3341161.3343690>
- <a id="ref-130"></a>[130] Tongtao Zhang, Ananya Subburathinam, Ge Shi, Lifu Huang, Di Lu, Xiaoman Pan, Manling Li, Boliang Zhang, Qingyun Wang, Spencer Whitehead, Heng Ji, Alireza Zareian, Hassan Akbari, Brian Chen, Ruiqi Zhong, Steven Shao, Emily Allaway, Shih-Fu Chang, Kathleen R. McKeown, Dongyu Li, Xin Huang, Kexuan Sun, Xujun Peng, Ryan Gabbard, Marjorie Freedman, Mayank Kejriwal, Ram Nevatia, Pedro A. Szekely, T. K. Satish Kumar, Ali Sadeghian, Giacomo Bergami, Sourav Dutta, Miguel E. Rodríguez, and Daisy Zhe Wang. 2018. GAIA - A Multi-media Multi-lingual Knowledge Extraction and Hypothesis Generation System. In Proceedings of the 2018 Text Analysis Conference, TAC 2018, Gaithersburg, Maryland, USA, November 13-14, 2018. NIST. <https://tac.nist.gov/publications/2018/participant.papers/TAC2018.GAIA.proceedings.pdf>