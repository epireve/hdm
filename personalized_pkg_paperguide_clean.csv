Papers,Authors,Published Year,Journal,DOI,Conclusion,Implementation Insights,Insights,Key Findings,Limitations,Methodology,Reproducibility,Research Question,Summary,TL;DR,Tags
Multimodal Object Detection Using Depth and Image Data for Manufacturing Parts,"Mahjourian Nazanin, Nguyen Vinh",2025,reference-manager,,,,,,,Multimodal object detection: Uses data from multiple sensor types (image and depth) to improve accuracy and reliability.,,How can an early fusion multimodal object detection model combining RGB images and 3D point cloud data improve the reliability and accuracy of object detection in manufacturing environments compared to RGB-only and depth-only approaches?,"The paper aims to improve object detection in manufacturing by integrating RGB images and 3D depth data using early sensor fusion within a modified Faster R-CNN model. The proposed multimodal approach significantly outperforms RGB-only and depth-only baselines, increasing mAP by up to 78%, enabling more robust and reliable detection.","The research goal is to improve object detection in manufacturing by using early sensor fusion of RGB and depth data with a single shared backbone; the approach modifies Faster R-CNN to accept four-channel RGB-D inputs, and results show the RGB-D model outperforms RGB-only and Depth-only variants.",
Sensor fusion: Combines information from different sensors at various stages (early,intermediate,late) to enhance object detection.,,,,,,,,,,,,,
"Experimental evaluation on a modified NIST manufacturing task board: Tests the system’s performance in realistic industrial scenarios.""","The research ensured reproducibility by training and testing all three model variants 10 times, confirming consistent results. However, there is no information provided about the availability of the source code for the project.","The RGB-D model achieved the highest mean mAP (0.480) and mean precision (0.474), outperforming the RGB-only (mAP: 0.425, precision: 0.424) and Depth-only models (mAP: 0.269, precision: 0.301).",,,,,,,,,,,,,
The RGB-D model’s mean mAP was 13% higher than RGB-only and 78% higher than Depth-only; mean precision was 11.8% and 57% higher,respectively.,,,,,,,,,,,,,,
"Integrating depth with RGB data significantly improved object detection
Results:",especially for challenging objects and scenes; results were consistent across 10 runs,"confirming robustness. No p-values were reported.""",Primary outcomes measured: mean mAP (mean Average Precision) and Mean Precision.,,,,,,,,,,,,
Depth-only: mean mAP 0.269,Mean Precision 0.301,,,,,,,,,,,,,,
RGB-only: mean mAP 0.425,Mean Precision 0.424,,,,,,,,,,,,,,
"RGB-D: mean mAP 0.480
Effects:",Mean Precision 0.474,,,,,,,,,,,,,,
RGB-D outperforms RGB-only (mean mAP +13%,Mean Precision +11.8%) and Depth-only (mean mAP +78%,Mean Precision +57%).,,,,,,,,,,,,,
RGB-D model shows improved detection,especially for objects missed by RGB-only,such as metallic or low-contrast items.,,,,,,,,,,,,,
"Results are consistent across 10 runs (error bars indicate standard deviation).""",Experiments did not include varying lighting conditions.,,,,,,,,,,,,,,
Model lacks a full 3D representation or point cloud data; only depth values per pixel were used.,,,,,,,,,,,,,,,
Depth-only model struggles with short and small objects,and may hallucinate detections.,,,,,,,,,,,,,,
Dataset size and diversity are limited; more diverse configurations needed.,,,,,,,,,,,,,,,
Sensor calibration and depth processing could be improved.,,,,,,,,,,,,,,,
Results may not meet industrial accuracy requirements yet.,,,,,,,,,,,,,,,
Future work needed for transparent,irregular,non-standard,and overlapping objects.,,,,,,,,,,,,
"Robotic manipulation integration and further multimodal model designs are suggested for future research.""",The RGB-D model outperforms both RGB-only and Depth-only models in object detection accuracy and precision.,,,,,,,,,,,,,,
Early sensor fusion of RGB and depth data enables better feature extraction and generalization,improving detection in complex scenes.,,,,,,,,,,,,,,
Using a single shared backbone for multimodal input reduces computational cost and simplifies deployment.,,,,,,,,,,,,,,,
Recommendation: Employ early fusion of RGB and depth data for efficient,"accurate object detection in manufacturing environments.""","Need for larger, more diverse datasets including varied object configurations, lighting, and occlusion scenarios.",,,,,,,,,,,,,
Improvement of sensor calibration,depth processing,and model fine-tuning in industrial settings.,,,,,,,,,,,,,
"Exploration of detection for transparent
Paper Title (use style: paper title)","irregular
Nouvanty Vanya, Suryanto T., Faroqi A.","non-standard
2023","fully 3D
reference-manager",and overlapping objects,,"and integration with robotic manipulation tasks.""",,,,"Use of ontologies and formal semantics to define and structure knowledge graphs, ensuring shared understanding and precise data interpretation.",,"How can knowledge graphs, supported by ontologies and formal semantics, be constructed and applied to integrate diverse healthcare data sources for improved analytics, decision-making, and personalized medicine?","The paper aims to revolutionize patient care by integrating knowledge graphs and closed-form continuous-time liquid neural networks (CfCs) to create digital twins. Using advanced analytics, machine learning, and semantic modeling, the study demonstrates improved real-time healthcare analytics, personalized medicine, early diagnosis, and enhanced clinical decision-making.","The research goal is to revolutionize patient care by integrating knowledge graphs and closed-form continuous-time liquid neural networks (CfCs); the approach synthesizes multimodal patient data using graph embeddings and CfCs, and the principal finding is that this enables real-time analytics and personalized medicine for improved patient outcomes.","Future research should use larger, more diverse datasets and improve sensor calibration and depth processing. Investigating transparent, irregular, and overlapping objects, integrating the framework into robotic manipulation tasks, and exploring alternative multimodal detection designs are recommended. Controlled robotic grasping experiments are also suggested to assess detection performance."
Application of mathematical models (deterministic and stochastic),advanced analytics,and machine learning (including deep learning) for healthcare analytics and prediction.,,,,,,,,,,,,,
"Implementation of real-time analytics and simulation through continuous knowledge graph updates and online learning techniques.""",No information available,"The integration of knowledge graphs and closed-form continuous-time liquid neural networks (CfCs) enables real-time healthcare analytics, personalized medicine, early diagnosis, and improved surgical planning.",,,,,,,,,,,,,
This approach allows for continuous model updates with new patient data,supporting more accurate and informed clinical decisions.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""","Primary outcomes include improved accuracy and reliability in time-series modeling, with recorded values of 0.7362 and 0.7373.",,,,,,,,,,,,,,
The approach outperforms advanced recurrent neural network models in scalability and performance.,,,,,,,,,,,,,,,
Enables real-time analytics,early diagnosis,intervention,and improved surgical planning.,,,,,,,,,,,,
"Facilitates personalized care and better patient outcomes.""",,"Combining knowledge graphs and closed-form continuous-time liquid neural networks (CfCs) enables real-time, adaptable digital twins for patient care.",,,,,,,,,,,,,
This approach supports personalized medicine,early diagnosis,intervention,and improved surgical planning.,,,,,,,,,,,,
Continuous updates and integration of multimodal data improve predictive accuracy and healthcare decision-making.,,,,,,,,,,,,,,,
"Recommendation: Implement this digital twin system to enhance patient outcomes and healthcare efficiency.""",Integration of real-time analytics and simulation with continuously updated digital twin systems for healthcare remains underexplored.,,,,,,,,,,,,,,
Combining detailed anatomical and physiological models with knowledge graphs and predictive models for personalized surgical planning needs further research.,,,,,,,,,,,,,,,
Advanced analytics techniques,including deep learning and simulation,"require more development for dynamic healthcare process evaluation.""",,,,,,,,,,,,,
A human digital twin for the M-Machine,"Saariluoma Pertti, Myllylä Mari, Karvonen Antero, Luimula Mika, Aho Jami",2024,reference-manager,10.1007/s44163-024-00164-x,,,,,,Designing an interaction design process model using Minsky’s M-Machine as an example to model human-AI design processes.,,"How can human information-processing limitations, intentions, and user roles be modeled to improve the design and usability of technical artifacts, such as paper machines, through frameworks like the IEC model and HDTs?","The paper investigates how cognitive mimetics, focusing on information processes, can unify human and machine thinking. Using simulation and protocol analysis, it develops the IEC model to represent operators’ mental processes in paper manufacturing. Findings show the model helps interpret operator actions, though it lacks comprehensive detail.",The research goal is to generalize the IEC model for modeling human-technology interaction; the approach uses cognitive mimetics and empirical analysis of operator thinking; the principal finding is that the IEC model effectively captures the logic of human actions in technical process control.,
Extending digital twinning from technology to human intelligent technology interaction processes.,,,,,,,,,,,,,,,
"Developing a new holistic conceptual tool for designing human digital twins.""","The research is not reproducible. No datasets were generated or analyzed, and code availability is stated as """"Not applicable."""" No source code is provided.","The IEC\_081 model provides a way to represent and interpret operators’ actions in paper machine control, aiding information gathering about their mental processes.",,,,,,,,,,,,,
The model does not offer comprehensive descriptions of ideal processes,states,or precise operator actions.,,,,,,,,,,,,,
"No datasets were generated or analyzed; no quantitative results or p-values are reported.""","The primary outcome is the development of the IEC\_0.81 model, a small-scale operator information processing model.",,,,,,,,,,,,,,
The IEC\_0.81 model allows representation and interpretation of operators’ actions in controlling paper machines.,,,,,,,,,,,,,,,
No statistical values,measured effects,or empirical results are reported.,,,,,,,,,,,,,
"No datasets were generated or analyzed in this study.""","The IEC\_081 model does not provide comprehensive descriptions of ideal processes, states, or remedial acts.",,,,,,,,,,,,,,
The model lacks precise information about operators’ actions.,,,,,,,,,,,,,,,
"Further advancement is needed by examining operator actions in differing circumstances.""",,,,,,,,,,,,,,,
KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge Graph Enhancement for Medical Diagnosis,"Zuo Kaiwen, Jiang Yirui, Mo Fan, Liò Pietro",2025,reference-manager,,,,,,,Context-based data chunking and segmentation: Medical documents are divided into contextually relevant chunks using segmentation rules.,,"How can a hierarchical multi-agent framework integrating LLMs and automated knowledge graph construction improve the accuracy, scalability, and reliability of clinical decision-making in medical diagnosis and treatment using unstructured and multimodal medical data?","KG4Diagnosis introduces a hierarchical multi-agent framework for automated medical knowledge graph construction and diagnostic reasoning. Using BioBERT, LLMs, and medical ontologies, the system segments medical texts, extracts entities/relations, and builds validated knowledge graphs. Results show improved accuracy in diagnosis and treatment recommendations, enhancing clinical decision-making.","KG4Diagnosis aims to enhance clinical decision-making by integrating a hierarchical multi-agent framework with automated medical knowledge graph construction, using advanced semantic extraction techniques, and demonstrates improved diagnostic accuracy and reduced hallucination compared to traditional approaches.",
Semantic-driven entity and relationship extraction: BioBERT and medical ontologies (e.g.,SNOMED-CT,UMLS) are used to extract entities and relationships from the data chunks.,,,,,,,,,,,,,
Hierarchical multi-agent framework: Integrates LLMs with specialized agents for automated knowledge graph construction,diagnosis,"and validation.""","The research will be made publicly available through a GitHub repository upon completion. No current source code is provided in the context, but future public release is planned to facilitate reproducibility.","KG4Diagnosis, a hierarchical multi-agent framework, integrates general and specialized agents for diagnosis across 362 diseases, demonstrating improved diagnostic accuracy and reduced hallucination compared to single-agent systems.",,,,,,,,,,,
The semantic-driven entity extraction and relationship reconstruction modules achieved higher precision than conventional methods; all relationships were validated by medical experts before graph expansion.,,,,,,,,,,,,,,,
"No explicit quantitative results or statistical significance (p-values) are provided in the context.""","KG4Diagnosis maintains diagnostic accuracy while preventing hallucination, representing an advancement over traditional single-agent approaches.",,,,,,,,,,,,,,
The semantic-driven entity extraction and relationship reconstruction modules achieve higher precision compared to conventional methods.,,,,,,,,,,,,,,,
The framework effectively simulates real-world clinical consultations,especially for obesity and related medications (Ozempic and Wegovy).,,,,,,,,,,,,,,
The system’s performance is strong for well-documented conditions but requires further investigation for rare diseases or unusual symptom combinations.,,,,,,,,,,,,,,,
"No explicit statistical values or quantitative results are provided in the context.""","System performance depends on the quality and completeness of the knowledge graph, especially for rare or complex conditions.",,,,,,,,,,,,,,
Challenges exist with rapidly evolving medical knowledge and rare disease combinations not well-represented in training data.,,,,,,,,,,,,,,,
Heavy reliance on high-quality medical data limits deployment in regions with scarce resources.,,,,,,,,,,,,,,,
"Effectiveness for rare diseases or unusual symptom combinations needs further investigation.""",KG4Diagnosis introduces a hierarchical multi-agent framework integrating LLMs and knowledge graphs for accurate medical diagnosis and treatment across 362 diseases.,,,,,,,,,,,,,,
The system achieves higher diagnostic accuracy and reduces hallucination compared to single-agent approaches.,,,,,,,,,,,,,,,
Expert validation ensures reliable,clinically relevant knowledge graph expansion.,,,,,,,,,,,,,,
The framework supports robust,"adaptable clinical decision-making and will set new evaluation standards.""",Limited effectiveness in handling rare diseases or unusual symptom combinations due to insufficient representation in training data.,,,,,,,,,,,,,
Heavy reliance on high-quality medical data,posing challenges for deployment in regions with limited data resources.,,,,,,,,,,,,,,
Need for further benchmarking against state-of-the-art models (e.g.,ESM-1b,Med-PaLM,"BioGPT) using datasets like MedQA to validate and improve the framework.""","Future research should address processing unstructured medical data, preventing and detecting LLM hallucinations, integrating multimodal information, refining agent coordination protocols, and developing standardized evaluation protocols for medical knowledge graph systems. These areas remain challenging and require innovative solutions for improved reliability and effectiveness in clinical applications.",,,,,,,,,,,
Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph,"Prahlad Deeksha, Lee Chanhee, Kim Dongha, Kim Hokeun",2025,reference-manager,10.1145/3701716.3715473,,,,,,"Knowledge Graphs (KGs): Used to structure and store factual data, enabling accurate and context-aware responses while reducing hallucinations.",,"How does integrating knowledge graphs with retrieval augmented generation improve the personalization, accuracy, and efficiency of large language models in generating responses for domain-specific applications such as calendar datasets?","The paper aims to personalize large language models (LLMs) using retrieval augmented generation (RAG) and knowledge graphs (KGs). Using calendar and conversation datasets, the approach builds KGs and leverages embeddings for efficient retrieval. Results show improved accuracy (ROUGE/BLEU scores), reduced hallucinations, and faster execution compared to the baseline.","The research goal is to personalize large language models using Retrieval Augmented Generation and Knowledge Graphs; the approach integrates knowledge graphs to guide LLMs, and results show improved text generation quality, higher ROUGE and BLEU scores, and reduced execution time compared to the baseline.",
Retrieval Augmented Generation (RAG): Integrates external knowledge sources into LLMs to improve reliability and minimize factual errors.,,,,,,,,,,,,,,,
"Evaluation Metrics (ROUGE and BLEU): Used to measure the quality and accuracy of generated responses by comparing n-gram overlaps with reference answers.""",The research is reproducible. The source code and dataset are publicly available for evaluation of personalization approaches.,"Our approach outperforms the baseline in all Llama-2-Chat models (7B, 13B, 70B) for BLEU and ROUGE scores, with average increases of 35.15% (ROUGE-1), 65.57% (ROUGE-2), 35.82% (ROUGE-L), and 61.11% (BLEU-1).",,,,,,,,,,,,,
Execution time is reduced by an average of 8.931% across all models using our approach.,,,,,,,,,,,,,,,
"The improvements are statistically significant
Results:","indicating enhanced text generation quality and better alignment with the golden answer.""","Primary outcomes: Our approach outperforms the baseline in BLEU-1, ROUGE-1, ROUGE-2, and ROUGE-L scores across all Llama-2-Chat models (7B, 13B, 70B).",,,,,,,,,,,,,
Average ROUGE-1 increase: 35.15%,,,,,,,,,,,,,,,
Average ROUGE-2 increase: 65.57%,,,,,,,,,,,,,,,
Average ROUGE-L increase: 35.82%,,,,,,,,,,,,,,,
Average BLEU-1 increase: 61.11%,,,,,,,,,,,,,,,
Execution time reduction: 8.931%,,,,,,,,,,,,,,,
Measured effects: Improved text generation quality,increased lexical overlap with the golden answer,"and faster response times.""","Models lack new domain-specific knowledge, leading to hallucinations.",,,,,,,,,,,,
Data privacy issues arise when using web-based information.,,,,,,,,,,,,,,,
The approach focuses on smaller models,which may limit performance on unseen data.,,,,,,,,,,,,,,
No concrete design for on-device RAG personalization using KGs is provided.,,,,,,,,,,,,,,,
"Further research is needed to improve smaller models' performance on unseen data.""
Average ROUGE-1","The proposed approach using knowledge graphs (KGs) outperforms the baseline in all evaluation metrics (ROUGE, BLEU, execution time) across Llama-2-Chat models (7B, 13B, 70B).
ROUGE-2",ROUGE-L,and BLEU-1 scores increased by 35.15%,65.57%,,35.82%,,,,,,,,respectively.,and 61.11%
Execution time decreased by 8.931% on average.,,,,,,,,,,,,,,,
Recommendation: Use KGs for domain adaptation,improved response quality,faster execution,"and enhanced privacy by enabling on-device LLMs.""",Improving the performance of smaller language models on unseen data for on-device applications.,,,,,,,,,,,
Addressing data privacy concerns by enabling knowledge graph-based personalization without sending sensitive data to external providers.,,,,,,,,,,,,,,,
"Further reducing hallucinations and factual errors in language model outputs through enhanced integration with knowledge graphs.""","Future research should focus on improving the performance of smaller models on unseen data, developing concrete designs for on-device retrieval augmented generation (RAG) with knowledge graphs (KGs), and addressing data privacy issues in personalized applications using KGs and LLMs.",,,,,,,,,,,,,,
Incremental Analysis of Legacy Applications Using Knowledge Graphs for Application Modernization,"Krishnan Saravanan, Mathai Alex, Singhee Amith, Kumar Atul, Agarwal Shivali, Raghunath Keerthi Narayan, Wenk David",2022,reference-manager,10.1145/3493700.3493735,,,,,,"Static analysis of legacy source code to extract information, stored in an MS-SQL database.",,"How can incremental analysis using knowledge graphs assist subject matter experts and architects in systematically identifying, isolating, and modernizing relevant portions of large legacy application codebases?","The paper presents a tool for incremental analysis of legacy software using knowledge graphs. The main objective is to help experts modernize large codebases by defining logical boundaries and dependencies. The methodology combines static analysis, knowledge graph construction, and iterative increment creation. Results show improved focus and efficiency in modernization tasks.","The research goal is to aid legacy application modernization by enabling subject matter experts to incrementally analyze large software systems; the approach uses static analysis and a customizable, language-agnostic knowledge graph; results show effective identification and isolation of relevant code portions for modernization tasks.",
Construction of a knowledge graph using a custom ontology,representing code and dependencies in a Neo4j graph database.,,,,,,,,,,,,,,
"Incremental analysis via neighborhood detection
In a demo scenario","allowing experts to iteratively define and analyze logical code boundaries.""
starting with one transaction (‘SSP3’) as a seed",the increment expanded to include 13 programs and 6 tables.,"The system enables incremental analysis of legacy applications by representing them as a knowledge graph using a language-agnostic ontology, allowing SMEs to focus on relevant code portions and dependencies.",,,,,,,,,,,,
"The approach facilitates modernizing code with minimal external dependencies and provides clear integration points
Results: Demonstrated on GENAPP","but no p-values or statistical significance are reported.""
creating increments (e.g.","Primary outcome: Developed a tool for incremental analysis of legacy applications using knowledge graphs and a customizable ontology.
seed transaction ‘SSP3’ led to an increment with 1 transaction",13 programs,and 6 tables).,,,,,,,,,,,
Measured effects: Aggregate metrics like lines of code (LOC) and Cyclomatic complexity (Cyclo) are tracked for each increment.,,,,,,,,,,,,,,,
"No explicit statistical values provided.""",Current tool leverages only static analysis.,,,,,,,,,,,,,,
Future work needed to understand application data (e.g.,tables) and extract insights from operational logs.,,,,,,,,,,,,,,
Demonstration limited to a mainframe application (GENAPP),"though approach is claimed to be extensible.""",Incremental analysis helps modernize legacy applications by focusing on relevant code portions and minimizing external dependencies.,,,,,,,,,,,,,
The knowledge graph approach is language-agnostic and supports integration of business functions and data domains.,,,,,,,,,,,,,,,
The system enables SMEs to iteratively refine increments,improving modernization efficiency.,,,,,,,,,,,,,,
"Future work includes analyzing application data (tables) and extracting insights from operational logs.""","Need to enhance understanding of application data (e.g., tables) and extract insights from operational logs.",,,,,,,,,,,,,,
Current tool relies mainly on static analysis; future work includes integrating additional data sources.,,,,,,,,,,,,,,,
"Expanding the knowledge graph to incorporate business functions and data domains beyond static analysis.""","Future research directions include understanding the data of the application (such as tables) and extracting insights from operational logs. These areas are suggested as next steps beyond the current tool, which primarily leverages static analysis.",,,,,,,,,,,,,,
A Selective Homomorphic Encryption Approach for Faster Privacy-Preserving Federated Learning,"Korkmaz Abdulkadir, Rao P.",2025,reference-manager,,,,,,,"Implementation of Security Techniques: Three security mechanisms—Homomorphic Encryption (HE), differential privacy, and the proposed Fast and Secure Homomorphic Encryption (FAS)—were implemented within a federated learning framework.",,"How can a federated learning framework integrate selective encryption, differential noise addition, and bitwise scrambling to achieve strong privacy protection and computational efficiency across diverse, privacy-sensitive domains without sacrificing model utility or requiring precomputed sensitivity masks?","The paper introduces FAS, a Fast and Secure Homomorphic Encryption method for federated learning (FL), integrating selective encryption, noise injection, and scrambling. Through comprehensive experiments, FAS demonstrates strong security, reduced computational overhead, and robustness to data skew, outperforming conventional methods and offering practical guidelines for secure, efficient FL deployment.","The research goal is to enhance privacy in federated learning; the approach introduces FAS, combining selective encryption, noise injection, and bitwise scrambling; results show FAS achieves strong security with significantly reduced computational overhead, outperforming traditional methods and maintaining high model utility, especially in healthcare applications.",
Development of FAS: Designed a novel method combining selective encryption,noise injection,and bitwise scrambling to enhance security and efficiency.,,,,,,,,,,,,,
Performance Evaluation: Conducted comprehensive comparisons of encryption techniques using standardized metrics (MSSIM,VIFP) to assess computational overhead,scalability,"and resistance to model inversion attacks.""","No source code for the project is mentioned in the context. The datasets used are publicly available on Kaggle, supporting data reproducibility, but there is no explicit mention of code availability or detailed reproducibility procedures.",,"FAS achieves security comparable to full encryption (FHE) but with significantly reduced training time and computational overhead; at 10% encryption, FAS passes all security tests and achieves MSSIM 58% vs. FedML-HE’s 52%.",,,,,,,,,
FAS demonstrates superior robustness to data skew across all datasets,maintaining stable accuracy and privacy (MSSIM and VIFP scores minimally affected),while other methods degrade under skew.,,,,,,,,,,,,,
Differential privacy offers lightweight protection with minimal computation impact; FHE provides highest security but incurs major computational costs,making FAS the most efficient and scalable for large-scale,"privacy-sensitive applications.""","FAS technique shows superior robustness to data skew across all evaluated datasets (Kidney, Lung, COVID, Diabetic Retinopathy) compared to MASKCRYPT and Fedml-HE.",,,,,,,,,,,,
FAS achieves consistent MSSIM and VIFP scores under both skewed and normal data,e.g.,Kidney dataset: FAS MSSIM 62/61,VIFP 16.5/15.,,,,,,,,,,,,
In COVID dataset with MobileNetV2,FAS achieves 42.50% faster encryption time and 73.91% lower overhead than FEDML-HE.,,,,,,,,,,,,,,
"In COVID dataset with EffNetB0
At 10% encryption","FAS provides 46.15% encryption improvement and 69.77% overhead reduction compared to FEDML-HE; vs. MASKCRYPT: 36.36% faster encryption
FAS achieves MSSIM score of 58%","60.61% lower overhead.
higher than FedML-HE’s 52%",with lower computational costs.,,,,,,,,,,,,
"FAS consistently achieves the lowest training times and computational overhead across all datasets compared to other methods.""",No discussion of limitations or shortcomings is provided in the context.,,,,,,,,,,,,,,
"No self-reported problems
,FAS (Fast and Secure Federated Learning) achieves security comparable to full encryption but with significantly reduced training time and overhead.",open questions,or suggestions for further research are mentioned.,,,,,,,,,,,,,
FAS combines selective encryption,bitwise scrambling,and differential noise,offering strong protection against model inversion attacks without accuracy loss.,,,,,,,,,,,,
FAS is efficient and scalable for large-scale,resource-constrained,and real-time privacy-sensitive applications.,,,,,,,,,,,,,
Recommendation: Use FAS for practical,secure federated learning,"especially in latency-sensitive fields like healthcare.""",Need for privacy-preserving methods that avoid pre-training and per-round mask recalibration while ensuring stable security and low computational overhead.,,,,,,,,,,,,
Balancing strong privacy guarantees with minimal impact on model utility,especially for precision-sensitive applications like medical imaging.,,,,,,,,,,,,,,
Developing cohesive solutions tailored for federated learning workflows,"rather than relying on generic privacy mechanisms.""",Future research should refine the FAS techniques and explore hybrid approaches across diverse datasets and federated environments to enhance scalability and applicability. There is a need to address gaps in optimizing privacy mechanisms tailored for federated workflows and further improve security-performance trade-offs.,"The study design is an experimental framework using federated learning (FL) applied to medical imaging datasets. It is not randomized, double-blind, controlled, or placebo-controlled. The setup uses 11 physical machines in a shared-nothing cluster and evaluates different cryptographic methods and deep learning models.",,,,,,,,,,,,
Towards a Flexible System Architecture for Automated Knowledge Base Construction Frameworks,Din Osman,2019,reference-manager,,,,,,,"Distributed weak supervision using Apache Spark and Snorkel, enabling parallel processing to reduce learning time.",,"How can a scalable, flexible, and extensible architecture for automated knowledge base construction frameworks address current limitations in scalability, extensibility, and usability, while meeting the functional and non-functional requirements needed for effective knowledge base development?","The paper addresses challenges in building knowledge bases and proposes a scalable, flexible, and extensible architecture for automated knowledge base construction (AKBC) frameworks. Using extensions to an existing engine, the methodology improves scalability, usability, and extensibility. Results highlight architectural gaps in current systems. The work aims to benefit AKBC developers and users.","The paper's research goal is to automate knowledge base construction from richly formatted data using a flexible, scalable, and extensible architecture; the key method is an improved framework with new APIs and distributed processing; the principal finding is that this approach outperforms existing systems in scalability, usability, and extensibility.",
Integration with a fairness API to detect and flag potentially discriminatory features in the data.,,,,,,,,,,,,,,,
"Modular architecture for figure analysis
Snorkel: Source code available at https://www.snorkel.org/.
SageKB: Source code available at https://github.com/SageKB/.","separating figure extraction and semantic understanding using NLP techniques.""",DeepDive: Source code available at https://deepdive.stanford.edu.,,,,,,,,,,,,,
"Alexandria: No explicit source code link; paper available online.
MinIE",Knowledge Vault,"Fonduer: No source code information provided.""","The paper presents a scalable, flexible, and extensible architecture for automated knowledge base construction (AKBC) frameworks, addressing current limitations in scalability, extensibility, and usability.",,,,,,,,,,,,
Initial implementation includes distributed weak supervision using Apache Spark and Snorkel,and integration with a fairness API to flag potentially discriminatory features.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""",Presented a flexible architecture for automating knowledge base development.,,,,,,,,,,,,,,
Identified challenges and requirements for AKBC (Automated Knowledge Base Construction) frameworks.,,,,,,,,,,,,,,,
Proposed an architecture more scalable,usable,and extensible than current approaches.,,,,,,,,,,,,,
Initial implementation included extensions to a knowledge base engine.,,,,,,,,,,,,,,,
"No statistical values or quantitative results reported.""","Scalability: Frameworks rely on vertical scaling, which is insufficient for large workloads.",,,,,,,,,,,,,,
Usability: Users must write complex rules and feature extractors,often requiring scripting knowledge.,,,,,,,,,,,,,,
Extensibility: No simple method to add domain-specific features or extend pipelines.,,,,,,,,,,,,,,,
Limited support for transparency and fairness.,,,,,,,,,,,,,,,
No user interface provided.,,,,,,,,,,,,,,,
Insufficient support for scale-out architectural patterns.,,,,,,,,,,,,,,,
Practical requirements like maintainability,security,and system management are out of scope.,,,,,,,,,,,,,
"Future work needed on user interfaces and domain feature integration.""","The study presents a scalable, flexible, and extensible architecture for automated knowledge base construction (AKBC) frameworks, addressing current limitations in scalability, extensibility, and usability.",,,,,,,,,,,,,,
Key implications include the need for easier addition of domain features,pipeline extensibility,user interfaces,and support for transparency and fairness.,,,,,,,,,,,,
Recommendations include developing user interfaces,integrating fairness APIs,"and sharing implementation experiences to guide future improvements.""",Lack of easy integration of domain-specific features; users must hard code these into frameworks.,,,,,,,,,,,,
Limited extensibility and absence of user interfaces,making pipelines hard to extend and less user-friendly.,,,,,,,,,,,,,,
Insufficient support for transparency and fairness,"with no mechanisms to filter or flag potentially discriminatory features.""","Future research should focus on developing user interfaces, enabling easier addition of domain features, improving pipeline extensibility, supporting transparency and fairness, and enhancing scalability and usability. Sharing implementation experiences and case studies is also suggested to address current limitations in AKBC frameworks.",,,,,,,,,,,,,
Enhancing Repository-Level Software Repair via Repository-Aware Knowledge Graphs,"Yang Bo, Tian Haoye, Ren Jiadong, Jin Shunfu, Liu Yang, Liu Feng, Le Bach",2025,reference-manager,,,,,,,Knowledge Graph Mining: Constructs a knowledge graph from codebases and GitHub artifacts to identify top candidate bug locations using both graph analysis and LLM suggestions.,,"How can repository-aware knowledge graphs be leveraged to enhance repository-level software repair by improving bug localization, patch generation, and decision interpretability compared to existing LLM-based approaches?","The paper aims to improve repository-level software repair by constructing a knowledge graph that models structural and semantic relationships among code and repository entities. Using AST parsing and regular expressions, it extracts and connects entities, then ranks function relevance. Results show enhanced bug localization and patch ranking, increasing repair accuracy to 45.67%.","The research goal is to improve repository-level software repair by addressing semantic ambiguity; the approach integrates repository-aware knowledge graphs (KGCompass) to connect issues and code entities, and the principal finding is that KGCompass achieves state-of-the-art repair accuracy (45.67%) and superior bug localization (51.33%) on SWE-bench-lite.",
Patch Generation: Uses LLM prompts augmented with structural path information to generate code patches at identified locations.,,,,,,,,,,,,,,,
Patch Ranking: Evaluates and ranks generated patches using LLM-generated reproduction tests,regression tests,"and patch size prioritization strategies.""",,"Most (69.7%) ground truth bug functions require multi-hop traversal in the knowledge graph, validating the need to model indirect relationships; 55.77% of ground truth patches are ranked first among top-20 relevant functions.",,,,,,,,,,,
KGCompass achieves 45.67% bug resolution,matching SWE-Agent 1.0 and outperforming Kodu (44.67%) and OpenHands (41.67%),with a low average cost per bug ($0.2013).,,,,,,,,,,,,,
"The hybrid KG+LLM approach yields 4.0% higher file-level and 9.1% higher function-level coverage than pure LLM; using entity path information increases successful patches from 102 to 108 (p-values not reported).""","KGCompass achieves a repair success rate of 45.67%, matching SWE-Agent 1.0 (45.33%) and outperforming Kodu (44.67%), OpenHands (41.67%), and PatchKitty (41.33%).",,,,,,,,,,,,,,
KGCompass achieves 51.33% function-level match rate vs SWE-Agent 1.0’s 49.67%.,,,,,,,,,,,,,,,
Average cost per repair for KGCompass is $0.20,compared to SWE-Agent 1.0’s $2.18.,,,,,,,,,,,,,,
KGCompass uniquely resolves 19 bugs not fixed by other open-source approaches and 11 not fixed by closed-source systems.,,,,,,,,,,,,,,,
Using ground-truth tests,KGCompass resolves 143 instances (47.67%).,,,,,,,,,,,,,,
Ablation studies show entity path information increases resolved cases from 102 (34.00%) to 108 (36.00%).,,,,,,,,,,,,,,,
Patch ranking with “Model Patch Size” and regression/reproduction tests increases accuracy to 45.67%.,,,,,,,,,,,,,,,
"Increasing candidate patches from 1 to 6 raises success from 36.00% to 45.67%; upper bound is 47.67%.
With Deepseek V3",KGCompass achieves a repair rate of 36.67%; with Qwen2.5 Max,"33.33%.""","Semantic ambiguity in LLM-based approaches limits precise bug localization, especially in large codebases with overlapping function names.",,,,,,,,,,,,
Less than 50% accuracy in function-level localization on SWE-Bench-Lite.,,,,,,,,,,,,,,,
Study focused only on Python repositories,possibly limiting generalizability.,,,,,,,,,,,,,,
"Further research needed for other programming languages.""",KGCompass achieves state-of-the-art repair success (45.67%) and superior function-level localization (51.33%) at low cost ($0.20/repair).,,,,,,,,,,,,,,
The knowledge graph-based approach generalizes well across LLMs,outperforming closed-source methods even with open-source models.,,,,,,,,,,,,,,
Multi-hop entity path information significantly improves localization and repair accuracy.,,,,,,,,,,,,,,,
"Future improvements in patch validation could further close the small gap to the upper bound (47.67%).""","Need for improved modeling of multi-hop relationships in knowledge graphs, as 69.7% of bug functions require multi-hop traversal.",,,,,,,,,,,,,,
Enhancing integration of repository artifacts (issues and pull requests) into knowledge graphs to better bridge semantic gaps.,,,,,,,,,,,,,,,
"Further optimization of patch ranking and candidate selection strategies to maximize repair precision and efficiency.""",Future research should explore integrating more repository artifacts (like issues and pull requests) and leveraging indirect entity relationships for patch generation. There is also a need to investigate the generalizability of KGCompass to programming languages beyond Python and further analyze the impact of structural context in repair tasks.,,,,,,,,,,,,,,
Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs,"Wei Yifan, Yu Xiaoyan, Pan Tengfei, Li Angsheng, Du Li",2025,reference-manager,,,,,,,Monte Carlo Tree Search (MCTS): An algorithm used to explore the knowledge graph and generate data by simulating different entity trajectories.,,"How can synthetic data generation and filtering techniques, as implemented in SENATOR, improve the quality and reliability of medical question-answer datasets for training and evaluating large language models?","The paper introduces SENATOR, a framework to identify and address knowledge gaps in large language models (LLMs) for the medical domain. Using synthetic data generation and targeted evaluation, SENATOR efficiently supplements missing knowledge, improving LLM performance on medical benchmarks by 11.98% (Llama-3-8B) and 9.15% (Qwen2-7B).","The research goal is to efficiently detect and repair knowledge deficiencies in medical domain LLMs; the SENATOR framework identifies gaps and synthesizes targeted data for fine-tuning, resulting in average performance improvements of 11.98% (Llama-3-8B) and 9.15% (Qwen2-7B) across four medical benchmarks.",
Supervised Fine-Tuning (SFT): The model is trained using cross-entropy loss with specified hyperparameters.,,,,,,,,,,,,,,,
"Data Filtering: Samples are filtered based on logical connection and hallucination criteria to ensure quality.""",,"Incorporating more synthetic data during supervised fine-tuning (SFT) consistently improves model performance, with Llama and Qwen models achieving up to 47.38 and 52.74 average scores, respectively.",,,,,,,,,,,,,
Synthetic data expands the coverage of pretraining data,effectively addressing both shared and model-specific knowledge deficiencies in medical LLMs.,,,,,,,,,,,,,,
SENATOR demonstrates effective deficiency correction even when synthetic data is swapped between models,"highlighting the generalizability of the approach; no explicit p-values are reported.""","Primary outcome: SENATOR framework improves Llama-3-8B and Qwen2-7B average performance on four medical benchmarks by 11.98% and 9.15%, respectively.",,,,,,,,,,,,,
Qwen2 model with SENATOR achieves 40% accuracy on the Genetics component of GPQA.,,,,,,,,,,,,,,,
"SENATOR uses less synthetic data (26k–128k samples) than previous methods (514k samples).""","Reliance on high-quality, domain-specific knowledge graphs limits applicability where such resources are incomplete or unavailable.",,,,,,,,,,,,,,
Synthetic data generation quality can be improved; current focus is more on detecting gaps than optimizing data synthesis.,,,,,,,,,,,,,,,
Manual analysis found 37.92% of QA samples had errors: formulaic (16.77%),logical (19.56%),hallucination (1.59%).,,,,,,,,,,,,,
Base model struggles with multi-hop reasoning,causing logical errors.,,,,,,,,,,,,,,
Knowledge boundaries in large models are unclear,"leading to unreliable or contradictory synthetic samples in specialized domains.""","The SENATOR framework effectively detects and repairs knowledge deficiencies in large language models (LLMs) for the medical domain, improving average benchmark performance by 11.98% (Llama-3-8B) and 9.15% (Qwen2-7B).",,,,,,,,,,,,,
Targeted synthetic data generation efficiently addresses specific knowledge gaps,even with less data than previous methods.,,,,,,,,,,,,,,
Expanding synthetic data further enhances model performance,highlighting the value of targeted data supplementation.,,,,,,,,,,,,,,
"Reliance on high-quality knowledge graphs and synthetic data quality are current limitations; future work should address these dependencies and improve data generation methods.""","Reliance on high-quality, domain-specific knowledge graphs limits applicability; future work aims to relax this dependency by constructing approximate knowledge graphs or using retrieval-augmented methods.",,,,,,,,,,,,,,
The current synthetic data generation process can be improved; future work will explore advanced techniques for more relevant,diverse,and factual data.,,,,,,,,,,,,,
"Efficient data synthesis must be tightly coupled with effective detection of knowledge deficiencies to avoid redundancy and better repair model gaps.""","Future research should focus on reducing reliance on high-quality, domain-specific knowledge graphs by exploring automatic KG construction or retrieval-augmented methods. Improving synthetic data generation—using advanced techniques for greater relevance and diversity—and adding entity type constraints for more precise domain exploration are also recommended.",No information available,,,,,,,,,,,,,
New generation Energy Performance Certificate: Development and application in an Italian case study as an EU proof of concept,"Salvalai Graziano, Sesana Marta Maria, Isacco Ilaria",2024,reference-manager,10.1051/e3sconf/202452306001,,,,,,Application of eight Evaluation Strategies: Each case study used one of eight strategies based on its characteristics and preliminary data analysis to assess the EPC RECAST approach.,,"How does the EPC RECAST testing method compare the standard national EPC procedure with the new generation EPC in terms of user-friendliness, reliability, and effectiveness for building energy performance assessment and renovation roadmap creation?","The paper aims to verify the user-friendliness and reliability of the EPC RECAST methodology for energy performance assessment. Using case studies across six EU countries, it benchmarks a new generation EPC process against standard procedures. Results highlight applicability and improvement potential, concluding the method is effective and reliable.","The research goal is to verify the user-friendliness and reliability of the EPC RECAST methodology by benchmarking it against standard national EPC procedures using case studies in six EU countries; the approach involves multi-step testing and comparative evaluation, and results highlight the methodology's applicability and potential for improvement.",
Four-step EPC RECAST testing method: (1) Site inspection preparation,(2) On-site data collection,(3) In-office modeling and calibration,(4) EPC and Renovation Roadmap creation.,,,,,,,,,,,,
"Comparative benchmarking: Energy consumption calculated by the standard national procedure is compared with the new generation EPC RECAST procedure to evaluate reliability and improvements.""",The research has completed two out of four validation steps. The methodology uses standard national calculation tools and the EPC RECAST online interface. No source code for the project is provided in the context. Full reproducibility will be addressed in future publications after all steps are completed.,"Two out of four steps of the EPC RECAST methodology have been applied and validated, demonstrating the approach's applicability and identifying areas for improvement.",,,,,,,,,,,,,
Long-Term Monitoring data collection using IoT sensors was found to be cost-effective,reliable,and precise for different room sizes and complexities.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are provided in the context.""","Primary outcomes focus on comparing energy consumption calculated by different methods (EPC RECAST, standard national procedures, calibrated/non-calibrated models) with measured consumption (LTM campaign).",,,,,,,,,,,,,,
Results assess reliability,effectiveness,and impact of EPC RECAST data collection and calculation engine.,,,,,,,,,,,,,
"No explicit statistical values or measured effects are provided in the context.""",Only two out of four steps of the developed methodology have been applied and validated.,,,,,,,,,,,,,,
Complete validation of the methodology is pending and will be addressed in future publications.,,,,,,,,,,,,,,,
"Further research is needed to apply and verify the remaining steps.""","The EPC RECAST approach has been successfully applied and partially validated in real case studies, demonstrating its applicability and potential for improvement.",,,,,,,,,,,,,,
Long-Term Monitoring using IoT sensors is cost-effective and provides reliable,high-quality data.,,,,,,,,,,,,,,
Tools like questionnaires and georeferenced photos help prevent information loss during data collection.,,,,,,,,,,,,,,,
"Full methodology validation is pending further application of the remaining steps.""",Full validation of the EPC RECAST methodology is pending; only two out of four steps have been completed.,,,,,,,,,,,,,,
Future work will focus on data mining,model calibration,and the development of the Renovation Roadmap.,,,,,,,,,,,,,
"Assessment through the Quantitative Verification Strategy remains to be completed and published.""",Future research should focus on the complete validation of the EPC RECAST methodology by applying and verifying the two remaining steps. Further studies are needed to assess the reliability and improvement potential of EPCs using the new generation data collection and calculation methods.,,,,,,,,,,,,,,
Unifying Large Language Models and Knowledge Graphs: A Roadmap,"Pan Shirui, Luo Linhao, Wang Yufei, Chen Chen, Wang Jiapu, Wu Xindong",2024,reference-manager,,,,,,,KG-enhanced LLM pre-training: Uses knowledge graphs (KGs) during the pre-training stage to improve large language models' (LLMs) knowledge expression.,,How can large language models (LLMs) and knowledge graphs (KGs) be synergistically integrated to improve knowledge representation and reasoning for tasks such as question answering?,"The paper investigates methods for unifying large language models (LLMs) and knowledge graphs (KGs) to enhance knowledge representation and reasoning. It categorizes integration approaches, describes frameworks like KG-enhanced LLMs and Synergized LLMs + KGs, and highlights improved performance and interpretability in downstream tasks through synergistic models.","The paper's research goal is to unify large language models (LLMs) and knowledge graphs (KGs) using a roadmap of three integration frameworks; its approach categorizes and reviews methods for KG-enhanced LLMs, LLM-augmented KGs, and synergized LLMs+KGs; results highlight complementary strengths and future research directions.",
KG-enhanced LLM inference: Utilizes KGs during the inference stage,allowing LLMs to access up-to-date knowledge without retraining.,,,,,,,,,,,,,,
LLM-augmented KG construction: Applies LLMs for entity discovery,coreference resolution,"and relation extraction in building KGs.""",No information available,"The synergy of Large Language Models (LLMs) and Knowledge Graphs (KGs) enhances knowledge representation and reasoning, leveraging LLMs' language understanding and KGs' factual accuracy.",,,,,,,,,,,
Representative synergized methods include JointGT (2021),KEPLER (2021),DRAGON (2022),HKLM (2023),LARK (2023),,KSL (2023),,,,,,,,,and StructGPT (2023).
"No explicit quantitative results or statistical significance (p-values) are provided in the context.""","No explicit primary outcomes, results, or measured effects (including statistical values) are provided for the listed methods in the context.",,,,,,,,,,,,,,
No numerical performance data or statistical analysis is reported for BertCR,Spanbert,CDLM,CrossCR,CR-RL,,SentRE,"LLMs represent knowledge implicitly, making it hard to interpret or validate.",,,,,,"or other methods mentioned.""",DREEAM,Curriculum-RE
LLMs lack interpretability and decisiveness; their reasoning is probabilistic and not directly explainable.,,,,,,,,,,,,,,,
LLMs may hallucinate or provide incorrect information,especially in high-stakes scenarios.,,,,,,,,,,,,,,
LLMs struggle with domain-specific or new knowledge due to limited training data.,,,,,,,,,,,,,,,
KGs are often incomplete and hard to construct,limiting comprehensive knowledge.,,,,,,,,,,,,,,
KGs may not model unseen entities or new facts effectively.,,,,,,,,,,,,,,,
KGs typically ignore textual information,focusing mainly on structure.,,,,,,,,,,,,,,
"KGs lack language understanding compared to LLMs.""",Integrating knowledge graphs (KGs) with large language models (LLMs) enhances performance and interpretability in downstream tasks.,,,,,,,,,,,,,,
Retrieval-Augmented Knowledge Fusion methods,like RAG,outperform baseline models in open-domain question answering and generate more factual,diverse text.,,,,,,,,,,,,
"Document-level relation extraction benefits from LLM-based and graph-based approaches for improved relation identification.""","Effective knowledge injection for black-box LLMs remains an open challenge, especially due to limited access and prompt length constraints.",,,,,,,,,,,,,,
Bridging the gap between multi-modal LLMs and knowledge graph structures is a significant challenge requiring further research.,,,,,,,,,,,,,,,
"Developing methods for accurate encoding and alignment of entities across different modalities in knowledge graphs is a key future direction.""","Future research should address effective knowledge injection for black-box LLMs, bridging multi-modal LLMs and KG structures, and enhancing the synergy between LLMs and KGs for bidirectional reasoning. Challenges include modality alignment, input length limits, and integrating advanced techniques like multi-modal learning and graph neural networks.",,,,,,,,,,,,,,
Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving,"Srinivas Sakhinana Sagar, Vaikunth Vijay Sri, Runkana Venkataramana",2024,reference-manager,,,,,,,"Human-centric evaluation: Eight aspects (e.g., user satisfaction, usability, task completion) rated by humans using Likert-scale surveys, Yes/No completion, and qualitative feedback.",,"How can a modular, AI-driven Process Engineering Operations Assistant (PEOA) framework leveraging graph retrieval-augmented generation and expert models automate and enhance complex problem-solving, error handling, and decision support in the chemical and process industry?","The paper introduces the PEOA framework for automating complex problem-solving in process engineering. Using a modular, tool-integrated approach evaluated by human-centric metrics and benchmark datasets, PEOA achieves strong performance, closely matching leading proprietary LLMs. Results highlight its adaptability, effective tool use, and potential for broader application and further enhancement.",The research goal is to automate complex process engineering tasks; the approach combines instruction-tuned small language models with Graph Retrieval-Augmented Code Generation (GRACG) and advanced knowledge graph techniques; results show the PEOA framework outperforms ablated variants and matches leading proprietary LLMs in accuracy and adaptability.,
Custom benchmark datasets: MathComp and ChemProc datasets developed for training and evaluation,covering mathematical and chemical engineering problems.,,,,,,,,,,,,,,
Quantitative metrics: Used metrics like BLEU,ROUGE-L,Exact Match,Recall,NDCG,,and percentage-based measures for tool usage,"The PEOA framework achieved high performance across MathComp and ChemProc datasets, with baseline scores such as Consistency (80.41%), Precision (78.66%), and Error Handling (77.05%) in MathComp.",,,,,,,"and error handling.""",parameter extraction
Ablation studies showed significant drops in performance when removing GRACG,instruction-tuning,or error-handling,confirming their importance.,,,,,,,,,,,,
The framework closely matches leading proprietary LLMs in effectiveness,"offering a modular and adaptable solution for complex chemical and process engineering tasks. No p-values or statistical significance values are reported.""","The PEOA framework (Baseline) consistently outperforms ablated variants across all evaluation metrics in task planning, tool selection, tool calling, and response generation.",,,,,,,,,,,,,
Key metrics for MathComp (Baseline): Recall 78.82%,NDCG 0.69,COMP 76.79%,TUA 78.87%,PR 73.83%,,Acc 75.94%.,,,,,,,,,
Key metrics for ChemProc (Baseline): Recall 77.77%,NDCG 0.68,COMP 75.55%,TUA 76.96%,PR 71.63%,,Acc 74.52%.,,,,,,,,,
Exact Match (EM) for MathComp: PEOA 73.68%,Triplex 59.50%,Graph RAG 72.50%.,,,,,,,,,,,,,
Exact Match (EM) for ChemProc: PEOA 74.13%,Triplex 63.20%,Graph RAG 73.80%.,,,,,,,,,,,,,
Response generation (MathComp): BLEU 83.77,ROUGE-L 79.53,EM 80.88.,,,,,,,,,,,,,
"Response generation (ChemProc): BLEU 83.12
Ablation results: Removing GRACG","ROUGE-L 80.19
Instruction-Tuning","EM 84.95.
or Error-Handling significantly reduces performance across all metrics.",,,,,,,,,,,,,
Human evaluation: User satisfaction and usability rated on a scale of 1–5; qualitative feedback categorized as High,Medium-High,or Medium.,,,,,,,,,,,,,
"The framework demonstrates high answer relevance
Conclusion: The complete PEOA framework achieves optimal performance in specialized engineering tasks and closely matches leading proprietary LLMs.""",context relevance,"faithfulness
The PEOA framework demonstrates effective performance across task planning, tool selection, tool calling, and response generation, with competitive scores on key metrics.",and correctness (e.g.,PEOA Math: AnwRel 83.47,,ConRel 82.03,,,,,,,,Correct 85.65).,Faith 81.79
Human-centric evaluation highlights strengths in user satisfaction,usability,adaptability,and error handling.,,,,,,,,,,,,
Qualitative feedback rates PEOA as Medium to Medium-High,suggesting room for improvement.,,,,,,,,,,,,,,
"Recommendation: Further enhance adaptability and user experience.""",Traditional RAG techniques struggle with global questions that require a holistic understanding of knowledge bases.,,,,,,,,,,,,,,
Instruction-tuning small-scale language models (SLMs) is needed for effective adaptation to complex,domain-specific tasks.,,,,,,,,,,,,,,
"Integration of graphs with LLMs and RAG remains an emerging research area
Paper Title (use style: paper title)","requiring further exploration for optimal performance.""
Nouvanty Vanya, Suryanto T., Faroqi A.",2023,reference-manager,,,,,,,"Modular cognitive architecture: The study presents a modular cognitive architecture centered on a """"cognitive ledger"""" for lifelogging user activities and digitizing user minds.",,"How can a modular cognitive architecture leveraging blockchain-based decentralized infrastructure return control of users' digital intelligence and personal data to the users themselves through secure, immutable cognitive ledgers for cognitive digital twins?","The paper proposes a modular cognitive architecture for developing cognitive digital twins, centered on a """"cognitive ledger"""" that lifelogs user activities. Using participatory design, the study defines requirements and suggests blockchain for secure, immutable storage. The main implication is empowering users to control their digital intelligence and privacy.","The research goal is to return control of users' digital intelligence to themselves by proposing a modular cognitive architecture using blockchain for secure, user-owned cognitive digital twins; the approach involves lifelogging and decentralized storage, and the principal finding is that blockchain can ensure information security and user ownership.",
Blockchain-based decentralized infrastructure: Blockchain is evaluated and proposed for secure,immutable storage of user information,memory pool records,and NFTs.,,,,,,,,,,,,
"Personalized machine learning (ML) models: The ML Trainer Engine trains personalized models to compose the user's cognitive digital twin.""",,,,The system is still a work in progress; evaluation of blockchain technologies is ongoing.,,,,,,,,,,,
Blockchain ledger size may become unmanageable (terabytes per node after a few years).,,,,,,,,,,,,,,,
Risk of closed source shell applications stealing user information.,,,,,,,,,,,,,,,
Some knowledge objects are too large for blockchain metadata fields; require external decentralized storage.,,,,,,,,,,,,,,,
"Further research needed on shell design and development.""",The proposed modular cognitive architecture enables users to own and manage their digital minds securely using blockchain-based cognitive ledgers.,,,,,,,,,,,,,,
Blockchain ensures the security and immutability of user data,addressing privacy concerns.,,,,,,,,,,,,,,
"Future work should focus on designing secure shell applications and further developing the cognitive blockchain infrastructure.""",Evaluating and selecting suitable blockchain technologies for storing memory pool records and NFTs.,,,,,,,,,,,,,,
Developing relevant ontologies for storing the mental layer.,,,,,,,,,,,,,,,
Advancing automated machine learning (AutoML) methods for the learning layer,"as current systems lack full automation.""","Future research should focus on evaluating the proposed architecture in more scenarios, selecting and developing ontologies for the mental layer, finalizing the blockchain network and storage protocol, and designing shells using cognitive blockchain. Further investigation is needed on the impacts of blockchain consensus protocols and achieving full autonomy.","Participatory design methodology; work in progress; recruited participants organized into two-person teams; observational and feedback-based; modular architecture design; not randomized, not controlled, not blinded, not retrospective, not a meta-analysis or systematic review.",,,,,,,,,,,,
Affordable AI Assistants with Knowledge Graph of Thoughts,"Besta Maciej, Paleari Lorenzo, Jiang Jia Hao Andrea, Gerstenberger Robert, Wu You, Iff Patrick, Kubíček Aleš, Nyczyk Piotr, Khimey Diana, Hannesson J'on Gunnar, Kwa'sniewski Grzegorz, Copik Marcin, Niewiadomski H., Hoefler Torsten",2025,reference-manager,,,,,,,Direct Retrieval: Used for broad contextual understanding by extracting relevant information directly from the knowledge graph.,,"How can prompt engineering techniques, specifically the use of generic few-shot examples embedded in prompt templates, improve decision-making processes and accuracy in solving problems using knowledge graphs?",No information available,Research goal: reduce AI task execution costs while maintaining high success rates; approach: iteratively build a structured knowledge graph (KG) from unstructured data for efficient reasoning; results: KGoT enables smaller models to match larger models' performance at lower cost by leveraging structured KG-based reasoning.,
Graph Queries and Scripts: Employed for structured reasoning,allowing more complex and logical task-solving.,,,,,,,,,,,,,,
KG-Based Representation: Converts outputs and reasoning steps into explicit triples,reducing noise,mitigating bias,"and improving fairness.""",No information available,,"KGoT achieved a 71.06% F1 score on SimpleQA, significantly surpassing the top reasoning model (49.4%) and all OpenAI models (best: 62.5%).",,,,,,,,,
Performance was statistically significant,with KGoT improving upon all mini-reasoning models by at least 3.5× (p-value not explicitly stated).,,,,,,,,,,,,,,
Toolset and prompt format optimizations improved accuracy,runtime,"and cost efficiency; XML-based prompts were retained for consistency.""","KGoT achieved a 71.06% F1 score on SimpleQA, outperforming all OpenAI and Claude models (best OpenAI: 62.5%, best reasoning model: 49.4%).",,,,,,,,,,,,
Accuracy rate was 70.34% with no performance degradation.,,,,,,,,,,,,,,,
Merged tool sets improved accuracy,runtime,and cost efficiency.,,,,,,,,,,,,,
Best configuration: HF DR XML solved 37 tasks in 11.87h at $7.84; merged GQ XML solved 31 tasks in 10.62h at $5.43.,,,,,,,,,,,,,,,
Markdown prompts improved accuracy and cost but were not consistently reproducible and interfered with optimizations.,,,,,,,,,,,,,,,
"Noise mitigation (irrelevance removal) improved reasoning by simplifying knowledge graphs.""","No explicit limitations, shortcomings, or suggestions for further research are stated in the provided context.",The study demonstrates a structured approach to determine if further mathematical or probabilistic calculations are needed after retrieving partial solutions.,,,,,,,,,,,,,
"It recommends returning """"""""True"""""""" if more calculations are required and """"""""False"""""""" otherwise",focusing strictly on the necessity for further computation.,,,,,,,,,,,,,,
"The process emphasizes not inventing information and using only explicitly provided data.""",No information available,"Suggested future research directions include: developing new knowledge graph construction methods using predictive graph models, integrating with neural graph databases, deploying over distributed-memory clusters for scalability, and refining reasoning strategies through advanced task decomposition schemes to improve performance on long-horizon tasks.",,,,,,,,,,,,,
KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models,"Mohbat Fnu, Zaki Mohammed J.",2025,reference-manager,,,,,,,"Fine-tuned LoRA (Low-Rank Adaptation) adapters were used for each task, with specific hyperparameters (r = 64, α = 16, dropout = 0.5).",,"How can the integration of food knowledge graphs and large language models be leveraged to provide personalized food recommendations, generate recipes, and deliver comprehensive nutritional information based on user queries with dietary constraints and preferences?","The paper introduces KERL, a system for recipe recommendation, generation, and nutrition estimation using knowledge graphs and large language models. Using benchmarks and metrics like BLEU, Rouge, and F1, KERL-Recom outperforms state-of-the-art LLMs, achieving a 56-point F1 improvement over Phi-3-mini-128K. The approach demonstrates superior constraint handling and recipe personalization.","The paper's research goal is recipe recommendation under complex constraints; it introduces KERL, a knowledge graph-based approach using fine-tuned LLMs, and finds that KERL-Recom significantly outperforms state-of-the-art models in F1 score for constrained recipe retrieval.",
Recipe generation was benchmarked using the Recipe1M dataset and template prompts for generating recipes from titles and/or ingredients.,,,,,,,,,,,,,,,
Performance was evaluated using standard retrieval (precision,recall,F1) and text generation metrics (BLEU,Rouge,METEOR,,"CIDer).""",,,,,,,,"The fine-tuned LoRA KERL-Recom model outperformed others, achieving a 56-point F1 improvement over Phi-3-mini-128K and 26 points over Llama-2-7B.","The research is reproducible. The source code and benchmark datasets are available at https://github.com/mohbattharani/KERL. Experimental setup details, training parameters, and evaluation metrics are explicitly described, supporting reproducibility."
KERL-Recom demonstrated superior handling of complex constraints in KGQA benchmark questions compared to state-of-the-art LLMs.,,,,,,,,,,,,,,,
"Statistical significance (p-values) is not reported in the provided context.""",KERL-Recom achieved a 56-point F1 score improvement over Phi-3-mini-128K and 26-point improvement over Llama-2-7B.,,,,,,,,,,,,,,
KERL-Nutri showed lower mean absolute error (MAE) per micro-nutrient compared to LLaVA-Chef (e.g.,sugar MAE: 1.84 vs. 5.43).,,,,,,,,,,,,,,
KERL outperformed baselines in recipe relevance,cooking step quality,"and nutrient value accuracy.""","KERL-Recom depends on recipe subgraphs from FoodKG; if no recipe meets all constraints, no recommendation is made.",,,,,,,,,,,,
The system may fail if incorrect context information is provided; results require safeguards.,,,,,,,,,,,,,,,
"KERL-Recom does not directly link health conditions to dietary restrictions; this is left for future research.""","KERL-Recom significantly outperforms state-of-the-art LLMs in recipe recommendation, achieving a 56-point F1 improvement over Phi-3-mini-128K and 26 points over Llama-2-7B.",,,,,,,,,,,,,,
KERL modules deliver more relevant recipes,higher quality cooking steps,and more accurate nutrient values than baselines.,,,,,,,,,,,,,
Limitations include dependency on FoodKG coverage and lack of direct health condition-dietary restriction mapping.,,,,,,,,,,,,,,,
Future work should incorporate ingredient substitution,health information,"and cultural preferences for improved personalization.""","Lack of unified food understanding systems that combine personalized recipe recommendation, cooking steps, and micro-nutrition information.",,,,,,,,,,,,
Limited research on integrating food-related Knowledge Graphs (KGs) with Large Language Models (LLMs),especially considering both health constraints and user preferences.,,,,,,,,,,,,,,
"The full potential of KG and LLM integration in food science remains underexplored.""","Future research should focus on integrating ingredient substitution, personal health information, and cultural preferences into the system. Additionally, establishing direct relationships between health conditions and dietary restrictions is suggested, as current methods do not accurately address these aspects.",,,,,,,,,,,,,,
A Human Digital Twin Architecture for Knowledge-based Interactions and Context-Aware Conversations,"Mohammed Abdul Mannan, Mohammad Azhar Ali, Ortiz Jason A., Neumann Carsten, Bochenek Grace, Reiners Dirk, Cruz-Neira Carolina",2024,reference-manager,,,,,,,"Performance evaluation using scenario-based testing: The HDT system was assessed across multiple phases (e.g., pre-check, identification, instructional guidance) with success, conditional success, and failure rates recorded.",,"How can an HDT system that integrates metacognition, affective computing, and advanced conversational agents provide effective instructional guidance, problem-solving, and emotional support during assembly tasks while addressing performance, usability, and data privacy challenges?","The paper investigates the HDT system’s ability to assist users in gun assembly tasks. Using scenario-based testing, HDT provided accurate instructions, solutions, and emotional support, achieving high success rates. The study concludes that HDT is effective and plans future improvements in performance, user studies, and data privacy.","The paper's main objective is to enhance Human Digital Twin (HDT) systems for Human-Autonomy Teams (HATs) using an open architecture with Large Language Models and metacognition; the key method integrates multimodal data and modular components, and the principal finding is improved task accuracy, user experience, and adaptability, with optimization needed.",
User feedback simulation: Pre-defined conversational paragraphs were used to simulate user feedback and evaluate system adaptability.,,,,,,,,,,,,,,,
Processing time analysis: Module processing times (e.g.,speech recognition,image capture,"LLM response) were measured to identify performance bottlenecks.""",,,"The HDT system achieved a 100% success rate in both instructional guidance (7/7 scenarios) and recommendations/solutions phases (10/10 scenarios), providing accurate instructions and effective problem-solving without user feedback.",,,,,,,,,
In the pre-check phase,the HDT succeeded in 5/10 scenarios without feedback and required feedback for the remaining 5; no failures occurred.,,,,,,,,,,,,,,
"No statistical significance or p-values were reported in the context.""
Results:",Primary outcomes measured: HDT system performance during a gun assembly task across five phases.,,,,,,,,,,,,,,
Pre-check: 10 scenarios (5 success,5 conditional success,0 failure),,,,,,,,,,,,,
Identification: 10 scenarios (10 success,0 conditional success,0 failure),,,,,,,,,,,,,
Instructional Guidance: 7 scenarios (7 success,0 conditional success,0 failure),,,,,,,,,,,,,
Recommendations/Solutions: 10 scenarios (10 success,0 conditional success,0 failure),,,,,,,,,,,,,
Emotional Support: 11 scenarios (11 success,0 conditional success,0 failure),,,,,,,,,,,,,
Assembly Verification: 6 scenarios (3 success,3 conditional success,0 failure),,,,,,,,,,,,,
"Measured effects: Success was defined as correct response without user feedback; conditional success required up to five feedback attempts; failure was inability to provide correct response after five attempts. No failures occurred in any phase.""",System performance varies with conversation complexity and user emotional cues; optimization needed.,,,,,,,,,,,,,,
High variability in image capture and LLM response times affects responsiveness.,,,,,,,,,,,,,,,
Assembly verification accuracy needs improvement.,,,,,,,,,,,,,,,
No current data privacy or security measures; must be addressed.,,,,,,,,,,,,,,,
"No comprehensive user studies conducted yet.""","The HDT system effectively delivers clear instructions, proactive problem-solving, and emotional support, enhancing user experience and task accuracy.",,,,,,,,,,,,,,
Limitations exist in assembly verification accuracy and processing times,requiring optimization.,,,,,,,,,,,,,,
Future work should focus on user studies,custom domain-specific models,"and improved data privacy and security.""","Improve overall system performance to maintain uniform interactive rates, including exploring distributed computing and custom LLMs trained on domain-specific data.",,,,,,,,,,,,
Conduct comprehensive user studies to evaluate effectiveness,usability,and trust in diverse HATs activities.,,,,,,,,,,,,,
Address data privacy and security,including customization of data sharing,content filtering,"and abuse monitoring.""","Three future research directions are suggested: 1) optimizing system performance and exploring custom domain-specific LLMs; 2) conducting comprehensive user studies to assess effectiveness, usability, and trust; 3) addressing data privacy and security, including customization of shared data and content filtering. Incorporating metacognition is also recommended.",,,,,,,,,,,
TLogic: Temporal Logical Rules for Explainable Link Forecasting on Temporal Knowledge Graphs,"Liu Yushan, Ma Yunpu, Hildebrandt Marcel, Joblin Mitchell, Tresp Volker",2022,reference-manager,,,,,,,"Experiments were conducted on three ICEWS datasets, split into training, validation, and test sets with time-aware filtering.",,"What is the effectiveness of learning and applying temporal logical rules for link prediction in temporal knowledge graphs, as evaluated on the ICEWS datasets using metrics such as mean reciprocal rank (MRR) and hits@k?","The paper introduces TLogic, a symbolic framework for learning temporal logical rules from temporal knowledge graphs to forecast links. Using rule confidences and time differences, TLogic outperforms baselines on ICEWS datasets. The method provides interpretable predictions and shows low variance. Future work includes integrating acyclic rules and improved sampling.","The research goal is explainable link forecasting on temporal knowledge graphs; the approach introduces TLogic, a symbolic framework using temporal random walks to learn temporal logical rules; results show TLogic outperforms state-of-the-art baselines and provides human-readable, time-consistent explanations.",
TLogic uses logical rule learning with temporal constraints,aggregating candidate scores using a noisy-OR calculation.,,,,,,,,,,,,,,
Hyperparameter tuning was performed,"selecting best values based on MRR (Mean Reciprocal Rank) on the validation set.""",The research is reproducible. The source code for the project is available at https://github.com/liu-yushan/TLogic. A random seed of 12 is fixed for the rule learning component to ensure reproducible results. All experimental details and hyperparameters are provided.,"TLogic achieves the best results for MRR, hits@3, and hits@10 across ICEWS14, ICEWS18, and ICEWS0515 datasets; only xERTE outperforms TLogic in hits@1 for ICEWS18 and ICEWS0515.",,,,,,,,,,,,
Exponentially weighted transition distributions consistently outperform uniform sampling,especially as the number of walks increases.,,,,,,,,,,,,,,
"The combination of rule confidence and exponential score components yields the best overall performance; statistical significance (p-values) is not reported.""","TLogic outperforms all baseline methods in MRR, hits@3, and hits@10 on ICEWS14, ICEWS18, and ICEWS0515.",,,,,,,,,,,,,,
xERTE performs better than TLogic for hits@1 on ICEWS18 and ICEWS0515.,,,,,,,,,,,,,,,
TLogic achieves MRR/h@1/h@3/h@10:,,,,,,,,,,,,,,,
ICEWS14: 0.4304 / 0.3356 / 0.4827 / 0.6123,,,,,,,,,,,,,,,
ICEWS18: 0.2982 / 0.2054 / 0.3395 / 0.4853,,,,,,,,,,,,,,,
ICEWS0515: 0.4697 / 0.3621 / 0.5313 / 0.6743,,,,,,,,,,,,,,,
Exponentially weighted transition outperforms uniform sampling.,,,,,,,,,,,,,,,
Increasing the number of walks improves all metrics.,,,,,,,,,,,,,,,
The combined score function yields the best overall performance; optimal balance depends on application and metric prioritization.,,,,,,,,,,,,,,,
Object distribution baseline does not significantly improve results.,,,,,,,,,,,,,,,
"Proportion of cases with no rules for test query relation:
ICEWS14: 0.00056
ICEWS18: 0.00021
ICEWS0515: 0.00007",,,,,,,,,,,,,,,
"Proportion of cases with no matching body groundings:
ICEWS14: 0.0333
ICEWS18: 0.0256
ICEWS0515: 0.0172""","Memory constraints require reducing the time window for large datasets (e.g., ICEWS18: 200, ICEWS0515: 1000).",,,,,,,,,,,,,,
Results are based on a single algorithm run due to small variance.,,,,,,,,,,,,,,,
Only rules with minimum confidence (0.01) and body support (2) are considered.,,,,,,,,,,,,,,,
"No explicit mention of generalizability beyond tested datasets.""","TLogic outperforms all baseline methods in most metrics (MRR, hits@3, hits@10) across ICEWS14, ICEWS18, and ICEWS0515 datasets.",,,,,,,,,,,,,,
Combining rule confidence and exponential time weighting yields the best performance.,,,,,,,,,,,,,,,
Exponentially weighted transition distributions consistently outperform uniform sampling.,,,,,,,,,,,,,,,
"Object distribution baseline adds little value; temporal rules are essential for strong predictive performance.""","Existing embedding-based methods lack transparency and interpretability, making predictions difficult for humans to understand.",,,,,,,,,,,,,,
Symbolic methods using logical rules face scalability issues,limiting their practicality on large real-world datasets.,,,,,,,,,,,,,,
Manual creation of logical rules is challenging due to the complexity of events,"leading to a knowledge acquisition bottleneck.""","As future work, it is suggested to integrate acyclic rules, which could provide additional relevant information and potentially improve performance for rules of length 2. Also, replacing the simple sampling mechanism for temporal walks with a more sophisticated approach to better identify promising walks is recommended.",,,,,,,,,,,,,
Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage,"Li Zhuohang, Zhang Jiaxin, Liu Luyang, Liu Jian",2022,reference-manager,,,,,,,"Differential Privacy (DP): A technique that adds random noise to gradients to limit privacy leakage, providing theoretical guarantees about information disclosure.",,How can privacy-preserving defenses in federated learning effectively prevent data reconstruction attacks from degraded gradients while maintaining model utility?,"The paper investigates privacy preservation in federated learning, comparing cryptography-based and gradient-degradation-based methods. Using various defenses, it evaluates image reconstruction attacks and proposes new optimization strategies. Results show that current defenses, including Soteria, are insufficient, as high-quality image reconstruction from gradients remains possible, highlighting ongoing privacy risks.","The research goal is to achieve privacy preservation in federated learning; the key approach compares cryptography-based and gradient-degradation-based defenses, highlighting Soteria, which perturbs data representations; the principal finding is that while such defenses limit information leakage, they may reduce model utility and struggle with out-of-distribution data.",
Gradient Compression/Sparsification: Reduces the amount of information in shared gradients by compressing or making them sparse,helping prevent information leakage.,,,,,,,,,,,,,,
"Secure Multi-Party Computation (MPC): Cryptographic protocols enabling parties to jointly compute functions over private data without revealing individual inputs.""","The research provides source code at https://github.com/PatrickZH/Improved-Deep-Leakage-from-Gradients and https://github.com/JonasGeiping/invertinggradients. Experimental results are reported, supporting reproducibility. No explicit mention of code for the proposed Generative Gradient Leakage (GGL) method is found.","The proposed GGL method achieves the lowest MSE-I (0.0780–0.0968), highest PSNR (10.1434–11.1902), and lowest LPIPS (0.1620–0.2561) across all tested defenses, outperforming DLG, iDLG, IG, and GI.",,,,,,,,,,,,,
GGL demonstrates superior data reconstruction accuracy under additive noise,gradient clipping,gradient sparsification,and Soteria defenses.,,,,,,,,,,,,
"No explicit p-values or statistical significance values are reported in the context.""","The primary outcome is that the proposed GGL method reconstructs high-quality images from gradients under all four defense schemes: additive noise, gradient clipping, gradient sparsification, and Soteria.",,,,,,,,,,,,,,
GGL achieves the lowest MSE-I (0.0780–0.0968),highest PSNR (10.1434–11.1902),and lowest LPIPS (0.1620–0.2561) across defenses,outperforming DLG,iDLG,,IG,,,,,,,,Merely relying on secure multi-party computation (MPC) is insufficient to resist inference attacks over the output.,"and GI."""
Differential privacy (DP) often requires adding too much noise,reducing the utility of trained models.,,,,,,,,,,,,,,
Many data reconstruction attacks assume idealized,bare-bone FL systems without real-world privacy defenses,contradicting industrial practices.,,,,,,,,,,,,,
Existing attack methods are limited to shallow networks and low-resolution images,"though some recent works extend to deeper networks and higher resolutions.""","Cryptography-based (e.g., secure multi-party computation) and gradient-degradation-based (e.g., differential privacy) approaches both help preserve privacy in federated learning, but each has limitations.",,,,,,,,,,,,,
Differential privacy offers strong privacy guarantees but can reduce model utility due to added noise.,,,,,,,,,,,,,,,
Gradient compression/sparsification and defenses like Soteria can further mitigate information leakage.,,,,,,,,,,,,,,,
"No single method fully prevents privacy leakage; combining strategies is recommended.""","Recovering high-resolution batch data with multiple local steps is still a major challenge, especially for batch sizes greater than 1 and large images.",,,,,,,,,,,,,,
Faithfully reconstructing out-of-distribution image samples remains difficult due to limitations imposed by the GAN latent space.,,,,,,,,,,,,,,,
"Existing privacy-preserving methods often reduce model utility or are insufficient against advanced inference attacks.""","Future research should address the challenge of recovering high-resolution batch data with multiple local steps, as current methods are limited to small images or specific scenarios. Further investigation is needed into privacy leakage under various defense strategies and the reconstruction of in-the-wild data.","The study design characteristics include: comparative evaluation of cryptography-based and gradient-degradation-based privacy approaches in federated learning; implementation of multiple attack baselines; use of various defense schemes; randomized trials with different seeds; multi-dataset (CelebA, ImageNet) experiments; and quantitative and qualitative analysis of reconstruction results.",,,,,,,,,,,,,
Named Entity Resolution in Personal Knowledge Graphs,Kejriwal M.,2023,reference-manager,,,,,,,"Rule-based systems: Use manually specified rules for entity resolution, especially in the Semantic Web community.",,"How can Entity Resolution methods be advanced to effectively address challenges of scalability, heterogeneity, and domain-independence in large, diverse, and structurally varied datasets, particularly within the context of Personal Knowledge Graphs and emerging AI technologies?","The paper examines Named Entity Resolution (ER) in Personal Knowledge Graphs, focusing on evaluating blocking and similarity steps. Using precision, recall, and F-Measure as key metrics, it highlights the tradeoffs between efficiency and effectiveness. The study concludes that a two-step, independent workflow remains standard due to insufficient evidence favoring integrated approaches.","The research goal is to improve Named Entity Resolution in Personal Knowledge Graphs using systematic, machine learning-based blocking key approaches, with results showing enhanced scalability and effectiveness compared to traditional manual methods.",
Machine learning approaches: Primarily supervised methods that learn from labeled pairs of duplicates and non-duplicates to optimize performance.,,,,,,,,,,,,,,,
"Hybrid and low-supervision techniques: Combine rule-based and machine learning methods
For blocking","including approaches like active learning.""
Pairs Completeness (PC) and Reduction Ratio (RR) are used","No information available
with their F-Measure as a single-point tradeoff metric.",,"Primary outcomes are measured using precision, recall, and their harmonic mean (F-Measure) for the similarity step.",,,,,,,,,,,
"No specific numerical results or statistical values are provided.""",Benefits of new blocking techniques over traditional methods are not extensively established.,,,,,,,,,,,,,,
Difficulty in addressing domain-independence,scalability,and heterogeneity simultaneously.,,,,,,,,,,,,,
Transfer learning is challenging to execute and has not achieved high quality compared to supervised approaches.,,,,,,,,,,,,,,,
Ambiguity and inconsistency in constructing ground truths for complex types introduce evaluation noise.,,,,,,,,,,,,,,,
Schema-free approaches are novel,"with unresolved conceptual and methodological questions.""",Low-supervision methods can effectively detect urgency in short crisis messages.,,,,,,,,,,,,,
Transfer learning enables these models to adapt to new crisis scenarios with minimal labeled data.,,,,,,,,,,,,,,,
The approach reduces the need for extensive manual annotation,"making it scalable for real-world crisis response.""","Advancing schema-free approaches to Entity Resolution (ER) to address data diversity and heterogeneity, including effective feature construction and automation.",,,,,,,,,,,,,
Improving property alignment and ground truth construction for complex,cross-domain knowledge graphs to reduce evaluation noise.,,,,,,,,,,,,,,
"Applying and enhancing transfer learning for ER to reduce training data requirements and improve scalability across domains.""","Future research should focus on schema-free approaches to Entity Resolution (ER), improving property alignment and ground truth construction, applying transfer learning for domain adaptation, addressing scalability in large distributed systems, developing domain-specific Knowledge Graphs (KGs), and integrating deep learning with traditional ER methods. Structural heterogeneity remains a key challenge.",,,,,,,,,,,,,,
Human Body Digital Twin: A Master Plan,"Tang Chenyu, Gao Shuo, Occhipinti Luigi G.",2023,reference-manager,,,,,,,"Use of advanced nanotechnology for designing and fabricating sensitive, adaptable, and comfortable sensors to collect human data over long periods.",,"What is a comprehensive, five-level roadmap for the development and deployment of human body digital twins (DTs) in healthcare, and what are the key technological, security, cost, and ethical challenges that must be addressed to advance this field?","The paper proposes a five-level roadmap for modeling the human body Digital Twin (DT), aiming to improve healthcare. It highlights Level 5 models, which focus on explainability using interpretability techniques and causal inference. Although still early, these models promise deeper insights and future clinical impact.","The paper's main objective is to propose a five-level roadmap for human body digital twin (DT) modeling; it introduces explainable models using advanced interpretability techniques as the key method, concluding that such models can enhance understanding but are still in early stages and not yet clinically guiding.",
Application of Self-supervised Learning (SSL) algorithms to utilize large amounts of unannotated data for model development.,,,,,,,,,,,,,,,
Development of low-cost,scalable,"and self-powered sensors using energy harvesting technology.""",,"The NHS has implemented strict data security measures, including GDPR compliance, secure networks (e.g., NHSmail), and access controls, to protect patient data.",,,,,,,,,,,
A national data opt-out system allows patients to control the use of their personal data for research and planning.,,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are provided.""","No primary outcomes, results, or measured effects (including statistical values) are explicitly stated in the provided context.","Scarcity of human data, especially clinician-annotated data, limits model development.",,,,,,,,,,,,,
Security concerns in data collection,transmission,and storage.,,,,,,,,,,,,,
Ethical issues: data privacy,informed consent,potential misuse/discrimination,and equitable access.,,,,,,,,,,,,
"Level 5 (explainable models) is still in its infancy and lacks real clinical guidance.""",A five-level roadmap for human body Digital Twin (DT) models is proposed to unify research and guide future development.,,,,,,,,,,,,,,
Level 5 models focus on explainability,using advanced interpretability techniques to enhance understanding and trust.,,,,,,,,,,,,,,
Key challenges include data scarcity,security,cost,and ethical issues such as privacy and equitable access.,,,,,,,,,,,,
"Recommendations include promoting collaboration
Ethical issues","addressing ethical concerns
including data privacy","and leveraging technological advances for data collection and model development.""
informed consent","Scarcity of human data, especially clinician-annotated data, limits human body Digital Twin (DT) development.
potential misuse",and equitable access,,must be addressed.,,,,,,,,,
"Need for a unified five-level roadmap to standardize modeling and foster interdisciplinary collaboration in human body DT research.""","Future research should address the scarcity of clinician-annotated human data, enhance data-sharing initiatives, ensure security and privacy, tackle ethical issues such as equitable access, and develop affordable, scalable solutions. Further investigation into interdisciplinary collaboration and technological advancements is also recommended.",,,,,,,,,,,,,,
"""""Describe and query semantic building digital twin data in temporal Knowledge Graphs""""","""""Yingying Zhang and Jakob Beetz""""",2023,reference-manager,,,,,,,"Temporal Knowledge Graph (TKG): Proposed to integrate and represent both static building context data and dynamic sensor data streams, enabling complex temporal queries.",,"How can temporal knowledge graphs be optimally represented and queried to efficiently integrate and reason over dynamic sensor data and static building context data in the AEC domain, enabling complex queries and supporting real-world applications?","The paper introduces a Temporal Knowledge Graph approach to better represent and query dynamic sensing and monitoring data in buildings. Using RDF and SPARQL, it integrates static and sensor data for complex queries. Findings highlight current limitations and propose future improvements for efficient, dynamic knowledge graph representation and reasoning.","The research goal is to integrate temporal data into semantic building digital twin graphs using a temporal knowledge graph (TKG) approach; the method involves extracting and linking BIM and sensor data via RDF and SPARQL; results show simple queries are feasible, but complex queries and efficient representation need further study.",
RDF (Resource Description Framework): Used to structure and describe building and sensor data,including temporal attributes.,,,,,,,,,,,,,,
"SPARQL Queries: Employed to retrieve and analyze temporal sensor observations and building context information from the knowledge graph.""",The research proposes a reusable structured method to add temporal attributes to RDF triples and a temporal knowledge graph-based data integration schema. There is no explicit mention of source code availability for the project.,"The study introduced time constraints into semantic building digital twin graphs and demonstrated that simple time or entity-specific queries are achievable, but complex cluster queries remain challenging.",,,,,,,,,,,,,
No quantitative results or p-values were reported.,,,,,,,,,,,,,,,
The proposed temporal knowledge graph-based schema enables integration of static and dynamic building data,"but further research is needed for efficient complex queries and optimal graph representation.""",Primary outcome: Introduction of a temporal knowledge graph-based data integration schema for linking static building data and dynamic sensor data using RDF format.,,,,,,,,,,,,,
Measured effects: Enables complex queries through SPARQL and supports integration of heterogeneous data.,,,,,,,,,,,,,,,
"Results/statistical values: No statistical values or quantitative results reported.""",Efficiency issues may arise with temporal knowledge graphs due to high data throughput in real projects; optimal representation needs further investigation.,,,,,,,,,,,,,,
Current query syntax struggles with complex,practical application-level queries; most methods only handle simple snapshot queries.,,,,,,,,,,,,,,
Further research is needed for complex queries (e.g.,cluster queries,event pattern matching).,,,,,,,,,,,,,
Integration of dynamic sensor data and building context data into a unified graph is not fully achieved,limiting complex reasoning.,,,,,,,,,,,,,,
Existing approaches for adding temporal attributes to knowledge graphs have limitations in documenting complete knowledge evolution.,,,,,,,,,,,,,,,
Extending semantic language syntax increases statement complexity,"though it adds flexibility.""","Integrating time constraints into semantic building digital twin graphs is feasible, but efficiency and optimal representation need further research due to high data volumes.",,,,,,,,,,,,,
Current query methods handle simple queries but struggle with complex,real-world scenarios; improved methods are needed.,,,,,,,,,,,,,,
Unified temporal knowledge graphs can enhance data integration,reasoning,and automation in the AEC industry.,,,,,,,,,,,,,
"Future work should focus on optimizing temporal knowledge graph representation and developing advanced query capabilities for complex event patterns.""","Efficient representation of temporal knowledge graphs for high-throughput, real-world AEC projects needs further investigation.",,,,,,,,,,,,,,
Current query methods for Temporal Knowledge Graphs struggle with complex queries; research is needed for advanced query capabilities like cluster queries and event pattern matching.,,,,,,,,,,,,,,,
"Integration of dynamic sensor data and building context data into a unified graph for comprehensive reasoning remains a significant challenge.""","Future research should focus on: integrating dynamic sensor and building context data into unified graphs; improving efficiency and representation of temporal knowledge graphs for real projects; developing advanced query methods for complex, event-based queries; and enhancing reasoning capabilities to extract new insights from evolving knowledge graphs.",No information available,,,,,,,,,,,,,
"Agentic Publications: An LLM-Driven Framework for Interactive Scientific Publishing, Supplementing Traditional Papers with AI-Powered Knowledge Systems","Pugliese Roberto, Kourousias George, Venier Francesco, Garlatti Costa Grazia",2023,reference-manager,,,,,,,"Automated data acquisition using APIs, web scraping, and researcher submission portals to collect research papers and findings.",,"How can an AI-powered system efficiently acquire, process, structure, and deliver scientific knowledge from diverse sources while enabling interactive, transparent, and personalized exploration and discovery for users in complex research domains?","The paper presents the Agentic Publication concept, aiming to automate scientific knowledge extraction, structuring, and interactive exploration. Using NLP, embeddings, and knowledge graphs, the system parses articles, extracts metadata, and enables semantic search. A limited demo was developed, demonstrating feasibility and informing further development (Pugliese, 2025).","The paper’s main objective is to develop an Agentic Publication system for scientific knowledge management; its key method combines automated data ingestion, NLP-based content parsing, semantic indexing, and transparent reasoning; the principal finding is a working demo that enables trustworthy, source-cited answers and continuous knowledge updates.",
Natural Language Processing (NLP) techniques for parsing,cleaning,and structuring raw documents into manageable content and extracting metadata.,,,,,,,,,,,,,
Embeddings indexing with pre-trained models (e.g.,"SciBERT) to enable semantic search and retrieval of relevant information.""","The research demonstrates reproducibility through a limited Agentic Publication demo, accessible via DOI (https://doi.org/10.34965/agenticpublication.3567a). The manuscript describes open documentation of criteria, model training, and algorithms, with a proposal for open access to model weights, code, and knowledge base (except sensitive content).","The system uses NLP and embedding models (like SciBERT) to extract, structure, and index scientific literature from sources such as PubMed, Web of Science, and Scopus for efficient semantic search and knowledge graph population.",,,,,,,,,,,,
Metadata and content extraction enable transparent,traceable answers,with visualizations and summaries aiding user understanding.,,,,,,,,,,,,,
No quantitative results,statistical significance (p-values),"or primary research findings are reported in the context.""","No specific primary outcomes, results, or measured effects (including statistical values) are reported in the provided context.",Risk of status-quo bias by over-trusting established sources.,,,,,,,,,,,
Potential misuse for unethical purposes.,,,,,,,,,,,,,,,
Privacy concerns with unpublished or sensitive data.,,,,,,,,,,,,,,,
Intellectual property and recognition challenges.,,,,,,,,,,,,,,,
Legal issues with copyrighted content.,,,,,,,,,,,,,,,
"LLMs may produce incorrect or unsupported statements (""""""""hallucinations"""""""").",,,,,,,,,,,,,,,
Limited context window for LLMs.,,,,,,,,,,,,,,,
LLMs lack genuine comprehension.,,,,,,,,,,,,,,,
Static knowledge in LLMs may hinder relevance in fast-changing fields.,,,,,,,,,,,,,,,
"Biases in LLM training data can affect fairness and accountability.""","Visual and summarization tools, paired with clear explanations, help users understand complex concepts quickly and efficiently.",,,,,,,,,,,,,,
Trust and transparency are essential; users should be able to trace answers to their sources and understand the system’s reasoning.,,,,,,,,,,,,,,,
Interactive exploration and adjustable detail levels let users tailor information depth to their needs.,,,,,,,,,,,,,,,
Multi-modal responses (e.g.,graphs,tables,"images) enhance engagement and knowledge delivery.""","Limited access, publication delays, and concentration of publishing channels hinder timely and equitable knowledge dissemination.",,,,,,,,,,,
Current AI systems struggle with representing complex knowledge types (e.g.,mathematical proofs,nuanced arguments,highly visual insights).,,,,,,,,,,,,
Need for AI systems that actively generate hypotheses,identify research gaps,and provide personalized,"dynamic knowledge curation.""","Future research should focus on enhanced reasoning and hypothesis generation, interactive exploration capabilities, integration of real-time experimental data, improved knowledge representation (especially for mathematical and visual content), robust evaluation frameworks, and evolving legal and policy governance for AI-driven scientific systems.",,,,,,,,,,,
SENAI: Towards Software Engineering Native Generative Artificial Intelligence,"Saad Mootez, López José Antonio Hernández, Chen Boqi, Ernst Neil, Varró Dániel, Sharma Tushar",2025,reference-manager,10.1145/nnnnnnn.nnnnnnn,,,,,,"Refactoring code snippets with low cohesion and high coupling to improve these attributes, evaluated using test suite execution and software metrics (e.g., lcom, cbo).",,"How can evaluation strategies and training processes for large language models be adapted to assess and improve their understanding and application of core software engineering principles such as modularity, cohesion, coupling, and abstraction, beyond mere functional correctness?","The paper aims to adapt evaluation strategies for language models to assess their understanding of software engineering (SE) principles like cohesion and coupling. Using taxonomy-based frameworks and new benchmarks, the study proposes tasks such as code refactoring and analysis. Findings highlight current limitations and suggest improvements for SE-native AI models.","The paper's main objective is to align large language models with software engineering (SE) principles by proposing taxonomy-based evaluation methods that assess design quality, and its principal finding is that current models lack SE knowledge, necessitating new benchmarks focused on maintainability, cohesion, and coupling.",
Ranking and analyzing code fragments by cohesion and coupling to assess model understanding of these concepts.,,,,,,,,,,,,,,,
"Evaluating model judgments on class rankings based on cohesion or coupling measures for validity.""",,"Current benchmarks mainly assess code correctness, not adherence to good software design principles like modularity, cohesion, and coupling.",,,,,,,,,,,,,
New evaluation strategies propose tasks such as refactoring and ranking code by cohesion/coupling to better assess software engineering knowledge.,,,,,,,,,,,,,,,
No quantitative results,conclusions,"or statistical significance (p-values) are provided in the context.""","Primary outcomes focus on evaluating models' understanding of software engineering principles, especially cohesion and coupling.",,,,,,,,,,,,
Results are measured using software metrics like lcom (lack of cohesion of methods) and cbo (coupling between objects),as well as test suite execution.,,,,,,,,,,,,,,
Token-based metrics (e.g.,BLEU,ROUGE,CodeBLEU) and execution-based metrics (e.g.,pass@k,,execution accuracy) are used.,,,,,,,,,
Benchmarks include HumanEval,MBPP,and BigCodeBench.,,,,,,,,,,,,,
The evaluation assesses correctness,code quality (cohesion/coupling),"and adherence to good software design practices.""","Current evaluation benchmarks focus on code correctness, not adherence to good software design practices.",,,,,,,,,,,,
Models may generate syntactically correct but poorly structured code,leading to inflexible and hard-to-maintain systems.,,,,,,,,,,,,,,
Existing models struggle with higher-level design patterns and object-oriented programming concepts like inheritance and encapsulation.,,,,,,,,,,,,,,,
There is a gap between software engineering expectations and model outputs.,,,,,,,,,,,,,,,
"Further research is needed to integrate higher-level design and architectural insights into model training and evaluation.""",Current language models focus on syntactic and functional correctness but neglect key software engineering (SE) principles like cohesion and coupling.,,,,,,,,,,,,,,
New benchmarks and evaluation strategies are needed to assess adherence to SE best practices,not just code correctness.,,,,,,,,,,,,,,
Training and evaluation should integrate SE knowledge to produce maintainable,scalable,and robust software.,,,,,,,,,,,,,
Instruction-based models and taxonomy-based frameworks (e.g.,"Bloom’s Taxonomy) are recommended for comprehensive assessment of SE knowledge.""",Current probing methods (like linear classifier probes) are limited in capturing complex and abstract software engineering (SE) concepts internalized by models.,,,,,,,,,,,,,
There is a need for comprehensive assessment frameworks,such as Bloom’s Taxonomy,to evaluate higher-order cognitive skills in code generation.,,,,,,,,,,,,,
Existing models overlook essential SE knowledge,including principles and best practices for maintainable,scalable,robust,"and reliable software.""",,"Future research should explore integrating higher-level design and architectural insights into model training, develop comprehensive assessment strategies using frameworks like Bloom’s Taxonomy, address data limitations for artifacts such as UML diagrams and ADRs, and investigate methods to enhance models’ understanding of complex software engineering concepts.",,,,,,,,,
A contrastive learning framework with dual gates and noise awareness for temporal knowledge graph reasoning,"Feng Siling, Chen Bolin, Liu Qian, Huang Mengxing",2025,reference-manager,10.1038/s41598-025-00314-w,,,,,,Multi-dimensional gated update module: Optimizes the ability to capture long-distance dependencies in temporal knowledge graphs using a dual-gate selection strategy.,,How can a dual-gate and noise-aware framework effectively model potential temporal connections in temporal knowledge graphs to improve the accuracy of extrapolative reasoning for predicting future facts?,"The paper aims to improve extrapolative reasoning for predicting future facts by proposing the DNCL model, which uses multi-layer embedding contrastive learning to enhance temporal relationship modeling. Experiments on four datasets show significant performance gains, especially from the contrastive learning module. The study concludes this module is crucial for model effectiveness.","The research goal is to improve temporal knowledge graph reasoning accuracy and robustness; the DNCL approach combines a multi-dimensional gated update module, noise-aware adversarial modeling, and multi-layer embedding contrastive learning; results show DNCL outperforms existing models on four benchmarks, especially in handling long-distance dependencies and noise.",
Noise-aware adversarial modeling: Enhances model robustness by generating and discriminating noise for adversarial training.,,,,,,,,,,,,,,,
"Multi-layer embedding contrastive learning: Mines temporal relationship connections using intra-layer and inter-layer comparative learning strategies.""",No information available,"The DNCL model achieved the highest MRR of 51.18% on ICEWS14, outperforming previous methods by up to 4.73%.",,,,,,,,,,,,,
Removing the multi-layer embedding contrastive learning module caused the largest performance drop (MRR decreased by 3.68% on ICEWS14),demonstrating its key role.,,,,,,,,,,,,,,
"No statistical significance (p-values) is reported in the context.""
On ICEWS14","DNCL achieved the best overall performance across four datasets (ICEWS14, ICEWS05-15, ICEWS18, GDELT) using MRR and Hits@1/3/10 metrics.
DNCL reached MRR 51.18%",outperforming others (L2TKG: 47.40%,LogCL: 48.87%,BH-TDEN: 39.3%).,,,,,,,,,,,
Removing the multi-layer embedding contrastive learning module caused a 3.68% drop in MRR on ICEWS14 (from 51.18% to 47.50%).,,,,,,,,,,,,,,,
DNCL consistently showed higher Hits@1/3/10 values compared to baseline models across all datasets.,,,,,,,,,,,,,,,
"The multi-layer embedding contrastive learning module significantly improved model robustness and generalization.""",Difficulty in capturing long-distance dependencies in sparse data environments.,,,,,,,,,,,,,,
Challenge in effectively handling noise interference in input data.,,,,,,,,,,,,,,,
Difficulty in modeling potential connections between temporal relationships.,,,,,,,,,,,,,,,
Need for further research on integrating multimodal information and optimizing for ultra-large-scale,ultra-sparse,and strongly noisy data.,,,,,,,,,,,,,
Suggestion to explore advanced methods like large language models,meta-learning,"and incremental learning.""",The multi-layer embedding contrastive learning module significantly improves model performance and embedding space richness.,,,,,,,,,,,,
Removing this module leads to the largest performance drop,confirming its key role.,,,,,,,,,,,,,,
DNCL outperforms other models across all evaluation metrics and datasets.,,,,,,,,,,,,,,,
"Recommendation: Retain both intra-layer and inter-layer contrastive learning for optimal results.""","Combining multimodal information (text, images, sensor data) in the DNCL model to improve time series reasoning.",,,,,,,,,,,,,,
Optimizing the DNCL model structure and algorithm for ultra-large-scale,ultra-sparse,and strongly noisy data.,,,,,,,,,,,,,
Introducing methods like large language models,meta-learning,"and incremental learning to enhance adaptability.""","Future research should explore integrating multimodal information (text, images, sensor data) into the DNCL model, optimizing its structure and algorithms for ultra-large-scale, ultra-sparse, and noisy data, and applying advanced methods like large language models, meta-learning, and incremental learning to improve adaptability.",,,,,,,,,,,,
Advances in Privacy Preserving Federated Learning to Realize a Truly Learning Healthcare System,"Madduri Ravi, Li Zilinghan, Nandi Tarak, Rodriguez Alex, Ryu Minseok, Kim Kibaek",2024,reference-manager,,,,,,,"Multimodal learning: Integrates different data types (e.g., images, tabular data, text) to improve predictions by capturing complementary information from each modality.",,How can Privacy-Preserving Federated Learning (PPFL) be integrated into the healthcare ecosystem to enable collaborative analysis of multimodal biomedical data across diverse sources while safeguarding patient privacy and advancing the vision of a learning healthcare system?,"The paper addresses challenges in integrating multimodal biomedical data across disparate sources for improved healthcare AI. Using embedding techniques and federated learning, it enables secure, privacy-preserving data analysis. The study highlights improved model performance and data readiness assessment, concluding that such integration enhances personalized prognosis, diagnosis, and treatment planning.","The paper's main objective is to integrate Privacy-Preserving Federated Learning (PPFL) into healthcare systems using a collaborative approach, with the principal finding that PPFL can overcome data sharing and privacy challenges to enable a truly learning healthcare system.",
Embedding generation: Transforms each data modality into a unified representation,enabling joint analysis and improved model performance.,,,,,,,,,,,,,,
Data fusion techniques: Includes early fusion (combining embeddings before prediction),joint fusion (updating embeddings during training),"and late fusion (combining predictions from separate models).""","The research emphasizes reproducibility through FAIR principles \[14], \[15] and mentions open-source frameworks such as APPFL \[16], \[17] and AIDRIN \[23]. Source code for APPFL is referenced in \[17] (https://arxiv.org/abs/2409.11585). No direct source code links are provided for other projects.","Multimodal biomedical data is often siloed due to administrative, privacy, and regulatory barriers, impeding comprehensive analysis and integration.",,,,,,,,,,,
Implementing cost-aware schedulers and data readiness tools like AIDRIN can reduce financial barriers and optimize data preparation for federated learning (FL) in healthcare.,,,,,,,,,,,,,,,
No explicit quantitative results,primary findings,"or statistical significance (p-values) are provided in the context.""",,"Vulnerability to model inversion and gradient inversion attacks, risking data privacy.",,,,,,,,,,,
Difficulty in calculating the optimal privacy budget for biomedical multimodal datasets.,,,,,,,,,,,,,,,
"Limited trust boundaries may cause biased
Small","localized datasets and restrict generalizability.
fragmented datasets for rare diseases hinder capturing complex patterns.",,,,,,,,,,,,,,
Infrastructure challenges: heterogeneous computational resources,identity/access management,ease of use,privacy for different data types,and reproducibility.,,,,,,,,,,,
Data silos due to administrative,privacy,and regulatory constraints impede data integration and sharing.,,,,,,,,,,,,,
Need for additional efforts in data extraction,transformation,"and loading.""",No information available,Integration of hierarchical federated learning (FL) to extend trust boundaries and enhance collaboration among diverse healthcare institutions.,,,,,,,,,,,
Development of federated evaluation and continuous learning mechanisms for real-time model updates and relevance.,,,,,,,,,,,,,,,
"Creation of cost-effective algorithms to enable adoption of federated learning by institutions with limited resources.""",Future research should focus on developing methods to calculate the optimal privacy budget for biomedical multimodal datasets to prevent model inversion attacks while improving model performance. Further investigation is also needed to address the complexities of multimodal federated learning (FL).,,,,,,,,,,,,,,
"Generative AI for Software Architecture. Applications, Challenges, and Future Directions","Esposito Matteo, Li Xiaozhou, Moreschini Sergio, Ahmad Noman, Cerny Tomas, Vaidhyanathan Karthik, Lenarduzzi Valentina, Taibi Davide",2025,reference-manager,,,,,,,Systematic search and selection: Used defined search terms and inclusion/exclusion criteria to identify relevant white and grey literature.,,"What is the current state of research and practice regarding the application of Generative AI in Software Architecture, including targeted architectural styles, maintenance tasks, analysis methods, use cases, and associated challenges?","The paper aims to review how Generative AI (GenAI) and Large Language Models (LLMs) are used in software architecture. Using systematic searches and quality assessments, 46 papers (36 white, 10 grey) were analyzed. The study identifies main research questions, methodologies, and significant findings, offering insights and future challenges in this field.","The paper's main objective is to review how GenAI and LLMs are used in software architecture, using a systematic search and selection approach, and finds that GenAI offers promising opportunities but highlights the need for explainable AI-generated architectural decisions.",
Quality assessment: Applied a checklist and Likert scale to evaluate study quality,involving multiple authors and resolving disagreements with a third reviewer.,,,,,,,,,,,,,,
Data extraction with open coding: Two authors extracted data using open coding,"with a third author resolving disagreements.""",,36 white literature (WL) and 10 gray literature (GL) primary studies were included after quality assessment; 37 out of 39 papers passed the quality threshold.,,,,,,,,,,,,
Study types: 40% case studies,14% peer-reviewed literature,10% tool reviews,3% exploratory studies,3% proof-of-concept,,1% survey.,,,,,,,,,
"No explicit quantitative results or statistical significance (p-values) are reported in the provided context.""","46 papers included: 36 white literature (78%), 10 grey literature (22%).",,,,,,,,,,,,,,
Study types: 40% Case Study,14% Experiment,3% Exploratory Study,3% Proof of Concept,1% Survey,,10% Tool Review,2% Position Paper.,,,,,,2% Industry Report,54% Full Paper,9% Blog Post
Quality assessment for grey literature: sources scoring below 0.5 (scale 0–1) were excluded.,,,,,,,,,,,,,,,
Data extraction used open coding by two authors,with a third author resolving disagreements.,,,,,,,,,,,,,,
"No explicit statistical values or measured effects reported in the context.""",Potential subjectivity in the analysis of selected studies (construct validity threat),,,,,,,,,,,,,,
Possible incorrect conclusions about causal relationships (internal validity threat),,,,,,,,,,,,,,,
Inability to evaluate the external validity of all included studies (external validity threat),,,,,,,,,,,,,,,
Potential non-inclusion of relevant studies (conclusion validity threat),,,,,,,,,,,,,,,
"Survey correctness and completeness may affect results""","AI-assisted programming offers significant short-term opportunities but requires explainability, especially for architectural decisions.",,,,,,,,,,,,,,
There are multiple future directions,including generating models or graphs (like UML sketches) to aid practitioners.,,,,,,,,,,,,,,
Validity threats exist,mainly regarding the completeness and correctness of the literature survey.,,,,,,,,,,,,,,
"Recommendations include careful study selection and quality assessment to mitigate validity threats.""",LLM accuracy and hallucinations: There is a need for systematic validation and formal verification of AI-generated architectural solutions to ensure reliability.,,,,,,,,,,,,,,
Ethical considerations and privacy: Integrating ethical AI practices,addressing biases,transparency,and privacy concerns are significant future directions.,,,,,,,,,,,,
Human oversight and explainability: Ensuring human interaction,oversight,"and explainability in AI-assisted programming is a key research gap.""","Future research should focus on improving LLM accuracy, systematic validation, and formal verification of AI-generated solutions. Other directions include ethical AI practices, privacy, explainability, integration across all SALC phases, handling heterogeneous input sources, DevOps integration, and frameworks for continuous architecture and system evolution.",,,,,,,,,,,,
"MARK: Memory Augmented Refinement of Knowledge
""llm\_response\_memory"""""""": \[","Ganguli Anish, Deb Prabal, Banerjee Debleena",2023,reference-manager,,,,,,,,,,,{{,
"""User acceptance is based on concise",single-sentence summaries that clearly state the research goal,approach,and results using keywords like objective,method,,"and conclusion.""""""""",,,,,,,,,
"""User expects explicit mention of main objective",key method,"and principal finding in the summary.""""""""",,,,,,,,,,,,,
"""User values inclusion of terms such as research goal
]
""user\_question\_memory"""""""": \[",approach,results,"or conclusion.""""""""",,,,,,,,,,,,
"""User requests concise","one-sentence summaries for academic papers.""""""""",,,,,,,,,,,,,,
"""User prefers explicit identification of research goal",method,"and principal finding.""""""""",,,,,,,,,,,,,
"""User expects use of keywords like research goal
]
""residual\_memory"""""""": \[",approach,results,"or conclusion.""""""""",,,,,,,,,,,,
"""User values clarity and brevity in academic summaries.""""""""",,,,,,,,,,,,,,,
"""User expects the Assistant to structure responses around main objective",method,"and finding.""""""""",,,,,,,,,,,,,
"""Future responses should prioritize explicitness and keyword inclusion for summarizing research papers.""""""""
]
}}""","The paper introduces metrics (ICS, KPCS, AICS) to evaluate LLM-generated responses, balancing semantic and factual alignment. Using memory agents, the system captures user and assistant knowledge, improving contextual understanding. Results show increased accuracy, concise responses (149 tokens vs. 415), and effective memory-driven learning without retraining.",,"How can the MARK framework be evaluated and refined to optimize memory augmentation, knowledge retention, and contextual accuracy in multi-user interactions, particularly by balancing semantic and factual alignment through metrics like AICS and adjusting weight parameters such as 𝛼?","Multi-turn, multi-user interaction experiments assessed MARK’s ability to maintain coherence, adapt, and persist corrections across sessions.",,,,,,,,,,,
Memory extraction and refinement used conversation history to create and categorize memories,stored in a vector search-enabled document store.,,,,,,,,,,,,,,
"Evaluation employed the MedMCQA dataset and introduced heuristic metrics to assess chatbot response effectiveness.""
With memory",correct responses increased from 7 to 20; information coverage was not negatively impacted.,"MARK’s refined memory system builds ~10 memory units per conversation (2.57 residual, 2.8 user question, 4.0 assistant answer), supporting efficient, structured knowledge updates without retraining.",,,,,,,,,,,,,
"Average response token count dropped from 415 to 149
Results: With memory augmentation","indicating improved efficiency; statistical significance is shown by maximized AICS difference at α = 0.1.""
AICS increased from 0.18 to 0.36 (100% improvement)","Primary outcomes measured: Information Capture Score (ICS), Key Point Coverage Score (KPCS), and Average Information Capture Score (AICS).
and KPCS increased from 0.12 to 0.32 (166.7% improvement). ICS remained constant at 0.76.",,,,,,,,,,,,,
"Effects: Memory augmentation improved contextual alignment and response accuracy.""",LLMs lack real-time adaptability and require manual updates for evolving knowledge.,,,,,,,,,,,,,,
RAG-based agents may retrieve knowledge in the wrong format or out of context.,,,,,,,,,,,,,,,
User corrections may not persist across sessions; repeated errors can occur.,,,,,,,,,,,,,,,
Users can introduce incorrect modifications,intentionally or unintentionally.,,,,,,,,,,,,,,
LLMs may generate plausible but incorrect (hallucinated) responses,especially in high-risk domains.,,,,,,,,,,,,,,
Conventional LLMs require full retraining for domain adaptation.,,,,,,,,,,,,,,,
No dataset was selected for personalization; personalization relies on user interaction.,,,,,,,,,,,,,,,
"Memory may override newer facts or ignore past user context.""","User acceptance indicates that concise, accurate, and context-specific responses are preferred.",,,,,,,,,,,,,,
The User values clear extraction of main conclusions,implications,and recommendations.,,,,,,,,,,,,,
Structured bullet-point format is favored for summarizing study findings.,,,,,,,,,,,,,,,
"Responses should focus on critical memory points relevant to future interactions.""",Develop a structured knowledge graph for improved context retrieval and reasoning.,,,,,,,,,,,,,,
Implement safety filters using metadata for data governance and protection of sensitive information.,,,,,,,,,,,,,,,
"Introduce collaborative planning refined memory agents to dynamically manage and adapt the refinement process based on user feedback.""","Future research should investigate long-term memory retention, strategies to prevent erroneous memory formation from incorrect user input, and methods to further reduce human intervention. Additional studies could explore scalability, security, and adaptability of memory-based refinement in diverse, high-stakes domains.",No information available,,,,,,,,,,,,,
Digital twins as global learning health and disease models for preventive and personalized medicine,"Li Xinxiu, Loscalzo Joseph, Mahmud A. K. M. Firoj, Aly Dina Mansour, Rzhetsky Andrey, Zitnik Marinka, Benson Mikael",2025,reference-manager,10.1186/s13073-025-01435-7,,,,,,Analysis of longitudinal electronic medical records and biobank data to identify disease evolution and initiating mechanisms.,,"How can digital twins be developed and implemented to address the dynamic, multi-scale characterization of health and disease, while overcoming social, psychological, organizational, ethical, regulatory, and financial challenges to advance predictive, preventive, and personalized medicine?","The paper examines how integrating genetic, environmental, and multi-omics data—especially through digital twins (DTs) and explainable AI—can improve disease prediction, prevention, and personalized treatment. It highlights methodological advances, key challenges (e.g., explainability, ethics, privacy), and calls for cross-sector collaboration to realize DTs’ healthcare potential.","The paper’s main objective is to accelerate scientific discovery and revolutionize health care using digital twins by advancing mathematical, statistical, and computational foundations; its key method is integrating explainability techniques like attribution maps; the principal finding highlights the need for cross-sector harmonization and solutions for transparency, ethics, and privacy.",
Single-cell-based methods,including scGWAS and integration of scRNA-seq with pharmacogenomic databases,to infer clinical traits and drug sensitivity.,,,,,,,,,,,,,
"Construction of multi-layer network models using machine learning to integrate diverse disease-associated variables for personalized treatment.""",,"The paper highlights significant social, psychological, organizational, ethical, regulatory, and financial challenges in implementing digital twins (DTs) in healthcare, including gender disparities and data ownership concerns.",,,,,,,,,,,,,
Regulatory progress includes FDA pre-qualification programs to accelerate digital tool approval,but privacy and data security remain critical issues.,,,,,,,,,,,,,,
A US National Academy of Science white paper recommends a cross-sector,integrated agenda to advance digital twin technologies,"emphasizing harmonization and realistic applications; no quantitative results or p-values are reported.""",,"Gender differences in perception, use, and leadership of digital technologies may disadvantage women, especially minorities.",,,,,,,,,,,
Unresolved issues of data ownership,ethics,data security,and regulatory challenges.,,,,,,,,,,,,
Lack of adopted standards in data generation hinders data interoperability.,,,,,,,,,,,,,,,
Fundamental challenges in aggregating uncertainty and addressing missing data.,,,,,,,,,,,,,,,
Machine learning models often lack explainability,"affecting trust and usability.""","Digital twins (DTs) can transform healthcare by enabling predictive, preventive, and personalized medicine.",,,,,,,,,,,,,
"Key challenges include ethical
Collaborative","regulatory
cross-sector efforts and harmonized standards are recommended to address these challenges and promote equitable global health.""","organizational
Lack of adopted standards in data generation hinders interoperability for digital twins.",and technical barriers,especially regarding data privacy,,explainability,,,,,,,,,and global interoperability.
Fundamental challenges exist in aggregating uncertainty across different data types and addressing missing data.,,,,,,,,,,,,,,,
"Need for solutions to disseminate digital twins globally for equitable and effective health in line with the 2030 agenda.""","Future research should address gender differences in digital technology use, data ownership, ethics, data security, and regulatory issues. Integrated, cross-sectoral studies are needed to harmonize research, advance computational foundations, and explore linking medical digital twins with those in other fields. Gaps remain in prioritizing disease mechanisms and integrating diverse data types.",Longitudinal observational study using electronic medical records and biobanks,,,,,,,,,,,,,
Population-wide analyses,,,,,,,,,,,,,,,
Multi-scale (population,individual,tissue,cell,molecular),,,,,,,,,,,
Use of multi-omics and single-cell RNA sequencing (scRNA-seq),,,,,,,,,,,,,,,
Integration of environmental,genetic,and clinical data,,,,,,,,,,,,,
"Application of machine learning and computational methods""",,,,,,,,,,,,,,,
Sustainable Development Goal indicator for measuring availability and affordability of medicines for children: a proof-of-concept study,"Joosse Iris R, Mantel-Teeuwisse Aukje K, Suleman Fatima, van den Ham Hendrika A",2023,reference-manager,10.1136/bmjopen-2022-065929,,,,,,"Data collection in six geographical survey areas, including a main urban center and five other areas, using systematic selection of health facilities across public, private, and up to two other sectors.",,"How can a standardized, child-specific methodology based on SDG indicator 3.b.3 be developed and applied to measure the availability and affordability of age-appropriate medicines for children across different countries?","The study aimed to adapt and validate a WHO/HAI methodology to measure the availability and affordability of medicines for children. Using historical, quality-assured datasets from several countries, the adapted tool showed proof of concept but was limited by small sample size and outdated data. Further research is needed.",The research goal was to adapt the WHO/HAI methodology for measuring access to medicines for children; the approach involved developing child-specific core medicine sets and a new affordability parameter (NUNT); results showed successful proof of concept using historical data from three countries.,
Surveying up to 50 medicines (including 14 core medicines) for price and availability at selected outlets.,,,,,,,,,,,,,,,
Adaptation of the WHO/HAI methodology,"including the creation of two core sets of pediatric medicines and the novel NUNT parameter for affordability calculations.""","The research is partially reproducible. Aggregated data per medicine and country are available from the HAI website, but individual facility data are not publicly available. There is no mention of source code for the project.","The study adapted an existing tool to measure availability and affordability of medicines specifically for children, successfully providing proof of concept using historical datasets.",,,,,,,,,,,,
Only a modest sample of age-appropriate medicines was included,limiting the findings; further analyses with larger,prospectively collected datasets are needed.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""",The primary outcome was to provide proof of concept for a child-specific tool measuring availability and affordability of medicines for children.,,,,,,,,,,,,,,
The adapted methodology was successfully applied using historical datasets,but only a modest sample of age-appropriate medicines was surveyed.,,,,,,,,,,,,,,
"No specific statistical values or measured effects are provided in the context.""","Adapted methodology inherits limitations from the original tool, such as burden of disease weighting and use of national poverty line for affordability.",,,,,,,,,,,,,,
Reliance on historical datasets,which may not reflect the current situation.,,,,,,,,,,,,,,
Only a modest sample of age-appropriate medicines was surveyed.,,,,,,,,,,,,,,,
Uncertainties in NUNT (Number of Units Needed for Treatment) calculation due to dosage ranges and weight-to-age conversions.,,,,,,,,,,,,,,,
Limited availability of international weight-for-age charts for children above 10 years.,,,,,,,,,,,,,,,
Individual facility data not publicly available; only aggregated data can be accessed.,,,,,,,,,,,,,,,
Further validation and analyses on larger,"prospectively collected datasets are needed.""",The adapted child-specific methodology provides proof of concept but was limited by reliance on historical data and a small sample of age-appropriate medicines.,,,,,,,,,,,,,
Further analyses using larger,prospectively collected datasets are recommended.,,,,,,,,,,,,,,
"Sensitivity analyses are needed to determine the minimum number of medicines required for reliable accessibility measurement.""",The burden of disease weighting method may lead to disproportionate results; further analysis of different weighting approaches is needed.,,,,,,,,,,,,,,
Only a modest sample of age-appropriate medicines was surveyed; larger,prospectively collected datasets are required.,,,,,,,,,,,,,,
"The adapted affordability measure combining national poverty line and lowest-paid government worker wage needs further validation.""","Future research should use larger, prospectively collected datasets with more age-appropriate medicines. Additional analyses are needed on weighting approaches for disease burden and on the minimum number of medicines required for reliable accessibility measures. More data on child medicines is needed for meaningful sensitivity analyses.","The study is an observational, cross-sectional survey using a standardized, adapted WHO/HAI methodology. It uses historical, quality-assured datasets, systematic sampling of medicine outlets across multiple sectors and areas, and collects data on medicine price and availability. The study is non-randomized, non-controlled, and retrospective.",,,,,,,,,,,,,
AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based Chatbots Utilizing Private Data,"Zhao Xinjie, Blum Moritz, Yang Rui, Yang Boming, Carpintero Luis Márquez, Pina-Navarro Mónica, Wang Tony, Li Xin, Li Huitao, Fu Yanran, Wang Rongrong, Zhang Juntao, Li Irene",2024,reference-manager,,,,,,,,,What are the key concepts I should explore to fully understand the applications of transfer learning in NLP?,,,
Which related topics should I investigate to deepen my knowledge of semantic role labeling?,,,,,,,,,,,,,,,
What additional areas should I study to grasp the full scope of multilingual NLP systems?,,,,,,,,,,,,,,,
Are there any important connections between coreference resolution and other NLP tasks I should be aware of?,,,,,,,,,,,,,,,
What topics am I missing if I want to master information extraction techniques in NLP?,,,,,,,,,,,,,,,
Which concepts are closely linked to word sense disambiguation that I should also learn about?,,,,,,,,,,,,,,,
What other areas should I explore to understand the challenges of low-resource language processing?,,,,,,,,,,,,,,,
Are there any overlooked relationships between text summarization and other NLP methods I should consider?,,,,,,,,,,,,,,,
What foundational ideas should I study to enhance my understanding of dialogue systems in NLP?,,,,,,,,,,,,,,,
"Which related fields or concepts can help me better comprehend the limitations of current language models?""",{,,,,,,,,,,,,,,
"""goal\_analysis"""""""": """"""""The main goal is to identify and summarize up to three key research methods
""tasks"""""""": \[
{
""id"""""""": 1",methodologies,"or techniques used in the study evaluating AGENTiGraph’s performance in NLP and CV domains.""""""""",,,,,,,,,,,,,
"""description"""""""": """"""""Review the study’s methodology section to extract primary research methods or techniques.""""""""
""dependencies"""""""": \[]
}
{
""id"""""""": 2",,,,,,,,,,,,,,,
"""description"""""""": """"""""Summarize up to three key methods or techniques
""dependencies"""""""": \[1]
}
]","ensuring concise and clear explanations.""""""""",,,,,,,,,,,,,,
"""execution\_strategy"""""""": """"""""First
""potential\_challenges"""""""": \[",examine the methodology section for explicit mentions of research methods. Next,distill and summarize the top three methods in bullet points,"focusing on clarity and relevance.""""""""",,,,,,,,,,,,
"""Limited explicit detail on specific methodologies in the context""""""""",,,,,,,,,,,,,,,
"""Distinguishing between general feedback and actual research methods""""""""
]",,,,,,,,,,,,,,,
"""success\_criteria"""""""": """"""""A concise list (up to three) of clearly explained research methods or techniques directly referenced in the provided context.""""""""
}""","The research is reproducible. The source code for the project is available at https://shorturl.at/axsPd. The dataset construction process is described in detail, including human verification steps, and experimental details are provided for legal and medical domain demonstrations.","AGENTiGraph provided concise and efficient responses, with 32 out of 50 NLP queries highlighting its brevity, but 5 queries noted missing key details and 4 reported misunderstandings or incorrect answers.",,,,,,,,,,,,,
In the computer vision domain,14 out of 34 queries were satisfactory; 20 suggested improvements,mainly requesting more detailed technical explanations.,,,,,,,,,,,,,
"User satisfaction was high for efficiency and focused answers
Results: 32 out of 50 queries highlighted AGENTiGraph's shorter","especially for users with prior knowledge
focused answers; 5 queries noted incompleteness or missing details; 4 queries reported misunderstandings or incorrect answers.","but completeness and technical depth were noted as areas needing improvement. No statistical significance (p-values) reported.""",Primary outcome: AGENTiGraph provided more concise responses than ChatGPT in the NLP domain.,,,,,,,,,,,,
Measured effects: High user satisfaction for efficiency and conciseness,"especially for users familiar with core concepts.""",The study may lack sufficient technical depth in explanations of complex concepts.,,,,,,,,,,,,,
Practical examples or applications are sometimes missing,especially for advanced topics.,,,,,,,,,,,,,,
There is a trade-off between conciseness and completeness in responses.,,,,,,,,,,,,,,,
Difficulty in handling broad,abstract queries.,,,,,,,,,,,,,,
"Inconsistencies in response quality across domains and question types.""","AGENTiGraph provides concise, efficient responses, especially valued by users with prior knowledge for quick reviews.",,,,,,,,,,,,,,
Users request more detailed explanations and practical examples,particularly for complex or technical topics in computer vision.,,,,,,,,,,,,,,
There is a need to balance brevity with completeness and improve consistency across domains.,,,,,,,,,,,,,,,
Recommendations include enhancing technical depth,handling abstract queries better,"and adding domain-specific improvements.""","Need to balance concise answers with completeness, especially for complex topics.",,,,,,,,,,,,
Improve handling of broad,open-ended questions to enhance versatility.,,,,,,,,,,,,,,
Enhance domain-specific explanations with more technical detail and practical examples,"particularly in Computer Vision.""",What are the main subfields connected to semantic role labeling that I should explore for a deeper understanding?,,,,,,,,,,,,,
Which related concepts should I study to fully grasp the applications of word embeddings in NLP?,,,,,,,,,,,,,,,
Are there any important links between coreference resolution and other NLP tasks that I might be missing?,,,,,,,,,,,,,,,
What additional topics should I investigate to understand the full scope of language modeling techniques?,,,,,,,,,,,,,,,
How does syntactic parsing relate to other areas in NLP,and what concepts should I connect it with?,,,,,,,,,,,,,,
What are the key relationships between sentiment analysis and other text classification methods?,,,,,,,,,,,,,,,
Which foundational ideas should I review to better understand the challenges in machine translation?,,,,,,,,,,,,,,,
Are there any emerging topics linked to dialogue systems that I should include in my study plan?,,,,,,,,,,,,,,,
What supporting concepts are essential for a comprehensive view of information extraction in NLP?,,,,,,,,,,,,,,,
How do named entity recognition and relation extraction interact,"and what other concepts bridge these tasks?""",,,,,,,,,,,,,,
Membership Inference Attacks and Defenses in Federated Learning: A Survey,"Bai Li, Hu Haibo, Ye Qingqing, Li Haoyang, Wang Leixia, Xu Jianliang",2024,reference-manager,10.1145/3704633,,,,,,Unified evaluation benchmark: Proposes a standardized framework to assess the effectiveness of defense mechanisms against membership inference attacks (MIAs).,,"How can membership inference attacks (MIAs) and their defenses be systematically evaluated and improved within federated learning (FL) to enhance privacy, utility, and robustness against evolving threats?","The paper surveys Membership Inference Attacks (MIAs) and defenses in Federated Learning (FL). It categorizes MIAs as update-based or trend-based, reviews defense mechanisms like partial sharing and noise perturbation, and highlights the need for unified benchmarks. The study concludes with future research directions to enhance FL privacy and security.",The research goal is to survey membership inference attacks (MIAs) and defenses in federated learning (FL); the approach categorizes MIAs into update-based and trend-based types and reviews defense strategies; the principal finding is a comprehensive taxonomy and identification of research gaps for enhancing FL privacy and security.,
Defense algorithm assessment: Evaluates defense mechanisms under realistic assumptions and attack scenarios,considering factors like non-IID data and model architecture.,,,,,,,,,,,,,,
"Taxonomy-based analysis: Categorizes MIA research into update-based and trend-based approaches to structure the review and comparison of methods.""",,"Robust defense mechanisms can prevent membership leakage in federated learning (FL), but some, like differential privacy (DP), may introduce new vulnerabilities such as model poisoning attacks.",,,,,,,,,,,,,
There is no standardized evaluation benchmark for defense mechanisms,making it difficult to compare results and select effective mitigations.,,,,,,,,,,,,,,
Many defense strategies either overstate effectiveness or cause significant utility loss; DP offers strong privacy but with high utility costs,"making it unsuitable for critical applications. No statistical significance (p-values) reported.""","There is a lack of a standardized evaluation benchmark for assessing defense mechanisms, making comparison difficult.",,,,,,,,,,,,,
Defense effectiveness is often overstated; non-IID data and model architecture significantly impact privacy leakage risk.,,,,,,,,,,,,,,,
DP (Differential Privacy) mechanisms provide strong privacy but cause large utility loss.,,,,,,,,,,,,,,,
Robust defense mechanisms may prevent membership leakage but can introduce new risks,such as model poisoning.,,,,,,,,,,,,,,
"No specific statistical values or measured effects are provided.""","Lack of a standardized evaluation benchmark, making comparison of defense mechanisms difficult.",,,,,,,,,,,,,,
Overstatement of defense effectiveness due to overlooked factors like non-IID data and model architecture.,,,,,,,,,,,,,,,
Secure aggregation may be insufficient when the server acts as an attacker.,,,,,,,,,,,,,,,
Scarcity of defenses balancing privacy,utility,and efficiency.,,,,,,,,,,,,,
Potential negative correlations between defenses against MIAs and other attacks,"such as model extraction or poisoning.""",There is a critical need for a unified evaluation benchmark to compare defense mechanisms against MIAs in federated learning (FL).,,,,,,,,,,,,,
Current defense mechanisms often overstate effectiveness and overlook realistic attack scenarios; robust,realistic evaluation is recommended.,,,,,,,,,,,,,,
Existing privacy-preserving methods often sacrifice utility or efficiency; more balanced solutions are needed.,,,,,,,,,,,,,,,
Future research should explore new attack vectors,refine defenses,"and consider correlations among different attack types.""",Lack of a unified evaluation benchmark makes it difficult to compare and select effective defense mechanisms.,,,,,,,,,,,,
Existing defense algorithms are often evaluated under unrealistic assumptions,overlooking factors like non-IID data and server-as-attacker scenarios.,,,,,,,,,,,,,,
There is a scarcity of defense mechanisms that balance privacy,utility,"and computational efficiency.""","Future research should focus on developing robust defense mechanisms considering correlations among attacks, comprehensive evaluation metrics for MIAs, relaxing unrealistic assumptions, exploring attacks on emerging FL frameworks, creating unified evaluation benchmarks, assessing defense algorithms under realistic scenarios, and designing privacy-preserving methods with minimal utility loss and computational cost.",,,,,,,,,,,,
Digital Personal Health Coaching Platform for Promoting Human Papillomavirus Infection Vaccinations and Cancer Prevention: Knowledge Graph-Based Recommendation System,"Ammar Nariman, Olusanya Olufunto A, Melton Chad, Chinthala Lokesh, Huang Xiaolei, White Brianna M, Shaban-Nejad Arash",2023,reference-manager,10.2196/50210,,,,,,"Collection and analysis of vaccine-related data from Reddit using the Reddit API and Python Reddit API Wrapper, including sentiment and content analysis to identify misinformation.",,"How can a personal health library (PHL) that integrates multimodal data and semantic technologies be developed and optimized to promote HPV vaccinations and cancer screening, address barriers to vaccination and healthcare delivery, and provide personalized support to improve health outcomes?",,"The paper's main objective is to promote HPV vaccinations and cancer screening using a Personal Health Library (PHL); the key method is developing a PHL prototype to deliver personalized health messaging, and the principal finding is that the PHL shows potential to improve vaccination outcomes and health behaviors.",
Multimodal multidimensional data integration from multiple sources,such as text,tabular data,and graphs.,,,,,,,,,,,,
"Application of natural language processing (NLP) to detect domain concepts and generate resource description framework (RDF) graphs.""",The research uses publicly available data via APIs (such as Reddit API and Open Data API) and does not include real patient data. There is no mention of source code availability for the project. Reproducibility is limited to data collection methods described; no code is provided.,The study proposes using Reddit data and sentiment analysis to identify vaccine misinformation in online communities.,,,,,,,,,,,,,
Multimodal data integration and semantic techniques are applied to detect domain concepts and generate knowledge graphs.,,,,,,,,,,,,,,,
No statistical significance (p-values) or quantitative results are reported; all case scenarios are illustrative,"not based on real patient data.""",No real patient data or human subjects were included; all scenarios were generated for demonstration.,,,,,,,,,,,,,
The PHL (personal health library) is at the prototype stage; no primary outcomes,results,or measured effects (including statistical values) are reported.,,,,,,,,,,,,,
"A formal user experience assessment is planned but not yet conducted.""",No real patient data or identifiable personal information used.,,,,,,,,,,,,,,
Case scenarios are solely generated and not based on actual cases.,,,,,,,,,,,,,,,
Study is not human subjects research; results may lack real-world applicability.,,,,,,,,,,,,,,,
"No ethics committee approval due to absence of human data.""","The PHL app can promote HPV vaccinations and cancer screening, and address barriers to other vaccinations and health care delivery.",,,,,,,,,,,,,,
The app provides tailored,motivational support to improve health behaviors and treatment plans.,,,,,,,,,,,,,,
"The PHL is in prototype development; formal user experience assessments are planned after full implementation.""","The PHL is currently a prototype; further research is needed to implement, optimize, and fully assess its functionalities.",,,,,,,,,,,,,,
Formal user experience assessment (qualitative and quantitative) is planned to evaluate effectiveness,usability,and popularity.,,,,,,,,,,,,,
"Future work includes expanding PHL applications beyond HPV to other vaccinations and chronic conditions.""","Future research should assess the PHL’s effectiveness, functionality, usability, and popularity through formal user experience studies after full implementation. Additional investigation is needed into expanding PHL’s application to other vaccinations, chronic conditions, and barriers to healthcare delivery. Data collection from social media for vaccine attitudes is also planned.","The study design is not human subjects research, does not include real patient data, and did not require ethics committee approval. All case scenarios are generated for demonstration purposes only. No data sets were generated or analyzed. No information on randomization, blinding, control, or observational methods.",,,,,,,,,,,,,
Research Knowledge Graphs: The Shifting Paradigm of Scholarly Information Representation,"Zloch Matthäus, Dessì Danilo, D’Souza Jennifer, Castro Leyla Jael, Zapilko Benjamin, Karmakar Saurav, Mathiak Brigitte, Stocker Markus, Otto Wolfgang, Auer Sören, Dietze Stefan",2025,reference-manager,10.1007/978-3-031-94578-6\_8,,,,,,Use of quality-controlled ground truth datasets for scholarly information extraction tasks.,,"How do Research Knowledge Graphs (RKGs) transform the representation, integration, and citability of scholarly research artifacts and entities, and what are their applications, challenges, and future roles in advancing the scholarly domain?","The paper investigates Research Knowledge Graphs (RKGs), focusing on their role in representing, interlinking, and crediting diverse research artifacts. Using Semantic Web practices, the study reviews RKG applications, construction methodologies, and challenges, concluding that RKGs enhance transparency, reproducibility, and knowledge discovery in scholarly research.","The paper's research goal is to investigate, describe, and categorize Research Knowledge Graphs (RKGs) for scholarly information; its approach involves analyzing RKG examples, construction methods, and challenges; its principal finding is that RKGs enhance research transparency, interoperability, and traceability by interlinking diverse scholarly artifacts using standardized semantic technologies.",
Modeling research artifacts using established vocabularies and ontologies (e.g.,schema.org) to enhance visibility,findability,and interoperability.,,,,,,,,,,,,
Application of deep learning methods,especially in natural language processing and image analysis,involving code,models,"and training data.""",,"The research highlights a reproducibility crisis due to unshared implementation details and lack of transparency. While platforms like GitHub exist for code sharing, no specific source code for the project is mentioned in the context. RKGs are proposed to improve reproducibility, but no code is provided.",,,,,,,,,"Research Knowledge Graphs (RKGs) enable interlinking of diverse research artifacts, improving knowledge discovery and supporting reproducibility and transparency in scholarly work."
RKGs facilitate structured,machine-actionable representations of research,benefiting researchers,publishers,and citizen scientists by enhancing data reusability and compliance with FAIR principles.,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the context.""",Primary outcomes focus on the benefits of Research Knowledge Graphs (RKGs) for scholarly information representation.,,,,,,,,,,,,,,
RKGs enable better research management,enhanced reproducibility,transparency,and consensus through standardized vocabularies.,,,,,,,,,,,,
RKGs facilitate discoverability,integration,and interoperability of research artifacts.,,,,,,,,,,,,,
"No explicit statistical values or measured effects are provided.""","Many research artifacts and their relationships are not properly referenced, limiting citability and credit.",,,,,,,,,,,,,,
Knowledge about scientific advancements is often buried in unstructured documents (PDFs),making extraction laborious.,,,,,,,,,,,,,,
"Limited availability of quality-controlled ground truth datasets for scholarly information extraction tasks.""","Research Knowledge Graphs (RKGs) transform how research artifacts are stored, managed, and shared, enhancing data reusability and machine-driven applications.",,,,,,,,,,,,,,
RKGs support FAIR principles,improving transparency,traceability,and accessibility for diverse stakeholders.,,,,,,,,,,,,
Well-structured RKGs facilitate efficient research management,discovery,and validation.,,,,,,,,,,,,,
"Continued development and integration of RKGs are recommended to advance scholarly knowledge.""",Insufficient reproducibility and transparency due to unshared or unreported implementation details in research.,,,,,,,,,,,,,,
Limited availability of quality-controlled ground truth datasets for scholarly information extraction tasks.,,,,,,,,,,,,,,,
"Challenges in integrating and interlinking diverse Research Knowledge Graphs (RKGs) to enable comprehensive knowledge discovery.""","Future research should address gaps in reproducibility and transparency, explore standardized approaches for building and interlinking Research Knowledge Graphs (RKGs), and investigate how RKGs can support large language models by providing verifiable, up-to-date information and improving fact verification and contextual understanding.",,,,,,,,,,,,,,
RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being,"Ferdousi Rahatara, Hossain M. Anwar",2025,reference-manager,,,,,,,"Synthetic Prompt Generation: Created structured, context-rich prompts for both patients and care providers using dataset attributes (e.g., symptoms, sensor data) to simulate real-world health dialogues.",,"How does the Responsible Prompt Engine (RPE) within the RHealthTwin framework enable structured, ethical, and personalized prompt generation to improve large language model response quality across diverse healthcare domains compared to standard prompting paradigms?","The study evaluates the Responsible Prompt Engine (RPE) within the RHealthTwin framework, focusing on generating ethical, structured, and personalized prompts for health-related LLM outputs. Using four benchmark datasets and various prompt strategies, RPE consistently outperformed baselines in factuality, contextual appropriateness, and responsibility, supporting its use in mid-resource healthcare settings.","The research goal is to evaluate the Responsible Prompt Engine within the RHealthTwin framework using slot-based prompting on four health datasets; the approach involves generating structured, ethical, and personalized prompts, and the principal finding is improved response quality and responsible, context-rich outputs compared to standard prompting methods.",
Comparative Prompt Evaluation: Assessed Responsible Prompt Engine (RPE) against standard prompting methods (zero-shot,few-shot,instruction-tuned) across four health-related datasets.,,,,,,,,,,,,,
Multimodal Input Testing: Incorporated both textual and screenshot data (e.g.,"wearable device summaries) to evaluate system performance in multimodal scenarios.""","The research is reproducible. The official implementation repository, including full output examples and code for ResponsibleHealthTwin-RHT, is available at https://github.com/turna1/ResponsibleHealthTwin-RHT-.","The Responsible Prompt Engine (RPE) consistently achieved the highest Factuality Score (FS) and Contextual Appropriateness Score (CAS) across all datasets and models, outperforming zero-shot, few-shot, and system instruction strategies.",,,,,,,,,,,,
RPE-generated prompts led to improved semantic similarity (BERTScore),content overlap (ROUGE-L),and word-level accuracy (BLEU) compared to baseline methods.,,,,,,,,,,,,,
RPE responses demonstrated higher instructional compliance and responsibility rubric scores,"but no explicit p-values or statistical significance measures are reported.""",Primary outcomes:,,,,,,,,,,,,,
The Responsible Prompt Engine (RPE) consistently achieved the highest Factuality Score (FS) and Contextual Appropriateness Score (CAS) across all datasets and models.,,,,,,,,,,,,,,,
"RPE outperformed zero-shot
Results:",few-shot,and instruction-tuned prompting strategies for both patient-side and provider-side prompts.,,,,,,,,,,,,,
GPT-4 and Gemini Pro achieved the highest absolute FS and CAS scores.,,,,,,,,,,,,,,,
RPE led to improved structured,ethical,and personalized prompt generation,enhancing response quality.,,,,,,,,,,,,
"Measured effects/statistical values:
ROUGE-L",BERT Score,BLEU,Instructional Compliance Score (ICS),and WHO-Aligned Responsibility Rubric (WRR) were used as evaluation metrics.,,,,,,,,,,,
"Exact numerical values for these metrics are not provided in the context.""","Limited adaptability of LLMs for long-term, personalized well-being interventions beyond clinical or mental health.",,,,,,,,,,,,,,
Evaluation restricted mainly to the Responsible Prompt Engine; full system not comprehensively tested.,,,,,,,,,,,,,,,
Direct performance comparisons with prior systems are invalid due to differing experimental setups,data sources,and model types.,,,,,,,,,,,,,
Standardized benchmarks and metrics are lacking for rigorous cross-system comparison.,,,,,,,,,,,,,,,
"Impact on workforce sustainability remains challenging to address at this early stage.""","The Responsible Prompt Engine generates structured, ethical, and personalized prompts that improve large LLM response quality.",,,,,,,,,,,,,,
Slot-based prompting and safety tuning enhance transparency,ethical guardrails,and factual grounding in health-related outputs.,,,,,,,,,,,,,
Multimodal data integration and predictive modeling enable personalized,"actionable recommendations for diverse health scenarios.""","Enhancing the adaptability of large language models (LLMs) for long-term, personalized well-being interventions beyond clinical or mental health applications.",,,,,,,,,,,,,
Overcoming limitations of the Responsible Prompt Engine’s reliance on predefined templates,especially in highly variable or ambiguous contexts.,,,,,,,,,,,,,,
"Developing algorithms to transform unstructured user input into structured prompts and system instructions for adaptive AI inference.""","Future research should focus on enhancing LLM adaptability for long-term, personalized well-being interventions, developing dynamic responsible prompt engines, improving algorithms for structuring unstructured queries, and creating more realistic synthetic test prompts. Establishing standardized benchmarks and metrics for rigorous cross-system comparison is also recommended.",Study design:,,,,,,,,,,,,,
Systematic review synthesizing over 100 studies. The study also includes experimental evaluation of the Responsible Prompt Engine using four benchmark datasets across diverse health domains. The evaluation is not randomized,controlled,"or blinded; it is primarily observational and comparative.""",,,,,,,,,,,,,
Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation,"Yan Bo, Cao Yang, Wang Haoyu, Yang Wenchuan, Du Junping, Shi Chuan",2024,reference-manager,10.1145/3589334.3645693,,,,,,"Two-stage perturbation mechanism: First perturbs user-related shared HINs using EM (Expectation-Maximization) for selection, then perturbs user-item interactions within selected shared HINs to preserve semantics and privacy.",,How can we recover broken meta-path semantics for HIN-based federated recommendation (FedRec) while rigorously protecting user privacy through formal privacy definitions and a semantic-preserving user interaction publishing mechanism?,"The paper proposes FedHGNN, a Federated Heterogeneous Graph Neural Network for privacy-preserving recommendations on HINs. It introduces a two-stage perturbation mechanism to protect user privacy while recovering semantic information. Experiments show FedHGNN outperforms baselines, achieving strong recommendation accuracy and rigorous privacy guarantees.","The research goal is privacy-preserving recommendation on distributed HINs; the approach introduces FedHGNN, combining a two-stage perturbation mechanism for semantic-preserving user interaction publishing with a heterogeneous GNN; results show FedHGNN achieves strong recommendation performance while providing rigorous privacy guarantees.",
Heterogeneous Graph Neural Networks (HGNN): Used for recommendation by modeling complex relationships in heterogeneous information networks.,,,,,,,,,,,,,,,
"Semantic-preserving user-item interaction publishing: Ensures privacy while maintaining semantic information by carefully perturbing and publishing user-item interactions.""",,"FedHGNN outperforms all FedRec models by up to 34% in HR@10 and 42% in NDCG@10, and also surpasses several centralized models.",,,,,,,,,,,,,
Ablation studies show the two-stage perturbation mechanism significantly improves performance; FedHGNN achieves HR@10 of 0.4185 (ACM),0.4373 (DBLP),and 0.2977 (Yelp).,,,,,,,,,,,,,
"Moderate privacy budget (π2 = 1) yields optimal performance; statistical significance (p-values) not reported.""
On ACM
On DBLP
On Yelp","FedHGNN outperforms all FedRec models by up to 34% in HR@10 and 42% in NDCG@10.
FedHGNN achieves HR@5: 0.3593
HR@5: 0.3376
HR@5: 0.2178","HR@10: 0.4185
HR@10: 0.4373
HR@10: 0.2977","NDCG@5: 0.2787
NDCG@5: 0.2481
NDCG@5: 0.1578","NDCG@10: 0.298.
NDCG@10: 0.2778.
NDCG@10: 0.1834.",,,,,,,,,,,
"Ablation study shows two-stage perturbation improves performance; FedHGNN outperforms all its variants in most metrics.""","Assumes centralized data storage, which may not be realistic due to privacy concerns and regulations (e.g., GDPR).",,,,,,,,,,,,,,
Difficulty in formally defining privacy for HIN-based FedRec; traditional definitions may not apply.,,,,,,,,,,,,,,,
Data sparsity issues are worsened by distributed data storage.,,,,,,,,,,,,,,,
"Performance depends on the number of shared HINs and privacy budgets.""","FedHGNN achieves superior or competitive performance compared to baselines, especially on ACM and DBLP datasets.",,,,,,,,,,,,,,
Semantic-preserving perturbation (+SDPRR) effectively addresses data sparsity and maintains data diversity.,,,,,,,,,,,,,,,
Both degree-preserving and feature-preserving are necessary for semantic-preserving privacy.,,,,,,,,,,,,,,,
First-stage perturbation is essential for privacy,"and the designed EM preserves user high-order patterns with minimal performance loss.""",How to rigorously define and protect privacy in HIN-based federated recommendation (FedRec) settings.,,,,,,,,,,,,,
How to recover broken semantic information for HIN-based FedRec while ensuring privacy,especially given distributed HIN storage and privacy constraints.,,,,,,,,,,,,,,
Investigating why PFedRec outperforms some GNN-based FedRec baselines,"particularly the impact of personal item embeddings and parameter initialization.""","Future research should address: (1) formally defining privacy in HIN-based FedRec, as traditional definitions may not apply; (2) recovering broken meta-path semantics in distributed, privacy-preserving settings; and (3) developing methods that balance semantic preservation, privacy protection, and recommendation utility in federated environments.",,,,,,,,,,,,,
PsyDT: Using LLMs to Construct the Digital Twin of Psychological Counselor with Personalized Counseling Style for Psychological Counseling,"Xie Haojie, Chen Yirong, Xing Xiaofen, Lin Jingkai, Xu Xiangmin",2024,reference-manager,,,,,,,Psychoanalysis: Focuses on exploring unconscious thoughts and past experiences to understand current behavior.,,No information available,"The paper examines counseling strategies, focusing on how counselors use inquiry, feedback, and therapy techniques to promote client self-reflection and problem-solving. The methodology involves analyzing counselor-client interactions. Key findings highlight the importance of openness, emotional regulation, and relationship-building. The study concludes that these approaches foster client autonomy and effective counseling outcomes.",,
Cognitive Behavioral Therapy (CBT): Targets the connection between thoughts,emotions,and behaviors to address psychological challenges.,,,,,,,,,,,,,
Person-Centered Therapy: Emphasizes empathy,acceptance,"and the client’s autonomy in the therapeutic process.""","The PsyDTCorpus dataset and PsyDTLLM model will be released upon decision of the paper. System hardware and software details are provided, but no source code is currently available. Data cleaning protocols are described to ensure privacy. No explicit source code link is given.","PsyDTCorpus significantly outperforms baseline datasets across four professional assessment dimensions, with expert evaluation scores: 8.39 (Conversation Strategy), 8.69 (State and Attitude), 8.29 (Relationship Building), and 8.12 (Application of Therapy Technique).",,,,,,,,,,,
Ablation studies show synthetic multi-turn dialogue fidelity over 60%,with performance drops when linguistic style (68.75%),therapy technique (75.0%),or client personality (62.5%) are removed.,,,,,,,,,,,,
"No explicit p-values or statistical significance values are reported in the context.""","The synthetic multi-turn dialogue achieved a fidelity of over 60% in integrating linguistic style, therapy technique, and client personality.",,,,,,,,,,,,,,
PsyDTCorpus outperformed baseline datasets across all four professional assessment dimensions.,,,,,,,,,,,,,,,
"Expert evaluation scores for PsyDTCorpus: Conversation Strategy 8.39
Ablation study results: 68.75% fidelity without linguistic style","State and Attitude 8.69
75.0% without therapy technique","Relationship Building 8.29
62.5% without client personality.",Application of Therapy Technique 8.12,Fluency 1.00.,,,,,,,,,,,
"Manual evaluation shows PsyDTCorpus excels in quality and professionalism compared to other datasets.""",Limited generalizability due to focus on a single client’s narrative.,,,,,,,,,,,,,,
Lack of detailed information restricts definitive assessment of some traits (e.g.,conscientiousness).,,,,,,,,,,,,,,
Self-reported experiences may introduce bias.,,,,,,,,,,,,,,,
Influence of external environment on decisions not fully explored.,,,,,,,,,,,,,,,
"Further research suggested for broader applicability.""","The PsyDT multi-turn dialogue synthesis method achieves over 60% fidelity, effectively integrating linguistic style, therapy technique, and client personality.",,,,,,,,,,,,,,
Manual evaluation shows PsyDTCorpus outperforms baseline datasets in conversation strategy,state and attitude,relationship building,and therapy technique application.,,,,,,,,,,,,
Ablation studies confirm each element (linguistic style,therapy technique,client personality) is crucial for high-quality dialogue synthesis.,,,,,,,,,,,,,
Recommendation: Use the PsyDT framework for generating professional,"consistent multi-turn mental health dialogues.""",The framework only constructs digital twins with specific counseling styles and cannot meet all clients' psychological needs.,,,,,,,,,,,,,
Relying on a single psychological counselor is arbitrary; future work should explore multiple counselors for joint diagnosis.,,,,,,,,,,,,,,,
"Further research is needed to address the complexity of psychological counseling beyond current model capabilities.""",,No information available,,,,,,,,,,,,,
Digital twin system for manufacturing processes based on a multi-layer knowledge graph model,"Su Chang, Tang Xin, Jiang Qi, Han Yong, Wang Tao, Jiang Dongsheng",2025,reference-manager,10.1038/s41598-024-85053-0,,,,,,"Comparative experiment: Compared the proposed three-layer knowledge model with OpenIE, Bi-LSTM, and BERT-Bi-LSTM-CRF for extracting knowledge triples from manufacturing documents.",,"How does the proposed three-layer knowledge model, with its concept layer ontology framework, enhance the extraction, integration, and application of domain-specific knowledge in industrial manufacturing, particularly for supporting digital twin models and outperforming existing knowledge extraction methods?","The paper aims to validate a three-layer knowledge model for extracting knowledge triples from unstructured manufacturing data. Using standard metrics (Precision, Recall, F1 Score), the proposed model outperformed BERT-Bi-LSTM-CRF, Bi-LSTM, and OpenIE. The model supports digital twin applications, enhancing decision-making in aero-engine blade production.","The research goal is to validate a three-layer knowledge model for extracting knowledge triples from manufacturing documents; using an ontology-based approach, the model outperforms OpenIE, Bi-LSTM, and BERT-Bi-LSTM-CRF methods, achieving the highest Precision (77.38%), Recall (83.14%), and F1 Score (80.15%).",
Quantitative evaluation: Used Precision,Recall,and F1 Score to measure extraction accuracy and effectiveness.,,,,,,,,,,,,,
Digital twin architecture: Implemented a three-layer knowledge graph (concept,model,"decision layers) integrating physical and virtual environments for process simulation and optimization.""",,"The proposed three-layer knowledge model achieved the highest Precision (77.38%), Recall (83.14%), and F1 Score (80.15%) in extracting knowledge triples, outperforming BERT-Bi-LSTM-CRF, Bi-LSTM, and OpenIE.",,,,,,,,,,,
Integration of domain knowledge and cognitive reasoning significantly improved extraction accuracy and generalizability.,,,,,,,,,,,,,,,
"No p-values or statistical significance values are reported in the context.""","The proposed three-layer knowledge model achieved Precision: 77.38%, Recall: 83.14%, and F1 Score: 80.15%.",,,,,,,,,,,,,,
It outperformed BERT-Bi-LSTM-CRF (Precision: 71.43%,Recall: 73.17%,F1 Score: 72.29%),Bi-LSTM (Precision: 67.39%,Recall: 69.42%,,F1 Score: 68.39%),,,,,,,F1 Score: 64.44%).,Recall: 64.98%,and OpenIE (Precision: 63.91%
The model demonstrated superior accuracy,generalizability,"and effectiveness in extracting knowledge triples from manufacturing documents.""","Data integration difficulty due to heterogeneous data structures, formats, and semantics, risking information loss or errors.",,,,,,,,,,,,
Limited real-time data processing capabilities for large volumes.,,,,,,,,,,,,,,,
Challenges in system scalability and adaptability to evolving manufacturing processes.,,,,,,,,,,,,,,,
Dependence on expert knowledge,"limiting universality and flexibility; slows adaptation to new information.""","The proposed three-layer knowledge model outperforms existing methods in extracting accurate and relevant knowledge triples, with Precision 77.38%, Recall 83.14%, and F1 Score 80.15%.",,,,,,,,,,,,,
Integrating domain knowledge and cognitive reasoning enhances accuracy,generalizability,and adaptability across technical environments.,,,,,,,,,,,,,
Over-reliance on expert input limits flexibility; future work should integrate automated machine learning and natural language processing for broader applicability.,,,,,,,,,,,,,,,
The model enables dynamic optimization,real-time monitoring,"and improved efficiency in smart manufacturing systems.""","Data integration difficulty: Integrating heterogeneous data from various sources remains challenging, risking information loss or errors. Future work should explore advanced integration techniques and universal standards.",,,,,,,,,,,,
Real-time data processing: Efficiently handling large volumes of real-time data is technically demanding. Research should focus on scalable frameworks and optimized algorithms.,,,,,,,,,,,,,,,
"Dependence on expert knowledge: Over-reliance on domain experts limits adaptability. Future directions include automated knowledge extraction using machine learning and natural language processing.""","Future research should address data integration difficulties by developing universal data exchange standards and ontology alignment methods, improve real-time data processing with scalable frameworks and edge computing, enhance system scalability with modular designs, and reduce dependence on expert knowledge through automated knowledge extraction and machine learning.","Study design: Case study. The research involves conducting case studies within a digital twin system architecture for the manufacturing process. It includes experimental comparison of knowledge extraction methods using standard metrics (Precision, Recall, F1 Score) but is fundamentally structured as a case study.",,,,,,,,,,,,,
AIstorian lets AI be a historian: A KG-powered multi-agent system for accurate biography generation,"Li Fengyu, Li Yilin, Zhu Junhao, Chen Lu, Zhang Yanfei, Zhou Jia, Zu Hui, Zhao Jingwen, Gao Yunjun",2025,reference-manager,,,,,,,"Regex-driven relation extraction: Prompts a language model to generate regular expressions (regex) for extracting biographical facts as triplets (entity, relation, value) from text chunks.",,How can a knowledge graph-powered multi-agent system be designed to generate accurate historical biographies from large corpora while minimizing factual errors and hallucinations?,"The paper addresses automatic biography generation, aiming to improve stylistic adherence and factual fidelity using a two-step training strategy: supervised fine-tuning and stylistic preference optimization. The proposed method, AIstorian, outperforms baselines in ROUGE scores and reduces factual errors, demonstrating significant advancements in biography generation for historical documents.","The research goal is to generate accurate, stylistically consistent historical biographies using a knowledge-grounded large language model (AIstorian); the approach combines KG-based indexing, two-step training, and error-aware generation; results show AIstorian outperforms Qwen2.5-72B-Instruct in factual fidelity and stylistic consistency.",
Knowledge graph construction: Builds a biographical knowledge graph by linking extracted entities and their relationships to source text.,,,,,,,,,,,,,,,
"Error-aware generation with multi-agents: Uses multiple agents to reduce hallucinations and improve factual accuracy during biography generation.""",,The proposed system reduces atomic fact errors by 35% and achieves higher ROUGE scores compared to baselines.,,,,,,,,,,,,,
Index construction time is 422.65s,much faster than GraphRAG (5279.65s) and NER baselines (885.68s,1037.31s),but slower than TongGu (26.69s).,,,,,,,,,,,,
Online retrieval is extremely fast (0.00146s),"and the longer generation time (1236s) significantly lowers hallucination rates and improves factual accuracy.""","Our system (AIstorian) achieves highest ROUGE-1 (83.69), ROUGE-2 (74.14), ROUGE-L (80.54), lowest hallucination rate (39.29%), and lowest average atomic fact error (0.43).",,,,,,,,,,,,,
Retrieval: Precision 0.936,Recall 0.944,F1 Score 0.923.,,,,,,,,,,,,,
Index construction: 422.65s (faster than GraphRAG's 5279.65s).,,,,,,,,,,,,,,,
Online retrieval: 0.00146s (faster than TongGu's 169.101s and GraphRAG's 1.68s).,,,,,,,,,,,,,,,
Generation stage: 1236s total,36.35s per biography.,,,,,,,,,,,,,,
"StylePO reduces atomic fact errors by 35%.""",LLMs struggle to maintain stylistic adherence and use of professional terminology due to scarce training data.,,,,,,,,,,,,,,
LLMs are prone to hallucination,generating plausible but incorrect facts.,,,,,,,,,,,,,,
Training-based methods require tens of thousands of samples.,,,,,,,,,,,,,,,
Decoding-based methods reduce model creativity.,,,,,,,,,,,,,,,
Retrieval-augmented methods depend on retrieval accuracy.,,,,,,,,,,,,,,,
"Information fragmentation occurs due to scattered sources and chunking.""","The proposed system achieves higher factual accuracy and stylistic consistency than baselines, notably reducing hallucination rates.",,,,,,,,,,,,,,
StylePO and multi-agent approaches significantly improve ROUGE scores and decrease atomic fact errors by 35%.,,,,,,,,,,,,,,,
The system offers much faster index construction and online retrieval compared to GraphRAG and other NER baselines.,,,,,,,,,,,,,,,
"The additional training time is justified by substantial performance gains in accuracy and reliability.""",Limited availability of professional biographical training data restricts effective fine-tuning of large language models.,,,,,,,,,,,,,,
Existing models struggle with resolving conflicting information in references,often requiring expert intervention.,,,,,,,,,,,,,,
Lack of domain expertise leads to errors,"highlighting the need for better knowledge retrieval and integration mechanisms.""",,"The study design includes: supervised fine-tuning, data synthesis, data augmentation, two-step training, ablation studies, error-aware generation with multi-agents, regex-driven relation extraction, knowledge graph construction, in-context learning, and evaluation using precision, recall, F1-score, ROUGE metrics, and hallucination rate. The dataset is split into training and test sets (8:2 ratio).",,,,,,,,,,,,
HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models,"Gao Fan, Zhao Xinjie, Xia Ding, Zhou Zhongyi, Yang Rui, Lu Jinghui, Jiang Hang, Park Chanjun, Li Irene",2025,reference-manager,,,,,,,Formative Study: Conducted to understand users’ needs for acquiring and exploring healthcare information.,,"How effective is HealthGenie in supporting users’ needs for acquiring and exploring healthcare information through personalized recommendations, visualized knowledge graphs, and interactive dialogue compared to a baseline system, in terms of information perception, user preference support, and overall user experience?","The study aimed to understand users’ needs in exploring healthcare information and evaluate HealthGenie’s visual and personalized recommendation features. Using mixed methods, results showed users preferred rapid, visual, and structured information. HealthGenie’s outputs were rated highly for organization and interpretability, improving task efficiency and supporting user preferences.","The research goal was to empower users with healthy dietary guidance; the approach combined Knowledge Graphs and Large Language Models for structured, visual, and personalized information delivery; results showed high user satisfaction with organization, interpretability, and efficiency, though some desired greater detail (granularity).",
Within-Subjects Study: Compared HealthGenie to a baseline system using counterbalanced task order and surveys.,,,,,,,,,,,,,,,
"Reflexive Thematic Analysis: Used to analyze participant viewpoints from questionnaires and co-design sessions.""",,"Visual outputs significantly improved task efficiency and comprehension, with high ratings for ease of recipe revision (A = 4.91, AA = 0.28) and understanding nutrition information (A = 4.88).",,,,,,,,,,,,,
Organization and interpretability received the highest ratings (KG: A = 4.66; LLM: A = 4.67; Interpretability: A = 4.58),while granularity was rated lower and showed statistical significance (A = 4.25,p = 0.042).,,,,,,,,,,,,,
Participants strongly preferred visual and hybrid formats over text-only,citing clarity,rapid information delivery,"and enhanced engagement as key benefits.""","Participants highly rated output quality, especially Organization (KG: mean = 4.66, SD = 0.49; LLM: mean = 4.67, SD = 0.49).",,,,,,,,,,,
Interpretability received the highest data characteristic rating (mean = 4.58,SD = 0.51).,,,,,,,,,,,,,,
Accuracy (mean = 4.25,SD = 0.75) and Granularity (mean = 4.25,SD = 0.62) were slightly lower.,,,,,,,,,,,,,
Visual output improved task efficiency: “The visual output made revising the recipe easy” (mean = 4.91,SD = 0.28); “The visual output made understanding nutrition-related information easy” and “helped select preferred recipes quickly” (both mean = 4.88).,,,,,,,,,,,,,,
"Participants preferred hybrid visualizations for clarity and rapid information delivery.""","High latency due to multiple prompts for better feedback, causing slower response times.",,,,,,,,,,,,,,
Trade-off between speed and performance to ensure output quality.,,,,,,,,,,,,,,,
Difficulty in distinguishing reliable from unreliable healthcare information,affecting trust and transparency.,,,,,,,,,,,,,,
Challenges in recognizing and verifying real-world data.,,,,,,,,,,,,,,,
"Users may struggle with complex concepts due to lack of domain knowledge.""","Both KG and LLM outputs were highly rated for organization and interpretability, making information easy to understand and use.",,,,,,,,,,,,,,
Visual outputs significantly improved task efficiency,comprehension,and decision-making.,,,,,,,,,,,,,
Participants desired greater granularity in some contexts.,,,,,,,,,,,,,,,
"Recommendation: Use structured visualizations and progressive disclosure to enhance user experience and support preferences.""
Lack of accessible
Demand for rapid","Difficulty distinguishing reliable from unreliable healthcare information, especially with LLM-generated content; need for enhanced trust and transparency.
well-supported explanations and logical reasoning in AI recommendations; need for explainability.
visual","and hybrid information delivery formats to improve comprehension and user experience.""","Future research should explore adaptive triggering mechanisms that balance automation with user control, develop robust intent prediction models, and design recommendation interfaces that preserve user agency and transparency. There is also a need to support richer, proactive interactions and personalized, visual information delivery in LLM-KG interfaces.",Formative study; within-subjects study; counterbalanced design; two conditions (HealthGenie and baseline system); participants divided into two groups with task order reversed; quantitative and qualitative measurements; mixed ANOVA and Wilcoxon signed-rank test used for analysis; reflexive thematic analysis for qualitative data.,,,,,,,,,,,
Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Maintenance,"Peng Xin, Wang Chong, Liu Mingwei, Lou Yiling, Wu Yijian",2025,reference-manager,,,,,,,"Structured Representation: Uses knowledge graphs, knowledge cards, and frames to formally organize and query software knowledge elements and their relationships.",,"How can a scalable code digital twin framework be developed to represent, extract, and continuously update complex software knowledge by integrating structured and unstructured information, managing ambiguity, and enabling intelligent applications for efficient software development, maintenance, and evolution?","The paper proposes the Code Digital Twin framework to capture and represent tacit software knowledge—domain concepts, functionalities, and design rationales—using both structured and unstructured methods. The methodology extracts and integrates this knowledge, enabling continuous alignment with evolving software. The approach aims to enhance LLM-assisted software maintenance and development.",The paper’s research goal is to empower large language models (LLMs) for complex software maintenance by proposing a Code Digital Twin framework that integrates structured and unstructured knowledge; the principal finding is that this approach enhances LLMs’ understanding of system-wide dependencies and evolving design rationales.,
Unstructured Representation: Captures flexible,context-rich knowledge through natural language explanations and unstructured documents.,,,,,,,,,,,,,,
"Synergistic Pipeline: Integrates large language models (LLMs) and static program analysis for comprehensive knowledge extraction and representation.""",,"The code digital twin model integrates key knowledge elements (domain concepts, functionalities, rationales) and their relationships, providing a unified, evolving representation closely linked to software artifacts.",,,,,,,,,,,,,
The framework emphasizes combining structured (e.g.,knowledge graphs) and unstructured (e.g.,documentation) knowledge for comprehensive software understanding.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the context.""",The primary outcome is a research agenda for implementing a code digital twin framework for software knowledge representation.,,,,,,,,,,,,,,
The framework aims to integrate structured and unstructured knowledge,manage ambiguity and versioning,and link key knowledge elements (domain concepts,functionalities,rationales) to software artifacts.,,,,,,,,,,,
"No statistical values or measured effects are reported.""",Loss of software development knowledge due to suboptimal documentation.,,,,,,,,,,,,,,
LLMs struggle with complex software maintenance and real-world issue resolution.,,,,,,,,,,,,,,,
Difficulty capturing tacit (implicit,undocumented) knowledge.,,,,,,,,,,,,,,
Challenges integrating structured and unstructured knowledge sources.,,,,,,,,,,,,,,,
Managing ambiguity and evolving knowledge is difficult.,,,,,,,,,,,,,,,
Motivating and ensuring quality human contributions is challenging.,,,,,,,,,,,,,,,
"Maintaining knowledge completeness as software evolves.""","The code digital twin framework integrates structured and unstructured software knowledge for scalable, adaptable representation.",,,,,,,,,,,,,,
It continuously aligns with evolving software,enhancing LLM and developer collaboration in maintenance tasks.,,,,,,,,,,,,,,
Key challenges include knowledge integration,ambiguity management,"and ensuring long-term scalability and adaptability.""","Developing scalable, hybrid frameworks that integrate structured and unstructured software knowledge for digital twins.",,,,,,,,,,,,
Establishing efficient,accurate,and scalable automatic pipelines for extracting and updating knowledge from evolving software artifacts.,,,,,,,,,,,,,
"Managing ambiguity and knowledge versioning to ensure adaptability and coherence as software evolves.""",Future research should focus on: developing scalable frameworks for integrating structured and unstructured software knowledge; creating efficient knowledge extraction pipelines; adapting to evolving software artifacts; leveraging human input through incentives and user-friendly interfaces; and maintaining knowledge completeness and accuracy as systems evolve.,,,,,,,,,,,,,,
Design for a Digital Twin in Clinical Patient Care,"Nitschke Anna-Katharina, Brandl Carlos, Egersdörfer Fabian, Görtz M., Hohenfellner Markus, Weidemüller Matthias",2025,reference-manager,,,,,,,Construction of a knowledge graph using the Resource Description Framework (RDF) to integrate and visualize multimodal clinical data and model relationships.,,"How can a modular, evolving, and interpretable Digital Twin design based on a knowledge graph using Resource Description Framework (RDF) effectively support and improve patient-centered clinical decision-making throughout the phases of the clinical patient journey?","The paper proposes a patient-centered Digital Twin (DT) system for medicine, using a knowledge graph built from multimodal data and models. The methodology includes provenance chains for interpretability and feedback loop control. Results show robust, explainable predictions. The study concludes that validation and regulatory compliance are essential for clinical translation.","The paper's main objective is to design a modular, interpretable Digital Twin (DT) for clinical patient care using a bipartite knowledge graph built from a Resource Description Framework (RDF); the key method integrates multimodal data and model provenance to ensure reliability, and principal findings show the approach meets essential requirements but needs further validation and scaling.",
Implementation of fusion models using various weighting schemes (accuracy weighting,entropy weighting,linear/logistic regression,neural networks) to combine outputs from different base models.,,,,,,,,,,,,
"Use of provenance chains to track the origin and influence of information for interpretability and to prevent feedback loop errors.""",,"The Digital Twin (DT) design enables holistic patient attribute estimation and interpretability, but its effectiveness depends on sufficient and diverse data and base models.",,,,,,,,,,,,,
In heart disease detection and glioma diagnosis,different fusion model weighting methods showed no significant performance difference; no p-values or quantitative results are provided.,,,,,,,,,,,,,,
The DT system’s modularity,feedback loops,and provenance chain support transparency and ongoing improvement,"but further validation and risk assessment are needed for clinical translation.""",The primary outcome is the development and evaluation of a Digital Twin (DT) design for clinical patient care.,,,,,,,,,,,
Results show the DT enables integration of mechanistic and AI-based models,supporting decision-making in various medical fields.,,,,,,,,,,,,,,
In heart disease detection and glioma diagnosis,no significant performance difference was found between different fusion weighting methods.,,,,,,,,,,,,,,
Statistical values: No specific numerical/statistical values (e.g.,ROC AUC,"F1 scores) are reported in the context.""",Reliance on the availability of data and base models limits applicability.,,,,,,,,,,,,
More and broader data are needed for holistic decision support.,,,,,,,,,,,,,,,
Supervised machine learning only identifies structures within data.,,,,,,,,,,,,,,,
Service accessibility must be ensured to avoid increasing health inequality.,,,,,,,,,,,,,,,
"Modularity trades off increased training data with potential information loss between features.
Further improvement",scaling,critical analysis,"and risk assessments are needed.""","The Digital Twin (DT) design supports interpretability, compliance, and privacy, enabling decision support throughout the patient journey.",,,,,,,,,,,
Clinical translation requires rigorous validation,regulatory compliance,robust control systems,and clinician trust.,,,,,,,,,,,,
Limitations include data availability,model modularity,and potential for increased health inequality.,,,,,,,,,,,,,
Future opportunities span multiple medical fields,but require large-scale,"diverse data and further development.""",Need for larger and more diverse data collection to enable holistic decision support and improve model performance.,,,,,,,,,,,,
"Ensuring broad service accessibility to prevent health-service inequality and socio-economic gaps.
Further improvement",scaling,critical analysis,"and risk assessments are required to generate guidelines for Digital Twin development in clinical environments.""","Future research should address data quantity and diversity, improve Digital Twin (DT) modularity, and ensure broad service accessibility. Further validation, scaling, risk assessment, and guideline development for DTs in clinical settings are needed. Large-scale routine, socioeconomic, and behavioral data collection is recommended.",,No information available,,,,,,,,,
ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant,"Xiang Yifan, Zhang Zhenxi, Li Bin, Weng Yixuan, Zhou Shoujun, He Yangfan, Li Keqin",2025,reference-manager,,,,,,,"Construction of Knowledge Graphs (KGs): Building structured representations of objects, their attributes, and relations for personalized knowledge integration.",,"How can integrating images, knowledge graphs, and Chain-of-Thought data enable the development and evaluation of personalized multimodal large language models capable of relational reasoning and contextual knowledge connection for personalized queries?","The paper investigates how answer length in Chain-of-Thought (CoT) QA affects model performance. Using experiments on closed-ended questions, it finds that longer CoT answers improve accuracy on difficult, multi-step reasoning tasks, but have little effect on simple tasks. The study concludes longer answers enhance reasoning for complex queries.","The research goal is to study how answer length in CoT QA pairs affects model performance; the approach refines answers by reducing reasoning steps and tests on closed-ended questions; results show longer answers improve performance on difficult tasks requiring reasoning, but have little effect on simple tasks.",
Chain-of-Thought (CoT) QA Pairs: Creating question-answer pairs that require step-by-step reasoning using the constructed knowledge graphs.,,,,,,,,,,,,,,,
"Robustness Analysis: Training multiple models independently on the same data to assess semantic consistency and reliability of outputs.""",The research is reproducible. The source code and datasets for ReGraP-LLaVA are released at: https://github.com/xyfyyds/ReGraP.,"ReGraP-LLaVA demonstrates robust, semantically consistent performance across repeated training runs, effectively leveraging personalized knowledge for diverse question types.",,,,,,,,,,,,,
Longer Chain-of-Thought (CoT) answers improve accuracy on difficult tasks requiring multi-step reasoning,while simple tasks remain unaffected by answer length.,,,,,,,,,,,,,,
"Human evaluation shows ReGraP-LLaVA outperforms baselines in 72 out of 100 cases; no explicit p-values reported.""",Reducing answer length in CoT QA pairs does not affect accuracy on simple tasks.,,,,,,,,,,,,,,
For difficult tasks needing multi-step reasoning,longer answers improve performance.,,,,,,,,,,,,,,
Example accuracies: Difficult MC Acc. up to 1.00,Simple MC Acc. up to 1.00.,,,,,,,,,,,,,,
Number of personalized objects has limited impact on performance.,,,,,,,,,,,,,,,
Hard-prompt method achieves highest accuracy,"with less than 0.4% difference from other methods.""",The model's answers can be overly detailed when a short response is sufficient.,,,,,,,,,,,,,
There may be variability in outputs due to stochasticity in training and inference.,,,,,,,,,,,,,,,
Some evaluation aspects (e.g.,multi-object QA,relational reasoning) are not supported by all compared benchmarks.,,,,,,,,,,,,,
"Further research suggested on robustness and answer length effects.""",Reducing answer length in CoT (Chain-of-Thought) QA pairs does not affect accuracy on simple tasks.,,,,,,,,,,,,,,
For difficult tasks needing multi-step reasoning,longer CoT answers significantly improve model performance.,,,,,,,,,,,,,,
Recommendation: Use longer,"detailed CoT answers for challenging queries; shorter answers suffice for basic tasks.""","Integrating images, knowledge graphs (KGs), and Chain-of-Thought (CoT) data to comprehensively encode personalized knowledge.",,,,,,,,,,,,,
Developing personalized Multimodal Large Language Models (MLLMs) whose training frameworks align with KGs for improved reasoning over personalized knowledge.,,,,,,,,,,,,,,,
Evaluating MLLMs’ relational reasoning and knowledge connection capabilities for personalized,"context-dependent queries.""",,,,,,,,,,,,,,
Steps to Knowledge Graphs Quality Assessment,Huaman Elwin,2022,reference-manager,,,,,,,"Proposed a user-driven assessment framework for Knowledge Graphs (KGs) quality, consisting of four steps: identification, setting, assessment, and exploitation.",,"How can a comprehensive, user-driven framework be developed to assess the quality of Knowledge Graphs (KGs) by identifying and integrating relevant quality dimensions and metrics from existing literature?","The paper reviews 30 studies on quality assessment, identifies 20 relevant quality dimensions (QDs) for knowledge graphs (KGs), and proposes a user-driven assessment framework with four steps: identification, setting, assessment, and exploitation. The framework enables flexible, weighted evaluation and aims to guide KG architects and future research.","The paper's main objective is to propose 20 relevant quality dimensions (QDs) and associated metrics for Knowledge Graph (KG) assessment, introduces a user-driven evaluation framework with four steps, and concludes that cost-effectiveness, traceability, and variety are key QDs for guiding KG architects and future assessment frameworks.",
Developed quantitative and qualitative metrics (QMs) and aggregated scoring formulas using alpha- and beta-weights for evaluating Quality Dimensions (QDs).,,,,,,,,,,,,,,,
Incorporated both manual and semi-automated methods,"combining human and machine efforts for assessment.""",,"The paper proposes 20 quality dimensions (QDs) relevant for knowledge graphs (KGs), emphasizing cost-effectiveness, traceability, and variety, and introduces a user-driven assessment framework with four steps: identification, setting, assessment, and exploitation.",,,,,,,,,,,,
The framework uses quantitative scores for QDs and quality metrics (QMs),with alpha- and beta-weights to aggregate and tune results; all scores and weights are within \[0,1].,,,,,,,,,,,,,
"No explicit statistical significance (p-values) or detailed quantitative results are reported; the framework's performance evaluation and surveys are planned as future work.""",Primary outcomes:,,,,,,,,,,,,,,
Identification and comparison of Quality Dimensions (QDs) and assessment tools across 30 papers.,,,,,,,,,,,,,,,
Introduction of additional QDs (e.g.,conciseness,interlinking,interoperability,licensing,,performance,,,,,,,versatility).,trustworthiness,syntactic validity
Implementation of 11 QDs evaluated through 34 Quality Metrics (QMs).,,,,,,,,,,,,,,,
Aggregated scoring formulas for QD Assessment (QDA) and Knowledge Graph Assessment (KGA) using weighted metrics.,,,,,,,,,,,,,,,
Results can be tuned and visualized by adjusting alpha- and beta-weights.,,,,,,,,,,,,,,,
"No explicit statistical values or measured effects reported.""","Most authors, except \[13,14], do not describe the methodology used to define quality dimensions (QDs).",,,,,,,,,,,,,,
Quality dimensions were created in the context of each author's focus,limiting generalizability.,,,,,,,,,,,,,,
Further research is needed to evaluate the proposed framework in specific use cases.,,,,,,,,,,,,,,,
No unique solution exists for assessing Knowledge Graphs (KGs).,,,,,,,,,,,,,,,
The framework may require a combination of human and machine efforts.,,,,,,,,,,,,,,,
"A unified quality assessment approach is lacking and needed for broader adoption.""
A user-driven","The study proposes 20 quality dimensions (QDs) and several metrics for assessing Knowledge Graphs (KGs), emphasizing cost-effectiveness, traceability, and variety.
4-step assessment framework is introduced: identification",setting,assessment,and exploitation.,,,,,,,,,,,
The framework guides KG architects and supports future KG assessment development.,,,,,,,,,,,,,,,
Future work includes developing,evaluating,and semi-automating the framework,"with input from domain experts.""","Further development and evaluation of the proposed Knowledge Graph (KG) assessment framework, including surveys from domain experts and KG researchers.",,,,,,,,,,,
Need for research on applying the framework to specific use cases,as there is no unique solution for KG quality assessment.,,,,,,,,,,,,,,
"Pursuit of a unified quality assessment approach to improve adoption and applicability of KGs globally.""","Future research should focus on developing and evaluating the proposed KG assessment framework, conducting expert surveys to refine quality dimensions (QDs) and the framework, exploring semi-automation of quantitative metrics, and testing the framework in specific use cases. A unified quality assessment approach for KGs is also recommended.",,,,,,,,,,,,,,
Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems,"Gupta Rajeev, Gupta Suhani, Parikh Ronak, Gupta Divya, Javaheri Amir, Shaktawat Jairaj Singh",2025,reference-manager,,,,,,,"Tri-Memory System: Utilizes three memory modules—Short-Term Memory (STM), Long-Term Memory (LTM), and Permanent Memory (PM)—with dynamic consolidation to manage learning at different timescales.",,"How can a biologically inspired, energy-efficient continual learning framework enable scalable, privacy-preserving, and personalized AGI on edge devices while addressing challenges such as catastrophic forgetting, expert management, memory consolidation, and the balance between generalization and personalization?","The paper investigates a Tri-Memory System—Short-Term Memory (STM), Long-Term Memory (LTM), and Permanent Memory (PM)—inspired by neuroscience principles like synaptic pruning and sparse coding. Using dynamic consolidation and sparse neural coding, the system efficiently retains, prunes, and consolidates knowledge, improving continual learning and resource efficiency.","The paper’s research goal is to enable Personalized AGI through neuroscience-inspired continual learning; its key approach is a Tri-Memory System (Short-Term, Long-Term, Permanent Memory) for dynamic knowledge consolidation; results show that combining multiple brain-inspired strategies is essential for lifelong adaptation and efficiency.",
Hybrid Learning: Combines Hebbian-like updates (local,correlation-based) with error-driven (gradient-based) learning for both rapid adaptation and global optimization.,,,,,,,,,,,,,,
"Replay-Based Rehearsal and Pruning: Uses replay of recent experiences and adaptive pruning based on usage counters and feedback to maintain memory efficiency and prevent forgetting.""",,,"The proposed framework uniquely combines dynamic memory allocation, lightweight Hebbian updates, nightly consolidation, and modular design for efficient continual learning.",,,,,,,,,,,,
Maintains bounded model size through pruning and selective memory retention,supporting edge deployment.,,,,,,,,,,,,,,
Addresses the stability–plasticity dilemma by separating learning timescales (STM,LTM,PM).,,,,,,,,,,,,,
Integrates user feedback (explicit ratings or sentiment analysis) to modulate consolidation and pruning.,,,,,,,,,,,,,,,
Table 1 conceptually compares the framework to existing methods on memory efficiency,catastrophic forgetting mitigation,edge compatibility,and modularity.,,,,,,,,,,,,
"No explicit statistical values or quantitative results are provided.""",Lack of strong theoretical guarantees for continual learning algorithms.,,,,,,,,,,,,,,
Need for optimal pruning schedules and thresholds.,,,,,,,,,,,,,,,
Unclear consolidation policies and promotion thresholds.,,,,,,,,,,,,,,,
Challenges in measuring and guaranteeing against catastrophic forgetting.,,,,,,,,,,,,,,,
Potential modular explosion from expert management.,,,,,,,,,,,,,,,
Difficulty in global knowledge reorganization for AGI-level scalability.,,,,,,,,,,,,,,,
"Tradeoff between plasticity and efficiency.""","The proposed architecture combines neuroscience-inspired strategies for continual learning, including synaptic pruning, Hebbian plasticity, sparse coding, and dual memory systems.",,,,,,,,,,,,,,
Selective memory consolidation and modular expert gating enable efficient,personalized,and energy-aware on-device AGI.,,,,,,,,,,,,,
Further research is needed on neuromorphic integration,theoretical guarantees,"and balancing generalization with personalization.""",Determining optimal pruning schedules and thresholds to balance adaptability and retention in continual learning systems.,,,,,,,,,,,,
Developing theoretical guarantees for stability–plasticity trade-offs and error bounds in online,pruned,and updated models.,,,,,,,,,,,,,
"Integrating the framework with neuromorphic hardware or spiking neural networks for enhanced energy efficiency.""",Suggested future research directions include: implementing the architecture on neuromorphic hardware or with spiking neural networks; developing theoretical guarantees for continual learning (such as stability–plasticity trade-offs and error bounds); optimizing pruning and consolidation strategies; managing expert modules; and balancing generalization with personalization.,,,,,,,,,,,,,,
A Lexical Approach to Assessing Stress: Development and Proof-of-Concept,"Driskell Tripp, Salas Eduardo, Burke C. Shawn, Driskell James E.",2021,reference-manager,10.1177/00187208211045167,,,,,,"Word-count approach: Analyzes text by counting keywords linked to specific psychological constructs, ignoring word order and context.",,"How effective are word-count approaches in analyzing psychological constructs within verbal content, considering their limitations in capturing sentence-level meaning and context?",,"The research goal is to unobtrusively assess stress effects in high-demand settings using a word-count analytic approach, and the principal finding is that the developed tool, STRESSnet, effectively evaluates cognitive and emotional states like stress, anxiety, and team orientation from ongoing communications.",
Unobtrusive assessment: Monitors communication without interfering,to evaluate cognitive/emotional states.,,,,,,,,,,,,,,
STRESSnet tool: Provides ongoing,"unobtrusive evaluation of individual and team communications for stress-related states.""",No information available,,,,Limited analysis based on available data.,,,,,,,,,
Not intended to replace traditional measures.,,,,,,,,,,,,,,,
May not generalize to all settings or populations.,,,,,,,,,,,,,,,
"Further research suggested for broader applicability and validation.""","STRESSnet offers an unobtrusive way to assess cognitive and emotional states, such as stress and anxiety, at both individual and team levels.",,,,,,,,,,,,,,
The tool effectively tracks constructs like Social Impairment and team orientation over time.,,,,,,,,,,,,,,,
Spoken language analysis provides valuable insights into psychological states.,,,,,,,,,,,,,,,
This method supplements,but does not replace,"other assessment approaches.""",Need for further validation of the STRESSnet tool in diverse operational settings.,,,,,,,,,,,,
Determining optimal methods for measuring and interpreting operator state using lexical analysis.,,,,,,,,,,,,,,,
"Addressing how lexical measures can be integrated with traditional assessment approaches for comprehensive monitoring.""",,,,,,,,,,,,,,,
Enhancing Temporal Knowledge Graph Representation with Curriculum Learning,"Liu Yihe, Shen Yi, Dai Yuanfei",2024,reference-manager,10.3390/electronics13173397,,,,,,"Curriculum learning: The framework uses curriculum learning, gradually training the model on data of increasing complexity to improve performance and efficiency.",,"How can a curriculum learning framework that progressively increases data difficulty and complexity during training enhance the performance, generalization, and robustness of temporal knowledge graph representation models?","The paper proposes a curriculum learning-guided framework for temporal knowledge graph representation. Using a difficulty assessor and curriculum temperature scaling, the method adapts training to data complexity. Experiments on ICEWS datasets show 1–2% MRR improvement and faster training, demonstrating enhanced performance, efficiency, and broad applicability.","The research goal is to improve temporal knowledge graph representation using a curriculum learning approach; the method segments training data by difficulty and adapts training progression, resulting in 1–2% higher MRR and faster training compared to baselines, demonstrating enhanced performance, efficiency, and generalization.",
Ablation study: The study removes or modifies specific modules (e.g.,difficulty assessor,sphere filtering) to analyze their individual contributions.,,,,,,,,,,,,,
Incremental training strategy: The model is trained incrementally,"focusing first on simpler data and progressively introducing more complex data.""",,"Curriculum learning improved Mean Reciprocal Rank (MRR) by 1–2% for De-SimplE and DistMult models on ICEWS14 and by ~1% on ICEWS0515, especially in Hits@10 and MRR.",,,,,,,,,,,,
The framework reduced training time,achieving similar or better results in only 60% (3/5) of the baseline models' time.,,,,,,,,,,,,,,
All improvements were statistically significant,"demonstrating enhanced efficiency and generalization without altering model structure.""","Curriculum learning (CL) improved MRR by 1–2% for DE-SimplE and DistMult on ICEWS14, and by ~1% on ICEWS0515.",,,,,,,,,,,,,
CL models achieved higher Hits@10 and MRR than baselines.,,,,,,,,,,,,,,,
CL reduced training time,achieving better results in 3/5 of the time.,,,,,,,,,,,,,,
"Ablation study: Removing CL components reduced performance; full CL achieved highest MRR (53.29 on ICEWS14).""","Improvements in MRR scores are relatively modest, ranging from 1–2%.",,,,,,,,,,,,,,
Temporal knowledge graphs are often sparse and incomplete,complicating training.,,,,,,,,,,,,,,
Existing methods face challenges due to complex dynamic interactions and temporal information.,,,,,,,,,,,,,,,
"No explicit mention of other limitations or suggestions for further research.""","The proposed curriculum learning framework improves model performance and training efficiency, achieving 1–2% higher MRR and faster convergence than baseline models.",,,,,,,,,,,,,,
Incremental training strategies enhance accuracy and robustness by focusing on simpler data first,then progressively tackling complex data.,,,,,,,,,,,,,,
The framework is adaptable,requiring only changes in training sample order,not model structure.,,,,,,,,,,,,,
"Recommendation: Apply curriculum learning strategies for better generalization and efficiency in temporal knowledge graph embedding tasks.""",Need for further exploration of curriculum learning strategies to address modest performance gains (1–2% in MRR) in some cases.,,,,,,,,,,,,,,
Investigating the application of the proposed framework to a broader range of temporal knowledge graph models and datasets.,,,,,,,,,,,,,,,
"Addressing challenges related to data sparsity and incomplete temporal knowledge graphs during training.""",,"Ablation study; experiments removing specific modules; curriculum learning framework; incremental training strategy; staged training with increasing data complexity; bucket-based grouping (3, 4, 5 buckets); comparison with baseline models; parameter optimization strategies; not randomized, double-blind, controlled, placebo-controlled, or observational; no mention of multi-site, retrospective, stratified, crossover, parallel, meta-analysis, or systematic review.",,,,,,,,,,,,,
Temporal Knowledge Graph Reasoning Based on Dynamic Fusion Representation Learning,"Chen Hongwei, Zhang Man, Chen Zexi",2024,reference-manager,10.1111/exsy.13758,,,,,,Dynamic local recurrent encoding layer (LRE): Captures neighboring historical facts to improve prediction accuracy.,,How can the proposed TD-RKG method effectively address the challenges of implicit correlation and time sensitivity in temporal knowledge graph reasoning by holistically modeling global dynamic information and encoding historical facts into entity and relationship representations?,"The paper investigates TD-RKG, a model for temporal knowledge graph reasoning. Using ablation studies on three datasets, it shows that dynamic local recurrent encoding and implicit encoding layers significantly improve performance. TD-RKG outperforms baselines in most metrics, effectively addressing implicit correlation and time sensitivity, though struggles with new entities.","The research goal is to improve temporal knowledge graph (TKG) completion; the TD-RKG approach uses dynamic local recurrent, implicit encoding, and attention layers; results show TD-RKG outperforms baselines in most metrics, especially in capturing implicit correlations and time sensitivity, though it underperforms on Hits@3 for YAGO.",
Dynamic implicit encoding layer (IE): Captures implicit correlations between entities,enhancing prediction timeliness and accuracy.,,,,,,,,,,,,,,
"Dynamic global information attention layer (GIA): Integrates global information for improved reasoning in temporal knowledge graphs.""","The paper provides a detailed experimental setup for reproducibility, including datasets, evaluation metrics, baseline models, and implementation details. For extrapolation reasoning models, publicly available source code from the papers and reproduced models with consistent parameter settings were used. No specific project source code link is provided.","TD-RKG significantly outperforms baseline models on ICEWS14, ICEWS05-15, and YAGO datasets in most metrics, especially MRR, Hits@1, and Hits@10.",,,,,,,,,,,,,
Ablation studies show the dynamic local recurrent encoding layer (LRE) has the greatest impact on performance; dynamic attention layer (GIA) improves ICEWS datasets.,,,,,,,,,,,,,,,
"No explicit p-values or statistical significance values are reported.""
On YAGO","Primary outcomes measured: Mean Reciprocal Rank (MRR), Hits@1, Hits@3, Hits@10.
TD-RKG achieved MRR: 57.96",Hits@3: 65.33,Hits@10: 75.01 (best results in bold).,,,,,,,,,,,,
TD-RKG outperformed baselines in MRR and Hits@1/10 but not Hits@3 on YAGO.,,,,,,,,,,,,,,,
"Ablation study showed LRE layer had the greatest impact on performance.""",Does not capture implicit relationships with global historical information.,,,,,,,,,,,,,,
Ignores the degree of influence of time on different entities.,,,,,,,,,,,,,,,
Relies on fixed entity representations and direct connections,limiting the capture of implicit associations.,,,,,,,,,,,,,,
Hits@3 metric does not meet state-of-the-art on the YAGO dataset due to new entities with randomly initialized representations.,,,,,,,,,,,,,,,
Only reports experimental results under the original setting,"which may yield incorrect higher-ranking scores in some cases.""","TD-RKG outperforms baseline models on most datasets, especially in relationship prediction MRR, Hits@1, and Hits@10.",,,,,,,,,,,,,
The local recurrent encoding layer (LRE) has the greatest positive impact; dynamic implicit encoding (IE) and attention (GIA) layers also improve performance.,,,,,,,,,,,,,,,
The model struggles with new entities in the YAGO dataset; meta-learning is recommended to address this.,,,,,,,,,,,,,,,
"TD-RKG effectively addresses implicit correlation and time-sensitivity challenges.""",The model struggles with new entities due to randomly initialized representations; future work will use meta-learning to improve adaptation.,,,,,,,,,,,,,,
Existing methods insufficiently address implicit correlation and time sensitivity between entities.,,,,,,,,,,,,,,,
"Current models do not fully consider the varying influence of time on different entities.""","Future research should address the model's limitation in handling new entities, especially in datasets like YAGO. Adopting a meta-learning strategy is suggested to improve adaptation and reasoning for new entities. Further exploration of implicit correlations and time sensitivity challenges is also recommended.",,,,,,,,,,,,,,
Contrastive Enhanced Learning for Multi-Label Text Classification,"Wu Tianxiang, Yang Shuqun",2024,reference-manager,10.3390/app14198650,,,,,,Contrastive learning: Used to construct a label space and align positive label representations for improved classification.,,How can a model that combines contrastive learning with attention mechanisms within a BERT-based framework effectively enhance both label correlation and textual semantics to improve multi-label text classification performance?,"The paper investigates document–label methods for multi-label text classification, focusing on improving label representation using textual semantics and label correlations. The proposed MPCM model, evaluated on AAPD and RCV1-v2 datasets, outperforms baselines. Ablation studies show each component’s contribution, with cosine distance best capturing label relationships. The approach enhances classification accuracy.","The research goal is to improve multi-label text classification by integrating label and document information using attention and contrastive learning; the approach introduces enhanced label representation, global representation learning, and positive label alignment; results show superior performance over existing methods on AAPD and RCV1-v2 datasets.",
Distance metrics (cosine,Euclidean,Manhattan): Evaluated for measuring vector distances in the label space,affecting model performance.,,,,,,,,,,,,
Attention mechanisms: Integrated to combine label and text information,"enhancing semantic representation of labels.""",,"The proposed model (""""Ours"""") achieved the highest results on both AAPD (P@1: 86.58, P@3: 62.37, P@5: 42.22, nDCG@3: 81.93, nDCG@5: 85.32) and RCV1-v2 (P@1: 98.35, P@3: 86.58, P@5: 62.19, nDCG@3: 95.21, nDCG@5: 95.75) datasets.",,,,,,,,,,,,
Cosine distance consistently outperformed Manhattan and Euclidean distances for constructing the label space,indicating the importance of metric selection in multi-label classification.,,,,,,,,,,,,,,
"Incorporating comprehensive label information and optimizing multiple loss components led to superior classification performance compared to existing models; no explicit p-values or statistical significance values are reported.""
On RCV1-v2","On AAPD, the proposed model (""""Ours"""") achieved the highest scores: P@1: 86.58, P@3: 62.37, P@5: 42.22, nDCG@3: 81.93, nDCG@5: 85.32.
""""""""Ours"""""""" also led: P@1: 98.35",P@3: 86.58,P@5: 62.19,nDCG@3: 95.21,,nDCG@5: 95.75.,,,,,,,,,
Euclidean distance performed worst among distance metrics for label space construction.,,,,,,,,,,,,,,,
Ablation tests showed removing contrastive label representation learning,positive label representation alignment,"or contrastive global representation learning reduced performance.""","Euclidean distance performs poorly, failing to capture nuanced label relationships due to sensitivity to scale and feature distribution.",,,,,,,,,,,,
Removing contrastive label representation learning significantly degrades performance and slows training,requiring about 50 epochs to match the performance others reach in 5 epochs.,,,,,,,,,,,,,,
Combining label and document information is often simplistic,"limiting full utilization of label information.""",Incorporating label information significantly improves multi-label text classification performance.,,,,,,,,,,,,,
The proposed model (MPCM) achieves competitive or superior results on both AAPD and RCV1-v2 datasets by comprehensively leveraging label information.,,,,,,,,,,,,,,,
Contrastive label representation learning is crucial for efficient and effective label space construction.,,,,,,,,,,,,,,,
Positive label representation alignment,though simple,"robustly enhances classification performance.""","Existing methods struggle to balance textual semantic and relational information of labels, often combining them simplistically and limiting classifier performance.",,,,,,,,,,,,
There is a need for improved approaches to fully utilize label information,especially for capturing both label correlation and textual semantics.,,,,,,,,,,,,,,
"Future research should explore advanced integration strategies for label and document information to enhance multi-label text classification.""","Future research should focus on improving label representation, especially balancing label textual semantics and label correlation. There is a need to develop methods that better utilize label information, address challenges with under-represented (tail) labels, and explore the impact of different distance metrics on label space construction.",,,,,,,,,,,,,,
FEDRKG: A Privacy-preserving Federated Recommendation Framework via Knowledge Graph Enhancement,"Yao Dezhong, Liu Tongtong, Cao Qi, Jin Hai",2024,reference-manager,,,,,,,"Federated Learning (FL): A privacy-preserving approach where each client trains on local user-item interaction data, and only model updates are shared with the server.",,How can a federated learning framework leveraging knowledge graphs be designed to provide accurate and privacy-preserving recommendations across diverse client devices without relying on sensitive user data or social network information?,"The paper proposes FEDRKG, a federated learning framework using graph neural networks (GNN) and knowledge graphs (KG) for privacy-preserving recommendations. By leveraging public item data and privacy mechanisms like local differential privacy (LDP), FEDRKG outperforms state-of-the-art federated methods and matches centralized models while protecting user data.","The research goal is to develop a privacy-preserving recommendation system; the approach introduces FEDRKG, a federated learning framework using graph neural networks and knowledge graphs; results show FEDRKG outperforms state-of-the-art federated methods and is competitive with centralized algorithms while protecting user privacy.",
Knowledge Graph (KG) Enhancement: The server maintains a knowledge graph to enrich recommendations by providing additional relational information between entities.,,,,,,,,,,,,,,,
Sensitivity Analysis: The study tests the impact of activated client numbers,receptive field depth,"and interaction item protection (privacy mechanisms) on model performance.""",No information available,"The proposed FEDRKG framework outperforms all federated learning baselines (FedMF, FedGNN) in AUC and F1 across MovieLens-20M, Book-Crossing, and Last.FM datasets, achieving AUCs up to 0.970 (±0.002).",,,,,,,,,,,
FEDRKG achieves competitive performance compared to centralized learning while preserving user privacy.,,,,,,,,,,,,,,,
"Statistical significance (p-values) is not explicitly reported in the context.""
FEDRKG achieved the best performance among federated learning methods:",Primary outcomes measured: AUC and F1 scores for click-through rate (CTR) prediction.,,,,,,,,,,,,,,
MovieLens-20M: AUC 0.970(±0.002),F1 0.919(±0.002),,,,,,,,,,,,,,
Book-Crossing: AUC 0.724(±0.004),F1 0.667(±0.006),,,,,,,,,,,,,,
Last.FM: AUC 0.785(±0.004),F1 0.708(±0.002),,,,,,,,,,,,,,
KGCN achieved the best AUC among centralized methods.,,,,,,,,,,,,,,,
The proposed method outperforms all federated baselines and is competitive with centralized learning in top-K recommendation.,,,,,,,,,,,,,,,
Sensitivity analysis showed:,,,,,,,,,,,,,,,
Fewer activated clients slightly decreased AUC on sparse datasets.,,,,,,,,,,,,,,,
Excessive receptive field depth reduced prediction accuracy.,,,,,,,,,,,,,,,
"Proper regularization with privacy mechanisms improved recommendation accuracy.""",Limited access to first-order interaction data reduces recommendation model effectiveness.,,,,,,,,,,,,,,
Privacy-preserving mechanisms can decrease recommendation accuracy.,,,,,,,,,,,,,,,
Excessive receptive field depth lowers prediction accuracy.,,,,,,,,,,,,,,,
Sparse client-side data may cause model overfitting.,,,,,,,,,,,,,,,
"Computationally intensive privacy methods are impractical for devices with limited resources.""",The proposed framework outperforms existing federated learning algorithms and achieves results competitive with centralized methods.,,,,,,,,,,,,,,
Incorporating knowledge graphs and relation-aware aggregation,especially using GNNs,significantly improves recommendation accuracy.,,,,,,,,,,,,,
Proper regularization and privacy-preserving mechanisms can enhance model performance despite limited client-side data.,,,,,,,,,,,,,,,
"Not all methods leveraging side information are effective; design and integration are critical.""","Lack of user connections: The current framework does not utilize user connections, limiting potential improvements.",,,,,,,,,,,,,,
Efficiency and interpretability: Future work aims to enhance how existing user connections are used without introducing new private data.,,,,,,,,,,,,,,,
Balancing privacy and performance: Excessive privacy measures (e.g.,high flip rates) can degrade system performance,"requiring careful trade-offs.""",,"The study design is an experimental framework using real-world datasets (MovieLens-20M, Book-Crossing, Last.FM). It employs a federated learning (FL) approach with a client-server architecture, privacy-preserving mechanisms, sensitivity analyses, and comparisons to centralized learning. No mention of randomization, blinding, control groups, or meta-analysis.",,,,,,,,,,,
A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction,"Bogachov Bogdan, Zhao Yaoyao Fiona",2025,reference-manager,,,,,,,Prompt engineering: Designing and refining prompts to interact with LLMs for efficient and accurate information generation.,,"How can a computationally efficient and accurate method be developed to adapt large language models for domain-specific engineering tasks, addressing the limitations of existing approaches such as prompt engineering, fine-tuning, and Retrieval-Augmented Generation?","The paper aims to develop a lightweight LLM adaptation method for engineering domains, focusing on maximizing accuracy and reducing hallucinations. Using a Cessna aircraft Structural Repair Manual, the methodology involves dataset preparation and SLG system construction. Significant findings highlight improved accuracy and applicability across engineering domains, though image data is excluded.","The research goal is to create a lightweight adaptation method for large language models in engineering; the approach, called Small Language Graph (SLG), uses a graph of small expert models; results show SLG outperforms conventional fine-tuning by 3x on Exact Match and is 1.7x faster.",
Retrieval-Augmented Generation (RAG): Chunking text,converting it to vectors,storing in a vector database,and retrieving relevant chunks to provide context for LLM responses.,,,,,,,,,,,,
Dataset preparation: Splitting engineering documents into text chunks,"generating question-answer pairs using Llama-3.3-70B-Instruct for fine-tuning and testing.""",The research describes its experimental setup and evaluation metrics but does not mention the availability of source code or other materials needed for full reproducibility. No links or references to project source code are provided.,"The approach retrieves relevant text chunks as vectors, compares them to user queries, and uses top-k vectors as context for a generator LLM, improving knowledge and reducing hallucinations.",,,,,,,,,,,,
The method enhances recall by breaking text into atomic statements and generating synthetic questions,but struggles with noisy data and multi-hop retrieval.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""
Best experiment results:
SLG: R-L 0.41","Primary outcomes measured: ROUGE-L (R-L), Exact Match (EM), and METEOR (M) scores.
EM 0.12",M 0.50,,,,,,,,,,,,,
Llama-3.1-8B: R-L 0.46,EM 0.05,M 0.55,,,,,,,,,,,,,
Llama-3.2-1B: R-L 0.43,EM 0.04,M 0.51,,,,,,,,,,,,,
"Average fine-tuning times:
SLG: 3475 seconds",,,,,,,,,,,,,,,
Llama-3.1-8B: 5891 seconds,,,,,,,,,,,,,,,
"Llama-3.2-1B: 2163 seconds""",Only two models (Llama-3.1-8B-Instruct and Llama-3.2-1B-Instruct) were compared; broader comparisons are needed.,,,,,,,,,,,,,,
High computational cost for full fine-tuning (updating all 137 billion parameters).,,,,,,,,,,,,,,,
Lack of hallucination tests in reported literature.,,,,,,,,,,,,,,,
Fine-tuned models risk knowledge overshadowing and hallucinations.,,,,,,,,,,,,,,,
Some methods struggle with ambiguous texts and require high computational resources.,,,,,,,,,,,,,,,
Limited performance of domain-specific models on general-language tasks.,,,,,,,,,,,,,,,
RAG-based techniques are overly reliant on complex models and lack standardized evaluation metrics.,,,,,,,,,,,,,,,
Some RAG methods assume each query has a single answerable chunk,do not handle multi-hop retrieval,and are only tested on small datasets.,,,,,,,,,,,,,
"RAG struggles with noisy data and sometimes cannot reject irrelevant queries.""","SLG outperforms stand-alone Llama-3.1-8B-Instruct and Llama-3.2-1B-Instruct models, especially in resisting hallucinations (EM metric: 3x better).",,,,,,,,,,,,,,
SLG experts and orchestrator train 1.7 times faster than Llama-3.1-8B-Instruct.,,,,,,,,,,,,,,,
Future work should expand model comparisons and tune additional hyperparameters.,,,,,,,,,,,,,,,
"Human evaluation and fact-checking are recommended for more robust hallucination assessment.""",The study only compares SLG to two models; future work will include broader comparisons with larger models like Llama-3.3-70B-Instruct and RAG.,,,,,,,,,,,,,,
There is a need for methods that combine computational efficiency,high accuracy,and effective handling of domain-specific tasks.,,,,,,,,,,,,,
"RAG-based techniques lack standardized evaluation metrics and are overly reliant on complex models.""","The study suggests future research should include more extensive comparisons with larger models like Llama-3.3-70B-Instruct and RAG. There is also a need to develop methods that combine computational efficiency with high accuracy for domain-specific tasks, addressing current limitations in model selection and evaluation.","The study design is an experimental comparative study. It involves fine-tuning and testing multiple language models (SLG, Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct) on a text-only engineering dataset, using quantitative evaluation metrics (ROUGE-L, Exact Match, METEOR). No mention of randomization, blinding, control, or observational elements.",,,,,,,,,,,,,
Privacy-Preserving Graph Machine Learning from Data to Computation: A Survey,"Fu Dongqi, Bao Wenxuan, Maciejewski Ross, Tong Hanghang, He Jingrui",2023,reference-manager,,,,,,,"Graph Summarization: Partitions the original graph into clusters, each serving as a node in an anonymized graph, with varied edge connection strategies to protect privacy.",,"What are the main privacy threats and protection techniques in graph machine learning, and how can privacy-preserving methods be systematically classified and integrated from both data and computational perspectives to achieve a comprehensive and secure graph machine learning system?","This paper reviews privacy-preserving techniques in graph machine learning, focusing on protecting sensitive information in graph data and computations. It systematically examines attackers, protection mechanisms (like randomized and differential privacy methods), and federated learning. The study highlights current challenges and suggests future research opportunities for secure graph machine learning systems.","The paper's research goal is to systematically review privacy-preserving techniques in graph machine learning, using a comprehensive approach covering both data and computation levels, and its principal finding is a classification of methods and future directions for secure graph machine learning systems.",
Switching-based Graph Generation: Uses iterative edge switching (Monte Carlo method) to anonymize graphs while preserving key structural features.,,,,,,,,,,,,,,,
Federated Learning (FL): Enables privacy-preserving graph computation by avoiding raw data transmission,"though model updates may still risk privacy.""",,"The paper systematically reviews privacy-preserving techniques in graph machine learning, focusing on both data generation and computation, especially under data sharing restrictions.",,,,,,,,,,,,
It analyzes attacker models (active and passive) and corresponding defense methods,and discusses federated learning frameworks addressing non-IID (non-independent and identically distributed) data challenges.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""","The paper systematically reviews privacy-preserving techniques in graph machine learning, focusing on both data and computational aspects.",,,,,,,,,,,,,,
It introduces various attacker types,required background knowledge,and corresponding protection mechanisms.,,,,,,,,,,,,,
Two general privacy protection mechanisms discussed are:,,,,,,,,,,,,,,,
Graph Summarization: Produces anonymized graphs by clustering nodes,resulting in varied anonymized graphs.,,,,,,,,,,,,,,
Switching-based Graph Generation: Uses iterative edge switching (Monte Carlo method) to anonymize graphs while preserving features like degree distribution,eigenvalues,eigenvectors,harmonic mean of geodesic path,and graph transitivity.,,,,,,,,,,,
"No explicit statistical values or quantitative results are provided.""","Assumption of shared model architecture across clients, despite optimal architectures differing due to graph size and over-smoothing in GNNs.",,,,,,,,,,,,,,
Privacy risks from transmitting hidden representations and cross-client information exchange.,,,,,,,,,,,,,,,
Privacy-preserving graph generation may reduce model utility,as techniques are not tailored for specific tasks.,,,,,,,,,,,,,,
Transmitting model parameters or gradients in FL may still leak private data.,,,,,,,,,,,,,,,
"Need to avoid cross-client transmission without degrading model performance.""","Current graph-level federated learning (FL) often assumes shared model architectures, but optimal architectures may differ across clients due to data and graph size differences.",,,,,,,,,,,,,,
Protecting raw data from reconstruction is essential for privacy in FL.,,,,,,,,,,,,,,,
Modeling client relationships as graphs can improve performance and unbiasedness.,,,,,,,,,,,,,,,
"Future research should address model heterogeneity and disentanglement of task-relevant information for better privacy-utility trade-offs.""","Addressing model heterogeneity in graph-level federated learning (FL), as optimal model architectures may differ across clients with varying graph sizes.",,,,,,,,,,,,,,
Avoiding cross-client transmission in subgraph-level FL to prevent privacy risks from direct information exchange along cross-client edges.,,,,,,,,,,,,,,,
Combining privacy-preserving data generation and computation to enhance privacy while maintaining model utility,"despite associated challenges.""","Suggested future research directions include: designing privacy-preserving graph generation methods that consider downstream machine learning tasks, addressing model heterogeneity in graph-level federated learning, avoiding cross-client transmission in subgraph-level FL, and combining privacy-preserving data generation with computation to enhance privacy and model utility.",,,,,,,,,,,,,
Exploring the role of dimensionality transformation in episodic memory,"Kerrén Casper, Reznik Daniel, Doeller Christian F., Griffiths Benjamin J.",2025,reference-manager,10.1016/j.tics.2025.01.007,,,,,,Principal Component Analysis (PCA) and demixed PCA (dPCA): Linear methods that reduce dimensionality by maximizing variance and separating components based on experimental parameters.,,"How does dimensionality transformation across neural regions support the encoding, storage, and retrieval of episodic memories, and what are its mechanisms, effects, and implications for memory function and cross-species differences?","The paper investigates how dimensionality transformation—reducing and expanding neural representations—supports episodic memory and other cognitive functions. Using insights from psychology, neuroscience, and computational models, it finds that low-dimensional representations enable efficient storage and flexible retrieval, though some memory detail is inevitably lost during this process.","The paper's main objective is to highlight how dimensionality transformation supports memory and cognition; using empirical and computational approaches, it shows that reducing complex information into low-dimensional representations enables efficient storage, flexible retrieval, and generalization, with results emphasizing its importance across episodic memory, object recognition, and goal-directed behavior.",
t-distributed stochastic neighbor embedding (t-SNE): A nonlinear method that captures complex relationships and preserves local data patterns.,,,,,,,,,,,,,,,
"Uniform Manifold Approximation and Projection (UMAP): Another nonlinear technique for uncovering intrinsic data structure.""",,"The paper proposes that dimensionality transformation, implemented by gamma oscillations (~30–100 Hz), is key for encoding, storing, and retrieving episodic memories.",,,,,,,,,,,,,
Dimensionality reduction/expansion is considered more viable than reducing neuron numbers for stable memory storage.,,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""","In rats, all cortical unimodal and transmodal areas project to the parahippocampal region; in humans, almost no direct unimodal associations exist.",,,,,,,,,,,,,,
Rodents show the least representational reduction (highest sensory precision); humans show the most (lowest sensory precision); nonhuman primates are intermediate.,,,,,,,,,,,,,,,
Neural oscillation frequencies (ripple,gamma,spindle,beta,alpha,,slow) are preserved across mammals.,,,,,,,,,
"Increased gamma oscillatory activity (~30–100 Hz) is hypothesized to enhance dimensionality transformation and benefit memory formation and retrieval.""",Variability in dimensionality transformation makes claims difficult to falsify and general rules hard to establish.,,,,,,,,,,,,,,
Loss of specific sensory details during dimensionality reduction can lead to memory distortions and reliance on semantic knowledge.,,,,,,,,,,,,,,,
Optimal dimensionality for episodic memory varies by memory and task,complicating generalization.,,,,,,,,,,,,,,
Need for biologically plausible computational models.,,,,,,,,,,,,,,,
"Suggestion for future research on representational dimensions across species.""","Episodic memories are dynamic and undergo dimensionality transformation, often losing or altering details over time.",,,,,,,,,,,,,,
Gamma oscillations may facilitate these neural transformations,impacting memory encoding,storage,and retrieval.,,,,,,,,,,,,
Dimensionality reduction supports efficient memory,object recognition,and goal-directed behavior.,,,,,,,,,,,,,
"Future research should empirically test these mechanisms and develop biologically plausible computational models.""",How representational dimensionality changes across brain regions during encoding and its effect on memory storage.,,,,,,,,,,,,,,
The neural trajectory of dimensionality signaling from the hippocampus to neocortical regions during episodic memory retrieval.,,,,,,,,,,,,,,,
"The relative importance of oscillatory power and frequency for dimensionality transformation.""","Future research should investigate how representational dimensionality changes across brain regions during encoding and retrieval, differences between unimodal and transmodal areas across species, the neural trajectory from hippocampus to neocortex, and develop biologically plausible computational models of memory transformation.",,,,,,,,,,,,,,
Towards a reference software architecture for human-AI teaming in smart manufacturing,"Haindl Philipp, Buchgeher Georg, Khan Maqbool, Moser Bernhard",2022,reference-manager,10.1145/3510455.3512788,,,,,,Developed a reference software architecture as a blueprint for research and validation.,,"How can a reference software architecture be designed and validated to enable effective, ethical, and scalable human-AI teaming in smart manufacturing across diverse industrial contexts?","The paper proposes a reference software architecture for human-AI teaming in smart manufacturing. Using the Lambda architecture pattern, it integrates batch, speed, and serving layers to manage data and support decision-making. The architecture aims to ensure scalability, interoperability, and ethical compliance, with validation planned across diverse industrial contexts.","The paper's main objective is to present an initial reference software architecture for human-AI teaming in smart manufacturing, using a Lambda architecture approach, with the principal finding being a preliminary blueprint that addresses latency, data management, and ethical compliance challenges.",
Applied the Lambda architecture pattern to organize components by latency requirements into batch,speed,and serving layers.,,,,,,,,,,,,,
"Used autonomous synchronization management to replicate and synchronize data stores for balancing large data processing.""",The research presents a preliminary reference software architecture developed by the Teaming.AI project. There is no explicit mention of the availability of source code for the project. Reproducibility details beyond the architectural description are not provided.,"A preliminary reference software architecture was developed, using the Lambda architecture pattern to address different latency requirements in smart manufacturing.",,,,,,,,,,,,,
The architecture is framework- and technology-agnostic,intended as a blueprint for diverse manufacturing contexts.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""",The primary outcome is the development of a preliminary reference software architecture for AI-enabled smart manufacturing systems.,,,,,,,,,,,,,,
No results or measured effects,including statistical values,are reported in the provided context.,,,,,,,,,,,,,
Future validation will assess applicability,scalability,and ethical compliance,"but no quantitative results are available yet.""",The reference software architecture is in a preliminary state and may change as research progresses.,,,,,,,,,,,
Monitoring teaming aspects in human-AI interaction is a main challenge.,,,,,,,,,,,,,,,
Scalability for near-realtime IIoT data processing remains a key challenge.,,,,,,,,,,,,,,,
Runtime monitoring and validation of ethical policies is difficult,especially since they can only be evaluated at runtime.,,,,,,,,,,,,,,
"Identifying and implementing functionalities for orchestrated machine learning in knowledge graphs is challenging.""","The reference software architecture is framework- and technology-agnostic, serving as a blueprint for various manufacturing contexts.",,,,,,,,,,,,,,
Validation will assess applicability,scalability,and ethical compliance in different domains.,,,,,,,,,,,,,
Expert and operator interviews,plus quantitative data,will be used for evaluation.,,,,,,,,,,,,,
"Subtle changes to the architecture are expected as research progresses.""",Formalizing and validating ethical policies and standards for AI systems at runtime remains a key challenge.,,,,,,,,,,,,,,
Identifying required functionalities for orchestrated machine learning within the framework abstraction layer is needed.,,,,,,,,,,,,,,,
"Examining approaches for calculating graph embeddings and understanding their computational costs is a significant research gap.""","Future research should address: monitoring teaming aspects in human-AI interaction, scalability for near-realtime IIoT data processing, runtime monitoring and validation of ethical policies, and relational machine learning for knowledge graphs. Gaps include modeling trust, continuous human feedback evaluation, and ensuring scalability and timing requirements.",No information available,,,,,,,,,,,,,
Web of Scholars: A Scholar Knowledge Graph,"Liu Jiaying, Ren Jing, Zheng Wenqing, Chi Lianhua, Lee Ivan, Xia Feng",2020,reference-manager,10.1145/3397271.3401405,,,,,,"Construction and use of a knowledge graph to extract, integrate, and profile explicit and implicit scholar relationships.",,"How can a knowledge graph-based system be designed to efficiently extract, integrate, and analyze both explicit and implicit relationships among scholars to enable comprehensive scholar profiling, relationship mining, and personalized academic services in large-scale academic networks?","The paper presents Web of Scholars, a knowledge graph-based system designed to extract, integrate, and profile both explicit and implicit relationships among scholars. Using data mining and visualization techniques on Microsoft Academic Graph data, it enables intelligent search, scholar ranking, and relationship analysis, supporting personalized academic services and open API access.","The research goal is to efficiently mine and visualize implicit scholar relationships; the approach uses a knowledge graph-based system called Web of Scholars, integrating data mining and visualization techniques; results show comprehensive profiling and interactive analysis of over 1.7 million researchers and their relationships in Computer Science.",
Application of data mining,information processing,social network analysis,and graphic rendering to analyze academic networks.,,,,,,,,,,,,
Utilization of a three-tier system architecture (Data Access,Business Logic,Application layers) with technologies like TITAN graph database,HBase,"and Spring MVC Framework.""",,,,,,,,,,,"Web of Scholars extracts and visualizes over 433 million relationships among 1.7 million scholars and 1.5 million publications in Computer Science, enabling advanced profiling and relationship mining."
The system reveals both explicit and implicit relationships (e.g.,advisor-advisee) using knowledge graph techniques,supporting intelligent queries and personalized recommendations.,,,,,,,,,,,,,
"No statistical significance (p-values) or quantitative evaluation results are reported in the context.""","Web of Scholars system enables efficient search, ranking, and mining of scholars and their relationships in large academic networks.",,,,,,,,,,,,,,
The system collects over 1.7 million scholars,1.5 million publications,and more than 433 million relationships (7 types).,,,,,,,,,,,,,
Provides visualization,intelligent query,academic ranking,scholar evaluation,and in-depth relationship mining.,,,,,,,,,,,
"No statistical values or measured effects reported.""",Difficult to obtain information quickly and accurately in large-scale networks.,,,,,,,,,,,,,,
Redundant and unstructured data on the Web can overwhelm users and are underutilized.,,,,,,,,,,,,,,,
Existing systems mainly focus on explicit relationships,making it hard to profile scholars based on user requirements.,,,,,,,,,,,,,,
"Challenges in mining complex implicit relationships among scholars.""","Web of Scholars effectively extracts and visualizes both explicit and implicit scholar relationships using knowledge graphs, enabling comprehensive scholar profiling and analysis.",,,,,,,,,,,,,,
The system supports personalized services such as advisor/advisee recommendations and expert finding for students,scholars,and institutions.,,,,,,,,,,,,,
It manages large-scale academic data efficiently,facilitating fast,intelligent semantic search and interactive analysis.,,,,,,,,,,,,,
"Open APIs and downloadable datasets promote further research and development.""","Difficulty in efficiently mining and analyzing implicit relationships (like advisor-advisee) among scholars in large, complex academic networks.",,,,,,,,,,,,,,
Lack of multidimensional profiling for scholars using multiple scholarly data sources.,,,,,,,,,,,,,,,
Existing systems mainly focus on explicit relationships,"limiting advanced services such as personalized recommendations and in-depth network analysis.""",,,,,,,,,,,,,,
"Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities","Chen Jiaoyan, Dong Hang, Hastings Janna, Jiménez-Ruiz Ernesto, Lopez Vanessa, Monnin Pierre, Pesquita Catia, vSkoda Petr, Tamma Valentina A. M.",2023,reference-manager,,,,,,,"Data source selection: Integrating structured, semi-structured, and unstructured data (e.g., databases, XML documents, free-text publications) as foundational sources for the knowledge graph (KG).",,"How can knowledge graphs be effectively constructed, integrated, and customized from diverse data sources to support scientific discovery and decision-making in the life sciences, while addressing technical challenges such as ontology alignment, entity recognition, and minimizing manual curation?","The paper reviews recent advances in Knowledge Graph (KG) research in life sciences, focusing on KG construction, knowledge discovery, and explainable AI (XAI). It highlights challenges in data integration, ontology development, and evaluating explanation quality, emphasizing the need for scalable systems and improved expert-validated evaluation methods.","The paper's main objective is to survey recent advances in graph-based technologies for life sciences, focusing on Knowledge Graph construction, knowledge discovery, and explainable AI; the key method is a review of exemplary use cases, and the principal finding is a vision outlining challenges and future research directions.",
Ontology construction: Creating a controlled vocabulary and schema using top-down (expert-driven or ontology reuse) or bottom-up (semi-automatic,data-driven) approaches.,,,,,,,,,,,,,,
"Knowledge extraction: Enriching the KG by extracting relationships from large-scale free-text sources using text mining techniques.""",,"Knowledge Graphs (KGs) are increasingly used in life sciences for integrating complex, large-scale, relational data, with examples like TERA and Hetionet demonstrating successful knowledge integration across domains.",,,,,,,,,,,,,
The paper identifies key challenges: incorporating KG semantics into machine learning,addressing long-tail label distributions,and creating efficient multi-modal KG representations.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""",No information available,Difficulty in constructing customized Knowledge Graphs (KGs) from multiple sources and integrating different domain KGs.,,,,,,,,,,,,,
Challenges in estimating semantic similarity and equivalence between knowledge elements.,,,,,,,,,,,,,,,
Need for better tool support to reduce manual curation while allowing user involvement.,,,,,,,,,,,,,,,
Scalability and efficiency issues in knowledge retrieval,query,and reasoning systems.,,,,,,,,,,,,,
Difficulty in updating KGs to reflect new data and concepts; risk of outdated schemas and facts.,,,,,,,,,,,,,,,
Quality assurance challenges,including error detection,correction,and robustness to noisy KGs.,,,,,,,,,,,,
Integrating KGs with deep learning and language models for better explainability (XAI) remains unresolved.,,,,,,,,,,,,,,,
Hallucination in large language models (LLMs); need for KGs to improve trustworthiness and interpretability.,,,,,,,,,,,,,,,
Incorporating KG semantics into machine learning models is still an open challenge.,,,,,,,,,,,,,,,
Addressing the long-tail phenomenon in classification tasks,especially for rare or unseen labels.,,,,,,,,,,,,,,
Creating efficient multi-modal representations of knowledge is still difficult.,,,,,,,,,,,,,,,
"Lack of widely accepted definitions and methods for explainability in AI models.""","There are significant technical challenges in constructing, integrating, and managing customized Knowledge Graphs (KGs) for life sciences.",,,,,,,,,,,,,,
Incorporating KG semantics into machine learning and addressing data imbalance (long-tail phenomenon) remain open research problems.,,,,,,,,,,,,,,,
"Continuous KG evolution
More scalable","quality assurance
efficient knowledge retrieval and reasoning systems are recommended.""","and robust evaluation methods for explainable AI (XAI) are needed.
Incorporating semantics from Knowledge Graphs (KGs) into machine learning models, especially for protein-related tasks, remains a major challenge.",,,,,,,,,,,,,
Addressing the long-tail phenomenon in machine learning with KGs,particularly for rare diseases or infrequent labels,is an open research gap.,,,,,,,,,,,,,
"Creating efficient multi-modal representations of knowledge to enable discovery is still an unresolved issue.""","Future research should focus on scalable and efficient knowledge retrieval, updating and quality assurance of knowledge graphs (KGs), integrating KGs with explainable AI (XAI) and language models, handling long-tail data in machine learning with KGs, developing multi-modal representations, and improving evaluation methods for XAI explanations.",,,,,,,,,,,,,,
PKG API: A Tool for Personal Knowledge Graph Management,"Bernard Nolwenn, Kostric Ivica, Łajewska Weronika, Balog Krisztian, Galusčáková Petra, Setty Vinay, Skjæveland Martin G.",2024,reference-manager,10.1145/3589335.3651247,,,,,,"Deployment of a RESTful API (PKG API) using Flask, with a React-based user interface (PKG Client).",,"How can a user-friendly, natural language-enabled personal knowledge graph (PKG) system be designed and implemented to allow individuals to intuitively manage and control their personal data?","The paper aims to create a user-friendly personal knowledge graph (PKG) system, enabling users to manage their data using natural language. Using a modular API (Flask backend) and a React-based client, the system translates user statements into structured data. Results show effective preference representation, supporting intuitive, user-centric PKG management.","The research goal is to enable user-friendly management of personal knowledge graphs (PKGs) via natural language; the approach combines a modular PKG API with NL2PKG translation and a web client, and the principal finding is that users can intuitively interact with PKGs using natural language statements.",
Use of the NL2PKG module to translate natural language statements into structured queries,leveraging large language models (LLMs) like Llama2-7b and Mistral-7b.,,,,,,,,,,,,,,
Entity linking using REL as the default method,"with DBPedia Spotlight as an alternative.""","The research provides an open-source demo of their solution, which includes a modular codebase for easy experimentation. The project uses a Flask-based RESTful API, a React user interface, and relies on frameworks like Ollama and RDFLib. No specific source code repository link is provided.","The proposed solution introduces a modular PKG API and user-friendly PKG Client, enabling natural language interaction with personal knowledge graphs (PKGs).",,,,,,,,,,,,
Preliminary experiments led to Mistral-7b being selected as the default large language model (LLM); however,no quantitative results or p-values are reported.,,,,,,,,,,,,,,
The open-source demo demonstrates the feasibility of translating natural language preferences into structured PKG statements,"advancing user-centric PKG management.""",The primary outcome is the development of a PKG API and Client enabling users to manage personal knowledge graphs (PKGs) using natural language statements.,,,,,,,,,,,,,
The system demonstrates the viability of translating natural language preferences into structured PKG data.,,,,,,,,,,,,,,,
"No statistical values or quantitative measured effects are reported.""","Most research on personal knowledge graphs (PKGs) remains conceptual; practical, user-facing implementations are lacking.",,,,,,,,,,,,,,
Existing solutions like Solid are complex and not user-friendly for ordinary users.,,,,,,,,,,,,,,,
Compatibility issues exist between Pod providers and Solid apps,leading to inconsistent user experiences.,,,,,,,,,,,,,,
"No further limitations or suggestions for future research are explicitly stated.""","Personal knowledge graphs (PKGs) are promising tools for organizing personal information, especially as digital data grows.",,,,,,,,,,,,,,
Existing tools are often too complex for non-expert users; this study developed a user-friendly PKG with natural language interaction.,,,,,,,,,,,,,,,
The open-source demo demonstrates the feasibility of intuitive,"user-centric PKGs and encourages further research into broader applications.""","Need for more intuitive, user-centric interaction methods with personal knowledge graphs (PKGs).",,,,,,,,,,,,,
Exploration of broader applications for PKGs beyond current use cases.,,,,,,,,,,,,,,,
"Further research into simplifying PKG tools for non-expert users.""",,,,,,,,,,,,,,,
"Large Scale Multimodal Data Capture, Evaluation and Maintenance Framework for Autonomous Driving Datasets",Lakshminarayana Nitheesh K.,2019,reference-manager,10.1109/iccvw.2019.00530,,,,,,Analysis and validation of multimodal sensor data: Examining and confirming the quality and accuracy of data from multiple sensors.,,"How can an open-source framework be developed to effectively capture, evaluate, and maintain large volumes of multimodal sensor data for autonomous driving, addressing challenges in data synchronization, quality analysis, storage, and visualization?","The paper aims to address challenges in capturing, evaluating, and maintaining large-scale multimodal sensor data for autonomous driving. Using an open-source framework, it analyzes and validates data from various sensors, visualizes sensor-specific data, and performs timeseries analysis. The study highlights the need for standardized evaluation processes and improved data management.","The paper's main objective is to develop an open-source framework for capturing, evaluating, and maintaining large-scale multimodal autonomous driving data; the key method involves sensor-specific analysis and visualization; the principal finding is that their framework addresses challenges in data quality, synchronization, and management for such datasets.",
Visualization of sensor-specific data: Creating visual representations for each type of sensor data to aid understanding and evaluation.,,,,,,,,,,,,,,,
"Timeseries analysis of sensor data: Studying how sensor data changes over time to identify patterns or issues.""","The research is reproducible as the framework and source code are open-sourced and available at https://github.com/intel/driving-data-collection-reference-kit. No source code is provided for ROS, Uber AVS, or Cruise WebViz in the context.","The web dashboard enables instant visualization and evaluation of multi-sensor data, reducing data access time by avoiding large file reads.",,,,,,,,,,,,,
Data integrity is verified through manual inspection for anomalies in camera,LiDAR,and OBD data; good data shows no frame loss or corruption.,,,,,,,,,,,,,
"No statistical significance (p-values) or quantitative results are reported in the context.""",Primary outcomes:,,,,,,,,,,,,,,
Developed an open-source framework for capturing,evaluating,and maintaining multimodal sensor data for autonomous driving.,,,,,,,,,,,,,
Enabled analysis and validation,visualization,and timeseries analysis of sensor-specific data.,,,,,,,,,,,,,
Manual data validation currently takes several minutes per dataset.,,,,,,,,,,,,,,,
Results and measured effects:,,,,,,,,,,,,,,,
Framework allows detection of malformed/bad data by comparing proportional changes in rpm and speed.,,,,,,,,,,,,,,,
Gear transitions to neutral as vehicle stops,confirming data validity.,,,,,,,,,,,,,,
Data rate for multi-camera platform: 764.65 MB/s; storage requirement: 4TB/hour.,,,,,,,,,,,,,,,
"Real-time synchronization of all sensors is challenging due to different FPS and lack of external control signals.""","Limited on-board storage restricts continuous data capture due to high data rates (up to 4TB/hour, 800MB/s).",,,,,,,,,,,,,,
Real-time synchronization of all sensors is extremely challenging; some sensors cannot be accurately synchronized.,,,,,,,,,,,,,,,
Accurate extrinsic calibration (e.g.,lidar-to-cams) remains an open research problem.,,,,,,,,,,,,,,
Not all sensor data can be directly visualized for human interpretation.,,,,,,,,,,,,,,,
No standardized process exists for frame-by-frame evaluation of large multimodal datasets.,,,,,,,,,,,,,,,
Issues like incoherent data,corrupted frames,invalid metadata,and incorrect sensor initialization may occur.,,,,,,,,,,,,
"Most existing works do not describe raw data evaluation and maintenance pipelines.""","High data rates (up to 4TB/hour) and varying sensor FPS make continuous, synchronized multimodal data capture and analysis challenging.",,,,,,,,,,,,,,
Real-time synchronization across all sensors is difficult,especially for those not supporting external signals.,,,,,,,,,,,,,,
There is a lack of standardized,frame-by-frame multimodal data evaluation and visualization processes.,,,,,,,,,,,,,,
The study recommends systematic,dashboard-based evaluation steps for data integrity,synchronization,"and anomaly detection.""","Accurate extrinsic calibration between different sensors (e.g., lidar-to-cameras) remains an open research problem.",,,,,,,,,,,
Real-time synchronization and joint visualization of multimodal sensor data for large-scale datasets is extremely challenging.,,,,,,,,,,,,,,,
"There is a lack of standardized processes and tools for frame-by-frame evaluation and quality analysis of raw multimodal data.""","Future research should focus on analysis and validation of multimodal sensor data, visualization of sensor-specific data, and timeseries analysis of sensor data. There is a need for standardized processes to evaluate and visualize large-scale, frame-by-frame multimodal data for dataset creation tasks.",,,,,,,,,,,,,,
Electronic Health Record–Oriented Knowledge Graph System for Collaborative Clinical Decision Support Using Multicenter Fragmented Medical Data: Design and Application Study,"Shang Yong, Tian Yu, Lyu Kewei, Zhou Tianshu, Zhang Ping, Chen Jianghua, Li Jingsong",2024,reference-manager,10.2196/54263,,,,,,Deployment of the system across three tertiary hospitals to integrate and analyze multicenter EHR data for CKD detection.,,How can a multicenter collaborative reasoning framework securely integrate and analyze fragmented electronic health record data from multiple hospitals to improve the early detection of overlooked chronic kidney disease?,"The paper investigates a multicenter collaborative reasoning framework for clinical decision support (CDS) using distributed EHR knowledge graphs. It employs secure, privacy-preserving sharing of intermediate reasoning results across hospitals. The system integrates findings to generate precise, explainable CDS responses, enhancing decision-making while maintaining data security and efficiency.","The research goal was to enable secure, collaborative clinical decision support (CDS) using fragmented multicenter EHR data; the approach used an EHR-oriented knowledge graph system with encrypted, blockchain-synchronized intermediate results; the principal finding was effective, privacy-preserving CDS generation without sharing original data, benefiting early chronic disease detection.",
Development of a disease-specific local ontology and semantic reasoning rules,created with input from nephrology experts,to identify CKD-related abnormalities.,,,,,,,,,,,,,
"Collaborative reasoning approach that combines fragmented patient data from multiple hospitals to detect overlooked CKD cases.""",,"Multicenter collaborative reasoning identified CKD risks earlier, with a median discovery lead time of 364 days versus 121 days for the transferred group.",,,,,,,,,,,,,
Duplicate examinations were reduced (mean: 3.34 tests in multicenter vs. 3.56 in transferred group).,,,,,,,,,,,,,,,
"Risk coverage ratio was higher in multicenter data (165%) compared to single-hospital data (133%). No p-values reported.""","The system identified CKD risks earlier than clinical assessment, with a median discovery lead time of 364 days (multicenter group) vs. 121 days (transferred group).",,,,,,,,,,,,,,
Duplicate examinations were reduced: mean reduction of 3.34 (SD 2.72) tests in the multicenter group vs. 3.56 (SD 4.12) in the transferred group.,,,,,,,,,,,,,,,
"Risk coverage ratio was higher in the multicenter group (165%) compared to the transferred group (133%).""",Communication efficiency may face bottlenecks when deployed across many hospitals and clinics.,,,,,,,,,,,,,,
Network and computational resource costs may increase due to patient alignment and semantic reasoning.,,,,,,,,,,,,,,,
Patient alignment relies on unique identifiers,which is challenging if unique citizen IDs are missing.,,,,,,,,,,,,,,
Data incompleteness remains a challenge during model application.,,,,,,,,,,,,,,,
Further systematic design and application of the Hyperledger method are needed for widespread deployment.,,,,,,,,,,,,,,,
"Alternative approaches for patient alignment using nonunique identifiers are necessary.""",Multicenter collaborative reasoning enables earlier CKD risk detection and reduces missed or delayed diagnoses.,,,,,,,,,,,,,,
The system decreases duplicate examinations by leveraging fragmented data across hospitals.,,,,,,,,,,,,,,,
Comprehensive risk coverage supports clinicians in making thorough assessments.,,,,,,,,,,,,,,,
Data security is maintained through encrypted,"high-level information sharing and local data isolation.""",Limited explainability and generality of the current model remain unresolved.,,,,,,,,,,,,,
Communication efficiency and computational costs pose challenges for large-scale,multicenter deployment.,,,,,,,,,,,,,,
"Patient alignment is difficult when unique identifiers are missing; alternative approaches using nonunique identifiers are needed.""","Future research should address communication bottlenecks and increased network/computational costs when scaling the system. Further systematic design, including the Hyperledger method, is needed. Research on patient alignment without unique IDs and adapting the system for other diseases and data-sensitive environments is also recommended.","The study design is a multicenter, observational application study conducted across three tertiary A-level hospitals. It involved collaborative reasoning using fragmented electronic health record data, without randomization, blinding, control groups, or placebo. The design is retrospective, focusing on early detection of overlooked chronic kidney disease.",,,,,,,,,,,,,
Merging heterogeneous clinical data to enable knowledge discovery,"Seneviratne Martin G., Kahn Michael G., Hernandez-Boussard Tina",2018,reference-manager,10.1142/9789813279827\_0040,,,,,,"Integration of heterogeneous data sources: Combining traditional health data with patient-generated health data (PGHD), social media, and wearable device data to enable knowledge discovery in precision medicine and population health.",,"How can the integration of heterogeneous clinical, molecular, and environmental data streams across institutions and modalities be achieved to drive knowledge discovery and advance precision medicine while addressing challenges of interoperability, data governance, and patient privacy?","The paper aims to advance precision medicine by integrating large-scale clinical, molecular, and environmental datasets across institutions and data types. Using data standards and ontology-driven integration, the study highlights improved prediction and phenotyping through multi-modal data, while addressing technical, semantic, and ethical integration challenges.",The research goal is to enable knowledge discovery in biomedicine by merging heterogeneous clinical data; the approach focuses on integrating multi-institutional and multi-modal datasets using interoperability standards and ontology-driven methods; the principal finding is that such integration improves phenotyping and prediction algorithms.,
Data mining: Identifying drug-drug interactions by analyzing social media posts and biomedical literature.,,,,,,,,,,,,,,,
"Incorporation of patient-reported outcomes (PROs) into electronic health records (EHRs) to promote patient-centric care.""",,Integrating diverse datasets from multiple institutions and data types offers significant opportunities for biomedical knowledge discovery and precision medicine.,,,,,,,,,,,,,
"Technical and operational challenges remain
Larger","especially in harmonizing
integrated datasets improve machine learning performance for diagnostics and prognostics","structuring
but privacy and data governance are critical concerns. No p-values reported.""","and learning from multi-modal datasets
Combining traditional health data with patient-generated health data (PGHD) or social media data enables knowledge discovery in precision medicine and population health.",despite progress with interoperability standards.,,,,,,,,,,,
Santillana et al.: Combined hospital visit data with Twitter,Google searches,and online health forum posts to predict influenza incidence.,,,,,,,,,,,,,
Vilar et al.: Identified drug-drug interactions by combining social media posts with biomedical literature.,,,,,,,,,,,,,,,
Integrating patient-reported outcomes (PROs) into electronic health records (EHRs) promotes patient-centric care and provides insights into relationships between PROs and clinical outcomes such as mortality.,,,,,,,,,,,,,,,
The rise of connected monitoring devices and ambient information (e.g.,geo-location) creates opportunities for richer multi-modal datasets.,,,,,,,,,,,,,,
Data integration is suggested to reduce overall healthcare costs.,,,,,,,,,,,,,,,
"No specific statistical values are provided in the context.""","Difficulty harmonizing data from different abstraction levels (e.g., diagnosis codes vs. proteomic data), complicating storage and feature generation.",,,,,,,,,,,,,,
Privacy and security risks,including increased risk of patient re-identification.,,,,,,,,,,,,,,
Need for improved de-identification,consent,and access control processes.,,,,,,,,,,,,,
Potential for bias due to demographic imbalances in training data.,,,,,,,,,,,,,,,
Technical and operational challenges in sharing data across institutions and modalities.,,,,,,,,,,,,,,,
Uncertainty regarding clinician liability and duty of care with aggregated health data.,,,,,,,,,,,,,,,
"Ongoing challenges in structuring and learning from multi-modal datasets.""",Integrating diverse datasets across institutions and data types offers major opportunities for biomedical knowledge discovery and precision medicine.,,,,,,,,,,,,,,
Technical and operational challenges in data sharing are being addressed with interoperability standards and data sharing platforms.,,,,,,,,,,,,,,,
The main challenge now is harmonizing,structuring,"and learning from multi-modal datasets while ensuring privacy and governance.""","Harmonizing and integrating diverse data types across different abstraction levels remains a major challenge, especially for data storage and machine learning.",,,,,,,,,,,,
Ensuring data stewardship,privacy,and security is critical,particularly regarding de-identification,consent,,and access control.,,,,,,,,,
"Addressing equity and inclusion to prevent bias in large-scale biomedical datasets is needed.""","Future research should focus on integrating diverse clinical data streams across institutions and modalities, developing flexible data storage and machine learning systems, and establishing effective data governance. Addressing technical and operational challenges in harmonizing and learning from multi-modal datasets remains a key area for further investigation.",,,,,,,,,,,,,,
"Issues and challenges in Cloud Storage Architecture: A Survey
Auditing","Ghani Anwar, Badshah Afzal, Jan Saeed Ullah, Alshdadi Abdulrahman A., Daud Ali
recording","2020
and data management methods are applied for data security","reference-manager
including trusted timestamps.",10.1109/RpJC.2020.DOI Number,,,,,,Encryption and decryption techniques are used to ensure data confidentiality in cloud storage systems.,,"What are the key challenges related to data security and data management in cloud storage architecture, and what solutions have been proposed to address these issues?","The paper surveys issues and challenges in cloud storage architecture, focusing on data security and data management. Using literature review, it identifies key problems like confidentiality, integrity, access, and virtualization vulnerabilities. The study discusses solutions such as cryptography and blockchain, concluding with future opportunities and recommendations for secure cloud storage.","The research goal is to survey challenges and solutions in cloud storage, the approach reviews security and data management issues and countermeasures, and the principal finding is that while cloud storage is scalable and convenient, significant security and management challenges remain, requiring further research.",
"Directory services are utilized for authentication and verification processes.""",,"By 2025, about 75 billion devices will connect to the internet, generating over 175 zettabytes (ZB) of data annually, highlighting a massive demand and bright future for cloud storage.",,,,,,,,,,,,,
Cloud storage is highly scalable and manageable but faces significant challenges,mainly in security (confidentiality,integrity,access,authentication,,authorization) and data management.,,,,,,,,,
Solutions include cryptography,digital certificates,attribute-based encryption,and global transaction managers; however,"further research is needed to address ongoing issues. No p-values or statistical significance reported.""",,Primary outcomes:,,,,,,,,,
Identification of key cloud storage challenges: Security,Confidentiality,Data Dynamics,Integrity,Data Access,,Data Segregation,and Virtualization vulnerabilities.,,,,,,Backup Problem,Data Breaches,Authentication and Authorization
Review of possible solutions: Cryptography,Digital signatures,Proxy Re-encryption,Obfuscation,Blockchain.,,,,,,,,,,,
"No statistical values or measured effects reported.""",Lack of proper storage management may lead to severe consequences.,,,,,,,,,,,,,,
Users have limited control over data location and deletion.,,,,,,,,,,,,,,,
Confidentiality and privacy depend on provider policies and legal jurisdictions.,,,,,,,,,,,,,,,
Legal obligations may force providers to disclose user data.,,,,,,,,,,,,,,,
Vulnerabilities in virtualization can expose sensitive data.,,,,,,,,,,,,,,,
"Off-the-shelf products may not meet cloud data center requirements.""","Cloud storage is rapidly evolving, driven by technologies like IoT, AI, and 5G, leading to massive data growth and a bright future for cloud storage.",,,,,,,,,,,,,,
Key challenges remain in security (confidentiality,integrity,access,authentication) and data management (dynamics,segregation,,backup,,,,,,,,,virtualization).
Solutions include cryptography,digital certificates,attribute-based encryption,and global transaction managers.,,,,,,,,,,,,
"Continued research and development are recommended to address ongoing security and management issues.""","Security concerns remain unresolved, especially regarding confidentiality, integrity, access, authentication, authorization, and data breaches.",,,,,,,,,,,,,,
Data management issues such as data dynamics,segregation,backup,and vulnerabilities in virtualization require further research.,,,,,,,,,,,,
"More research is needed to encourage business and enterprise customers to store sensitive data in cloud storage.""","Future research should address unresolved issues in cloud storage, especially security (confidentiality, integrity, access, authentication, authorization, data breaches) and data management (dynamics, segregation, backup, virtualization). Further investigation into scalable, cost-effective infrastructure and integration with AI, IoT, and 5G is recommended.",,,,,,,,,,,,,,
KnowCoder-V2: Deep Knowledge Analysis,"Li Zixuan, Liu Wenxuan, Bai Long, Zhang Chunmao, Li Wei, Zhang Fenghui, Jin Quanxin, He Ruoyun, Chen Zhuo, Hu Zhilei, Wang Fei, Xu Bingbing, Jiang Xuhui, Jin Xiaolong, Guo Jiafeng, Cheng Xueqi",2025,reference-manager,,,,,,,"Knowledge Organization Phase: Involves ontology expansion, knowledge extraction (using Information Extraction tasks like Named Entity Recognition, Relation Extraction, and Event Extraction), and continuous knowledge update from multiple documents.",,"How can the Knowledgeable Deep Research (KDR) framework systematically extract, organize, and reason over large-scale, heterogeneous research data to enable deep knowledge analysis and generate comprehensive research reports?","The paper presents a framework for extracting, organizing, and reasoning over structured knowledge from unstructured documents using ontologies and Python classes. It decomposes research tasks into subtasks, applies information extraction methods, and uses reasoning to generate comprehensive reports. The approach yields deeper insights and improved analysis compared to existing frameworks.","The research goal was to analyze Geoffrey Hinton's papers from 2015–2024, using ontology-based methods to organize and summarize his evolving focus from foundational neural network techniques to hierarchical representations and biologically plausible, efficient, and interpretable learning models, concluding with a clear trend toward self-supervision and neuroscience-inspired approaches.",
Knowledge Reasoning Phase: Decomposes research tasks into subtasks,then applies knowledge computation and text generation to generate comprehensive reports.,,,,,,,,,,,,,,
Task Decomposition: Uses a human-in-the-loop approach and LLMs to break down research tasks into subtasks,"which are structured into Python classes for further processing.""",No information available,"KNOWCODER-V2 under the KDR framework significantly improves the “Insight” score in report generation, offering more comprehensive and deep analysis compared to previous frameworks.",,,,,,,,,,,,
Hinton’s research from 2015-2024 shifted from neural network training (e.g.,layer normalization: 14,776 citations) to capsule networks (~5,000 citations) and later to self-supervision and biological plausibility.,,,,,,,,,,,,
"No explicit p-values or statistical significance data are provided in the context.""","KNOWCODER-V2, under the KDR framework, generates more comprehensive and insightful experimental results, significantly improving the """"Insight"""" score.",,,,,,,,,,,,,,
KNOWCODER-V2 can compute citation counts and analyze publication trends for researchers.,,,,,,,,,,,,,,,
Table 1 shows KnowCoder-V2 achieves Accuracy up to 83.3 and Wu\&Palmer Similarity up to 95.9 on taxonomy expansion tasks.,,,,,,,,,,,,,,,
Figure 1 demonstrates KnowCoder-V2 outperforms counterparts in Comprehensiveness,Thoroughness,Factuality,Coherence,Insight,,"and Average report generation scores.""",,,,,,,,,"Coarse knowledge management: Only web pages are used as knowledge units, making it hard to manage fine-grained, organized knowledge."
Inefficient operation manner: The framework operates entirely online,causing repeated searches and unnecessary computational overhead.,,,,,,,,,,,,,,
Shallow knowledge computation: Only shallow text-based computations are possible,"lacking deep logical deduction and statistical inference.""","KNOWCODER-V2 with the KDR framework generates more comprehensive, insightful, and deep analyses than previous frameworks, especially in complex knowledge analysis tasks.",,,,,,,,,,,,,
The system excels at extracting,organizing,and updating structured knowledge from diverse,unstructured sources.,,,,,,,,,,,,
Task decomposition and knowledge computation enable detailed,novel findings beyond simple summarization.,,,,,,,,,,,,,,
Recommendation: Use KNOWCODER-V2 for tasks requiring fine-grained,"multi-step knowledge analysis and reasoning.""","Coarse Knowledge Management: Existing frameworks manage knowledge at the web page level, making it difficult to use fine-grained, organized information from many documents.",,,,,,,,,,,,,
Inefficient Operation Manner: Operating entirely online leads to repeated searches and unnecessary computational overhead.,,,,,,,,,,,,,,,
Shallow Knowledge Computation: Current systems only perform basic text processing,"lacking advanced reasoning and analysis abilities.""","Future research should address limitations in coarse knowledge management, inefficient operation, and shallow knowledge computation. Suggested directions include developing finer-grained knowledge organization, improving operational efficiency, and enhancing computational capabilities for deeper logical deduction, statistical inference, and dynamic querying over structured knowledge.",,,,,,,,,,,,,
Synaptic proteins in neuron-derived extracellular vesicles as biomarkers for Alzheimer’s disease: novel methodology and clinical proof of concept,"Eitan Erez, Thornton-Wells Tricia, Elgart Katya, Erden Eren, Gershun Eve, Levine Amir, Volpert Olga, Azadeh Mitra, Smith Daniel G., Kapogiannis Dimitrios",2023,reference-manager,10.20517/evcna.2023.13,,,,,,"Retrospective analysis of de-identified samples from commercial and government biobanks, collected under strict regulatory and ethical standards.",,"How can neuronal-derived extracellular vesicles (NDEVs) isolated from blood serve as specific, minimally invasive biomarkers for detecting and monitoring Alzheimer’s disease pathology in living patients?","The study aimed to determine if changes in synaptic proteins in cerebrospinal fluid occur before neurodegeneration markers in preclinical Alzheimer’s disease. Using de-identified samples and statistical analyses, the study found synaptic protein changes precede neurodegeneration markers, suggesting early biomarker potential for Alzheimer’s disease detection.","The research goal was to evaluate synaptic proteins in neuron-derived extracellular vesicles as Alzheimer’s disease biomarkers using a novel methodology; the approach involved clinical proof of concept and statistical analyses; results showed these biomarkers can distinguish early-stage Alzheimer’s disease from controls, supporting their diagnostic potential.",
Statistical analysis using linear mixed models,discriminant classifier analysis (Wilks’ Lambda method),and receiver operator characteristic (ROC) analysis.,,,,,,,,,,,,,
Biomarker assessment through group comparisons,diagnostic classification,"and correlation analyses.""","Reproducibility between cohorts was emphasized, with multiple cohorts used to test robustness. Absolute biomarker levels varied widely, likely due to inconsistent preanalytical conditions. NDEV isolation by ExoSORT was reproducible between experiments, operators, and labs. No source code for the project is mentioned.","p181-Tau, proBDNF, and GluR2 showed significant differences between early AD and controls, reproducible across cohorts and surviving Bonferroni correction (P < 0.003125).",,,,,,,,,,,
"The best diagnostic model (GluR2
p181-Tau","proBDNF
proBDNF","NRGN
and GluR2 were significantly correlated with MMSE and CDR-SOB clinical scores (P ≤ 0.001–0.037).""","GAP43) achieved 81.3% accuracy
Differences in GAP43 and proBDNF between early AD and controls were reproducible across cohorts and survived Bonferroni correction (P < 0.003125).",94.7% sensitivity,,and 61.5% specificity (P < 0.001).,,,,,,,,,
No age- or sex-associated differences in biomarkers.,,,,,,,,,,,,,,,
"p181-Tau had the highest diagnostic accuracy (AUC = 0.832
GluR2 (AUC = 0.755)","P < 0.001).
proBDNF (AUC = 0.736)",GAP43 (AUC = 0.729),NRGN (AUC = 0.716),STXN1 (AUC = 0.709),,PSD95 (AUC = 0.675).,,,,,,,,,
"Discriminant model using GluR2
p181-Tau","proBDNF
proBDNF","NRGN
and GluR2 significantly correlated with MMSE and CDR-SOB clinical scores.""","GAP43: 81.3% accuracy
All participants had early Alzheimer’s disease, resulting in a narrow cognitive score range and limiting associations with cognition across the full AD severity spectrum.",94.7% sensitivity,,61.5% specificity (P < 0.001).,,,,,,,,,
Absolute biomarker levels varied widely between cohorts,likely due to inconsistencies in preanalytical parameters (e.g.,blood draws,plasma processing).,,,,,,,,,,,,
"Variability in preanalytical conditions remains a challenge for analytical validation and generalizability.""",,Technical difficulties in isolating and analyzing extracellular vesicles (EVs) hinder their clinical implementation as biomarkers.,,,,,,,,,,,,,
The heterogeneity of plasma EVs complicates identification and requires improved methods for capturing cell-origin-specific sub-populations.,,,,,,,,,,,,,,,
Immunoaffinity methods like ExoSORT depend on marker specificity,which is rarely complete,"potentially affecting the accuracy of neuron-derived EV enrichment.""","Future research should address mechanistic studies to clarify the role of NRGN in early AD, improve control and monitoring of preanalytical conditions to reduce biomarker variability, and develop standardized preanalytical protocols and quality controls for NDEV-based diagnostic assays to enhance reproducibility and analytical validation.","Retrospective, observational, non-randomized, non-controlled, multi-cohort study using de-identified samples from biobanks. Analyses were blinded. No placebo or intervention. Separate cohorts (NIA and BioIVT/PMED) were analyzed. Statistical analyses included linear mixed models, discriminant analysis, and ROC analysis.",,,,,,,,,,,
Solving problems of research information heterogeneity during integration – using the European CERIF and German RCD standards as examples,"Azeroual Otmane, Saake Gunter, Abuosba Mohammad, Schöpfel Joachim",2019,reference-manager,10.3233/isu-180030,,,,,,Extraction: Selecting and replicating research information from various heterogeneous source systems using defined interfaces.,,"How can problems of data heterogeneity and distribution across different systems be solved, and which methods, processes, and techniques can ensure the long-term and sustainable quality of research information during its integration into research information management systems (RIMS)?","The paper examines how to solve data heterogeneity and ensure long-term quality during research information integration into RIMS. Using empirical studies and literature review, it discusses methods like data transformation, harmonization, and merging. The study concludes that RIMS improve information supply but face challenges in data integration and quality management.","The paper's main objective is to address research information heterogeneity during integration into RIMS by empirically examining methods for harmonizing and merging heterogeneous data, concluding that harmonization and integration processes are essential for reliable, high-quality research information management.",
Cleaning and Transformation: Identifying and removing errors,inconsistencies,and translating data into a uniform format for analysis.,,,,,,,,,,,,,
Harmonization and Merging: Aligning,consolidating,"and aggregating data from different systems into a consistent structure for integration.""",,"Research Information Management Systems (RIMS) improve targeted and faster information supply, becoming strategic in research ecosystems.",,,,,,,,,,,
Data harmonization and merging are essential to address heterogeneity,ensuring consistent and high-quality research information.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are provided in the context.""",The primary outcome is the identification and solution of problems related to data heterogeneity during integration into research information management systems (RIMS).,,,,,,,,,,,,,,
The paper presents methods such as filtering,harmonization,aggregation,and enrichment to ensure long-term and sustainable quality of integrated research information.,,,,,,,,,,,,
"No statistical values or measured effects are provided.""","Inaccurate, irrelevant, outdated, incomplete, or spurious data encountered by all surveyed researchers.",,,,,,,,,,,,,,
Difficulty integrating heterogeneous and non-compliant data formats from various sources.,,,,,,,,,,,,,,,
Data often scattered across multiple locations,making access and processing challenging.,,,,,,,,,,,,,,
Need for continuous data quality monitoring and control.,,,,,,,,,,,,,,,
Data quality issues can negatively impact decision-making and institutional funding.,,,,,,,,,,,,,,,
"Lack of reliable and credible data sources may affect RIMS usability and researcher motivation.""","RIMS improve targeted and faster information supply, becoming key for research success.",,,,,,,,,,,,,,
Data quality issues (inaccurate,outdated,incomplete data) impact researcher acceptance and system usage.,,,,,,,,,,,,,
Effective integration and transformation processes (cleaning,harmonization,aggregation,enrichment) are essential for reliable RIMS.,,,,,,,,,,,,
"Recommendation: Prioritize data quality and user needs to ensure RIMS effectiveness.""","Insufficient data quality in RIMS, including issues with inaccurate, irrelevant, outdated, incomplete, or spurious data, impacts system acceptance and researcher motivation.",,,,,,,,,,,,,,
Challenges in integrating heterogeneous research information from various internal and external sources due to non-compliant formats and scattered data.,,,,,,,,,,,,,,,
"Need for improved harmonization and merging processes to ensure consistent and meaningful aggregation of research information.""",,,,,,,,,,,,,,,
Leveraging Knowledge Graph-Based Human-Like Memory Systems to Solve Partially Observable Markov Decision Processes,"Kim Taewoon, François-Lavet Vincent, Cochez Michael",2024,reference-manager,,,,,,,"KG-based Environment Design: Developed a Knowledge Graph (KG)-based simulation environment for agents to navigate and answer questions, enabling systematic study of memory management.",,"How can an agent effectively manage and utilize long-term memory, represented as a dynamic knowledge graph, to optimize exploration and question answering in partially observable environments with biased sampling and large state spaces?","The paper aims to improve AI agents’ ability to solve partially observable tasks by modeling human-like long-term memory using knowledge graph-based systems. Using a custom Rooms Environment, the HumemAI agent, which integrates episodic and semantic memory, outperforms a baseline. The study suggests further improvements in memory management and exploration strategies.","The research goal is to improve AI memory management in partially observable environments using a knowledge graph-based human-like memory system; the approach involves HumemAI agents with episodic and semantic memory in a configurable maze, and results show these agents outperform baselines by better capturing hidden states.",
Comparative Analysis of Memory Systems: Compared agents using different memory systems (short,episodic,semantic) to analyze their effectiveness.,,,,,,,,,,,,,
"Baseline Observation History Method: Used an agent that relies on a sequence of past observations to estimate the current state and answer questions.""",,"The HumemAI agent, which uses episodic and semantic memory, consistently outperformed the baseline agent across all tested memory capacities, with average test episode rewards up to 235 (±37) compared to the baseline's maximum of 200 (±15).",,,,,,,,,,,,,
Ablation studies showed that both episodic and semantic memory contribute to performance,with HumemAI (E) and HumemAI (S) variants achieving lower rewards than the full HumemAI.,,,,,,,,,,,,,,
"No explicit p-values or statistical significance measures are reported in the provided context.""",The HumemAI agent (with both episodic and semantic memory) outperforms the baseline agent across all memory capacities.,,,,,,,,,,,,,,
Average test episode rewards (±σ) for HumemAI (capacity 48): Phase 1: 118 (±18),Phase 2: 235 (±37); Baseline: 200 (±15).,,,,,,,,,,,,,,
Longer observation histories in baseline agents do not necessarily improve performance.,,,,,,,,,,,,,,,
HumemAI agent (capacity 48) training time: 40 minutes; baseline agent: 50 minutes.,,,,,,,,,,,,,,,
Attention weights show a shift from short-term to long-term memory,"and from episodic to semantic memory over time.""",No comprehensive understanding of how long-term memory is managed.,,,,,,,,,,,,,
Precise regulation and optimization of memory processes remain unclear.,,,,,,,,,,,,,,,
Sampling bias in the environment affects exploration policy and question-answering.,,,,,,,,,,,,,,,
"Prior works do not address structured knowledge graphs or human-like memory systems as in this study.""",HumemAI agents with episodic and semantic memory outperform baseline agents in partially observable environments.,,,,,,,,,,,,,,
The two-phase training approach (memory management,then exploration) is effective.,,,,,,,,,,,,,,
Longer observation histories in baselines can harm performance.,,,,,,,,,,,,,,,
Future work: jointly learn memory management and exploration,use graph neural networks,"and learn question-answering functions directly.""",Jointly learning memory management and exploration policies could improve agent performance.,,,,,,,,,,,,
Exploring other function approximators,such as graph neural networks (GNNs),may offer advantages over the currently used LSTM.,,,,,,,,,,,,,
"Transitioning from handcrafted question-answering functions to learning them directly is a promising future direction.""","Suggestions for future research include jointly learning memory management and exploration policies, exploring other function approximators like graph neural networks (GNNs) instead of LSTM, and transitioning from handcrafted question-answering functions to learning them directly.",No information available,,,,,,,,,,,,,
Designing decentralized knowledge management systems to effectuate individual and collective generative capacities,Schmitt Ulrich,2019,reference-manager,10.1108/k-03-2019-0215,,,,,,"Design Science Research (DSR): The study uses DSR to guide the creation and evaluation of a personal knowledge management system (PKMS), focusing on evolving artifacts and design theories.",,How can the integration of psychological notions of generativity into digital and systemic knowledge management inform the design and development of a novel personal knowledge management system (PKMS) to address current shortcomings and foster sustainable individual and collective knowledge practices?,"The paper aims to design and prototype a personal knowledge management system (PKMS) using design science research (DSR). It synthesizes transdisciplinary concepts, especially generativity, to address limitations in current knowledge management. Findings highlight a holistic framework that complements organizational KM, supporting lifelong learning and creative collaboration.","The research goal is to address shortcomings in current knowledge management (KM) by developing a novel personal knowledge management system (PKMS) using design science research and prototyping; the approach integrates generativity concepts for holistic, systemic KM, and results show this enables new, sustainable interventions for individuals and organizations.",
Prototyping: A prototype PKMS device application and cloud-based WHOMER server were developed and tested.,,,,,,,,,,,,,,,
"Transdisciplinary Approach: The research applies transdisciplinary methods to integrate knowledge across disciplines and disrupt disciplinary silos.""","The research follows design science research (DSR) practices and reports on early visions and prototype development. However, the prototype is still in progress, and there is no mention of source code availability. No explicit details on reproducibility or access to project code are provided.","The study reports early visions of the impact of a personal knowledge management system (PKMS), focusing on feasibility, suitability, acceptability, and potential as a general-purpose or disruptive technology.",,,,,,,,,,,,,
The PKMS prototype aims to address knowledge fragmentation and promote transdisciplinary collaboration by reducing structural holes and enabling broader access to collective intelligence.,,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are provided in the context.""",The primary outcome is the development and prototyping of a personal knowledge management system (PKMS) using design science research (DSR).,,,,,,,,,,,,,,
The PKMS was positively benchmarked against 12 objective criteria for disruptive innovations and general-purpose technologies.,,,,,,,,,,,,,,,
The study reports early visions of technology impact,feasibility,suitability,acceptability,and potential for innovation spawning.,,,,,,,,,,,
"No specific statistical values are provided.""",Prototype development is still in progress.,,,,,,,,,,,,,,
Results may be limited by early-stage reporting of technology impact.,,,,,,,,,,,,,,,
Some aspects have been detailed in earlier publications,not fully within this article.,,,,,,,,,,,,,,
The study aims to complement,not replace,traditional approaches,"which may affect generalizability.""","The study presents a holistic, transdisciplinary framework for personal knowledge management systems (PKMS) to complement, not replace, traditional organizational KM.",,,,,,,,,,,
It highlights the need to overcome outdated,linear document models and disciplinary silos by enabling structural reuse and generativity.,,,,,,,,,,,,,,
The PKMS approach aims to empower individuals for lifelong learning,creativity,and teamwork,enhancing both personal and organizational performance.,,,,,,,,,,,,
"Prototype development is ongoing; early findings indicate feasibility and potential as a disruptive innovation.""",Need for digital structural reuse and referencing to share knowledge beyond traditional publication limits and reduce redundant effort.,,,,,,,,,,,,,,
Requirement for transdisciplinary approaches to overcome disciplinary silos and prevent fragmented,disconnected knowledge.,,,,,,,,,,,,,,
Further development and evaluation of the PKMS prototype,focusing on feasibility,suitability,"and acceptability as a general-purpose technology.""","Future research should address the current limitations of knowledge management (KM) structures, especially knowledge entropy and fragmentation. Further investigation is needed into the holistic and systemic integration of generativity concepts in digital KM systems, and the ongoing development and impact of the PKMS prototype.",,"Design science research (DSR) methodology; prototyping project; longitudinal streams of research; generative design approach; multidisciplinary and decentralized focus; theory effectiveness principle; evaluation of evolving artifacts and design theories; not randomized, not controlled, not double-blind, not placebo-controlled, not observational, not a meta-analysis or systematic review.",,,,,,,,,
Using a Personal Health Library–Enabled mHealth Recommender System for Self-Management of Diabetes Among Underserved Populations: Use Case for Knowledge Graphs and Linked Data,"Ammar Nariman, Bailey James E, Davis Robert L, Shaban-Nejad Arash",2021,reference-manager,10.2196/24738,,,,,,"User-centered design and formative evaluation: Iterative feedback from focus groups (patients, caregivers, clinicians, health education professionals) to assess usability, clinical, and educational content.",,"How can a Personal Health Library (PHL) leveraging semantic technologies and federated Linked Data querying be designed and evaluated to integrate, manage, and support dynamic knowledge discovery from patient data while addressing usability, clinical, and educational requirements?","The paper aims to enhance physicians’ understanding of treatment recommendations’ applicability using the PHL platform, which leverages semantic technologies and Linked Open Data. Through user-centered design and formative evaluation, the study finds that PHL empowers patients and providers, supports knowledge sharing, and enables third-party application development.","The research goal is to help physicians understand the applicability of treatment recommendations by using the PHL platform, which leverages Linked Open Data, semantic annotations, and technologies like SPARQL and Solid; the principal finding is that these tools enable effective pattern discovery and knowledge capture for clinical practice.",
Semantic technologies: Use of ontologies and annotations for data mining,knowledge representation,and pattern detection.,,,,,,,,,,,,,
"Linked Data querying: Utilization of LDflex language for dynamic querying and integration of external Linked Data resources.""",,The PHL enables physicians to better understand the applicability and generalizability of treatment recommendations using visual aids and semantic annotations.,,,,,,,,,,,,,
The PHL leverages innovative technologies,including Linked Open Data and ontologies,to support pattern discovery and knowledge capture.,,,,,,,,,,,,,
"The PHL helps patients and caregivers make informed health decisions and supports healthcare providers with informatics tools; it is available as an open service to encourage third-party development.""",Three primary outcomes:,,,,,,,,,,,,,,
Number of days in the previous week participants ate healthy meals,,,,,,,,,,,,,,,
Number of days participated in at least 30 minutes of physical activity,,,,,,,,,,,,,,,
Number of days took medications as prescribed,,,,,,,,,,,,,,,
Effects will be measured by mean changes from baseline over 12 months.,,,,,,,,,,,,,,,
Expected effect sizes: small (standardized difference=0.375) to medium (0.50).,,,,,,,,,,,,,,,
"No actual results or statistical values reported.""",,The study highlights the need for automatic tools to help scientists discover patterns and associations in research data.,,,,,,,,,,,,,
The PHL (Population Health Library) uses semantic annotations and Linked Open Data to support data mining and knowledge sharing.,,,,,,,,,,,,,,,
"Visual aids are recommended to help physicians understand the relevance of clinical guidelines.""","Need for more automatic tools to support scientists in pattern discovery, including link predictions and discovering complex annotation patterns across diseases and drug interventions.",,,,,,,,,,,,,,
Focus on enriching patients' health knowledge graphs to improve reasoning capabilities of the knowledge layer.,,,,,,,,,,,,,,,
Further implementation of an end-to-end intelligent recommender and digital librarian framework,including text summarization,knowledge mapping,"and personalized resource suggestions.""","Future research should focus on developing more automatic tools for pattern discovery, enriching patients' health knowledge graphs, implementing an end-to-end intelligent recommender and digital librarian framework, incorporating artificial intelligence, and conducting clinical trials to fully evaluate the app and recruit participants.",,"Study 1: Descriptive, iterative, user-centered design, and formative evaluation; includes pre-development focus groups, workshops, scenario and dashboard simulations, and thematic assessment.",,,,,,,,,
"Study 2: Pragmatic randomized controlled trial (RCT) with intervention (TM and IR-TM) and control (usual care) arms; comparative effectiveness; multi-arm; follow-up over 12 months.""",,,,,,,,,,,,,,,
Towards a Personal Health Knowledge Graph Framework for Patient Monitoring,"Bloor Daniel, Ugwuoke Nnamdi, Taylor David, Lewis Keir, Mur Luis, Lu Chuan",2023,reference-manager,,,,,,,"Use of the MIMIC-III dataset for testing and validating the prototype system, including both structured (vital signs) and unstructured (clinical notes) data.",,"How can a modular framework integrating ontologies, semantic technologies, and multimodal data be developed and implemented to construct and personalize personal health knowledge graphs (PHKGs) for chronic disease monitoring, specifically for patients with chronic obstructive pulmonary disease (COPD)?","The paper proposes a modular framework for constructing temporal knowledge graphs (KGs) to monitor COPD patients. Using ontologies, data harmonization, and NLP tools like MedCAT, the system summarizes and annotates time series and free text data. Key findings include efficient data storage, personalized risk scoring, and improved reasoning for patient care.","The paper's main objective is to construct personal health knowledge graphs (PHKGs) for chronic disease monitoring using a modular framework that integrates expert knowledge and multimodal health data; the key method combines semantic technologies, ontologies, and machine learning; the principal finding is a prototype system for COPD patient monitoring using Neo4j and MongoDB.",
Time series data summarization,such as calculating averages and modes for 6-hour intervals,and statistical tests like the Kolmogorov-Smirnov test to detect condition shifts.,,,,,,,,,,,,,
Application of language models and the Medical Concept Annotation Toolkit (MedCAT) for extracting and annotating clinical concepts from free text,"followed by integration into a graph database.""","The research is reproducible. The code and demo for the Personal Health Knowledge Graph (PHKG) for COPD are available at https://github.com/Bluer01/COPH. The MIMIC-III dataset was used for testing, which is a freely accessible critical care database.","The PHKG system for COPD monitoring was built with 3.5 million nodes and 4 million relationships, using Neo4j and MongoDB for data management.",,,,,,,,,,,,
Personalized thresholds triggered alerts of three severity grades,providing actionable insights and recommended responses for abnormal patient measurements.,,,,,,,,,,,,,,
"No explicit quantitative results or statistical significance (p-values) were reported in the context.""",Developed a modular framework for personalized health knowledge graphs (PHKGs) focused on COPD management.,,,,,,,,,,,,,,
Used the MIMIC-III dataset to test risk scoring and alert generation for COPD patients.,,,,,,,,,,,,,,,
Summarized time-series data into 6-hour intervals for temporal analysis.,,,,,,,,,,,,,,,
Applied the Kolmogorov-Smirnov test to detect acute condition shifts.,,,,,,,,,,,,,,,
Used MedCAT for extracting and annotating clinical concepts from free text.,,,,,,,,,,,,,,,
"No explicit statistical results or measured effects reported.""","The system was only tested on the MIMIC-III dataset, which may limit generalizability.",,,,,,,,,,,,,,
Research in the development of personal health knowledge graphs (PHKGs) is still in its infancy.,,,,,,,,,,,,,,,
The prototype represents an initial step and lacks advanced machine learning capabilities.,,,,,,,,,,,,,,,
"Only summary or compressed time-series data are stored in the graph database.""",Summarizing time series data and using semantic technologies enables efficient construction of temporal knowledge graphs for patient monitoring.,,,,,,,,,,,,,,
Combining graph databases (Neo4j) and document storage (MongoDB) supports both efficient querying and scalable data management.,,,,,,,,,,,,,,,
"The framework is effective for COPD monitoring and is adaptable for future machine learning enhancements.""","Integration of multimodal personal health and environmental data into PHKGs remains challenging, especially outside clinical settings.",,,,,,,,,,,,,,
Enhancing machine learning capabilities to fill gaps in PHKGs and improve reasoning and inference.,,,,,,,,,,,,,,,
"Data harmonization and efficient storage solutions for heterogeneous and time-series health data require further research.""",,,,,,,,,,,,,,,
Pattern-based design applied to cultural heritage knowledge graphs,"Carriero Valentina Anita, Gangemi Aldo, Mancinelli Maria Letizia, Nuzzolese Andrea Giovanni, Presutti Valentina, Veninata Chiara",2021,reference-manager,10.3233/sw-200422,,,,,,"Iterative Design Methodology: The study reports results for three versions of ArCo (v0.1, v0.5, v1.0), each resulting from separate iterations of the design methodology.",,"How can a modular, pattern-based ontology network be designed to effectively capture, generalize, and address competency questions and requirements for representing cultural heritage knowledge in the ArCo project?","The paper aims to improve mapping between input data and ontology models for Italian Cultural Heritage. Using the ArCo approach, it emphasizes institutional control over data and ontological commitment. The methodology includes inference, error, and competency tests, showing ArCo's soundness and terminological coverage. The study concludes ArCo effectively supports rich, interoperable cultural data.","The research goal is to enhance cultural heritage knowledge graphs; the approach extends the eXtreme Design methodology with a root-thematic-foundations ontology pattern and formal evaluation; results show homogeneous, cohesive modules and improved data quality through iterative refinement and data cleansing.",
Structural and Comparative Analysis: Metrics are computed and compared across ArCo,CIDOC CRM,and EDM to assess structural evolution and quality indicators.,,,,,,,,,,,,,
Use of OntoMetrics Tool: OntoMetrics,a web-based tool,"is used to compute statistics and metrics about the ontologies.""","The research is reproducible: all test cases (for inference verification, error provocation, and competency question verification) are publicly available on GitHub at https://github.com/ICCD-MiBACT/ArCo/tree/master/ArCo-release/test. The TESTaLOD tool, used for testing, is also described and available as a web application.","ArCo v1.0 shows high terminological coverage (0.72) compared to EDM (0.07) and CIDOC CRM (0.2), indicating greater expressiveness.",,,,,,,,,,,
"Module metrics for ArCo report optimal appropriateness (=1)
Across ArCo versions","high encapsulation (~1)
key metrics (relationship richness","and low atomic size (4.85–6.63)
inheritance richness","supporting cohesive
class/property ratio) remain consistent","pattern-based design.
indicating a homogeneous design strategy; no p-values or statistical significance reported.""",,"ArCo v1.0 (latest release) has 13,792 axioms, 3,416 logical axioms, 340 classes, 616 object properties, 154 datatype properties, 8,734 annotation assertions, and 20,030,941 individuals.",,,,,,,,,
Transparency metric for ArCo v1.0 is 0.44; for CIDOC CRM: 0.74; for EDM: 0.84.,,,,,,,,,,,,,,,
Class/property ratio for ArCo v1.0 is 0.44.,,,,,,,,,,,,,,,
Number of root classes (NoR) for ArCo v1.0: 16 (0.05).,,,,,,,,,,,,,,,
Number of leaf classes (NoL) for ArCo v1.0: 277 (0.81).,,,,,,,,,,,,,,,
"Number of external classes (NoC) for ArCo v1.0: 38 (0.11).""",Lack of clear guidelines for designing the architecture of an ontology network in wide and complex domains.,,,,,,,,,,,,,,
ArCo ontologies reflect ICCD standards but are not exclusively committed to them,limiting generalizability beyond cataloguing practices.,,,,,,,,,,,,,,
"The evolving and heterogeneous community poses challenges for requirement collection and prioritization.""","ArCo modules show optimal appropriateness (=1), excellent encapsulation (∼1), and extremely low coupling (=0), indicating high quality, independence, and flexibility.",,,,,,,,,,,,,,
The pattern-based approach ensures homogeneous design and supports easy updating and reuse of modules.,,,,,,,,,,,,,,,
Direct reuse of ontologies is limited to stable,reference standards to avoid instability.,,,,,,,,,,,,,,
"Data cleansing and iterative refinement improve knowledge graph quality.""","Lack of automatic methods for detecting, annotating, and reusing Ontology Design Patterns (ODPs) in knowledge graphs.",,,,,,,,,,,,,,
Insufficient well-documented,high-quality ODPs and supporting tools for ODP-driven ontology engineering.,,,,,,,,,,,,,,
Challenges in reusing existing ontologies due to poor documentation,large size,"and lack of explicit alignments.""","Future research should focus on extending the ontologies to cover additional aspects like naturalistic heritage characteristics, improving requirement collection from a more diverse audience, extracting structured data using NLP techniques, and addressing challenges in designing ontology network architecture for complex domains.","The study design is a test-driven, pattern-based ontology engineering methodology called XD. It is modular, collaborative, iterative, and inspired by agile (eXtreme Programming). It involves parallel and interactive work between design and testing teams, uses unit testing, and emphasizes reuse of ontology design patterns (ODPs).",,,,,,,,,,,
MULTIBENCH: Multiscale Benchmarks for Multimodal Representation Learning,"Liang Paul Pu, Lyu Yiwei, Fan Xiang, Wu Zetian, Cheng Yun, Wu Jason, Chen Leslie, Wu Peter, Lee Michelle A., Zhu Yuke, Salakhutdinov Ruslan, Morency Louis-Philippe",2021,reference-manager,,,,,,,"Temporal alignment (WORDALIGN): Aligns information from different modalities to the same time granularity, especially for time-series data involving text.",,"How can MULTIBENCH serve as a unified, large-scale benchmark to comprehensively evaluate and accelerate the development of general-purpose multimodal machine learning models across domains, modalities, and tasks, considering generalization, complexity, and robustness to noisy or missing modalities?","The paper introduces MULTIBENCH, a large-scale benchmark designed to evaluate general-purpose multimodal models. Using 15 datasets across 10 modalities and 20 prediction tasks in 6 research areas, it assesses generalization, training/inference complexity, and robustness to noisy/missing data. MULTIBENCH aims to unify and advance multimodal learning research.","The research goal is to accelerate multimodal machine learning by introducing MULTIBENCH, a comprehensive benchmark; the approach involves evaluating diverse models across multiple modalities and tasks; results show simple fusion methods perform competitively, highlighting tradeoffs between performance and complexity.",
Fusion paradigms: Implements various methods for combining unimodal representations into a multimodal representation.,,,,,,,,,,,,,,,
Robustness evaluation: Tests models using modality-specific and multimodal imperfections,"measuring relative and effective robustness.""",MULTIBENCH emphasizes reproducibility and ease of use but does not explicitly mention the availability of source code for the project. No direct link or reference to the project's source code is provided in the context.,"Out-domain models showed improvements up to 8.5% over unimodal baselines on some datasets, with specific gains: 4.7% (MUSTARD), 6.0% (UR-FUNNY), and 8.4% (V\&T EE).",,,,,,,,,,,,
Simple fusion techniques (LF) balance high performance and low complexity,while complex models (MFAS,MULT) perform only slightly better on average.,,,,,,,,,,,,,
"No p-values or statistical significance values are reported in the context.""","MULTIBENCH evaluates 15 datasets across 10 modalities and 6 research areas, testing over 20 prediction tasks.",,,,,,,,,,,,,,
Out-domain methods achieved stronger performance on 9 out of 15 datasets.,,,,,,,,,,,,,,,
Example improvements: MUSTARD (4.7%),UR-FUNNY (6.0%),STOCKS-HEALTH (2.8%),STOCKS-TECH (4.0%),ENRICO (8.5%),,V\&T EE (8.4%).,,,,,,,,,
Metrics: Accuracy (↑ higher is better),AUPRC (↑),MSE (↓ lower is better).,,,,,,,,,,,,,
MULTIZOO toolkit enables standardized,"reproducible multimodal model training and evaluation.""",Tradeoffs between generality and specificity: MULTIBENCH may not leverage domain knowledge as well as task-specific models.,,,,,,,,,,,,,
Limited coverage: Currently excludes areas like question answering,retrieval,grounding,and reinforcement learning.,,,,,,,,,,,,
Limited models and metrics: Expansion is planned but not yet implemented.,,,,,,,,,,,,,,,
"Interpretability for non-image/text modalities remains an open question.""","MULTIBENCH accelerates multimodal ML research by standardizing evaluation and enabling robust, transferable, and multitask experiments.",,,,,,,,,,,,,,
Simple fusion methods (like LF) are competitive,especially for new datasets,but complex models excel on well-studied data.,,,,,,,,,,,,,
Transfer and multitask learning show promise but may trade off performance on smaller datasets.,,,,,,,,,,,,,,,
"Ongoing expansion and addressing limitations are recommended.""",Defining “interpretability” for modalities beyond image and text remains an open question.,,,,,,,,,,,,,,
MULTIBENCH currently lacks coverage of important research areas such as question answering,retrieval,grounding,and reinforcement learning.,,,,,,,,,,,,
Current benchmarks often overlook the tradeoffs between performance,robustness,"and complexity in real-world deployment.""","Suggested future research directions include: defining interpretability for modalities beyond image and text; expanding MULTIBENCH to cover more domains, tasks, and metrics; developing new evaluation metrics (e.g., for fairness, uncertainty); investigating multimodal transfer learning, co-learning, and multitask learning across modalities and datasets.",No information available,,,,,,,,,,,
Benchmarking Scalability and Elasticity of Distributed Database Systems,"Kuhlenkamp Jörn, Klems Markus, Röss Oliver",2014,reference-manager,,,,,,,Scalability benchmarking: Measures performance before and after scaling actions by changing load or system capacity between workload runs.,,"How reproducible and reliable are performance, scalability, and elasticity benchmarking results for distributed database systems like HBase and Cassandra, and what factors influence discrepancies in experimental outcomes when using tools such as YCSB under different infrastructure configurations?","The paper aims to reproduce and verify performance and scalability benchmarks for Cassandra and HBase, focusing on Enterprise Application Performance Management. Using the YCSB benchmarking tool, the study finds Cassandra has lower read latency, while HBase has better write latency. Some original results could not be reproduced, highlighting variability in benchmarking outcomes.","The research goal is to benchmark and reproduce performance and scalability results for Cassandra and HBase using updated setups; the approach involves repeating prior experiments with newer software and hardware configurations; the principal finding is that some results were reproducible, but significant differences in absolute throughput and latency were observed.",
Elasticity benchmarking: Measures performance during workload runs while executing scaling actions,capturing side-effects of scaling.,,,,,,,,,,,,,,
"YCSB benchmarking tool: Used to generate workloads and measure database performance with different read/write mixes and thread counts.""",The research reproduces performance and scalability experiments of HBase and Cassandra using Amazon EC2 infrastructure and the Yahoo Cloud Serving Benchmark (YCSB). No information about the availability of source code for the project is provided.,"Both Cassandra and HBase scale nearly linearly; Cassandra has better read performance, while HBase has better write performance.",,,,,,,,,,,,,
Cassandra throughput increased by 107% (Workload R) and 67% (Workload W) on c1.xlarge EC2 instances compared to m1.large; HBase throughput increased by 194% and 286%,respectively.,,,,,,,,,,,,,,
"Performance differences are significant and depend on EC2 instance type and data volume; no explicit p-values reported.""",Primary outcomes focus on performance and scalability benchmarking of Cassandra and HBase for write-intensive workloads.,,,,,,,,,,,,,,
Results show similar absolute throughput for HBase workload R between experiments E1 and E2.,,,,,,,,,,,,,,,
Read operations in HBase can experience high latency beyond 1000 ms (99th-percentile latency above 1000 ms).,,,,,,,,,,,,,,,
Throughput and latency metrics (average,95th,and 99th percentiles) are reported for different workloads and node counts.,,,,,,,,,,,,,
Most absolute performance measurements in E2 differ significantly from E1,"though general trends are consistent.""","Some results from experiment E1 could not be reproduced, especially most absolute throughput measurements.",,,,,,,,,,,,,
Absolute Cassandra throughput was considerably less,and latencies were higher than reported for E1.,,,,,,,,,,,,,,
HBase experiments required changes in YCSB server and thread counts due to performance issues.,,,,,,,,,,,,,,,
"Differences in Cassandra version used may affect comparability.""","Both Cassandra and HBase scale nearly linearly; Cassandra has better read performance, while HBase has better write performance.",,,,,,,,,,,,,,
Performance differences are influenced by EC2 instance type and data volume.,,,,,,,,,,,,,,,
Optimizing the number of open connections and client servers is crucial for each infrastructure.,,,,,,,,,,,,,,,
Recommendations: Test with different CPU core sizes and monitor disk,network,"and CPU bottlenecks.""","Difficulty in reproducing absolute throughput and latency results from previous experiments, especially for Cassandra, indicating a need for improved experiment reproducibility.",,,,,,,,,,,,
Significant performance differences attributed to running experiments on virtualized infrastructure (Amazon EC2) versus physical infrastructure,suggesting further study on infrastructure impact.,,,,,,,,,,,,,,
Trade-off between speed of scaling and performance variability during elasticity evaluation,"highlighting the need for research on optimizing both objectives.""",,"The study design includes performance and scalability benchmarking experiments of Cassandra and HBase using the YCSB tool. It involves scalability benchmarking (SB1, SB2, SB3) and elasticity benchmarking (EB1, EB2), with experiments conducted on both physical infrastructure and Amazon EC2. The design is comparative and experimental.",,,,,,,,,,,,
Secure Federated Learning With Fully Homomorphic Encryption for IoT Communications,"Hijazi Neveen Mohammad, Aloqaily Moayad, Guizani Mohsen, Ouni Bassem, Karray Fakhri",2024,reference-manager,10.1109/jiot.2023.3302065,,,,,,"Federated Learning (FL): Each user trains a local model on their own data, keeping data private, and only shares model updates.",,"How can federated learning (FL) combined with fully homomorphic encryption (FHE) be used to address security and privacy issues in IoT-enabled smart cities, and how effective are the proposed approaches in detecting intrusions and injection attacks in the Internet of Medical Things?","The paper aims to address security and privacy in IoT systems by proposing four scenarios using federated learning (FL) and encryption techniques. Experiments on benchmark IoT data sets show high accuracy (above 0.997% for most approaches), with MECM performing best. The approaches enhance IoT security and privacy.","The research goal is to address privacy and security in IoT-enabled smart cities using Federated Learning (FL) and Fully Homomorphic Encryption (FHE); the approach involves clustering users, local model training, encryption, and secure aggregation; results show effective security and efficiency in four evaluated scenarios.",
Encryption Techniques (including Fully Homomorphic Encryption - FHE): Model weights are encrypted locally using public key cryptography before being sent to the server.,,,,,,,,,,,,,,,
"Aggregation via FedAvg: The server aggregates encrypted model weights from users to update the global model without accessing raw data.""
OECM","The research is reproducible. The pseudocode for the proposed approaches is provided (Algorithms 1–4), and the source code is available on GitHub \[29]. Experimental settings and datasets are described in detail, supporting reproducibility.
MUCM","All four proposed approaches (OUCM, OECM, MUCM, MECM) performed well, with MECM achieving the highest accuracy and F1-score in detecting general intrusions and injection attacks.
and MECM achieved accuracy",recall,precision,,and F1-score all above 0.997%; OUCM achieved 0.911% accuracy.,,,,,,,,,
"The proposed approaches outperformed most previous methods and
OECM","by using FL (federated learning) and encryption techniques
MUCM","enhanced privacy and security in IoT systems. No p-values reported.""
and MECM achieved accuracy","All proposed approaches performed well in accuracy and F1-score; MECM achieved the highest scores for both.
recall",precision,,and F1-score all above 0.997%.,,,,,,,,,
OUCM achieved an accuracy of 0.911%.,,,,,,,,,,,,,,,
"Proposed approaches outperformed most previous methods in all four measures.""",,"The proposed approaches significantly improve security, privacy, communication overhead, and latency compared to the baseline in IoT-enabled smart cities.",,,,,,,,,,,,,
MECM consistently achieves the highest security scores across all IoT devices.,,,,,,,,,,,,,,,
Comprehensive evaluation using multiple criteria is recommended when selecting an approach.,,,,,,,,,,,,,,,
The approaches maintain high accuracy,precision,recall,"and F1-score.""",Need for further investigation into each use case’s specific security and privacy needs.,,,,,,,,,,,
Exploration of more diverse IoT environments and attack types beyond those studied.,,,,,,,,,,,,,,,
Assessment of the scalability and efficiency of the proposed approaches in larger,"real-world deployments.""",,Case study; experimental; uses real-world data set (WUSTL-EHMS-2020); evaluates four scenarios; not randomized; not double-blind; not placebo-controlled; not controlled; not multi-site; not retrospective; not stratified; not crossover; not parallel; not observational; not meta-analysis; not systematic review.,,,,,,,,,,,,
Temporal Knowledge Graph Reasoning with Historical Contrastive Learning,"Xu Yi, Ou Junjie, Xu Hui, Fu Luoyi",2022,reference-manager,,,,,,,Historical contrastive learning: Involves two stages—learning contrastive representations and training a binary classifier to distinguish historical from non-historical dependencies.,,"How can the proposed CENET (Contrastive Event Network) model improve event forecasting in temporal knowledge graphs by leveraging contrastive learning to effectively predict both repetitive and new events, considering dependencies on historical and non-historical entities?","The paper proposes CENET, a novel model for forecasting future events in temporal knowledge graphs (TKGs) using contrastive learning. By modeling both historical and non-historical dependencies and employing a copy mechanism and supervised contrastive learning, CENET achieves significant improvements in event prediction accuracy across multiple benchmark datasets.","The research goal is to improve future event forecasting on temporal knowledge graphs; the approach introduces CENET, a model using historical contrastive learning to distinguish historical and non-historical dependencies; results show CENET significantly outperforms previous methods, achieving at least 8.3% relative improvement in Hits@1 on event-based datasets.",
Temporal knowledge graph (TKG) extrapolation: Predicts missing entities in future events using time-stamped quadruples (s,p,o,t).,,,,,,,,,,,,
"Ablation study: Evaluates the impact of different model components and masking strategies on performance.""",,"CENET outperforms all baselines on public KGs (WIKI and YAGO), achieving improvements up to 23.68% (MRR), 25.77% (Hits@1), and 7.08% (Hits@3) over state-of-the-art methods.",,,,,,,,,,,,,
Experimental results show CENET achieves MRR of 68.39 and Hits@1 of 68.33 on WIKI,and MRR of 84.13 and Hits@1 of 84.03 on YAGO.,,,,,,,,,,,,,,
The improvements are statistically significant,"attributed to CENET's ability to handle imbalanced recurrence rates by learning both historical and non-historical dependencies.""","CENET outperforms baselines on WIKI and YAGO, with improvements up to 23.68% (MRR), 25.77% (Hits@1), and 7.08% (Hits@3) over SOTA.",,,,,,,,,,,,,
On WIKI: CENET achieves MRR 68.39,Hits@1 68.33,Hits@3 68.36.,,,,,,,,,,,,,
On YAGO: CENET achieves MRR 84.13,Hits@1 84.03,Hits@3 84.23.,,,,,,,,,,,,,
CENET achieves up to 8.25%,8.48%,and 20.80% improvements of Hits@1 on ICEWS18,ICEWS14,and GDELT respectively.,,,,,,,,,,,
"CENET’s concurrent learning of historical and non-historical dependency addresses imbalanced recurrence rates in datasets.""",No explicit limitations or shortcomings are stated in the provided context.,,,,,,,,,,,,,,
"No self-reported problems or suggestions for further research are mentioned.
,CENET significantly outperforms existing methods on most metrics",especially Hits@1,across both event-based and public temporal knowledge graphs.,,,,,,,,,,,,,
The model effectively learns from both historical and non-historical dependencies,improving prediction of new events.,,,,,,,,,,,,,,
"Contrastive learning is key to CENET’s performance; future work should explore better contrastive pairs.""",Need to explore the ability of contrastive learning in knowledge graphs further.,,,,,,,,,,,,,,
Finding more reasonable contrastive pairs for contrastive learning.,,,,,,,,,,,,,,,
"Addressing the challenge of imbalanced recurrence rates in datasets for better event forecasting.""",,"Historical contrastive learning (CENET) uses a two-stage design: (1) learning contrastive representations with a supervised contrastive loss, and (2) training a binary classifier using binary cross-entropy loss with sigmoid activation. The study is experimental, compares multiple baseline models, and uses five benchmark datasets.",,,,,,,,,,,,,
"Patient-centric knowledge graphs: a survey of current methods, challenges, and applications","Al Khatib Hassan S., Neupane Subash, Kumar Manchukonda Harish, Golilarz Noorbakhsh Amiri, Mittal Sudip, Amirlatifi Amin, Rahimi Shahram",2024,reference-manager,10.3389/frai.2024.1388479,,,,,,"Quantitative assessment: Uses statistical and computational techniques to measure metrics like accuracy, recall, and precision for evaluating graph effectiveness.",,"How can patient-centric knowledge graphs (PCKGs) be designed, constructed, and applied to integrate heterogeneous healthcare data for improved clinical decision-making and personalized patient care, while addressing challenges related to data integration, robustness, and scalability?","This paper surveys Patient-Centric Knowledge Graphs (PCKGs) in healthcare, aiming to improve personalized patient care by integrating diverse data. It reviews methodologies, applications, and challenges, highlighting advanced techniques like reasoning and semantic search. The study concludes PCKGs can transform healthcare through more accurate, efficient, and predictive patient-centered solutions.","The paper's main objective is to advance personalized patient care by integrating diverse healthcare data using patient-centric knowledge graphs (PCKGs); it surveys methodologies and real-world applications, concluding that PCKGs enhance clinical decision-making but face challenges in data integration and semantic inference.",
Axioms and rules utilization: Applies fundamental principles and logical statements to structure,interpret,and infer new knowledge from patient data.,,,,,,,,,,,,,
Qualitative assessment: Employs usability studies,content analysis,"and expert panel reviews to evaluate alignment with clinical best practices and user experience.""",,The review highlights that PCKGs (Patient-Centric Knowledge Graphs) improve personalized patient care by integrating diverse healthcare data and using advanced processing techniques like reasoning and semantic search.,,,,,,,,,,,
Quantitative assessment of PCKGs uses metrics such as accuracy,recall,and precision to numerically evaluate performance; completeness and consistency are emphasized,but no specific p-values are reported.,,,,,,,,,,,,
PCKGs show promise in disease prediction,personalized treatment recommendations,and clinical trials,with challenges in scalability,data transparency,,"and maintaining high accuracy as graphs expand.""",,,,,,,,,Introduction of the SMR framework improved precision in clinical trial patient selection.
Evolving graph frameworks enabled automatic diagnosis and efficient medical case searching.,,,,,,,,,,,,,,,
PCKGs enhanced accuracy,efficiency,and outcomes in clinical trial patient selection.,,,,,,,,,,,,,
Usability studies and feedback loops improved PCKG alignment with user needs.,,,,,,,,,,,,,,,
"No explicit statistical values reported.""","Difficulties in knowledge acquisition, completion, and temporal KG development (Ji et al., 2020).",,,,,,,,,,,,,,
Sample size limitations and unmeasured confounders affecting robustness (Chen et al.,2019).,,,,,,,,,,,,,,
Construction of PCKGs is time-consuming and heavily reliant on source data quality (Gyrard et al.,2018; Cong et al.,2018).,,,,,,,,,,,,,
Traditional KG embedding methods struggle with structural sparsity and neglect rich auxiliary texts (Hu et al.,2021).,,,,,,,,,,,,,,
Probability of certain paths in the KG may not be accurate; lack of details on classification reasoning (Xiang et al.,2019).,,,,,,,,,,,,,,
"Data quality and standardization remain significant challenges.""","PCKGs (Personalized Clinical Knowledge Graphs) enhance personalized patient care by integrating diverse healthcare data, improving treatment accuracy and efficiency.",,,,,,,,,,,,,,
Innovative processing techniques (reasoning,semantic search,inference) make PCKGs more actionable for clinical decision-making.,,,,,,,,,,,,,
Key challenges include data fragmentation,integration complexity,and scalability.,,,,,,,,,,,,,
Future work should focus on improving data integration,scalability,"and patient-centric approaches in clinical trials.""","Advancing technological and methodological innovations, such as machine learning and semantic web technologies, to enhance PCKGs’ predictive capabilities.",,,,,,,,,,,,
Addressing regulatory and ethical considerations as PCKGs become more integrated into healthcare delivery.,,,,,,,,,,,,,,,
Overcoming challenges in knowledge acquisition,completion,"and temporal KG development to maintain up-to-date and comprehensive patient profiles.""","Future research should focus on advanced analytics, machine learning, and semantic web technologies to improve PCKGs’ predictive capabilities. Key areas include data quality, standardization, regulatory and ethical considerations, methodological innovations, scalability, and integrating personalized medicine through linking genomic and clinical data. Cross-disciplinary collaboration is essential.",,,,,,,,,,,,
The effects of university students’ fragmented reading on cognitive development in the new media age: evidence from Chinese higher education,"Liu Wei, Huang Heng, Saleem Atif, Zhao Zhongping",2022,reference-manager,10.7717/peerj.13861,,,,,,Structural equation modeling (SEM) was used to analyze survey data and examine relationships between variables.,,"How do the three dimensions of fragmented reading—content fragmentation, temporal fragmentation, and attentional fragmentation—affect the cognitive breadth and cognitive depth of Chinese university students?","The study aimed to examine how fragmented reading (content, time, attention) affects Chinese university students’ cognitive development. Using questionnaires and structural equation modeling (SEM) on 916 students, results showed fragmented reading increases cognitive breadth but decreases cognitive depth. The study suggests balancing fragmented and in-depth reading for optimal learning.","The research goal was to examine how fragmented reading affects Chinese university students' cognitive development using SEM; results showed fragmented reading increases cognitive breadth but decreases cognitive depth, especially with higher temporal fragmentation.",
The cognitive development questionnaire (CDQ) and Fragmented Reading Questionnaire (FRQ) were used as measurement instruments.,,,,,,,,,,,,,,,
"Confirmatory factor analysis (CFA) and Harman’s single factor test were conducted to assess validity and internal consistency.""",Raw data is available in the Supplemental Files online at http://dx.doi.org/10.7717/peerj.13861#supplemental-information. There is no mention of source code for the project.,"Fragmented reading (content, time, attention) was positively associated with cognitive breadth: content (β = 0.43, p < 0.001), time (β = 0.47, p < 0.01), attention (β = 0.39, p < 0.001); temporal fragmentation had the strongest effect.",,,,,,,,,,,,,
All three fragmentation types were negatively associated with cognitive depth: content (β = −0.56,p < 0.01),time (β = −0.35,p < 0.01),attention (β = −0.50,,p < 0.001).,,,,,,,,,
The structural equation model showed good fit (X2/df = 2.51,CFI = 0.92,NFI = 0.91,TLI = 0.94,"RMSEA = 0.07).""",,Primary outcomes:,,,,,,,,,
Content fragmentation,temporal fragmentation,and attentional fragmentation each had a positive effect on cognitive breadth.,,,,,,,,,,,,,
Content fragmentation,temporal fragmentation,and attentional fragmentation each had a negative effect on cognitive depth.,,,,,,,,,,,,,
Results and measured effects:,,,,,,,,,,,,,,,
Reliability and validity were confirmed: Cronbach’s Alpha ranged from 0.70 to 0.79.,,,,,,,,,,,,,,,
Factor loading (FL): 0.62–0.83; Average variance extracted (AVE): 0.62–0.68; Composite reliability (CR): all >0.7.,,,,,,,,,,,,,,,
"Harman’s single factor test: first common factor explained 30% variance (below 40% threshold).""",Limited generalizability due to a sample of only 916 Chinese participants from six mainland universities.,,,,,,,,,,,,,,
Study focused only on five variables; did not include factors like reading motivation,engagement,personality traits,or socioeconomic status.,,,,,,,,,,,,
Results may not apply to Western or more diverse populations.,,,,,,,,,,,,,,,
"Further research needed to explore nested relationships and cultural/individual differences.""",Fragmented reading increases cognitive breadth but reduces cognitive depth in Chinese university students.,,,,,,,,,,,,,,
Educators and institutions should help students integrate fragmented information for deeper understanding.,,,,,,,,,,,,,,,
Fragmented reading is unavoidable; strategies are needed to balance its dual effects.,,,,,,,,,,,,,,,
Future research should include larger,"more diverse samples and additional variables.""",Limited generalizability due to the sample being only 916 Chinese participants from six universities; future research should use larger and more diverse samples.,,,,,,,,,,,,,
Need to examine additional variables such as reading motivation,reading engagement,personality traits,and socioeconomic status.,,,,,,,,,,,,
"Explore the effects of individual and cultural differences on fragmentation and cognitive development.""","Future research should use larger and more diverse samples, including Western populations, to improve generalizability. Additional variables like reading motivation, engagement, personality traits, and socioeconomic status should be examined. Investigating nested relationships and the Matthew effect in fragmented reading is also recommended.",,,,,,,,,,,,,,
Identifying and Consolidating Knowledge Engineering Requirements,"Allen Bradley Paul, Ilievski Filip, Joshi Saurav",2023,reference-manager,,,,,,,Surveys were used to evaluate and prioritize requirements from stakeholders.,,What are the consolidated requirements and quality attributes necessary for developing a comprehensive reference architecture for knowledge engineering that addresses the needs of diverse stakeholders and knowledge engineering eras?,"The paper aims to identify and consolidate knowledge engineering (KE) requirements across stakeholders and eras, analyzing gaps between current KE and reference architecture (RA) practices. Using requirement analysis and architecture evaluation, it finds no existing architecture fully meets all needs. The study proposes iterative, community-driven RA development for comprehensive KE support.","The paper's research goal is to identify and consolidate knowledge engineering (KE) requirements across stakeholders and eras; its approach profiles stakeholder needs, derives quality attributes and functional requirements, and evaluates existing architectures; the principal finding is that no current architecture fully satisfies all requirements, especially socio-technical ones.",
Instantiation and use of software architectures were assessed,including mapping requirements to quality attributes and functional requirements.,,,,,,,,,,,,,,
"Comparative evaluation of three candidate knowledge engineering architectures against 23 quality attributes and 8 functional requirements.""","BioCypher is open-source software, openly developed and available under an MIT license. It uses modern software engineering practices (continuous integration, deployment, version control, automated testing). The source code is openly accessible, supporting reproducibility and transparency. No specific source code repository link is provided in the context.","None of the three knowledge engineering architectures fully satisfy all 23 quality attributes; KGTK comes closest, but all lack support for editability, curatability, and affordability.",,,,,,,,,,,,,
BioCypher excels in maintainability,ethicality,and explainability but is not affordable or editable by humans.,,,,,,,,,,,,,
"No statistical significance (p-values) or quantitative results are reported.""",23 quality attributes (QAs) were used to formally assess and compare three knowledge engineering (KE) architectures.,,,,,,,,,,,,,,
None of the architectures satisfied all QAs; each aligned with its focus (e.g.,KGTK on usability,BioCypher on maintainability).,,,,,,,,,,,,,
All architectures satisfied reliability (Q01),robustness to noise (Q10),efficiency (Q02),scalability (Q04),distributivity (Q05),,and interoperability (Q06).,,,,,,,,,
"BioCypher cannot function without a schema
Editability (Q22)","limiting its applicability.
curatability (Q18)",and affordability (Q21) were not satisfied by any architecture.,,,,,,,,,,,,,
BioCypher enforces ethicality (Q19) by requiring source,license,and version parameters,ensuring legal and ethical data use.,,,,,,,,,,,,
BioCypher supports sustainability (Q20) through ontology mapping and scalable project databases,making the process economically sustainable for the use case.,,,,,,,,,,,,,,
BioCypher does not support human editing of produced knowledge (editability).,,,,,,,,,,,,,,,
KGTK’s process is explainable,storing intermediate outputs for reproducibility and inspection.,,,,,,,,,,,,,,
"All architectures are expected to be applicable to large knowledge graphs due to strong scalability.""",Manual editing of knowledge is not feasible; editability is lacking.,,,,,,,,,,,,,,
"Explainability is insufficient due to lack of stakeholder involvement
Editability","clear documentation
curatability","and use of non-interpretable algorithms.
and affordability are missed by all architectures.",,,,,,,,,,,,,
BioCypher strictly requires a schema,"limiting applicability without high-quality schemas.""",The 23 quality attributes (QAs) are effective for assessing and comparing knowledge engineering (KE) architectures; future KE development should consider all QAs.,,,,,,,,,,,,,
No evaluated architecture satisfies all QAs; socio-technical requirements like editability,curatability,and affordability are missing in all.,,,,,,,,,,,,,
Future KE architectures should address these gaps and be evaluated against the 23 QAs and 8 functional requirements (FRs).,,,,,,,,,,,,,,,
The study recommends a human-centric,"iterative approach involving stakeholder input to develop a comprehensive reference architecture (RA) for KE.""","Current KE architectures lack support for key socio-technical requirements, such as editability, curatability, and affordability.",,,,,,,,,,,,,
Future KE architectures should be evaluated against the 23 quality attributes (QAs) and 8 functional requirements (FRs) identified in this paper.,,,,,,,,,,,,,,,
There is a need for a human-centric,"iterative methodology involving stakeholder prioritization to guide comprehensive RA development for KE.""","Future research should focus on developing a comprehensive reference architecture (RA) for knowledge engineering (KE) using standard methodologies, prioritizing stakeholder needs, evaluating more KE and data engineering architectures, and addressing socio-technical requirements such as curatability and affordability that current architectures lack. Community collaboration is encouraged.",,,,,,,,,,,,,
GUARANTEED IMPROVEMENT OF THE PRIVACY-UTILITY TRADEOFF IN FEDERATED LEARNING,"Ye Jiayuan, Kang Anmin, Shen Zebang, Hassani Hamed, Shokri Reza",2023,reference-manager,,,,,,,"Private Power Method (PPM): An algorithm using random initialization, local computations, noise injection for privacy, and QR decomposition to ensure differential privacy in spectral analysis.",,"What are the theoretical guarantees and conditions under which CENTAUR achieves an optimal utility-privacy tradeoff in federated learning, and how do the problem parameters and assumptions affect the required number of clients and data points to ensure both utility and differential privacy?","The paper investigates the utility-privacy tradeoff in federated learning (FL) under differential privacy (DP). It proposes CENTAUR, which separates representation learning and classifier heads, training a global DP representation while personalizing classifier heads. Theoretical results show improved utility-privacy tradeoff, especially with large client numbers. Assumptions align with prior work.","The research goal is to improve utility in federated learning (FL) under differential privacy (DP); the approach, CENTAUR, separates representation learning and classifier head optimization; results show CENTAUR achieves better utility-privacy trade-off than prior DP FL methods by training a global DP representation and personalized classifier heads.",
Cross-Validation Scheme: Used to boost the success probability of PPM by running multiple trials and selecting the best output based on utility.,,,,,,,,,,,,,,,
Matrix Sensing Formulation: Reformulates the problem as a matrix sensing task,"enabling analysis via inexact gradient descent methods.""",,"CENTAUR achieves higher testing accuracy than prior methods under a tight differential privacy (DP) budget (ϵdp = 1, δ = 10−5), e.g., 77.80% (±0.52) on CIFAR10 (n=1000, S=2) versus 74.63% (±0.76) for PPSGD and 63.97% (±0.98) for DP-FedAvg-fb.",,,,,,,,,,,,
CENTAUR demonstrates a better utility-privacy trade-off,outperforming both prior federated learning (FL) methods and local stand-alone training across various privacy budgets (ϵ),as shown in Figure 1 and Figure 3.,,,,,,,,,,,,,
"Theoretical analysis (Corollary 5.1) shows that
Results for CIFAR10","for sufficiently large client numbers n
n=1000","CENTAUR can guarantee target accuracy ϵa within a DP budget ϵdp; statistical significance is not explicitly reported.""
S=2","Primary outcome: Testing accuracy (%) on CIFAR10 under various data allocation settings and privacy budgets.
ϵdp=1:",,,,,,,,,,,,
Stand-alone-no-FL: 74.06 (0.45),,,,,,,,,,,,,,,
DP-FedAvg-fb: 63.97 (0.98),,,,,,,,,,,,,,,
"PMTL-ft: 67.71 (0.78)
PPSGD: 74.63 (0.76)",,,,,,,,,,,,,,,
"CENTAUR: 77.80 (0.52)
Results for CIFAR10",n=1000,S=5,ϵdp=1:,,,,,,,,,,,,
Stand-alone-no-FL: 44.60 (1.30),,,,,,,,,,,,,,,
DP-FedAvg-fb: 41.12 (0.40),,,,,,,,,,,,,,,
"PMTL-ft: 45.75 (0.81)
PPSGD: 48.29 (1.79)",,,,,,,,,,,,,,,
CENTAUR: 51.05 (0.35),,,,,,,,,,,,,,,
CENTAUR consistently achieves the highest testing accuracy across all settings.,,,,,,,,,,,,,,,
CENTAUR demonstrates a better privacy-utility trade-off than all baselines,outperforming even local stand-alone training at ϵ = 0.5.,,,,,,,,,,,,,,
Data augmentation improves testing accuracy only for CENTAUR during local head fine-tuning; it worsens or does not improve results for other methods.,,,,,,,,,,,,,,,
"Statistical values are reported as mean (standard deviation) across runs.""","No theoretical guarantee that CENTAUR achieves target accuracy ϵa within a small DP budget ϵdp; only an upper bound is established, not a lower bound.",,,,,,,,,,,,,,
Assumptions 5.1–5.3 are required and may limit generalizability; they match prior works but still impose constraints.,,,,,,,,,,,,,,,
"Utility drop is inevitable in DP FL due to gradient clipping and sensitivity bounding.""","The main bottleneck in differentially private federated learning (DP FL) is the conflict between learning the representation function and classification head under gradient clipping, causing utility loss.",,,,,,,,,,,,,,
CENTAUR addresses this by training a global DP representation extractor and personalized classifier heads,improving utility over prior full-model DP approaches.,,,,,,,,,,,,,,
Theoretical analysis shows that with sufficient clients (n),CENTAUR achieves target utility within a given privacy budget,and better dependence on data dimension (d) reduces required clients.,,,,,,,,,,,,,
"Recommendations: Decompose models into shared representations and personalized heads for DP FL to improve utility and scalability.""","Theoretical guarantees for achieving target accuracy within a strict differential privacy (DP) budget are not fully established; only upper bounds are provided, not lower bounds.",,,,,,,,,,,,,,
The dependence on problem dimension d and number of clients n suggests that improving the dependence on d could reduce required client numbers for the same utility-privacy guarantee.,,,,,,,,,,,,,,,
"Existing approaches mainly address linear embedding or non-DP settings; extending results to more general or non-linear settings remains open.""",,,,,,,,,,,,,,,
Research Trends for the Interplay between Large Language Models and Knowledge Graphs,"Khorashadizadeh H., Amara Fatima Zahra, Ezzabady M., Ieng Frédéric, Tiwari Sanju, Mihindukulasooriya Nandana, Groppe Jinghua, Sahri S., Benamara Farah, Groppe Sven",2024,reference-manager,,,,,,,Database-First (DB-first) approach: Combines Large Language Models (LLMs) with traditional Database Management Systems (DBMSs) to create a hybrid query execution environment.,,"What are the emerging trends, challenges, and solutions in the interplay between Large Language Models (LLMs) and Knowledge Graphs (KGs), particularly regarding their integration for tasks such as KG question answering, multi-hop question generation, KG validation, and natural language query generation?","This paper surveys the integration of Large Language Models (LLMs) and Knowledge Graphs (KGs), focusing on how LLMs enhance KGs in tasks like text generation, ontology creation, inconsistency detection, and query generation. It also explores how KGs improve LLMs, and highlights new directions in KG question answering.","The paper's main objective is to survey the integration of LLMs and KGs, using a categorization-based approach, and its principal finding is that LLMs enhance KGs in tasks like question answering and query generation, with Freebase, Bert, and GPT-3 being most frequently used.",
Detailed comparison: Examines and compares conversational AI models (like ChatGPT) with traditional Question-Answering Systems (QASs) for Knowledge Graphs (KGs).,,,,,,,,,,,,,,,
"Fine-tuning LLMs: Uses fine-tuned LLMs for pre-annotating data in ontology property identification to assist annotators and reduce annotation times.""",,"The survey highlights that integrating LLMs with KGs enhances tasks like generating descriptive text, ontology creation, inconsistency detection, fact-checking, KG completion, and embedding.",,,,,,,,,,,,,
Freebase is the most used KG; Bert and GPT-3 are the most frequently employed LLMs in reviewed literature.,,,,,,,,,,,,,,,
"No explicit quantitative results or p-values are reported in the context.""",Freebase is the most commonly used Knowledge Graph (KG); Bert and GPT-3 are the most frequently used Large Language Models (LLMs).,,,,,,,,,,,,,,
LLMs enhance KGs in generating descriptive text,ontology generation,inconsistency detection,fact-checking,KG completion,,and KG embedding.,,,,,,,,,
KGs can improve LLMs; cooperation enables advanced question answering,including query generation from text and multi-hop question answering.,,,,,,,,,,,,,,
Fine-tuned LLMs reduce annotation times in ontology property identification.,,,,,,,,,,,,,,,
"No explicit statistical values or measured effects are provided.""","Current rule-based techniques often overlook the semantics of relations, focusing mainly on structural information.",,,,,,,,,,,,,,
LLMs can be manipulated to generate misinformation at scale.,,,,,,,,,,,,,,,
LLMs may have outdated or insufficient knowledge for detecting factual errors.,,,,,,,,,,,,,,,
SQL-based approaches struggle with unstructured text.,,,,,,,,,,,,,,,
"Further research is needed for reliable knowledge incorporation and parameter reduction in LLMs.""","The integration of LLMs and KGs enhances tasks like KG completion, fact-checking, ontology generation, and question answering.",,,,,,,,,,,,,,
A clear separation between knowledge (in KGs) and language understanding (in LLMs) is recommended to reduce complexity and energy use.,,,,,,,,,,,,,,,
New research directions include efficient training data selection,smaller LLMs,and advanced architectures for AGI.,,,,,,,,,,,,,
"Merging LLM and KG strengths can enable innovative applications like personal KG-enhanced LLMs and improved chatbots.""",Developing reliable techniques to incorporate knowledge from Knowledge Graphs (KGs) into Large Language Models (LLMs) without increasing model size or losing reasoning capabilities.,,,,,,,,,,,,,,
Reducing LLM parameter count by limiting training data to core programming/query language fragments and using automated code/query transformations.,,,,,,,,,,,,,,,
Advancing under-explored areas such as multi-hop question answering,query generation from text,"and KG-powered chatbots.""","Future research should explore integrating LLMs with traditional database systems for hybrid query execution, improving query generation from text, reducing LLM complexity and energy use, separating knowledge from language understanding, developing Personal KG-enhanced LLMs, and designing complex architectures toward artificial general intelligence (AGI).","The study design is a comprehensive survey. It systematically categorizes and reviews prior surveys and research, addresses new research questions, and analyzes tasks not previously covered. It includes statistical analysis, comparative evaluation, and highlights supervised fine-tuning and zero-shot learning approaches. No mention of randomization, control, blinding, or experimental design.",,,,,,,,,,,
A framework towards digital twins for type 2 diabetes,"Zhang Yue, Qin Guangrong, Aguilar Boris, Rappaport Noa, Yurkovich James T., Pflieger Lance, Huang Sui, Hood Leroy, Shmulevich Ilya",2024,reference-manager,10.3389/fdgth.2024.1336050,,,,,,"Data filtering and cleaning: Included subjects with complete demographic, clinical, proteomic, and metabolomic data for at least two time points; excluded features and subjects with high missingness.",,"How can the integration of multiomic data, clinical features, and knowledge graph-based modeling improve the prediction and interpretation of clinical outcomes in type 2 diabetes over 6 months to 1 year?",,"The research goal was to develop a digital twin framework for type 2 diabetes using machine learning, multiomic data, knowledge graphs, and mechanistic models; the approach integrated these methods to predict disease progression, and the principal finding was the identification of both known and novel disease components for precision medicine.",
Machine learning: Trained models to forecast disease progression and identify relevant clinical measurements.,,,,,,,,,,,,,,,
"Knowledge graph analysis: Used the SPOKE knowledge graph to map relationships between biomedical entities and inform predictive modeling.""
At 6 months","The research is reproducible with some restrictions. The Arivale dataset is available upon request. The SPOKE knowledge graph is accessible at https://spoke.ucsf.edu/. Analysis code is available at https://github.com/IlyaLab/t2d-dt-modeling and additional knowledge graph code at https://github.com/yjzhang/kg\_feature\_engineering.
mean changes included d\_HbA1c: −0.05","Predictive models using clinical, proteomic, and metabolomic data enabled significant prediction of short-term changes in clinical variables despite data noise.
d\_Glucose: −0.48",d\_GFR: 1.04,d\_Insulin: −0.89,,d\_HOMA-IR: −0.24 (N=1,,,,,,,,,131).
Knowledge graph analyses identified molecular pathways linking predictive proteins to T2D,"suggesting potential treatment paths; no explicit p-values reported.""","Primary outcomes measured: Changes (deltas) in HbA1c, glucose, insulin, HOMA-IR, and eGFR at 6 months and 1 year.",,,,,,,,,,,,,
Results at 6 months (mean change): HbA1c −0.05,glucose −0.48,eGFR 1.04,insulin −0.89,HOMA-IR −0.24.,,,,,,,,,,,
Results at 1 year (mean change): HbA1c −0.11,glucose −0.18,eGFR −0.05,insulin −0.98.,,,,,,,,,,,,
"Machine learning models (regression/classification) predicted these changes using multiomic data; performance assessed with R² (regression) and F1 (classification) metrics.""",Sparsity of longitudinal data; lack of densely sampled time courses limits high-resolution disease dynamics analysis.,,,,,,,,,,,,,,
Limited sample size complicates predictive modeling,especially for high-dimensional multiomic data.,,,,,,,,,,,,,,
Predominantly healthy participants; unable to model T2D progression specifically.,,,,,,,,,,,,,,,
Need for more T2D-specific data,"though this may introduce confounders.""",The study successfully used multiomic and clinical data to predict changes in T2D-related clinical variables over time.,,,,,,,,,,,,,
Knowledge graph analyses identified relationships among predictive features and suggested new hypotheses for T2D and CKD progression.,,,,,,,,,,,,,,,
Data filtering ensured high-quality,longitudinal datasets for robust modeling.,,,,,,,,,,,,,,
Recommendations include further investigation of significant,yet unmeasured,"related features.""",Limited application of digital twins in biomedical research due to the complexity of biological systems.,,,,,,,,,,,,
Need for improved dashboards and user interfaces,including natural language interfaces,to increase adoption.,,,,,,,,,,,,,
"Further integration of mechanistic information into models to better estimate biological parameters.""","Future research should focus on collecting densely sampled longitudinal phenomic data, increasing sample size, including more participants with T2D, integrating mechanistic models with multiomic data, and developing user interfaces (e.g., dashboards, natural language tools) to broaden adoption and improve disease modeling and prediction.","The study is a longitudinal, observational study using the Arivale dataset. It analyzes deeply phenotyped individuals over multiple time points (∼6 months, ∼1 year) without randomization, blinding, or control groups. Data were filtered for completeness and analyzed retrospectively. No mention of placebo, crossover, or multi-site design.",,,,,,,,,,,,,
"Data Management Opportunities in Unifying Large Language Models+Knowledge Graphs
Data Cleaning","Khan Arijit, Wu Tianxing, Chen Xi
Integration","2024
and Augmentation: Methods for error detection","reference-manager
repairing",entity extraction,,entity resolution,,,,"Data and Input Modeling: Techniques for serializing graph structures for LLM input, integrating multi-modal data, extracting relevant subgraphs, and designing prompts with graph data.",,"How can the unification of Large Language Models (LLMs) and Knowledge Graphs (KGs) advance data management by improving data modeling, integration, augmentation, validation, explainability, and addressing interoperability and reasoning challenges across multi-modal and domain-specific tasks?","The papers and panel discuss integrating Large Language Models (LLMs) and Knowledge Graphs (KGs) to improve data management, accuracy, and explainability. Methodologies include neural-symbolic systems, few-shot learning, graph machine learning, and ontology-guided approaches. Key findings highlight challenges in scalability, bias, privacy, and multi-modal data, with implications for more efficient, fair, and explainable AI systems.","The workshop's main objective is to explore the synergy between large language models and knowledge graphs, using approaches like neural-symbolic collaboration and multi-modality, with the principal finding that integrating knowledge graphs can enhance LLM performance, accuracy, and application in data management.
and synthetic data generation using LLMs.",linking
Vector Data Management: Approaches for high-dimensional data indexing,join,geometric querying,"and retrieval-augmented LLMs using vector representations.""",,,,,,,,,,,High monetary costs and environmental impacts limit access for smaller organizations and researchers.,
LLMs retain and amplify biases from training data,especially against long-tail entities.,,,,,,,,,,,,,,
Challenges in explainability and associating content with provenance information.,,,,,,,,,,,,,,,
Security and privacy risks,including potential exposure of confidential information.,,,,,,,,,,,,,,
Complex graph structures require specialized optimization in databases.,,,,,,,,,,,,,,,
"Open problems in data and input modeling
Data cleaning","especially integrating multi-modal data.
integration",and augmentation remain fundamental challenges.,,,,,,,,,,,,,
"Querying and managing high-dimensional vector data is difficult.
Issues with accuracy",consistency,hallucinations,and harmful content generation.,,,,,,,,,,,,
Efficiency and scalability are constrained by high hardware requirements.,,,,,,,,,,,,,,,
Data and AI model market challenges,including data/model fitness and pricing.,,,,,,,,,,,,,,
"Lack of ground truth datasets and benchmarks in emerging domains.""","Knowledge Graphs (KGs) and Large Language Models (LLMs) together improve multi-modal data management, accuracy, and explainability.",,,,,,,,,,,,,,
Challenges include vector data management,efficiency,scalability,bias,fairness,,security,,,,,,,,,and privacy.
Optimizing KG databases and LLM systems is critical for performance and accessibility.,,,,,,,,,,,,,,,
Recommendations include developing techniques for provenance,bias mitigation,"and secure knowledge extraction.""","Integrating graph structures with multi-modal data (text, tables, images) as LLM input and designing prompts with graph data for better generalization.",,,,,,,,,,,,
Developing methods for data cleaning,integration,and augmentation using the synergy of LLMs and knowledge graphs.,,,,,,,,,,,,,
"Creating ground truth datasets and experimental benchmarks for evaluating LLM and knowledge graph integration in various domains.""","Future research should address: integrating graph structures with multi-modal data, extracting relevant subgraphs, designing prompts with graph data, data cleaning and integration, benchmarking with ground truth datasets, multi-modal and vector data management, improving LLM accuracy and consistency, optimizing efficiency and scalability, and enhancing validation and explainability using KGs.",,,,,,,,,,,,,,
StreamE: Lightweight Updates of Representations for Temporal Knowledge Graphs in Streaming Scenarios,"Zhang Jiasheng, Shao Jie, Cui Bin",2023,reference-manager,10.1145/3539618.3591772,,,,,,"Inductive forecasting task: Predicts future knowledge for unseen entities using observed knowledge, evaluating with mean reciprocal rank (MRR) and Hit@k metrics.",,How can the proposed StreamE framework effectively forecast future knowledge in temporal knowledge graphs by modeling both direct and propagated influence of newly arriving knowledge on entity representations?,"The paper proposes StreamE, a framework for inductive forecasting of future knowledge for unseen entities. Using entity representation as memory, StreamE outperforms baselines on four datasets, achieving up to 16.7% higher mean reciprocal rank. The study demonstrates StreamE's robustness and effectiveness through ablation and hyperparameter analysis.","The research goal is to improve inductive forecasting on temporal knowledge graphs; the approach is StreamE, which uses entity representations as memory and a propagation unit to model knowledge influence; results show StreamE outperforms baselines by up to 16.7% on key metrics, demonstrating superior robustness and accuracy.",
"Ablation study: Assesses the impact of different model components and update functions on performance.
Comparison with baseline methods: Benchmarks StreamE against other models (e.g.",xERTE,"RE-GCN) using multiple datasets and performance metrics.""",The research is reproducible. The source code for the project is available at https://github.com/zjs123/StreamE.,"StreamE outperforms all baseline methods on four datasets, achieving up to 16.7% higher performance on the AAA metric compared to the second-best method.",,,,,,,,,,,
StreamE maintains robust performance for both seen and unseen entities,and its effectiveness does not drop drastically with less training data.,,,,,,,,,,,,,,
Ablation studies show significant drops in Hit@1 and Hit@10 when key components are removed,"confirming their importance; no explicit p-values are reported.""","Primary outcomes were measured using Mean Reciprocal Rank (MRR) and Hit@k (Hit@1, Hit@3, Hit@10).",,,,,,,,,,,,,
StreamE achieved the best results on all four datasets,e.g.,on ICEWS 14: MRR 23.15,Hit@1 17.13,Hit@3 25.09,,Hit@10 34.59.,,,,,,,,,
StreamE outperformed the second-best method by 14.1%,1.6%,9.1%,and 16.7% on the AAA metric across datasets.,,,,,,,,,,,,
Ablation studies showed significant drops in Hit@1 and Hit@10 when removing the propagation unit (StreamE-P: ICEWS 14 Hit@1 15.41,Hit@10 31.06).,,,,,,,,,,,,,,
StreamE maintained robust performance with different training data proportions and for both seen and unseen entities.,,,,,,,,,,,,,,,
StreamE had lower training time per epoch (40s) and fewer parameters (1.09M) compared to xERTE (1096s,3.36M) and RE-GCN (118s,"5.96M).""",No information available,"StreamE outperforms all baseline methods on four datasets, achieving up to 16.7% higher on the AAA metric.",,,,,,,,,,,
The propagation unit and read function are crucial for accurate forecasting and representation updates.,,,,,,,,,,,,,,,
StreamE is robust to different operations and training data proportions,handling both seen and unseen entities effectively.,,,,,,,,,,,,,,
"Recommendation: Use StreamE for inductive forecasting tasks requiring robust handling of new knowledge.""",Existing methods struggle to use complete historical data due to high time complexity; future research should focus on efficient utilization of full history for forecasting.,,,,,,,,,,,,,,
Current models inadequately update entity representations during inactive intervals; further work is needed on dynamic representation updates.,,,,,,,,,,,,,,,
"The propagation influence of new knowledge is underexplored; future studies should better model and leverage propagation paths.""",,,,,,,,,,,,,,,
A SysML-centric integration framework for helicopter fuel system development,"Zhao H, Wu W K, Hu X M, Guo Y Q, Zhang C, Hao G X",2023,reference-manager,10.1088/1742-6596/2472/1/012040,,,,,,"SysML-centric integration framework: Integrates SysML modeling tools with simulation and analysis tools for system development, supporting concept design through verification and validation.",,"How can a SysML-centric integration framework, utilizing FMI-based co-simulation, improve the conceptual design, verification, and validation of helicopter fuel systems to meet complex stakeholder requirements and regulatory standards?","The paper aims to develop a SysML-centric integration framework for helicopter fuel system conceptual design. Using FMI-based co-simulation, it integrates simulation models (e.g., from AMESim) into SysML tools to verify performance. Results show improved fidelity, reusability, and efficiency, supporting complex system analysis and traceability.","The research goal is to develop a SysML-centric integration framework for helicopter fuel system development; the key method uses an FMI-based co-simulation approach to link SysML models with simulation tools; the principal finding is improved model fidelity, reusability, and efficiency in system analysis and verification.",
FMI-based co-simulation: Uses the FMI (Functional Mock-up Interface) standard to enable tool-independent model/data exchange and co-simulation between SysML and commercial simulation software.,,,,,,,,,,,,,,,
Black box and white box modeling: Utilizes use case diagrams and internal block diagrams for black box modeling,"and block definition diagrams and activity diagrams for white box modeling to analyze and allocate system requirements.""",,"The SysML-centric integration framework, using FMI-based co-simulation, enables high-fidelity, executable system models for helicopter fuel systems, reducing time costs and improving virtual verification.",,,,,,,,,,,,
The approach allows maximum reuse of physical simulation models from commercial software and supports multi-domain model libraries.,,,,,,,,,,,,,,,
"The SysML-centric method is concluded to be more reasonable for complex system analysis and traceability than alternatives; no statistical significance (p-values) is reported.""",Primary outcomes:,,,,,,,,,,,,,,
FMI-based co-simulation enables integration between SysML models and simulation tools,improving model fidelity and reducing time cost.,,,,,,,,,,,,,,
The approach allows reuse of physical simulation models from commercial software.,,,,,,,,,,,,,,,
SysML-centric integration is shown to be more reasonable for complex system analysis and traceability.,,,,,,,,,,,,,,,
Results and measured effects:,,,,,,,,,,,,,,,
Measures of performance (MoPs) include fuel pressure,fuel flow,and longitudinal/lateral center of gravity (CoG) constraints.,,,,,,,,,,,,,
Fuel supply flow must be no less than 0.013 kg/s.,,,,,,,,,,,,,,,
Fuel supply pressure must be no less than 3 bar.,,,,,,,,,,,,,,,
Longitudinal CoG (CoGX) must be within -3.5 m to 3.5 m.,,,,,,,,,,,,,,,
Lateral CoG (CoGY) must be within -0.5 m to 0.5 m.,,,,,,,,,,,,,,,
No explicit statistical values (e.g.,"p-values) are provided.""",,Model/data exchange is essential for effective SysML-centric integration in complex system analysis and traceability.,,,,,,,,,,,,
The SysML-centric integration approach is more reasonable than alternatives,enabling reuse of multi-domain model libraries.,,,,,,,,,,,,,,
FMI-based co-simulation bridges SysML models and simulation tools,increasing model fidelity and reducing time cost.,,,,,,,,,,,,,,
"Expanded commercial and open-source tool support is recommended.""",Traditional design and verification approaches are insufficient for the increasing complexity and integration of helicopter fuel systems.,,,,,,,,,,,,,,
SysML models lack the capability to perform mathematical or physical performance calculations,limiting their value in system development.,,,,,,,,,,,,,,
"There is a need for improved model/data exchange technologies to maintain consistency between SysML models and simulation environments.""",,"The study design is a simulation-based, model-driven engineering approach using SysML-centric integration. It involves black box and white box modeling, numerical analysis with commercial simulation software (e.g., AMESim), and FMI-based co-simulation. The study is not randomized, controlled, or observational.",,,,,,,,,,,,,
Knowledge Graphs: Opportunities and Challenges,"Peng Ciyuan, Xia Feng, Naseriparsa Mehdi, Osborne Francesco",2023,reference-manager,10.1007/s10462-023-10465-9,,,,,,"Comprehensive survey: The study conducts a thorough review and analysis of existing knowledge graph research, technologies, and applications.",,"What are the opportunities and challenges of using knowledge graphs for question-answering systems, particularly in addressing simple and multi-hop questions, knowledge reasoning, and the verification of inferred knowledge?","This paper surveys knowledge graphs, aiming to analyze their technological advancements, applications, and challenges. Using a comprehensive literature review, it highlights knowledge graphs’ benefits for AI systems and diverse fields, but also details technical challenges in areas like knowledge acquisition and reasoning. The study concludes by encouraging future research and development.","The research goal is to comprehensively survey knowledge graphs, the approach is an in-depth analysis of their technologies, applications, opportunities, and challenges, and the principal finding is that while knowledge graphs offer significant benefits for AI and various fields, they face major technical limitations.",
Technical challenge analysis: The study examines and elaborates on limitations and challenges in five key knowledge graph technologies,such as knowledge graph embeddings and reasoning.,,,,,,,,,,,,,,
"Categorization and comparison: The study categorizes state-of-the-art research and compares traditional recommender system methods (content-based and collaborative filtering-based).""",,"Knowledge graph embedding methods show varying link prediction performance, with QuatE achieving the highest Hits@10 on FB15K at 90%, and TransG at 88.2%.",,,,,,,,,,,,,
Knowledge graphs are widely applied in education,scientific research,healthcare,and social networks,enabling functions like drug discovery and fake news detection.,,,,,,,,,,,
Multi-hop reasoning in knowledge graphs faces high computational costs and uncertainty in inferred knowledge,"highlighting the need for multi-source reasoning and error reduction; no p-values reported.""",KGNN (Lin et al. 2020): Predicted drug-drug interactions by mining relationships between drugs and their neighborhoods in medical knowledge graphs.,,,,,,,,,,,,,
COVID-KG (Wang et al. 2020e): Generated COVID-19-related drug repurposing reports using multimedia knowledge graphs.,,,,,,,,,,,,,,,
"No explicit statistical values or measured effects reported in the context.""",High computational cost for multi-hop knowledge reasoning.,,,,,,,,,,,,,,
Uncertainty and need for verification of inferred new knowledge.,,,,,,,,,,,,,,,
Difficulty in detecting and resolving conflicts between new and existing knowledge.,,,,,,,,,,,,,,,
Major limitations in acquiring and integrating knowledge from multiple sources.,,,,,,,,,,,,,,,
"Incomplete representation of knowledge; many entities and relationships are missing.""","Knowledge graphs offer significant opportunities for improving AI systems and applications in various fields, including healthcare and education.",,,,,,,,,,,,,,
Major technical challenges remain,especially in knowledge acquisition,integration from multiple sources,and multi-hop reasoning.,,,,,,,,,,,,
Verification and reduction of erroneous or conflicting knowledge are critical issues.,,,,,,,,,,,,,,,
"Future research should focus on multi-source reasoning and addressing technical limitations.""",Multi-hop knowledge reasoning remains challenging due to high computational costs.,,,,,,,,,,,,,,
Verification and conflict detection of inferred new knowledge are critical but unresolved issues.,,,,,,,,,,,,,,,
"Integrating and acquiring knowledge from multiple sources into a unified knowledge graph faces significant technical limitations.""","Future research should focus on multi-hop knowledge reasoning, verification of inferred knowledge, detection of conflicts between new and existing knowledge, multi-source knowledge reasoning, and erroneous knowledge reduction. There are also gaps in acquiring and integrating knowledge from multiple sources and in knowledge graph completion.","Study design: Systematic review; comprehensive survey; observational study. The study analyzes and categorizes existing research, technologies, applications, and challenges related to knowledge graphs across multiple fields. It does not involve randomized, controlled, or experimental designs.",,,,,,,,,,,,,
Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling,"Yang Linyao, Chen Hongyang, Li Zhao, Ding Xiao, Wu Xindong",2021,reference-manager,,,,,,,"Data augmentation: Refines training data during pretraining, emphasizing informative words to improve factual knowledge in language models.",,"How can integrating knowledge graphs (KGs) with pre-trained language models (PLMs) and large language models (LLMs) enhance semantic representation, factual knowledge modeling, and reduce issues such as hallucination and inconsistency in tasks like sentiment analysis, knowledge graph completion, and question answering?","The paper systematically reviews methods for enhancing large language models (LLMs) with knowledge graphs (KGs), focusing on knowledge graph enhanced large language models (KGLLMs). It categorizes and compares approaches, discusses challenges like hallucination and inconsistency, and concludes that KG integration improves factual accuracy and reasoning in LLMs.","The paper's main objective is to review and categorize methods for enhancing large language models (LLMs) with knowledge graphs (KGs), using a systematic approach, and concludes that integrating KGs into LLMs (KGLLMs) improves factual knowledge modeling; keywords: research goal, systematic review, knowledge graph enhancement, LLMs, results.",
Retrieval augmentation: Allows models to retrieve external data from databases or tools and incorporate it via prompts or embeddings.,,,,,,,,,,,,,,,
Knowledge graph enhancement: Includes before-training,during-training,"and post-training methods to integrate structured knowledge into language models.""",,"KGPLMs (Knowledge Graph Pre-trained Language Models) outperform traditional PLMs in capturing factual and relational information, leading to stronger language understanding and generation abilities.",,,,,,,,,,,
Incorporating external knowledge,especially via knowledge graphs,improves performance in tasks like Named Entity Recognition,particularly for domain-specific applications.,,,,,,,,,,,,
"No explicit quantitative results or statistical significance (p-values) are provided in the context.""","KGPLMs outperform traditional PLMs in capturing factual and relational information, leading to improved language understanding and generation.",,,,,,,,,,,,,,
KGPLMs show better performance in Named Entity Recognition,Relation Extraction,and Sentiment Analysis,especially in domain-specific tasks.,,,,,,,,,,,,
Table III: Compared to BERT,KGPLMs show the following improvements (in percentage points):,,,,,,,,,,,,,,
CoLAKE: Entity typing +2.8,Relation classification +5.6,,,,,,,,,,,,,,
LUKE: Entity typing +4.6,Relation classification +6.7,Question answering +19.2,,,,,,,,,,,,,
KEPLER: Entity typing +2.6,Relation classification +6,,,,,,,,,,,,,,
ERNIE: Entity typing +2,Relation classification +3.4,,,,,,,,,,,,,,
CokeBERT: Entity typing +1.3,Relation classification +2.7,,,,,,,,,,,,,,
K-Adapter: Entity typing +4.1,Relation classification +1.9,Question answering +5.4,,,,,,,,,,,,,
ERICA: Entity typing +4.4,Relation classification +2.2,Question answering +1.5,,,,,,,,,,,,,
KP-PLM: Entity typing +4.6,Relation classification +3.8,,,,,,,,,,,,,,
Post-training enhancement methods are low-cost,easy to implement,"and effectively improve LLMs’ performance on specific tasks but may limit generation flexibility.""","LLMs may produce correct answers with invalid reasoning or incorrect answers with valid reasoning, causing inconsistency.",,,,,,,,,,,,
Performance is highly influenced by prompt templates and entity co-occurrence frequencies.,,,,,,,,,,,,,,,
Poor generalization and inference abilities; struggle with relational reasoning.,,,,,,,,,,,,,,,
Difficulty constraining outputs to fine-grained or structural criteria.,,,,,,,,,,,,,,,
Hallucination: generating unsupported or contradictory content.,,,,,,,,,,,,,,,
Inability to explicitly access,edit,or update knowledge.,,,,,,,,,,,,,
Inconsistent answers to the same factual questions.,,,,,,,,,,,,,,,
Outputs are hard to interpret due to black-box nature.,,,,,,,,,,,,,,,
Reliance on biased prompts and dataset artifacts.,,,,,,,,,,,,,,,
Limited ability to infer relationships between actions and events.,,,,,,,,,,,,,,,
Accuracy depends on fact frequency in pre-training data.,,,,,,,,,,,,,,,
"Further research needed for LLMs as reliable knowledge bases.""","LLMs have limitations in factual knowledge modeling, reasoning, and consistency compared to knowledge graphs (KGs).",,,,,,,,,,,,,,
Integrating external KGs can improve LLMs’ knowledge traceability and correctability.,,,,,,,,,,,,,,,
Data and retrieval augmentation methods help but do not fundamentally enhance LLMs’ knowledge modeling.,,,,,,,,,,,,,,,
"Further research is needed to enable LLMs to serve as reliable knowledge bases.""",Investigating the scaling law of KGLLMs to determine optimal model size for efficiency and resource reduction.,,,,,,,,,,,,,,
Developing methods to merge different types of knowledge,especially integrating rarely used or implicit knowledge into LLM parameters.,,,,,,,,,,,,,,
"Incorporating multimodal and temporal knowledge from knowledge graphs to enhance KGPLMs beyond single-modality and static data.""","Suggested future research directions include: investigating the scaling law of KGLLMs to optimize model size and efficiency; merging different types of knowledge; incorporating multimodal and temporal knowledge; improving prompt template selection with KGs; addressing hallucination, inconsistency, and structured text generation; and enhancing LLMs’ factual knowledge modeling.","The study design is a systematic review. It categorizes and summarizes existing research on knowledge graph enhanced pre-trained language models (KGPLMs), including various enhancement methods (before-training, during-training, post-training). No information is provided about randomization, blinding, control groups, or other experimental design features.",,,,,,,,,,,,,
The role of working memory capacity in the temporal compression of episodic memories: An individual differences approach.,"Leroy Nathan, Majerus Steve, D'Argembeau Arnaud",2025,reference-manager,10.1037/xlm0001350,,,,,,"Used the DAStau procedure from the robustlmm package to estimate robust linear mixed-effects models, handling outliers and missing values.",,"How does event segmentation ability influence episodic memory performance, and what are the methodological considerations for measuring these constructs in online behavioral research platforms?",,The research goal was to examine working memory (WM) task performance and its relation to temporal compression rates using robust linear mixed-effects models; the approach involved robust statistical modeling with random effects for participants and stimuli; results showed descriptive statistics and correlations among WM tasks and outcome variables.,
Included random intercepts for both participants and stimuli (videos) in the statistical models.,,,,,,,,,,,,,,,
"Assessed explained variance using Nakagawa’s R² and computed Wald’s 95% confidence intervals and p-values for inference.""","The gridExtra package (v2.3; Auguie, 2017) was used for figure creation. No explicit mention of source code for the project or reproducibility details specific to gridExtra are provided in the context.","The interaction between WM (working memory) capacity and EBs (event boundaries) density significantly predicted temporal compression in Experiment 2 (interaction coefficient = -0.15, SE = 0.07, 95% CI \[-0.28, -0.02], p = 0.024).",,,,,,,,,,,,,
EBs density alone was a significant predictor of temporal compression (coefficient = 0.44,SE = 0.17,95% CI \[0.11,0.77],p = 0.009),,while WM capacity alone was not significant (p = 0.166).,,,,,,,,,
High inter-rater reliability was observed for scoring recalled experience units (ICC = 0.96,95% CI \[0.94,"0.97]); results were robust to exclusion criteria.""","Primary outcomes measured: Temporal compression rates (TCR), number of recalled experience units, and duration of recalled experience units.",,,,,,,,,,,,
"In Experiment 2 (N=210):
TCR: Mean = 3.61",SD = 2.75,Skew = 1.71,Kurtosis = 3.26,,,,,,,,,,,,
Recalled experience units: Mean = 8.76,SD = 3.21,,,,,,,,,,,,,,
Recalled experience units duration: Mean = 3.93,SD = 3.26,,,,,,,,,,,,,,
Correlations (Spearman’s rho) for EB- stimuli:,,,,,,,,,,,,,,,
WM Score & TCR: -0.07,,,,,,,,,,,,,,,
WM Score & recalled experience units: -0.05,,,,,,,,,,,,,,,
WM Score & recalled experience units duration: 0.10,,,,,,,,,,,,,,,
Correlations (Spearman’s rho) for EB+ stimuli:,,,,,,,,,,,,,,,
WM Score & TCR: -0.04,,,,,,,,,,,,,,,
WM Score & recalled experience units: -0.01,,,,,,,,,,,,,,,
WM Score & recalled experience units duration: -0.02,,,,,,,,,,,,,,,
Statistical power analysis: With N=210,power was estimated for detecting an interaction with effect sizes 5%-20% smaller than Experiment 1.,,,,,,,,,,,,,,
"Robust linear mixed-effects models were used due to unmet assumptions of classical models.""",Only global working memory (WM) capacity was evaluated; specific WM sub-processes were not examined.,,,,,,,,,,,,,,
Cannot determine if WM affects temporal compression during encoding,retrieval,or both.,,,,,,,,,,,,,
No segmentation task; unknown how participants segmented events during perception.,,,,,,,,,,,,,,,
"Temporal distance between event boundaries (EBs) in videos was not controlled.""",The interaction between working memory (WM) capacity and event boundaries (EBs) density significantly predicts temporal compression.,,,,,,,,,,,,,,
Higher EBs density increases temporal compression,especially for individuals with lower WM capacity.,,,,,,,,,,,,,,
Results are robust to exclusion criteria; findings hold when only high-performing participants are included.,,,,,,,,,,,,,,,
"Recommendation: Consider both WM capacity and EBs density when studying temporal memory.""",No information available,"Future research should investigate the specific role of working memory (WM) sub-processes in temporal compression of events, clarify whether WM affects encoding or retrieval, include segmentation tasks to assess individual differences, and control for temporal distance between event boundaries (EBs) in experimental designs.",,,,,,,,,,,,,
Quality-focused design patterns for digital twin systems,"Humana Carlo, Basson Anton H., Kruger Karel",2023,reference-manager,,,,,,,"Application of design patterns: Design patterns focused on quality attributes were applied to three case studies (heliostat field, water distribution network, smart city) to assess effectiveness and utility.",,"What quality-attribute focused design patterns can be developed to guide the architectural and implementation decisions for digital twin systems in complex environments, and how do these patterns address trade-offs between performance efficiency, reliability, reconfigurability, and interoperability?","The paper aims to enhance digital twin system design by introducing quality-attribute-focused design patterns. Using case studies, it shows these patterns help identify key design aspects and trade-offs, improving system design and speed. However, their quantitative impact and broader applicability require further research.","The paper’s research goal is to present four quality-attribute focused design patterns for digital twin (DT) systems, using a reference architecture approach, and the principal finding is that these patterns help design better and faster DT systems by guiding trade-offs for performance, reliability, reconfigurability, and interoperability.",
Case study methodology: Case studies were selected to represent diverse challenges and evaluate the applicability of design patterns in different digital twin systems.,,,,,,,,,,,,,,,
Design framework: The methodology included needs analysis,identification of digital twins,service mapping,architectural refinement,"and validation testing.""",,,,,,,,,,,"The proposed design patterns improved implementation suitability in three case studies: heliostat (performance efficiency, reliability), water distribution network (reliability, interoperability), and smart city (interoperability, performance efficiency, reconfigurability)."
Quality attributes effectively guided design choices,often requiring trade-offs between conflicting requirements.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""",Primary outcomes:,,,,,,,,,,,,,,
Design patterns were applied in three case studies: heliostat field,water distribution system,and smart city.,,,,,,,,,,,,,
Quality attributes (e.g.,performance efficiency,reliability,interoperability,reconfigurability) guided design choices.,,,,,,,,,,,
Results and measured effects:,,,,,,,,,,,,,,,
Heliostat case: Performance efficiency and reliability patterns led to suitable implementations.,,,,,,,,,,,,,,,
Water distribution: Reliability and interoperability patterns aided implementation.,,,,,,,,,,,,,,,
"Smart city: Interoperability
Testing metrics:",performance efficiency,and reconfigurability patterns were effective.,,,,,,,,,,,,,
Time required to integrate new components.,,,,,,,,,,,,,,,
"Time required to establish interfaces with external components.""",No explicit limitations are stated in the provided context.,,,,,,,,,,,,,,
"No self-reported problems or suggestions for further research are mentioned.""","Quality attributes are central to effective system design; multiple design patterns may be applied, requiring trade-offs.",,,,,,,,,,,,,,
Separation of concerns,modularity,and scalability are recommended for performance and reliability.,,,,,,,,,,,,,
Security should not be sacrificed,but industry standards can minimize performance impact.,,,,,,,,,,,,,,
"Case studies confirm the utility of these design patterns across diverse scenarios.""",Difficulty in evaluating the general validity and utility of design patterns due to their broad applicability.,,,,,,,,,,,,,,
Challenges in demonstrating the range of application across different physical systems and user expertise.,,,,,,,,,,,,,,,
"Need for more case studies to measure effectiveness and utility in diverse scenarios.""",,,,,,,,,,,,,,,
A temporal knowledge graph reasoning model based on recurrent encoding and contrastive learning,"Liu Weitong, Hasikin Khairunnisa, Khairuddin Anis Salwa Mohd, Liu Meizhen, Zhao Xuechen",2025,reference-manager,10.7717/peerj-cs.2595,,,,,,"Used time-aware filtering setting for temporal knowledge graph reasoning, separating quadruples by query time.",,How can a Temporal Reasoning with Recurrent Encoding and Contrastive Learning (TRCL) model effectively capture the dynamic relationships among historical facts and address the positive and negative influences of repeated historical facts to improve extrapolation reasoning in temporal knowledge graphs?,"The paper proposes TRCL, a new temporal knowledge graph (TKG) reasoning model focused on extrapolation reasoning. Using a recurrent encoder, global historical matrix, and contrastive learning, TRCL improves entity prediction. Experiments on four datasets show TRCL outperforms existing methods. Optimal performance occurs with α = 0.3 and learning rate 0.001.","The research goal is to improve temporal knowledge graph (TKG) extrapolation reasoning; the key method is the proposed TRCL model combining sequential information, a history matrix, contrastive learning, and a time decoder; results show TRCL is effective but needs better data efficiency for complex datasets like ICEWS05-15.",
Employed one-dimensional convolution-based GCN (Graph Convolutional Network) layers with specific hyperparameters (embedding size 200,2 layers,dropout 0.2).,,,,,,,,,,,,,
"Optimized model parameters using the adam optimizer with a learning rate of 0.001.""",,"The TRCL model achieved optimal entity prediction performance on ICEWS14 and ICEWS18 when α (history weight) = 0.3 and learning rate = 0.001, for both MRR (Mean Reciprocal Rank) and H@3 (Hits@3) metrics.",,,,,,,,,,,,,
Excessive focus on repetitive historical facts reduced prediction effectiveness,indicating a balance is needed.,,,,,,,,,,,,,,
TRCL outperformed existing methods in most metrics across four benchmark datasets; however,"its performance on ICEWS05-15 highlighted challenges with limited training data. No p-values were reported.""","The proposed TRCL model achieved the highest performance on entity prediction tasks across ICEWS14, ICEWS18, ICEWS05-15, and GDELT datasets.",,,,,,,,,,,,,
On ICEWS14: MRR 45.07%,H@1 34.71%,H@3 50.22%,H@10 65.37%.,,,,,,,,,,,,
On ICEWS18: MRR 33.78%,H@1 23.26%,H@3 38.20%,H@10 54.39%.,,,,,,,,,,,,
TRCL outperformed TiRGN except for H@1 on ICEWS05-15.,,,,,,,,,,,,,,,
Incorporating repetitive historical information and contrastive learning improved MRR and H@3 by up to 9.1% (ICEWS14) and 7.4% (ICEWS18) compared to ablated models.,,,,,,,,,,,,,,,
Optimal performance was achieved with α = 0.3 for history weight.,,,,,,,,,,,,,,,
Sensitivity analysis showed that excessive focus on repetitive historical facts reduced effectiveness.,,,,,,,,,,,,,,,
"TRCL improved by 2.3% (ICEWS14) and 1.2% (ICEWS18) in MRR over TRCL-h; by 8.3% (ICEWS14) and 6.8% (ICEWS18) over TRCL-nh.""",TRCL’s performance on the ICEWS05-15 dataset shows difficulty handling complex data with limited training samples.,,,,,,,,,,,,,,
The model relies heavily on extensive data; reducing this dependence is suggested for future work.,,,,,,,,,,,,,,,
"Expansion to relationship prediction tasks and emerging facts is needed for broader applicability.""","The TRCL model outperforms existing methods in most metrics for temporal knowledge graph (TKG) reasoning, especially in extrapolation tasks.",,,,,,,,,,,,,,
TRCL effectively leverages repeated historical facts but avoids over-reliance,as excessive focus can reduce prediction accuracy.,,,,,,,,,,,,,,
Optimal performance is achieved with α = 0.3 and learning rate = 0.001.,,,,,,,,,,,,,,,
"Future work should improve data efficiency and extend TRCL to relationship prediction tasks.""","Limited research on extrapolation reasoning in temporal knowledge graphs, which is more practical and challenging than interpolation reasoning.",,,,,,,,,,,,,,
Need for improved methods to handle incomplete data in temporal knowledge graphs.,,,,,,,,,,,,,,,
"Lack of explainable approaches for link forecasting and reasoning in temporal knowledge graphs.""","Future research should investigate the reasoning ability of models towards emerging facts. There is a need to address limitations in predicting new or previously unseen events, as current models like TRCL focus mainly on historical and repetitive facts for entity prediction.","The study design is experimental, involving comparative experiments with multiple models on several datasets. It uses time-aware filtering, ablation studies, and sensitivity analyses of hyper-parameters (such as α and learning rate). The design is not randomized, double-blind, controlled, or placebo-controlled. No mention of observational, retrospective, or meta-analysis approaches.",,,,,,,,,,,,,
AMEND 2.0: module identification and multi-omic data integration with multiplex-heterogeneous graphs,"Boyd Samuel S., Slawson Chad, Thompson Jeffrey A.",2025,reference-manager,10.1186/s12859-025-06063-x,,,,,,Random Walk with Restart for multiplex-heterogeneous networks (RWR-MH): A network diffusion method for analyzing multi-omic data across multiple network layers and components.,,"How does the updated AMEND algorithm enable effective multi-omic data integration and active module identification on multiplex-heterogeneous networks, while addressing challenges such as degree bias and multi-objective node selection?","AMEND 2.0 is introduced as a flexible method for active module identification and multi-omic data integration using multiplex-heterogeneous network diffusion. The study applies AMEND to TCGA-KIRC and OGT knockout datasets, demonstrating effective module identification and degree bias mitigation. AMEND is broadly applicable but network-dependent.","The research goal was to identify key molecular modules in complex multi-omic data; the approach used AMEND 2.0, which integrates multi-omic data via multiplex-heterogeneous network diffusion with degree bias mitigation; results show AMEND effectively identifies biologically relevant modules in diverse datasets.",
Biased Random Walk for multi-objective node selection (B-RWR): Integrates additional node-specific information to guide node selection.,,,,,,,,,,,,,,,
Degree bias adjustment: Adjusts for node connectivity differences,"applied in a component/layer-specific manner.""","The research is reproducible. All datasets and computer code used are available: chemical and protein-chemical links from STITCH v5, TCGA-KIRC data from Broad GDAC Firehose, and all other data and code on GitHub (https://github.com/samboyd0/AMEND2.0\_study).","AMEND accurately retains nodes with large seed values, even when these nodes have low degree; empirical cumulative probability differences remain above 0.75 across datasets.",,,,,,,,,,,,
The IN degree bias adjustment method significantly reduces correlation between diffusion score and degree compared to control and is recommended for real-world analyses.,,,,,,,,,,,,,,,
B-RWR increases diffusion scores for nodes with large BRW attribute values,"with the effect growing as the rate parameter approaches 0.""",Over-representation analysis (ORA) identified significant Gene Ontology and Disease Ontology terms in the AMEND module from TCGA-KIRC data at a significance level of 0.01 (Benjamini-Hochberg adjusted).,,,,,,,,,,,,,
Significant Reactome pathways were identified in the AMEND module from OGT-KO data at a significance level of 0.05 (Benjamini-Hochberg adjusted).,,,,,,,,,,,,,,,
In OGT-KO mouse liver samples,mitochondrial fatty acid beta-oxidation enzymes (ECI1,ECI2) showed equivalent down-regulation across omic datasets,except ECI2,which was up-regulated in transcriptomics but down-regulated in proteomics.,,,,,,,,,,,
Peroxisomal lipid metabolism genes (ACOX1,ACOX3,ALDH3A2,CRAT,ACAAB1,,EHHADH) mostly showed equivalent down-regulation across 1 and 2 weeks post-KO,,,,,,,,,except CRAT and ACOX3.
"These results suggest O-GlcNAcylation is a novel regulator of lipid metabolism in mouse liver.""","PPI networks suffer from degree bias due to technical and study biases, affecting node degree.",,,,,,,,,,,,,,
Certain technologies favor detection of highly-abundant or frequently-studied proteins.,,,,,,,,,,,,,,,
"Degree bias presents problems for network diffusion methods that use node degree to determine importance.""",AMEND 2.0 enables multi-omic data integration and identifies highly relevant modules using multiplex-heterogeneous network diffusion and degree bias mitigation.,,,,,,,,,,,,,,
AMEND is broadly applicable,especially for complex,noisy data with high-quality molecular interaction networks,but may not suit all research questions or clustering tasks.,,,,,,,,,,,,
Results depend heavily on network choice; alternative non-network-based methods exist for multi-omic integration.,,,,,,,,,,,,,,,
"AMEND is accessible as an R package with full documentation and code available.""",AMEND may not be optimal for specific research questions or experimental designs where identifying densely-connected subsets is not the main goal.,,,,,,,,,,,,,,
Results are highly dependent on the choice of network,highlighting the need for further research on network selection.,,,,,,,,,,,,,,
AMEND is not suitable for sample clustering,"suggesting a gap for unsupervised multi-omic integration methods.""",The study notes that AMEND may not be optimal for all research questions or experimental designs and that results depend on the choice of network. Future research could compare AMEND with non-network-based multi-omic integration methods and explore its performance in different experimental contexts.,"The study design is experimental, using multi-omic datasets (transcriptomic, proteomic, phospho-proteomic, metabolomic, RNA-seq, miRNA-seq, methylation, survival data) and network-based analyses. It includes fivefold cross-validation, differential expression analysis, and mixed effects models. No mention of randomization, blinding, or control groups.",,,,,,,,,,,,
"Patient-Centric Knowledge Graphs: A Survey of Current Methods, Challenges, and Applications","Al Khatib Hassan S., Neupane Subash, Manchukonda Harish Kumar, Golilarz Noorbakhsh Amiri, Mittal Sudip, Amirlatifi Amin, Rahimi Shahram",2024,reference-manager,,,,,,,"Ontology design: Creating structured frameworks (ontologies) to represent and integrate diverse healthcare data, ensuring accurate and standardized patient-centric knowledge representation.",,"What are the key methodologies, categorizations, challenges, and applications involved in the construction, evaluation, processing, and utilization of patient-centric knowledge graphs (PCKGs) in healthcare?","The paper surveys Patient-Centric Knowledge Graphs (PCKGs), aiming to classify their construction, evaluation, processing, and applications. Using a taxonomy developed from literature, it details methodologies like ontology design, data integration, and knowledge extraction. Key findings highlight robust frameworks for representing complex patient data and emphasize effective evaluation and real-world applications.","Constructing knowledge graphs and their biomedical applications \[121] aims to examine biomedical knowledge graph construction and applications, reviews manual curation and text mining approaches, highlights the need for advanced techniques for complex data, and concludes that current methods have limitations in representing diverse relationships and scalability.",
Knowledge extraction: Using techniques like Named Entity Recognition (NER) and Relationship Extraction (RE) to identify key entities (e.g.,drugs,symptoms) and their relationships.,,,,,,,,,,,,,
Evaluation methodologies: Employing qualitative (usability studies,expert reviews) and quantitative (accuracy,recall,precision,completeness,,consistency,,,,,,,"Patient-Centric Knowledge Graphs (PCKGs) integrate structured, semi-structured, and unstructured healthcare data using techniques like Named Entity Recognition and Relationship Extraction for comprehensive patient representation.",,"embedding techniques) assessments to measure the PCKG’s effectiveness."""
Quantitative assessments of PCKGs use metrics such as accuracy,recall,and precision to evaluate completeness and consistency; specific numerical results or p-values are not provided.,,,,,,,,,,,,,
PCKGs show promise in enhancing clinical decision support and clinical trials,but challenges remain in construction,robustness,"and handling complex data; future research is recommended.""","Primary outcomes focus on evaluating PCKGs using qualitative (usability, content analysis, expert review) and quantitative (accuracy, recall, precision, completeness, consistency) methods.",,,,,,,,,,,
Results show significant improvements in accuracy,efficiency,and outcomes for clinical trial patient selection.,,,,,,,,,,,,,
Measured effects include enhanced precision in patient selection using frameworks like SMR.,,,,,,,,,,,,,,,
"No explicit statistical values provided.""",Probability of certain paths in the knowledge graph (KG) may not be accurate.,,,,,,,,,,,,,,
Lack of details on the classification model used.,,,,,,,,,,,,,,,
Need for advanced techniques to handle complex sentence structures.,,,,,,,,,,,,,,,
Limitations in current methods to represent diverse relationships in KGs.,,,,,,,,,,,,,,,
Scalability and memory limitations in matrix factorization techniques.,,,,,,,,,,,,,,,
Challenges in data fragmentation,noncommensurability,and semantic inference within cardiovascular data.,,,,,,,,,,,,,
Complexity and rising costs of clinical research.,,,,,,,,,,,,,,,
Ensuring data transparency and building trust with patients in clinical trials.,,,,,,,,,,,,,,,
Sample size limitations and unmeasured confounders affecting robustness.,,,,,,,,,,,,,,,
Difficulties in knowledge acquisition,completion,"and temporal KG development.""","Patient-centric knowledge graphs (PCKGs) enable personalized treatment recommendations, improving precision and patient outcomes.",,,,,,,,,,,,
Integrating diverse data sources into PCKGs enhances clinical trial design,patient selection,and engagement.,,,,,,,,,,,,,
Key challenges include data fragmentation,complexity,and scalability.,,,,,,,,,,,,,
"Continued research and collaboration are recommended to maximize PCKGs’ impact on healthcare.""","Improving knowledge acquisition, completion, and temporal knowledge graph (KG) development to maintain comprehensive and up-to-date patient profiles.",,,,,,,,,,,,,,
Addressing data quality,standardization,and integration challenges due to heterogeneous data structures and varying medical standards.,,,,,,,,,,,,,
"Enhancing KG embedding methods to better utilize auxiliary texts and overcome structural sparsity for richer patient information representation.""","Future research should address challenges in knowledge acquisition, completion, and temporal knowledge graph development, robustness against sample size limitations and unmeasured confounders, advanced techniques for complex sentence structures, scalability issues, and improved representation of diverse relationships in knowledge graphs. Further exploration of individualized and patient-centric approaches is recommended.","The study design is an exhaustive survey and analysis of existing literature and practices, resulting in a taxonomy that classifies PCKGs into construction, evaluation, process, and applications. It is comprehensive, structured, and informed by foundational literature and cutting-edge research. No mention of randomization, blinding, or control groups.",,,,,,,,,,,,,
Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering,"Karras Oliver, Wernlein Felix, Klünder Jil, Auer Sören",2023,reference-manager,,,,,,,Case studies and experiments have been the main research methods for empirical research in RE and SE for over 20 years.,,"How can the ORKG be used to organize, publish, and evaluate an openly available and sustainable knowledge graph of empirical research in requirements engineering to enable comprehensive, up-to-date, and long-term literature reviews on the state and evolution of the field?","The paper aims to organize and evaluate empirical research in Requirements Engineering (RE) using the ORKG knowledge graph. It analyzes related publications, focusing on themes like research methods and data collection. Findings show evolving research methods but limited data sharing. The study highlights the need for standardized terminology and sustainable literature reviews.","The research goal is to organize and evaluate empirical research in Requirements Engineering (RE) using a knowledge graph; the approach involves systematic data extraction from related publications, and the principal finding is a positive trend in empirical methods and data sharing, with 71.3% of recent studies providing their data.",
Surveys and systematic literature reviews have become more common since 2015.,,,,,,,,,,,,,,,
Descriptive statistics are the primary data analysis technique,"used in 92% of recent studies.""","Most related publications do not provide reproducible data or source code. Only four out of 14 publications offer their data; three have broken links, and only one (\[15]) uses a public data repository. The current research made all data and materials openly available for reproducibility.","The use of inferential statistics is low (19.2% overall, 26.3% for the target state), while descriptive statistics are used in 87.6% overall and 92% for the target state.",,,,,,,,,,,,
Reporting of threats to validity (91.3%),data provision (71.3%),and research questions/answers (23.7%) increased over time.,,,,,,,,,,,,,
Case study usage decreased (22.3% for the target state),"and action research is rarely used (0% for the target state); no p-values reported.""","The proportion of papers reporting an empirical study increased over time, averaging 94.3% for 2020–2025.",,,,,,,,,,,,,
"Use of three to five empirical methods per paper increased (22% for three
Experiments (35.7%)","25.3% for four
secondary research (40%)","26.7% for five).
and surveys (18.7%) are the most used data collection methods.",,,,,,,,,,,,,
Case study use decreased (22.3% for 2020–2025); action research is rarely used (0%).,,,,,,,,,,,,,,,
Descriptive statistics are mainly used for data analysis (92% for 2020–2025); inferential statistics are less common (26.3%).,,,,,,,,,,,,,,,
Reporting of threats to validity,data provision,and research questions increased (91.3% for threats to validity).,,,,,,,,,,,,,
Proportion of papers providing at least one URL to their data increased over time,reaching 80% in 2020.,,,,,,,,,,,,,,
Experiments and case studies have been the main research methods for over 20 years.,,,,,,,,,,,,,,,
"There is a need for standardized terminology and theories for more consistent empirical research representation.""","Only papers from one publication venue were selected, limiting generalizability.",,,,,,,,,,,,,,
KG-EmpiRE is initial and lacks papers from other important venues.,,,,,,,,,,,,,,,
Data extraction and classification schema bias may exist,though mitigated by review processes.,,,,,,,,,,,,,,
Research method bias from using only a literature review.,,,,,,,,,,,,,,,
"Publication bias is present.""","There is a positive trend toward more empirical studies providing their data, with reported data URLs increasing from 25.4% (pre-2010) to 71.3% (2020–2025).",,,,,,,,,,,,,,
More case studies and action research are recommended.,,,,,,,,,,,,,,,
Standardized terminology and theories should be developed and used for consistency.,,,,,,,,,,,,,,,
"Collaboration among researchers is needed to reduce redundancy and improve cumulative research.""","Lack of comprehensive, up-to-date, and long-term available overviews of empirical research in requirements engineering (RE).",,,,,,,,,,,,,,
Insufficient availability and sharing of data from earlier works,hindering collaborative updates and cumulative research.,,,,,,,,,,,,,,
"Need for technical infrastructures (like ORKG) with additional features to enable sustainable and continuous literature reviews.""","Future research should focus on developing, expanding, and using standardized terminology and theories, increasing data availability, enabling cumulative research, and building comprehensive, up-to-date, and long-term overviews through collaborative efforts and technical infrastructures like ORKG. More case studies and action research are also recommended.",,,,,,,,,,,,,,
A novel customizing knowledge graph evaluation method for incorporating user needs,"Zhang Ying, Xiao Gang",2024,reference-manager,10.1038/s41598-024-60004-x,,,,,,"Accuracy verification using external data resources: Compares entities and triples from different sources, assigns weights, and aggregates confidence scores for accuracy assessment.",,"How can knowledge graph accuracy evaluation methods be improved by integrating user search behavior and needs to enhance the utility, applicability, and efficiency of evaluation results in practical application scenarios?","The paper proposes a new method to evaluate knowledge graph accuracy by integrating user requirements and search behavior into the evaluation process. Using iterative sampling and Margin of Error control, the method efficiently achieves reliable results. Applied to multiple knowledge graphs, it closely matches real accuracy rates, improving practical relevance.","The research goal is to improve knowledge graph accuracy evaluation by integrating user needs; the key method is an entity popularity-weighted clustering sampling approach; results show this method significantly enhances evaluation accuracy and efficiency, closely aligning with actual user requirements.",
Model stacking: Uses multiple mini-models to evaluate correlations,translational invariance,and path existence between entities for comprehensive knowledge graph assessment.,,,,,,,,,,,,,
EP-TWCS sampling method: Clusters entities by user needs,assigns weights based on entity popularity,"and iteratively samples to efficiently estimate accuracy with minimal bias.""",,"The entity popularity-weighted clustering sampling method significantly improved evaluation accuracy and stability for NELL, YAGO, and MOVIE knowledge graphs, closely matching actual accuracy rates while reducing sample size.",,,,,,,,,,,
All experiments were conducted with an error threshold (ε) of 5% and a confidence level of 95%.,,,,,,,,,,,,,,,
"No explicit p-values or detailed quantitative results are provided in the context.""","Primary outcome: The Entity Popularity Weighted Clustering Sampling (EP-TWCS) method was evaluated on NELL, YAGO, and MOVIE knowledge graphs.",,,,,,,,,,,,,,
NELL: EP-TWCS achieved 95.5% ± 3.0% true accuracy (sample size: 67–77).,,,,,,,,,,,,,,,
YAGO: EP-TWCS achieved 0.974 ± 0.025 to 0.968 ± 0.016 true accuracy (sample size: 30–39).,,,,,,,,,,,,,,,
MOVIE: EP-TWCS achieved 0.889 ± 0.045 true accuracy (sample size: 144–153).,,,,,,,,,,,,,,,
Compared to other methods (SRS,RCS,WCS,TWCS),"EP-TWCS showed higher or comparable accuracy with smaller sample sizes.""",,Quality problems due to unreliable or incomplete data sources and untimely updates.,,,,,,,,,
Automated extraction introduces bias and misinformation,especially with complex or ambiguous content.,,,,,,,,,,,,,,
Annotation errors from human misunderstanding cause omissions,misjudgments,and inconsistencies.,,,,,,,,,,,,,
"Difficulty maintaining accuracy
High time","completeness
computational","and timeliness during continuous updates.
and labor costs for comprehensive evaluation.""","The proposed method achieves stable and accurate evaluation results with only 67 samples when n ≥ 8, outperforming previous methods that require 102–196 samples.",,,,,,,,,,,,
Accuracy assessment results,confidence intervals,and sample sizes stabilize at n = 8,making it a critical threshold.,,,,,,,,,,,,
Integrating user needs into the evaluation process enhances the utility and applicability of knowledge graph assessments.,,,,,,,,,,,,,,,
Recommendations include flexible,"user-oriented evaluation and parameter adjustment to improve efficiency and accuracy.""",No information available,"Future research should address the lack of practical application of the “Fitness for Use” concept, refine quality dimension definitions to avoid overlap, and develop user-centered evaluation methods that flexibly adapt to diverse user needs rather than relying on unified accuracy metrics.",,,,,,,,,,,,
A Decade of Scholarly Research on Open Knowledge Graphs,"Turki Houcemeddine, Owodunni Abraham Toluwase, Hadj Taieb Mohamed Ali, Bile Ren´e Fabrice, Aouicha Mohamed Ben, Zouhar Vil´em",2023,reference-manager,,,,,,,"Retrieval of bibliographic metadata from Scopus for publications on open knowledge graphs (2013–2022), using a specific query to filter relevant papers.",,"What are the quantitative trends, main topics, and scholarly impacts of research on open knowledge graphs between 2013 and 2022, and how have these aspects evolved over time?","The paper analyzes the quantitative evolution of scholarly research on open knowledge graphs from 2013 to 2022 using bibliometric analysis based on Scopus data. It finds increasing importance and emerging concepts, especially since 2019, with a shift toward machine learning integration. The study highlights regional publication disparities and suggests rethinking collaboration models.","The research goal is to quantitatively analyze the evolution of open knowledge graph research (2013–2022); the approach uses bibliometric analysis; the principal finding is increasing importance and emerging concepts, with impactful work often originating from development and industry applications rather than research projects.",
Analysis of publication years,venues,citation counts,country affiliations,and keyword usage.,,,,,,,,,,,
"Construction and visualization of keyword co-occurrence networks using VoSViewer software.""
31% of papers were never cited; about 12% had 20+ citations; only 2% had over 100 citations.",No information available,"From 2013 to 2022, 4,445 scholarly articles on open knowledge graphs were published, increasing from 226 in 2013 to 751 in 2022.",,,,,,,,,,,,,
"No statistical significance (p-values) reported.""",The paper studies the quantitative evolution of scholarly research on open knowledge graphs from 2013 to 2022.,,,,,,,,,,,,,,
"It observes increasing importance and emerging concepts in the field.
Since 2019",there is more focus on integrating open knowledge graphs with advanced machine-learning techniques for improved coverage and quality.,,,,,,,,,,,,,,
"Breakthroughs often originate from development and industry applications rather than research projects.""",No information available,"Research on open knowledge graphs has grown significantly from 2013 to 2022, with increasing integration of advanced machine-learning techniques since 2019.",,,,,,,,,,,,,
Breakthroughs often originate from development and industry applications rather than academic research.,,,,,,,,,,,,,,,
Application-oriented journals and high-impact conferences are key publication venues.,,,,,,,,,,,,,,,
The community should reconsider collaboration between government,research,civil society,"and industry.""",Integration of open knowledge graphs with advanced machine learning techniques for improved coverage and quality.,,,,,,,,,,,
Addressing legal and technical barriers to achieve Findability,Accessibility,Interoperability,and Reusability (FAIR) in open knowledge graphs.,,,,,,,,,,,,
"Developing robust cybersecurity algorithms for social network analysis and language processing using open knowledge graphs.""","Future research should explore the evolution and current use of open knowledge graphs, especially their integration with advanced machine-learning techniques, the triple helix relation between government, research, and industry, and address legal and technical barriers related to FAIR principles. Existing studies have not emphasized these aspects.","The study design is a bibliometric study. It analyzes scholarly research on open knowledge graphs from 2013 to 2022 using data from the Scopus database. The study includes quantitative analysis, topic identification through keyword analysis, and co-occurrence network evaluation. No mention of randomization, control, blinding, or intervention.",,,,,,,,,,,,,
Cognitive boundary signals in the human medial temporal lobe shape episodic memory representation,"Zheng Jie, Gómez Palacio Schjetnan Andrea, Yebra Mar, Mosher Clayton, Kalia Suneil, Valiante Taufik A., Mamelak Adam N., Kreiman Gabriel, Rutishauser Ueli",2021,reference-manager,10.1101/2021.01.16.426538,,,,,,"Behavioral tasks: Subjects completed encoding (memorizing video clips), scene recognition (identifying previously seen frames), and time discrimination tasks.",,"How do different types of event boundaries within video clips (no boundary, soft boundary, hard boundary) affect memory encoding, scene recognition, and time discrimination in human subjects?","The study investigates how cognitive boundaries define the beginning and end of memory episodes. Using tasks involving video clip encoding, scene recognition, and time discrimination, the researchers analyzed neural responses to different boundary types. Findings show that neural activity changes at boundaries, influencing memory formation and recognition.","The research goal was to investigate how cognitive boundaries affect episodic memory; using a task with video clips containing different boundary types, the study found that neural state shifts at boundaries enhance recognition memory but impair temporal order memory, revealing a trade-off in memory organization.",
Electrophysiological recordings: Neural activity was recorded using depth electrodes implanted in brain regions such as the amygdala and hippocampus.,,,,,,,,,,,,,,,
"Online similarity ratings: Amazon Mechanical Turk workers rated visual similarity between target and foil frames.""",The data and analytic code supporting the findings will be deposited at Open Science Framework upon acceptance. No current source code is provided in the context.,"Significant differences (p < 0.001 or p < 0.01) were found between visual boundary types (NB, SB, HB) in luminance, contrast, complexity, entropy, LAB color channels, Alexnet fc7 Euclidean distance, and similarity.",,,,,,,,,,,,,
Correlation between neural population response and scene recognition differed by trial type and confidence,with significant thresholds at p < 0.01.,,,,,,,,,,,,,,
"Conclusions highlight that visual and neural reinstatement measures are statistically significant across conditions; all p-values for main effects are < 0.001 or < 0.01.""","Primary outcomes measured differences in visual attributes (luminance, contrast, complexity, entropy, LABL, LABA, LABB, Alexnet fc7 Euclidean distance, similarity) between pre- and post-boundary frames for three boundary types: NB (No boundary), SB (Soft boundary), HB (Hard boundary).",,,,,,,,,,,,,,
Significant effects found for all attributes across boundary types (all P < 0.001),except for some pairwise comparisons (e.g.,NB vs SB for luminance,P = 0.27).,,,,,,,,,,,,
ANOVA tested neural context reinstatement at boundaries and targets,covarying with confidence,boundary type,time distance,and similarity; significant main effects and interactions were identified (P < 0.05,,P < 0.01,,,,,,,Data and analytic code will be made available on the Open Science Framework after acceptance.,No information available,"P < 0.001)."""
No explicit study conclusions,key implications,or recommendations are provided in the context.,,,,,,,,,,,,,
"No information available regarding main findings or recommendations.""",,,,,,,,,,,,,,,
Digital twin for healthcare systems,Vallée Alexandre,2023,reference-manager,10.3389/fdgth.2023.1253050,,,,"Digital twin technology enables personalized treatment, predictive analytics, and optimized clinical operations, improving patient care and safety.",,No information available,,"How can digital twin technology transform healthcare systems by enhancing patient care, enabling predictive analytics, optimizing clinical operations, and supporting training and simulation to improve patient outcomes and operational efficiency?","The paper reviews the transformative role of digital twin technology in healthcare, focusing on its ability to enhance patient care, enable predictive analytics, and optimize clinical operations. Using real-time data integration and advanced analytics, digital twins improve patient outcomes and operational efficiency, supporting personalized and proactive healthcare.","The paper's main objective is to explore how digital twin technology can revolutionize healthcare by integrating real-time data, advanced analytics, and virtual simulations; its key method is reviewing applications and benefits; the principal finding is that digital twins enable personalized care, predictive analytics, and improved patient outcomes.",
Digital twins allow real-time monitoring,accurate diagnoses,proactive interventions,and empower patient participation in care.,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""",Digital twins enable enhanced patient care through real-time data integration and personalized treatment plans.,,,,,,,,,,,,,,
They support predictive analytics,allowing prediction of disease progression,identification of high-risk individuals,and recommendation of preventive measures.,,,,,,,,,,,,
Digital twins optimize clinical operations by streamlining workflows,improving resource allocation,and increasing operational efficiency.,,,,,,,,,,,,,
"No specific statistical values reported.""",Challenges related to data privacy,,,,,,,,,,,,,,
Interoperability issues,,,,,,,,,,,,,,,
Data quality concerns,,,,,,,,,,,,,,,
Ethical considerations,,,,,,,,,,,,,,,
High resource intensity,,,,,,,,,,,,,,,
"Integration with existing workflows
Need for validation",,,,,,,,,,,,,,,
Educational requirements,,,,,,,,,,,,,,,
Scalability challenges,,,,,,,,,,,,,,,
"Necessity for cultural shifts in adoption""","Digital twin technology can revolutionize healthcare by enabling personalized treatment, predictive analytics, and immersive training.",,,,,,,,,,,,,,
It empowers healthcare professionals with accurate diagnoses,real-time patient monitoring,and proactive interventions.,,,,,,,,,,,,,
Patients benefit from active participation and collaborative decision-making.,,,,,,,,,,,,,,,
Digital twins optimize resource allocation,streamline workflows,"and improve operational efficiency.""",Lack of standardized frameworks for developing and implementing digital twins in healthcare.,,,,,,,,,,,,
Insufficient integration of real-time,multi-source patient data for accurate digital twin modeling.,,,,,,,,,,,,,,
Limited research on ethical,privacy,"and regulatory challenges associated with digital twins in healthcare.""",,"Systematic review, literature review, rapid literature review, concept analysis (hybrid model), critical review, perspective, viewpoint, opinion articles, original research. No mention of randomized, double-blind, controlled, placebo-controlled, non-controlled, multi-site, retrospective, stratified, crossover, or parallel design.",,,,,,,,,,,
Key Management for Large-Scale Distributed Storage Systems,Lim Hoon Wei,2009,reference-manager,,,,,,,"Adoption of lightweight Diffie-Hellman key agreement techniques for session key establishment between client and storage device, providing forward secrecy.",,"How can lightweight key management techniques and a stronger security model, including forward secrecy, improve the efficiency, usability, and security of large-scale distributed file systems compared to existing proposals?","The paper addresses key management challenges in large-scale distributed file systems. Using lightweight Diffie-Hellman key agreement and short-lived keys, the proposed FSSA protocol improves security (including forward secrecy), usability, and key management. Results show FSSA matches Maat in computational cost but offers stronger security and simpler management.","The research goal is to improve key management and security in large-scale distributed file systems; the approach uses lightweight Diffie-Hellman-based authenticated key agreement with short-term keys (FSSA-KA); results show comparable computational cost to Maat protocols but with stronger security, forward secrecy, and simplified key management.",
Implementation and comparison of protocols (FSSA and Maat) in C++ using Crypto++ library,with performance measured on specified hardware.,,,,,,,,,,,,,,
Use of hybrid symmetric and asymmetric cryptographic methods,"combining authenticated key establishment and capability issuance.""",,"The proposed FSSA-KA protocol simplifies key management, achieves forward secrecy, and improves usability compared to Maat protocols.",,,,,,,,,,,,
FSSA-KA’s total computational cost per protocol run (4.086 ms) is comparable to Maat-I & Maat-II combined (3.953 ms),with individual times: Server 1.227 ms,Client 1.440 ms,Device 1.420 ms.,,,,,,,,,,,,
"No explicit p-values or statistical significance measures are reported.""",FSSA-KA protocol simplifies key management and provides forward secrecy using Diffie-Hellman key agreement.,,,,,,,,,,,,,,
FSSA-KA achieves stronger security,improved usability,and simplified key management compared to Maat.,,,,,,,,,,,,,
Computational cost (average time in ms for one protocol run):,,,,,,,,,,,,,,,
Server: FSSA-KA 1.227,Maat-I 1.232,Maat-II 1.316,,,,,,,,,,,,,
Client: FSSA-KA 1.440,Maat-I 1.346,Maat-II 0.085,,,,,,,,,,,,,
"Device: FSSA-KA 1.420
Total: FSSA-KA 4.086","Maat-I 1.375
Maat-I & II 3.953 (I) + 1.470 (II)",Maat-II 0.070,,,,,,,,,,,,,
"FSSA-KA is comparable to Maat in computational cost but offers stronger security and better usability.""",Only a single metadata server scenario is considered; extension to multiple servers is not implemented.,,,,,,,,,,,,,,
Real distributed file system implementation and analysis are left for future work.,,,,,,,,,,,,,,,
"Potential limitations or advantages of the approach are yet to be identified through real-world deployment.""",The proposed FSSA-KA protocol simplifies key management and achieves forward secrecy using lightweight Diffie-Hellman key agreement.,,,,,,,,,,,,,,
FSSA-KA offers stronger security,improved usability,and easier key management compared to Maat,while maintaining comparable computational cost (FSSA-KA: 4.086 ms vs. Maat-I & II: 3.953 ms and 1.470 ms).,,,,,,,,,,,,
Using only short-lived keys reduces the need for key revocation and enhances security in large-scale distributed file systems.,,,,,,,,,,,,,,,
"Future work includes implementing FSSA-KA in a real distributed file system to evaluate key management cost savings and identify further benefits or limitations.""",Lack of realistic key management assumptions in existing security proposals for large-scale file systems.,,,,,,,,,,,,,,
Need for a stronger security model that considers forward secrecy and the exposure of long-term secret keys.,,,,,,,,,,,,,,,
"Necessity to implement and evaluate the proposed approach in real distributed file systems to assess cost savings and identify further advantages or limitations.""","Future research should implement the proposed approach in a real distributed file system to analyze key management cost savings and identify additional advantages or limitations. Further investigation is needed into security impacts from hardware or software failures exposing secret keys, which current models do not explicitly address.",,,,,,,,,,,,,,
The Role of Physical Activity on Spatial and Temporal Cognitive Processing in Young Women,"Castillo-Escamilla Joaquín, Salvador-Viñas María del Mar, Cimadevilla José Manuel",2025,reference-manager,10.3390/brainsci15050431,,,,,,"Repeated measures ANOVA was used to analyze group (athlete/sedentary), type (faster/slower), and time (above/below 1000 ms) effects on accuracy and response times in the Time Comparison Task.",,"How does physical activity influence time processing in young female adults, and is this influence similar to its effects on spatial processing?","The study aimed to compare spatial and temporal cognitive performance between physically active and sedentary female students. Using repeated measures ANOVA on Time Comparison and Boxes Room tasks, it analyzed accuracy, response times, errors, and latencies. The methodology included group comparisons and correlation analyses to assess relationships between variables.","The research goal was to examine the benefits of physical activity on time processing in young female adults using the Time Comparison Task and spatial memory measures; the approach involved comparing physically active and sedentary groups, and the principal finding was that physical activity improved time processing, paralleling spatial memory results.",
Repeated measures ANOVA was also applied to mean errors and latencies in The Boxes Room Task across different trial blocks.,,,,,,,,,,,,,,,
"Pearson correlations and linear regression analyses were conducted to examine relationships between variables and tasks.""",The research's reproducibility is supported by a clear description of methods and analysis. The raw data can be requested from the authors. There is no mention of source code availability for the project.,"The Sport Group showed significantly better precision than the Sedentary Group in faster trials of the Time Comparison Task (M = 0.62 vs. M = 0.48; p = 0.021), but no difference in slower trials (p = 0.384).",,,,,,,,,,,,,
Main effects were found for time (F(1,40) = 23.48; p = 0.000; ηp2 = 0.370) and type (F(1,40) = 57.77; p = 0.000; ηp2 = 0.591),with significant group × type (F(1,40) = 5.66; p = 0.022; ηp2 = 0.124) and time × type (F(1,,40) = 35.01; p = 0.000; ηp2 = 0.467) interactions.,,,,,,,,,
"The study concludes that physical activity benefits time processing in young female adults
For faster trials","paralleling findings in spatial memory
accuracy was significantly worse for trials above 1000 ms compared to below 1000 ms and above 1000 ms slower trials (\*p < 0.05).","with all significant results at p < 0.05.""","In the Time Comparison Task, accuracy and response time (RT) were measured by trial type (faster/slower) and time difference (above/below 1000 ms).",,,,,,,,,,,,
Repeated measures ANOVA for RT showed a main effect of type (F(1,40) = 5.66; p = 0.022; ηp² = 0.124) and group (F(1,40) = 5.42; p = 0.025; ηp² = 0.120),but not time (F(1,40) = 3.22; p = 0.080).,,,,,,,,,,,
"The Sport Group was slower (M = 1573.94 ms; SD = 558.74) than the Sedentary Group (M = 1215.08 ms; SD = 362.97).
Regardless of group",faster trials required more response time (M = 1573.94 ms; SD = 685.36) than slower trials (M = 1215.08 ms; SD = 586.28).,,,,,,,,,,,,,,
In The Boxes Room Task,mean error and latency per block were measured and analyzed by repeated measures ANOVA.,,,,,,,,,,,,,,
Significant Pearson correlations were found between several measures within and between tasks (e.g.,BOX-B1 and BOX-B2: r = 0.655,p < 0.01; RT-BOX-B1 and RT-BOX-B2: r = 0.782,"p < 0.01).""","Only female, young adult participants were included, limiting generalizability.",,,,,,,,,,,
All participants were university students from a single institution.,,,,,,,,,,,,,,,
Differences between open-skill and closed-skill sports,and exercise intensity,were not addressed.,,,,,,,,,,,,,
Potential effects of gaming and academic performance were not examined.,,,,,,,,,,,,,,,
"Limited sample availability.""","Physical activity (PA) benefits time processing in young female adults, similar to its effects on spatial memory.",,,,,,,,,,,,,,
Future research should examine different types of sports,age,and sex influences on time processing.,,,,,,,,,,,,,
Investigating factors like gaming proficiency and academic performance is recommended.,,,,,,,,,,,,,,,
Combining behavioral and neurophysiological data (e.g.,"EEG) may reveal underlying neural mechanisms.""",Examine how different types of sports (sport typologies) influence the relationship between physical activity and time processing.,,,,,,,,,,,,,
Investigate the roles of age,sex,gaming proficiency,and academic performance in temporal processing.,,,,,,,,,,,,
"Integrate behavioral data with neurophysiological measures (like EEG) to reveal neural mechanisms underlying time processing.""","Future research should examine the influence of sport typology (open-skill vs. closed-skill), exercise intensity, age, and sex on time processing. The roles of gaming proficiency and academic performance, as well as integrating behavioral and neurophysiological (EEG) data, also warrant further investigation.","The study was a controlled, between-groups, observational design with two groups (physically active vs. sedentary). It used repeated measures (within–between interaction), was not randomized or blinded, and involved laboratory-based, computerized cognitive tasks with individual testing. No mention of placebo, crossover, or multi-site features.",,,,,,,,,,,,,
CONNECTED: leveraging digital twins and personal knowledge graphs in healthcare digitalization,"Carbonaro Antonella, Marfoglia Alberto, Nardini Filippo, Mellone Sabato",2023,reference-manager,10.3389/fdgth.2023.1322428,,,,,,"Use of Eclipse Hono for integrating and communicating with various IoT devices, supporting multiple protocols and custom data transformation.",,"How can a modular, standardized architectural framework like CONNECTED, integrating Personal Knowledge Graphs and Digital Twins, address challenges of data fragmentation, interoperability, and secure data management to improve healthcare applications and patient-centric services?","The paper proposes CONNECTED, a modular framework to integrate diverse healthcare data using modern standards, enabling general-purpose Digital Twins (DTs) supported by Personal Knowledge Graphs (PKGs). The methodology involves adopting technologies like Eclipse Hono, Apache Kafka, and Stardog. Results highlight PKGs’ effectiveness for semantic data modeling. The framework’s implications include improved patient-centric applications and data interoperability.","The research goal is to address healthcare data fragmentation by proposing CONNECTED, a multi-level architectural framework using Digital Twins and Personal Knowledge Graphs for data integration; the approach leverages technologies like Eclipse Hono, Apache Kafka, and Stardog, with results focused on enabling interoperable, patient-centric digital health ecosystems.",
Implementation of a distributed,asynchronous message bus using Apache Kafka to channel standardized data streams.,,,,,,,,,,,,,,
Storage and querying of data in a graph database (Stardog),"enabling knowledge inference through a reasoner.""","The original contributions are included in the article/supplementary material. The prototype is intended to be released as open-source software for validation, but no source code is currently provided. Further inquiries can be directed to the corresponding author.",The CONNECTED framework integrates diverse healthcare data sources using modern standards to create general-purpose Digital Twins (DTs) supported by Personal Knowledge Graphs (PKGs).,,,,,,,,,,,,
PKGs enable semantic search and knowledge inference,supporting complex,dynamic healthcare environments and informed decision-making.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""",,Integrity and validity of collected data must be guaranteed to ensure reliability.,,,,,,,,,,,,,
Difficulty in tracing clear boundaries between distinct patients if many PKGs are extracted from a single graph.,,,,,,,,,,,,,,,
Possible data duplication and inconsistencies if PKGs are physically separated.,,,,,,,,,,,,,,,
"Need for access-control policies for data privacy.
Complex legal",ethical,and regulatory issues due to healthcare data sensitivity.,,,,,,,,,,,,,
"Further research needed on technology selection and integration of computational models for simulation.""",CONNECTED is a modular framework integrating diverse healthcare data sources to create general-purpose digital twins (DTs) of patients.,,,,,,,,,,,,,,
Personal Knowledge Graphs (PKGs) are key for modeling heterogeneous data and enabling semantic search.,,,,,,,,,,,,,,,
Key challenges include ensuring data integrity,privacy,and implementing effective access-control policies.,,,,,,,,,,,,,
Future work involves selecting optimal technologies,addressing practical implementation issues,"and exploring simulation capabilities.""",Selecting and evaluating the best-suited technologies for implementing the proposed framework in real healthcare scenarios.,,,,,,,,,,,,
Addressing practical challenges and regulatory issues,such as differing privacy policies across organizations.,,,,,,,,,,,,,,
Investigating further extensions,"including integrating computational models into Digital Twins to enable simulation capabilities.""","Future research should focus on selecting optimal technologies for framework implementation, addressing practical challenges in real-world healthcare scenarios, investigating integration of computational models for simulation, ensuring data integrity, managing PKG boundaries, implementing access-control policies, and handling complex legal, ethical, and regulatory issues.",No information available,,,,,,,,,,,,
SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding,"Yang Ruiyi, Salim Flora D., Xue Hao",2024,reference-manager,10.1145/3589334.3645441,,,,,,"Construction of Simple Spatio-Temporal Knowledge Graph (SSTKG): Entities and their spatio-temporal relationships are modeled using embeddings and an “influence” metric, focusing on flexible, interpretable representations.",,"How can a Simple Spatio-Temporal Knowledge Graph (SSTKG) framework be designed to efficiently and interpretably model dynamic spatio-temporal relationships among entities, enabling accurate prediction and knowledge graph completion across diverse real-world datasets?","The paper proposes the Simple Spatio-Temporal Knowledge Graph (SSTKG) framework to improve recommendation and prediction tasks by modeling both spatial and temporal relations using three types of embeddings and """"influence."""" Experiments on real-world datasets show SSTKG outperforms baseline models and offers strong interpretability. Future work aims to refine and enhance SSTKG.","The research goal is to develop a Simple Spatio-Temporal Knowledge Graph (SSTKG) framework using embeddings and """"influence"""" to model entity relations; the approach integrates spatial-temporal data for prediction tasks, and results show SSTKG outperforms baseline models in accuracy and interpretability on real-world datasets.",
Embedding-based Recommendation: Entities and relations are converted into embeddings for use in recommendation algorithms,enhancing adaptability and interpretability.,,,,,,,,,,,,,,
Comparative Evaluation and Case Study: SSTKG is benchmarked against models like SVR,LSTM,GRU,T-GCN,and ST-GCN,,"and a case study is conducted to validate interpretability using real-world data.""",,,,,,,,"The SSTKG model outperformed SVR, GRU, and LSTM, and showed superior results over T-GCN and ST-GCN in acc@15, RMS, and RSD metrics on the Spend-Ohio and TFNSW datasets.",
T-test results showed no statistically significant difference between predictions before and after masking related entities (p-values: 0.9998975,0.999873,0.6717662).,,,,,,,,,,,,,
"SSTKG demonstrated strong interpretability and effectively modeled both spatial and temporal relationships for prediction tasks.""","The SSTKG model was evaluated using the Spend-Ohio dataset, focusing on interpretability and prediction accuracy.",,,,,,,,,,,,,,
Entities close to the sample store had higher influence values; distant entities had minimal influence.,,,,,,,,,,,,,,,
Masking influential entities (A,B,C) changed prediction results,as shown in Table 7.,,,,,,,,,,,,
t-test results (Table 8): p-values (0.9998975,0.999873,0.6717662) led to rejecting the null hypotheses,confirming significant prediction changes after masking.,,,,,,,,,,,,
SSTKG outperformed SVR,GRU,LSTM,T-GCN,and ST-GCN in acc@15,,RMS,,,,,,,,Strict categorization by NAICS code may cause excessive fragmentation of entity types.,"and RSD metrics on the Spend-Ohio dataset."""
Dynamics of relationships can vary significantly based on spatial and temporal factors,making modeling complex.,,,,,,,,,,,,,,
Detailed numerical time and location are hard to transfer as distinct entities.,,,,,,,,,,,,,,,
"Future work is needed to refine the SSTKG construction algorithm and enhance model dynamism.""","The proposed SSTKG framework effectively models both spatial and temporal relations, outperforming baseline models (SVR, GRU, LSTM) and even T-GCN and ST-GCN in key metrics.",,,,,,,,,,,,,,
SSTKG demonstrates strong interpretability and prediction accuracy,especially in real-world datasets.,,,,,,,,,,,,,,
Future work includes refining the SSTKG algorithm,enhancing its ability to capture entity mobility,"and balancing model size with efficiency.""",Refining the SSTKG construction algorithm.,,,,,,,,,,,,
Enhancing dynamism in SSTKG to reflect entities’ temporal mobility,such as shifting user locations.,,,,,,,,,,,,,,
"Balancing model size and efficiency.""","Future research should focus on refining the SSTKG construction algorithm, enhancing the model’s ability to reflect dynamic entity mobility (such as shifting user locations), and balancing model size with efficiency. These directions address current limitations in adaptability, dynamism, and computational efficiency.",,,,,,,,,,,,,,
Temporal Knowledge Graph Completion: A Survey,"Cai Borui, Xiang Yong, Gao Longxiang, Zhang He, Li Yunfeng, Li Jianxin",2023,reference-manager,10.24963/ijcai.2023/734,,,,,,Time-included tensor decomposition: Represents the knowledge graph as a 4-way tensor to learn hidden patterns using tensor decomposition techniques.,,"How can temporal knowledge graph completion (TKGC) methods effectively incorporate time validity, historical context, attention mechanisms, heuristic-based relevance, and temporal logical rules to improve link prediction accuracy and interpretability in dynamic knowledge graphs?","This paper surveys recent advances in temporal knowledge graph completion (TKGC), focusing on methods that incorporate time validity for link prediction. It categorizes TKGC approaches, highlights improved prediction with temporal information, discusses open challenges, and suggests future research directions like using external knowledge and time-aware negative sampling.","The research goal is to survey recent advances in temporal knowledge graph completion (TKGC); the approach organizes TKGC methods by how they incorporate temporal validity for link prediction; the principal finding is that while progress is significant, challenges remain, especially in prediction accuracy and dataset limitations.",
Dynamic embedding: Uses dynamic representations to capture how entities and relationships change over time.,,,,,,,,,,,,,,,
"Reasoning with historical context: Predicts missing links by analyzing relevant historical facts related to the query.""",,"TKGC methods are categorized into six types, including time-included tensor decomposition, time-based transformation, dynamic embedding, graph snapshots, reasoning with historical context, and temporal logical rules.",,,,,,,,,,,,,
Significant progress and promising results have been achieved on benchmark datasets,but prediction accuracy—especially on GDELT—remains limited due to dataset issues.,,,,,,,,,,,,,,
"No explicit quantitative results or statistical significance (p-values) are provided in the context.""",The primary outcome measured is the accuracy of TKGC methods in predicting missing links.,,,,,,,,,,,,,,
Results are evaluated using Hits@k,Mean Ranking (MR),and Mean Reciprocal Ranking (MRR).,,,,,,,,,,,,,
"No specific statistical values or measured effects are provided in the context.""","Limited prediction accuracy, especially on the GDELT dataset.",,,,,,,,,,,,,,
In-balanced distribution of facts,leading to a long tail structure for entities and relations.,,,,,,,,,,,,,,
Semantics of entities/relations (names and types) are largely ignored by existing methods.,,,,,,,,,,,,,,,
"Time-aware negative sampling is rarely explored in TKGC.""","TKGC methods have advanced link prediction by incorporating temporal validity, but challenges remain, especially with datasets like GDELT.",,,,,,,,,,,,,,
Incorporating external knowledge and semantics can improve prediction accuracy and representation learning.,,,,,,,,,,,,,,,
Time-aware negative sampling and reasoning with historical context are underexplored but promising directions.,,,,,,,,,,,,,,,
"Temporal logical rules enhance explainability and coverage for link prediction.""","Incorporating external knowledge to improve prediction accuracy, especially by enriching structural and temporal information.",,,,,,,,,,,,,,
Developing time-aware negative sampling methods,as current approaches rarely explore this in temporal knowledge graph completion (TKGC).,,,,,,,,,,,,,,
Utilizing semantics of entities and relations,such as names and types,"through pre-trained language models for better link prediction.""","Future research should focus on improving prediction accuracy by incorporating external knowledge, addressing dataset issues like imbalanced distributions, developing time-aware negative sampling methods, leveraging semantics of entities/relations, and using heuristic or domain knowledge for relevance measurement. These areas remain open challenges in temporal knowledge graph completion (TKGC).",,,,,,,,,,,,
Matching disparate dimensions for analytical integration of heterogeneous data sources,"Korobko Anna, Korobko Aleksei",2019,reference-manager,10.1145/3297662.3365809,,,,,,"Integration of OLAP (On-Line Analytical Processing), FCA (Formal Concept Analysis), and semantic analysis methods to reconcile and merge heterogeneous data sources.",,How can an original integration methodology based on the integral analytical model (IAM) be developed to automatically reconcile and merge heterogeneous data sources for enhanced online analytical processing (OLAP)?,"The paper aims to develop a new integration methodology for heterogeneous data using an Integral Analytical Model (IAM) and a Simplified Multidimensional Model (SMM). It combines OLAP, FCA, and semantic analysis to match and merge disparate data dimensions. Results support automated integration and exploratory analysis, enabling self-service analytical queries.","The research goal is to develop an original integration methodology for heterogeneous data using a simplified multidimensional model and IAM; the approach uses reference dimensions and matching algorithms; results show successful merging of disparate sources, supporting automatic data integration and exploratory analysis.",
Use of a matching policy and algorithm to map and merge disparate dimensions based on names,alternative names,and descriptions.,,,,,,,,,,,,,
"Application of open source libraries to convert documents into TF-IDF (Term Frequency-Inverse Document Frequency) matrices with 3-grams and compute cosine similarity for dimension matching.""","The research provides a Python code fragment for the matching algorithm (Figure 2) and states that the implementation uses open source libraries for data structures and similarity computation. However, there is no explicit mention of a public source code repository or full project code.",The proposed integration methodology enables automatic mapping and merging of heterogeneous data sources using textual features (alternative names and descriptions) and cosine similarity of TF-IDF 3-gram features.,,,,,,,,,,,,,
Quantitative results show a significant similarity score (e.g.,FOKEI and OKEI: 0.600247262; MOSTATUS: 0.649865971),supporting merging of these dimensions.,,,,,,,,,,,,,
The approach forms a formal foundation for automatic merging,"but human affirmation is required; further research is needed to enhance attribute matching and user interface development. No explicit p-values reported.""",The proposed algorithm integrates heterogeneous sources using additional textual features of dimensions (alternative names and descriptions).,,,,,,,,,,,,,
Implementation uses TF-IDF features (with 3-grams) and cosine similarity to compute similarity between dimension samples.,,,,,,,,,,,,,,,
A score matrix fragment is presented,showing precise similarity values for various dimensions (e.g.,FOKEI: 0.358797982,OKEI: 0.075714473).,,,,,,,,,,,,
The approach demonstrates the ability to match dimensions between sources,"aiding integration into the IAM system.""","The technology requires a specialist to design a common storage scheme, limiting adaptability to rapid data growth and """"hot"""" integration of new sources.",,,,,,,,,,,,,
Results of matching dimensions must be confirmed by humans before finalizing.,,,,,,,,,,,,,,,
"Further research is needed on intelligent merging algorithms and user interface development.""",The new integration methodology enables automatic merging of heterogeneous data sources using shared dimensions based on names and descriptions.,,,,,,,,,,,,,,
The proposed algorithm supports exploratory analysis and self-service integration,but human validation is recommended before committing merged dimensions.,,,,,,,,,,,,,,
"Further research is needed to improve matching algorithms and develop user interfaces for querying the integral analytical model (IAM).""",Further research is needed on intelligence merging algorithms.,,,,,,,,,,,,,,
Future studies will focus on intellectualizing the matching of dimension attributes.,,,,,,,,,,,,,,,
"Development of a user interface to support user queries to the Integral Analytical Model (IAM) is required.""","Future research should focus on developing more advanced intelligence merging algorithms, enhancing the intellectualization of matching dimension attributes, and creating improved user interfaces to support queries to the Integral Analytical Model (IAM). These areas address current limitations and aim to improve automatic integration and user interaction.",,,,,,,,,,,,,,
Local-Global History-aware Contrastive Learning for Temporal Knowledge Graph Reasoning,"Chen Wei, Wan Huaiyu, Wu Yuting, Zhao Shuyuan, Cheng Jiayaqi, Li Yuxin, Lin Youfang",2023,reference-manager,,,,,,,"Global Entity-Aware Attention Encoder: Models global historical facts related to queries, capturing important historical patterns beyond recent events.",,How can both global and local historical information be effectively captured and filtered to improve the accuracy and robustness of temporal knowledge graph extrapolation for predicting future unknown facts?,"The paper aims to improve future fact prediction in temporal knowledge graphs (TKG) by integrating both global and local historical information. It proposes LogCL, which uses entity-aware attention encoders and a local-global contrast module. Experiments show LogCL achieves higher prediction accuracy, highlighting the importance of combining global and local patterns.",The research goal is to improve temporal knowledge graph (TKG) extrapolation by better integrating local and global historical information; the approach uses a Local-global history-aware Contrastive Learning model (LogCL) with entity-aware attention and a contrast module; results show LogCL achieves more robust and accurate predictions than state-of-the-art methods.,"Tags: Temporal Knowledge Graph (TKG), TKG extrapolation, global historical information, local historical information, entity-aware attention, embedding, attention encoder, recurrent encoder, prediction, historical fact patterns, mean pooling, time semantic component, subgraph sample."
Local Entity-Aware Attention Recurrent Encoder: Focuses on modeling recent local history at adjacent timestamps to capture temporal dependencies.,,,,,,,,,,,,,,,
"Local-Global Query Contrast Module: Integrates global and local historical information to enhance prediction robustness through contrastive training.""",,"The proposed LogCL model outperforms variants and baselines on ICEWS14, ICEWS18, and ICEWS05-15 datasets, achieving MRRs of 48.87, 35.67, and 57.04, and Hits@1 of 37.76, 24.53, and 46.07, respectively.",,,,,,,,,,,,,
Ablation studies show that removing the entity-aware attention mechanism significantly reduces prediction accuracy.,,,,,,,,,,,,,,,
"No explicit p-values or statistical significance measures are reported in the context.""
On ICEWS14
On ICEWS18
On ICEWS05-15","Primary outcomes were measured using mean reciprocal rank (MRR) and Hits@k (k=1,3,10), reported as percentages.
LogCL achieved: MRR 48.87
LogCL achieved: MRR 35.67
LogCL achieved: MRR 57.04","Hits@1 37.76
Hits@1 24.53
Hits@1 46.07","Hits@3 54.71
Hits@3 40.32
Hits@3 63.72","Hits@10 70.26.
Hits@10 57.74.
Hits@10 77.87.",,,,,,,,,,,
Comparative models (e.g.,DisMult,ComplEx) showed lower performance across datasets.,,,,,,,,,,,,,
Ablation studies showed performance drops when removing components (e.g.,entity-aware attention).,,,,,,,,,,,,,,
"Sensitivity analysis and noise studies were conducted but specific statistical values are not provided in the context.""","Importance of historical information related to the query is often neglected, impairing prediction accuracy.",,,,,,,,,,,,,,
Existing methods lack effective filtering of irrelevant knowledge graph (KG) snapshots for queries.,,,,,,,,,,,,,,,
Weak anti-noise ability: models are vulnerable to noise in input data,leading to degraded or incorrect predictions.,,,,,,,,,,,,,,
"Robustness to noise is rarely considered in TKG reasoning research.""","LogCL outperforms state-of-the-art methods on four benchmark datasets, with MRR improvements up to 7.9%.",,,,,,,,,,,,,,
Modeling recent local entities and relations evolution is more effective than using only long historical information.,,,,,,,,,,,,,,,
The entity-aware attention mechanism significantly enhances prediction accuracy.,,,,,,,,,,,,,,,
"Recommendation: Combine both global and local historical information for best results in TKG extrapolation tasks.""","Existing methods often focus only on either global or local historical patterns, lacking effective integration of both.",,,,,,,,,,,,,,
Current approaches tend to overlook query-related historical information,reducing prediction accuracy.,,,,,,,,,,,,,,
"There is a need for better methods to robustly combine global and local historical information for improved future fact forecasting.""",Future research should address filtering irrelevant KG snapshots based on queries to improve prediction accuracy and enhance model robustness against noise. There is also a need to better integrate both global and local historical information and to overcome current limitations in capturing important historical patterns.,,"The discussion concludes that combining global and local historical information using LogCL improves entity prediction accuracy in temporal knowledge graph extrapolation tasks, as demonstrated by superior results across multiple datasets and analyses of model components and parameters.","The objectives of the study are to improve forecasting of future unknown facts (extrapolation) in temporal knowledge graphs (TKG) by comprehensively modeling both global repeating/cyclic historical patterns and local adjacent fact evolution patterns, aiming for more accurate predictions through combined global and local historical information.",,,,,,,,,,,
A Survey on Collaborative Mechanisms Between Large and Small Language Models,"Chen Yi, Zhao JiaHao, Han HaoHao",2025,reference-manager,,,,,,,"Model Fusion and Result Integration: Techniques like weight averaging, ensemble learning, and majority voting are used to merge model parameters or outputs, creating unified systems that leverage the strengths of both LLMs and SLMs.",,"How can collaborative mechanisms between large and small language models be designed and optimized to leverage their complementary strengths, address key challenges such as efficiency, consistency, and privacy, and enable broader, more effective applications across diverse real-world scenarios?","The paper explores collaborative mechanisms between large and small language models, aiming to create smarter, more integrated systems. It reviews methods like dynamic task allocation, structured communication, model fusion, and state synchronization. Key findings highlight improved efficiency and flexibility but note challenges in consistency, evaluation, and security. Future research directions are outlined.","The paper's research goal is to systematically review large-small language model collaboration; it uses a survey approach to analyze collaboration modes and key technologies, and concludes that such collaboration can enhance efficiency and applicability but faces challenges in efficiency, consistency, evaluation, and security.",
State Synchronization and Context Management: Synchronizing internal states (like memory and attention weights) and managing shared contextual information between models,especially in multi-turn dialogues or sequential tasks.,,,,,,,,,,,,,,
"Task Allocation and Intelligent Routing: Assigning tasks and directing data intelligently between LLMs and SLMs to optimize collaboration and performance.""",,"The paper identifies significant progress and potential in LLM-SLM (large and small language model) collaboration, especially for real-time, privacy-sensitive, and energy-constrained scenarios.",,,,,,,,,,,,,
Key challenges include collaboration efficiency,inter-model consistency,lack of standardized evaluation metrics,and privacy/security risks; no quantitative results or p-values are reported.,,,,,,,,,,,,
Future research should focus on dynamic collaboration strategies,privacy-preserving protocols,"and robust conflict resolution mechanisms for reliable outputs.""","No explicit primary outcomes, results, or measured effects (including statistical values) are reported in the provided context.",,,,,,,,,,,,
The context discusses challenges,open issues,and communication methods in LLM-SLM collaborative systems,"but does not present specific experimental results or statistical measurements.""","Increased deployment and operational complexity due to multiple components (LLM, SLM, router) needing individual and interaction monitoring.",,,,,,,,,,,
Difficulty in pinpointing faults,requiring advanced logging and root cause analysis.,,,,,,,,,,,,,,
Lack of standardized deployment patterns and best practices for collaborative systems.,,,,,,,,,,,,,,,
Absence of comprehensive monitoring frameworks for multi-model environments.,,,,,,,,,,,,,,,
Need for automated anomaly detection and fault diagnosis tools.,,,,,,,,,,,,,,,
Challenges in managing and updating individual components in distributed deployments.,,,,,,,,,,,,,,,
Privacy risks from data flow between different models.,,,,,,,,,,,,,,,
Potential for bias amplification and security vulnerabilities in collaborative systems.,,,,,,,,,,,,,,,
Additional latency and costs from task routing and inter-model communication.,,,,,,,,,,,,,,,
Difficulty ensuring consistency in output style,knowledge scope,and factual accuracy across models.,,,,,,,,,,,,,
Lack of standardized evaluation metrics and benchmarking platforms for collaborative effectiveness,efficiency,and cost.,,,,,,,,,,,,,
"Challenge in balancing privacy preservation with collaborative performance.""","Combining Large Language Models (LLMs) and Small Language Models (SLMs) can improve AI efficiency, economy, and applicability.",,,,,,,,,,,,,,
Key collaboration modes include pipeline,hybrid/routing,auxiliary/enhancement,knowledge distillation-driven,and integration/fusion.,,,,,,,,,,,
Main challenges: collaboration efficiency,inter-model consistency,evaluation metrics,and security/privacy.,,,,,,,,,,,,
"Future research should focus on dynamic collaboration strategies and standardized interfaces.""","Develop privacy-preserving data sharing protocols and formalize privacy guarantees for LLM-SLM collaboration, balancing privacy with collaborative performance.",,,,,,,,,,,,,,
Create robust knowledge fusion and conflict resolution mechanisms to handle conflicting outputs from LLMs and SLMs,ensuring reliable and trustworthy results.,,,,,,,,,,,,,,
Establish comprehensive monitoring frameworks,standardized deployment patterns,"and automated fault diagnosis tools for collaborative systems.""","Future research should focus on dynamic collaboration strategies using reinforcement learning and meta-learning, privacy-preserving data sharing protocols, formal privacy guarantees, robust security and attack-resistant mechanisms, standardized evaluation metrics, comprehensive monitoring frameworks, automated fault diagnosis tools, and best practices for deployment and management of LLM-SLM collaborative systems.",,,"Research on collaboration between large and small language models has advanced significantly, but future work must address challenges in efficiency, consistency, evaluation, and privacy to enable smarter, more integrated, and broadly applicable collaborative systems.",,,,,,,,,"The objectives of the study are to develop privacy-preserving data sharing protocols for LLM-SLM collaboration, formalize privacy guarantees in hybrid systems, investigate trade-offs between privacy and collaborative performance, audit data flows for leakage, and create robust knowledge fusion and conflict resolution mechanisms."
Improving Temporal Reasoning of Large Language Models via Recounted Narratives,"Zhang Xinliang Frederick, Beauchamp Nick, Wang Lu",2024,reference-manager,,,,,,,"NARRATIVE-OF-THOUGHT (NOT): Converts unordered event sets into Python classes and prompts language models to generate temporally grounded narratives, then sorts events into a temporal graph.",,"How can small, open-weights large language models effectively perform temporal reasoning and temporal graph generation under budget constraints, and what strategies can bridge the performance gap between these models and proprietary large models?","The paper investigates how different meta prompt types, input formats, and models affect the quality of reference narratives for temporal graph generation. Using F1 and GED scores, it finds that the meta prompt type—especially Simple Report—has the most impact. Effective narratives are concise, simple, and factual.","The main objective of the paper is to improve temporal reasoning in narrative generation by using concise, simple, and factual reference narratives; the key method involves evaluating different prompting strategies and meta prompt types, and the principal finding is that 5-shot examples optimize performance, with human baseline surpassed by 30 F1 points.",
Structural Representation: Frames temporal reasoning as a code completion task using Python methods.,,,,,,,,,,,,,,,
Narrative-aware Demonstrations: Uses concise,"simple narrative examples to guide model outputs.""",,"5-shot prompting is optimal for temporal reasoning; performance plateaus after 5 shots, likely due to long-context challenges.",,,,,,,,,,,,
Simple Report-style GPT-4 narratives achieve the highest F1 scores (up to 65.7 on ProScript),emphasizing conciseness,simplicity,and factuality.,,,,,,,,,,,,
"Medium-to-high alignment (72.8%) was found between automated and human evaluations; statistical significance (p-values) not reported.""",Primary outcomes were measured using F1 (higher is better) and GED (lower is better).,,,,,,,,,,,,,,
NOT-augmented LLAMA3-8B achieved best overall F1 (63.5,3-shot variant) and best GED on Schema-11.,,,,,,,,,,,,,,
NOT boosts F1 over its base model by 16% to 71%,sometimes improving GED.,,,,,,,,,,,,,,
Fine-tuned LLAMA3-8B achieved 71.9 F1 and 1.40 GED on ProScript,outperforming GPT-4 (63.9 F1,1.64 GED).,,,,,,,,,,,,,
"Average alignment between automated responses and human inspections was 72.8%.""","Only three evaluation benchmarks were used, which do not cover all possible domains (e.g., healthcare, biomedical).",,,,,,,,,,,,,,
Human baseline comparison was conducted only on the ProScript dataset,not on the other two benchmarks.,,,,,,,,,,,,,,
"Further research is needed to test generalizability and credibility.""","Small language models (LLMs) perform much worse than GPT-4 in temporal reasoning tasks, reaching only 40–60% of its F1 scores.",,,,,,,,,,,,,,
Chain-of-thought (CoT) prompting does not always help and can even lower performance in temporal reasoning.,,,,,,,,,,,,,,,
Simple Report-style prompts with GPT-4 give the best results due to their conciseness,simplicity,and factual accuracy.,,,,,,,,,,,,,
"Using NOT can help small LLMs match or surpass GPT-3.5 performance.""","There is a significant gap in temporal reasoning performance between AI systems and humans, and between proprietary large language models (LLMs) and open-weights, small LLMs.",,,,,,,,,,,,,,
Current benchmarks are limited in domain coverage; future work should extend evaluation to more diverse domains like healthcare and biomedical.,,,,,,,,,,,,,,,
Human baseline comparisons are incomplete,"as evaluations were only conducted on one dataset; further human evaluations on additional datasets are needed.""","Future research should investigate temporal graph generation (TGG) in more diverse domains, such as healthcare and biomedical fields, to test generalizability. Additional human baseline comparisons across all datasets are needed. Further exploration of methods to improve small LLMs’ global temporal reasoning without training is also recommended.",,"The discussion concludes that concise, simple, and factual reference narratives lead to the best performance, with 5-shot examples being optimal for temporal reasoning before performance plateaus.",,No information available,,,,,,,,,
Dealing With Data Heterogeneity in a Data Fusion Perspective,"Mandreoli Federica, Montangero Manuela",2019,reference-manager,10.1016/b978-0-444-63984-4.00009-0,,,,,,Schema mapping: Addresses structural differences between data sources to solve heterogeneity at the structure level.,,How can systematic computer science solutions address data heterogeneity challenges in integrating disparate life science data sources to improve knowledge and support novel application scenarios such as big data and crowdsourcing?,"The paper reviews computer science-based models, methodologies, and algorithms for addressing data heterogeneity in life science data integration. It overviews key data sources, discusses schema mapping, entity resolution, and data fusion techniques, highlights recent advances, and concludes that systematic approaches are essential for effective integration and future research.","The research goal is to address data heterogeneity in life science data integration using computer science solutions; the approach reviews models, methodologies, and algorithms for entity resolution and data fusion; the principal finding is that systematic computer science methods can effectively tackle heterogeneity challenges in life science applications.",
Entity resolution: Groups different descriptions of the same real-world entity to solve heterogeneity at the value level.,,,,,,,,,,,,,,,
"Data fusion: Combines various descriptions into a single representation to resolve value-level heterogeneity.""",,"No single truth inference algorithm consistently outperforms others across datasets; 17 algorithms were compared on five datasets, showing instability and no clear winner.",,,,,,,,,,,,,
Integrating heterogeneous,publicly available life science data is essential for knowledge discovery,especially in health informatics and tumor growth research.,,,,,,,,,,,,,
Main objectives in addressing data heterogeneity are completeness,conciseness,"and correctness of unified data; no p-values or quantitative statistical significance reported.""","No primary outcomes, results, or measured effects (including statistical values) are explicitly stated in the provided context.",Algorithms for the Truth Inference problem are not stable across datasets; no single algorithm consistently outperforms others.,,,,,,,,,,,
The Truth Inference problem has not been fully solved.,,,,,,,,,,,,,,,
"Further research is suggested to address these unresolved issues.""",Computer science solutions effectively address data heterogeneity in data integration for life sciences.,,,,,,,,,,,,,,
A systematic approach is recommended for integrating disparate data sources.,,,,,,,,,,,,,,,
Major challenges remain in providing valuable answers for advanced applications.,,,,,,,,,,,,,,,
The truth inference problem is unresolved; no single algorithm consistently outperforms others,"indicating a need for further research.""",Handling data heterogeneity and inconsistency in Big Data integration remains a significant challenge.,,,,,,,,,,,,,
The Truth Inference problem in crowdsourcing lacks stable,consistently effective algorithms across datasets.,,,,,,,,,,,,,,
"Integrating textual data with nontextual data (such as images) presents new research opportunities and challenges.""","Future research should address the instability of truth inference algorithms across datasets, as no single method consistently outperforms others. Additional directions include handling Big Data integration, leveraging crowdsourcing, addressing privacy issues, and integrating textual with nontextual data such as images.",,"The chapter reviews computer science-based models, methodologies, and algorithms for addressing data heterogeneity in life science data integration, highlights main challenges, and promotes systematic solutions such as schema mapping, entity resolution, and data fusion.","The objectives are to address data heterogeneity by ensuring completeness (amount of data), conciseness (uniqueness of object representations), and correctness (conformity to the real world) in the unified view provided to users and applications.",,,,,,,,,,,
Bring Privacy To The Table: Interactive Negotiation for Privacy Settings of Shared Sensing Devices,"Zhou Haozhe, Goel Mayank, Agarwal Yuvraj",2024,reference-manager,10.1145/3613904.3642897,,,,,,"Iterative design process: The study used repeated cycles of designing, building, and evaluating the negotiation system, ThingPoll.",,"How can a negotiation system like ThingPoll be designed and evaluated to effectively mediate privacy and functionality preferences among smart home users, compared to non-negotiation approaches such as Veto Vote and Majority Vote?","The paper investigates ThingPoll, a negotiation system for privacy settings in shared smart homes. Using simulated scenarios and a two-phase user study with 30 participants, it evaluates user workload, experiences, and willingness to adopt such apps. Findings suggest ThingPoll is practical, though results are exploratory due to sample size limitations.","The research goal was to design and evaluate ThingPoll, an IoT privacy negotiation system; using an iterative approach with formative and summative studies, the principal finding highlights the need for cooperative, efficient, and fair privacy negotiations between smart home visitors and homeowners.",
Formative and summative studies: Researchers conducted an initial formative study (N=12) to observe human negotiation,followed by a summative evaluation (N=30) of ThingPoll’s practicality and efficiency.,,,,,,,,,,,,,,
"Simulated negotiation scenarios: Negotiation situations were simulated to reflect real-world contexts due to recruitment and device constraints.""",,"The Negotiation approach achieved the highest satisfaction rate at 83.3% overall (70% homeowners, 90% guests), outperforming Majority Vote and Veto Vote approaches.",,,,,,,,,,,,,
Participants valued the Negotiation approach for allowing everyone to express privacy preferences,though it required more time and effort.,,,,,,,,,,,,,,
"No p-values or statistical significance data are provided in the context.""","Negotiation approach achieved the highest satisfaction rate: overall 83.3% (homeowners 70%, guests 90%).",,,,,,,,,,,,,,
High satisfaction attributed to ability to share needs and concerns.,,,,,,,,,,,,,,,
No explicit statistical values (e.g.,"p-values) for comparisons between approaches provided in the context.""",Simulated negotiation scenarios used due to difficulty recruiting real-world participants with compatible devices and privacy disclosure.,,,,,,,,,,,,,
Limited to participants familiar with IoT devices; unfamiliar users may have different perspectives.,,,,,,,,,,,,,,,
Small sample size (30 participants); findings are indicative,not definitive.,,,,,,,,,,,,,,
Assumed homeowners are sole device owners; real-world ownership/social relationships can be more complex.,,,,,,,,,,,,,,,
Assumed complete and accurate device behavior disclosure,which may not reflect reality.,,,,,,,,,,,,,,
Assumed participants made well-informed privacy choices; actual awareness may be limited.,,,,,,,,,,,,,,,
Social dynamics and design requirements may need adaptation for varied living arrangements.,,,,,,,,,,,,,,,
Unequal opportunity for users to express preferences during negotiation.,,,,,,,,,,,,,,,
Explaining preferences was time-consuming and sometimes unnecessary.,,,,,,,,,,,,,,,
Participants sometimes lost track of each other's preferences,"prolonging negotiations.""",Negotiation using ThingPoll helps balance privacy and functionality but can be time-consuming and may lead to unequal participation.,,,,,,,,,,,,,
Simulated scenarios were used due to recruitment challenges; real-world studies are recommended for deeper insights.,,,,,,,,,,,,,,,
The study's findings are exploratory due to a small,IoT-familiar participant sample.,,,,,,,,,,,,,,
Design improvements should address workload,equity in participation,"and preference management.""",Determining the acceptable balance and ethical boundaries of power distribution between homeowners and guests remains an open question.,,,,,,,,,,,,
The need for asynchronous negotiation mechanisms and exploring the impact of allowing more freedom in expressing preferences during privacy negotiations.,,,,,,,,,,,,,,,
Conducting in-the-wild studies to capture real-world negotiation dynamics,"as current findings are based on simulated scenarios and a limited participant pool.""","Suggested future research directions include: exploring the ethical balance of power between homeowners and guests, investigating social and cultural influences on negotiation, conducting in-the-wild studies, including participants unfamiliar with IoT, increasing sample size, enabling asynchronous negotiation, and examining the impact of allowing freer expression during negotiations.",,"Negotiations for shared IoT device privacy are time-consuming due to preference explanations, can lead to unequal participation, and may benefit from effective mediation and better management of user preferences to improve efficiency and equity.",,Objectives:,,,,,,,,,
The study aims to (1) observe how humans negotiate smart home privacy configurations,(2) inform the design of the negotiation system ThingPoll,(3) evaluate ThingPoll’s practicality,effectiveness,and user satisfaction,,"and (4) compare negotiation with baseline non-negotiation approaches.""",,,,,,,,,
"A Survey on Temporal Knowledge Graph: Representation Learning and Applications
Reinforcement learning-based methods: Model prediction as a Markov Decision Process","Cai Lianshang, Mao Xin, Zhou Yuhao, Long Zhaoguang, Wu Changxu, Lan Man
where an agent learns actions through rewards to improve model transparency.",2024,reference-manager,,,,,,,"Subgraph reasoning-based methods: Use explainable frameworks like xERTE to predict future facts by constructing subgraphs through iterative sampling and pruning, enhancing interpretability.",,"What are the current methods, challenges, and future directions in temporal knowledge graph representation learning (TKGRL) and its applications, with a focus on interpretability, information fusion, and comprehensive categorization of existing approaches?","The paper aims to provide a comprehensive summary of temporal knowledge graph representation learning (TKGRL) methods and applications. It categorizes methods, including interpretability-based approaches, analyzes their strengths and weaknesses, reviews applications, and highlights future research directions such as scalability, interpretability, and information fusion.","The paper's main objective is to review temporal knowledge graph representation learning (TKGRL), summarizes key methods like subgraph reasoning and large language models, and concludes that enhancing interpretability, scalability, and information fusion are crucial for advancing TKGRL research and applications.",
Information fusion techniques: Combine multi-modal data (textual and structured) and use attention mechanisms to dynamically weight information sources,"improving model accuracy and reliability.""",,"Interpretability-based methods, such as subgraph reasoning and reinforcement learning, aim to provide explanations for model predictions, improving transparency and reliability.",,,,,,,,,,,,
Models like Know-Evolve,GHNN,and EvoKG address dynamic event prediction using various neural and probabilistic frameworks,but quantitative results and p-values are not provided.,,,,,,,,,,,,
Scalability and interpretability remain key challenges; proposed solutions include distributed computing,sampling techniques,and attention mechanisms,"but no statistical significance is reported.""",Primary outcomes focus on evaluating Temporal Knowledge Graph Representation Learning (TKGRL) methods using MRR (mean reciprocal rank) and Hit@k metrics.,,,,,,,,,,,
MRR measures the average rank of correct answers; higher values indicate better performance.,,,,,,,,,,,,,,,
Hit@k measures the proportion of correct answers in the top k predictions.,,,,,,,,,,,,,,,
"No specific numerical results or statistical values are provided.""",Available TKG datasets are too small compared to real-world knowledge graphs.,,,,,,,,,,,,,,
TKGRL methods often overlook scalability and focus mainly on task-specific performance.,,,,,,,,,,,,,,,
Lack of interpretability and transparency in model results.,,,,,,,,,,,,,,,
Difficulty in representing new entities and relations with limited historical data.,,,,,,,,,,,,,,,
"Need for improved methods for information fusion and interpretability.""","Incorporating large language models (LLMs) into TKGRL can significantly improve model accuracy and effectiveness, but challenges like computational complexity and bias must be addressed.",,,,,,,,,,,,,,
Enhancing interpretability through attention mechanisms and visualization is crucial for practical applications and understanding model predictions.,,,,,,,,,,,,,,,
Improving scalability via distributed computing and sampling techniques is essential for handling large real-world datasets.,,,,,,,,,,,,,,,
T2TKG introduces a novel approach to capture intra-time and inter-time latent relations,"enhancing entity prediction in temporal knowledge graphs.""","Addressing scalability challenges in Temporal Knowledge Graph Representation Learning (TKGRL), including efficient negative sampling and distributed computing.",,,,,,,,,,,,,
Enhancing interpretability of TKGRL models through attention mechanisms and visualization techniques.,,,,,,,,,,,,,,,
"Incorporating information from multiple modalities and leveraging large language models to better represent dynamic and evolving temporal knowledge graphs.""","Future research should address scalability challenges, enhance interpretability, incorporate multi-modal information, and leverage large language models for dynamic TKGs. Suggested directions include distributed computing, sampling techniques, attention mechanisms, visualization methods, and improved question answering over temporal knowledge graphs. Gaps remain in dataset size, scalability, and interpretability.",,"Interpretability-based methods are highlighted as essential for developing reliable and transparent models by providing explanations for predictions, with subgraph reasoning and reinforcement learning approaches offering distinct techniques for enhancing interpretability in temporal knowledge graph representation learning.",,,,,,,,,,,,
Heart murmur detection from phonocardiogram recordings: The George B. Moody PhysioNet Challenge 2022,"Reyna Matthew A., Kiarashi Yashar, Elola Andoni, Oliveira Jorge, Renna Francesco, Gu Annie, Perez Alday Erick A., Sadr Nadi, Sharma Ashish, Kpodonu Jacques, Mattos Sandra, Coimbra Miguel T., Sameni Reza, Rad Ali Bahrami, Clifford Gari D.",2022,reference-manager,10.1101/2022.08.11.22278688,,,,,,Use of open-source algorithms for identifying heart murmurs and clinical outcomes from phonocardiogram (PCG) recordings.,,"How can open-source algorithms using phonocardiogram (PCG) recordings be developed and evaluated to enable automated pre-screening for heart murmurs and abnormal clinical outcomes, particularly in resource-constrained environments?","The paper describes a Challenge focused on developing open-source algorithms to identify heart murmurs and clinical outcomes from phonocardiogram recordings, aiming to improve pre-screening in resource-limited settings. The methodology involved algorithm submissions evaluated with novel metrics. Results and conclusions are pending, as the Challenge is ongoing.","The research goal was to develop open-source algorithms for detecting heart murmurs and clinical outcomes from phonocardiogram recordings using novel evaluation metrics; the approach involved a Challenge with 294 entries from 81+ teams, and results are pending as the official phase is ongoing.",
Application of novel evaluation metrics to assess algorithm performance.,,,,,,,,,,,,,,,
"Algorithmic pre-screening to reduce human screening of patients with normal cardiac function.""
294 entries from over 81 teams were received during the unofficial phase; the official phase is ongoing.",The research supports reproducibility through open-source algorithms and required teams to submit code for training and running models. Code was submitted via private GitHub or Gitlab repositories and will be publicly released after the Challenge and publication of conference papers.,No primary findings or quantitative results are reported; analysis and conclusions will be shared after the Challenge concludes.,,,,,,,,,,,,,
"No statistical significance (p-values) or final conclusions are provided in the current version.""",Primary outcome: Introduction and use of a cost-based evaluation metric to assess clinical outcome classifiers.,,,,,,,,,,,,,,
Measured effects: Metric evaluates ability to reduce costs by decreasing unnecessary expert screenings.,,,,,,,,,,,,,,,
"Statistical values: Weighted accuracy metric defined as aoutcome = (5nTP + nTN) / \[5(nTP + nFN) + (nFP + nTN)].
Results: No results reported; analysis pending Challenge conclusion.""",,"Algorithmic pre-screening using open-source algorithms can reduce human screening of patients with normal cardiac function, lowering healthcare costs and increasing screening capacity.",,,,,,,,,,,,,
Novel evaluation metrics supported reproducibility,generalizability,and relevance of the research.,,,,,,,,,,,,,
"Cost-based scoring is controversial but important for improving access in resource-constrained environments.""",,,,"This year’s Challenge tasked participants with developing open-source algorithms to identify heart murmurs and clinical outcomes from phonocardiogram recordings, aiming to reduce human screening for normal cardiac function and thereby lower healthcare costs and increase screening capacity.",,"The objectives of the study were to improve murmur detection and clinical outcome identification using algorithmic approaches, evaluate methods with novel and traditional metrics, and assess the cost-effectiveness of algorithmic pre-screening in reducing human diagnostic and treatment costs.",,,,,,,,,
Temporal Reasoning in AI Systems,Sharma Abhishek,2023,reference-manager,,,,,,,Discrete time survival analysis: Used to estimate how long facts (fluents) persist by modeling risk periods and hazard functions.,,"What knowledge representation and reasoning schemes are required for robust commonsense temporal projection in cognitive systems, and how can these methods improve the inference of fact persistence and Q/A performance in large knowledge bases?","The paper aims to improve commonsense temporal reasoning in AI by enhancing how systems infer the duration of facts (fluents). Using discrete survival analysis and hazard functions within the Cyc Knowledge Base, the proposed methods achieved a 49% average improvement in Q/A performance, demonstrating robust temporal projection capabilities.","The research goal is to improve commonsense temporal reasoning in AI; the approach integrates knowledge representation, hazard functions, and survival analysis for robust temporal projection; results show a 49% average improvement in Q/A performance, demonstrating significant gains in inferring fact persistence over time.",No information available
Temporal projection algorithm: Extrapolates the persistence of facts over time using knowledge representation schemes and temporal constraints.,,,,,,,,,,,,,,,
"Event calculus integration: Maintains correct temporal intervals for fluents and is combined with survival analysis to handle incomplete knowledge.""",,"Temporal projection methods improved Q/A performance across all query sets, with an average improvement of 49% over mode M1.",,,,,,,,,,,,,
Results are statistically significant (p < 0.05).,,,,,,,,,,,,,,,
The largest improvement was 141% (Query Set 3),"and the smallest was 31% (Query Set 5).""",Significant improvement in Q/A performance in all query sets with temporal projection methods.,,,,,,,,,,,,,
Overall improvement in performance with respect to M1 is 49%.,,,,,,,,,,,,,,,
Results are statistically significant (p < 0.05).,,,,,,,,,,,,,,,
Query set improvements:,,,,,,,,,,,,,,,
Set 1: 100% improvement (28% to 56%),,,,,,,,,,,,,,,
Set 2: 46% improvement (39% to 57%),,,,,,,,,,,,,,,
Set 3: 141% improvement (29% to 70%),,,,,,,,,,,,,,,
Set 4: 47% improvement (43% to 63%),,,,,,,,,,,,,,,
"Set 5: 31% improvement (26% to 34%)""",The study does not address reasoning about when individuals enter a state or transition among states.,,,,,,,,,,,,,,
It does not cover recurrent or periodic events.,,,,,,,,,,,,,,,
It lacks the capability to reason about probabilistic effects and estimate event likelihoods within time intervals.,,,,,,,,,,,,,,,
"Further research is suggested for these areas.""","The proposed knowledge representation and reasoning methods significantly improve temporal reasoning, leading to a 49% average increase in Q/A performance.",,,,,,,,,,,,,,
Integrating survival analysis and event calculus helps address incomplete knowledge about fact persistence.,,,,,,,,,,,,,,,
Future work should focus on reasoning about state transitions,recurrent events,"and probabilistic event effects.""",Reasoning about when individuals are likely to enter a given state and how they transition among states.,,,,,,,,,,,,
Extending research to handle recurrent and periodic events,such as sleeping or going to a grocery store.,,,,,,,,,,,,,,
"Developing the capability to reason about probabilistic effects of events and estimate the likelihood of event occurrence within a time interval.""",The study suggests future research should focus on: (1) reasoning about when individuals enter or transition between time-dependent states; (2) extending methods to handle recurrent and periodic events; and (3) reasoning about probabilistic effects and estimating event likelihoods within time intervals.,,"The study demonstrates that integrating survival analysis and temporal constraints into knowledge representation significantly improves AI systems' ability to reason about fact persistence, resulting in a 49% average improvement in Q/A performance across over 7,000 queries.","The objectives of the study are to propose and evaluate knowledge representation schemes that improve temporal projection and Q/A performance in AI systems, specifically by using risk periods, hazard functions, temporal constraints, and covariates to better reason about how long facts persist.",,,,,,,,,,,
An open source knowledge graph ecosystem for the life sciences,"Callahan Tiffany J., Tripodi Ignacio J., Stefanski Adrianne L., Cappelletti Luca, Taneja Sanya B., Wyrwa Jordan M., Casiraghi Elena, Matentzoglu Nicolas A., Reese Justin, Silverstein Jonathan C., Hoyt Charles Tapley, Boyce Richard D., Malec Scott A., Unni Deepak R., Joachimiak Marcin P., Robinson Peter N., Mungall Christopher J., Cavalleri Emanuele, Fontana Tommaso, Valentini Giorgio, Mesiti Marco, Gillenwater Lucas A., Santangelo Brook, Vasilevsky Nicole A., Hoehndorf Robert, Bennett Tellen D., Ryan Patrick B., Hripcsak George, Kahn Michael G., Bada Michael, Baumgartner William A., Hunter Lawrence E.",2024,reference-manager,10.1038/s41597-024-03171-w,,,,,,"Systematic comparison: Methods were systematically compared using a survey based on five criteria—KG construction functionality, maturity, availability, usability, and reproducibility.",,"How does PheKnowLator enable the automated, FAIR construction and analysis of ontologically grounded biomedical knowledge graphs, and how does it compare to existing open-source KG construction methods in terms of functionality, usability, maturity, availability, and reproducibility?","The paper presents PheKnowLator, a semantic ecosystem for automating the FAIR construction of ontologically grounded knowledge graphs (KGs). Using a systematic survey of 16 open-source KG construction methods, the study found PheKnowLator comparable or superior in functionality, usability, maturity, and reproducibility, highlighting its flexibility and advanced features.","The paper's main objective was to systematically compare open-source biomedical knowledge graph (KG) construction methods; using a survey-based approach, it evaluated 15 tools across five criteria, and found that PheKnowLator offers unique features for quality assessment, flexible KG construction, and advanced analysis compared to other methods.",
Keyword search: Open-source biomedical KG construction methods were identified using a keyword search against the GitHub API.,,,,,,,,,,,,,,,
"Survey assessment: A 44-question survey was used to evaluate each method across the five criteria.""","75% (n = 12) of the methods provided tools for reproducible workflows and installation, such as Docker containers (n = 6) and Jupyter or R Notebooks (n = 8). Source code for Graph Toolkit, ProNet, and SEmantic Modeling machIne is available on GitHub at the provided links.","The average coverage score across five assessment criteria was 3.93 (min = 2.79, max = 4.90), with 81.3% of methods supporting data download, 56.3% processing experimental data, and 37.5% processing clinical data.",,,,,,,,,,,,,
PheKnowLator was comparable to 15 other open-source KG construction methods but uniquely offered tools for assessing code quality and more semantically consistent molecular interactions.,,,,,,,,,,,,,,,
"Statistical significance (p-values) is not reported in the provided context.""
81.3% (n = 13) of methods included data download functionality.
56.3% (n = 9) processed experimental data; 37.5% (n = 6) processed clinical data.","The average coverage score across five assessment criteria was 3.93 (min = 2.79, max = 4.90).",,,,,,,,,,,,,,
"75% (n = 12) were written in Python; 43.8% (n = 7) in Java-based languages.
93.8% (n = 15) provided sample data; 75% (n = 12) provided tutorials.",,,,,,,,,,,,,,,
"Number of commits per year ranged from 17 to 1
68.8% (n = 11) had been published; 43.8% (n = 7) provided collaboration guidelines.
75% (n = 12) provided tools for reproducible workflows","000.
including Docker (n = 6) and Jupyter/R Notebooks (n = 8).""","Systematic comparison was subjective, involved only three researchers, and may have missed new methods.",,,,,,,,,,,,,
Only qualitative comparison was performed; no direct method benchmarking.,,,,,,,,,,,,,,,
Computational performance metrics computed from a single build run.,,,,,,,,,,,,,,,
Embeddings available only for one small build due to size constraints.,,,,,,,,,,,,,,,
No existing benchmarks for systematic evaluation of knowledge representation.,,,,,,,,,,,,,,,
Existing KG construction methods have limited flexibility in knowledge modeling.,,,,,,,,,,,,,,,
"Future work needed to compare data integration and ontology alignment pipelines to other tools.""",The PheKnowLator ecosystem is comparable to other open-source biomedical KG construction methods across key criteria.,,,,,,,,,,,,,,
Most methods provide good usability,availability,and reproducibility,but differ in data processing capabilities and collaboration guidelines.,,,,,,,,,,,,
Limitations include subjective assessment,incomplete inclusion of new methods,and lack of quantitative benchmarking.,,,,,,,,,,,,,
"Future work should include formal benchmarking and evaluation against additional tools.""","The comparison of open-source KG construction methods was subjective, limited to three researchers, and may have missed newer methods.",,,,,,,,,,,,,,
Only qualitative comparisons were performed; future work should include fair,quantitative evaluations using the same data.,,,,,,,,,,,,,,
"Embeddings are only available for small KGs; generating embeddings for larger KGs remains a challenge.""","Future research should include: (1) objective, quantitative comparisons of KG construction methods using the same data; (2) formal evaluation of data integration and ontology alignment pipelines against tools like Web Karma, OpenRefine, and R2RML; (3) generating embeddings for larger KGs using new embedding tools.",,"PheKnowLator is a flexible semantic ecosystem for constructing, analyzing, and benchmarking ontologically grounded knowledge graphs, offering features and performance that distinguish it from 15 other open-source methods.",,,,,,,,,,,,
A generative model of memory construction and consolidation,"Spens Eleanor, Burgess Neil",2024,reference-manager,10.1038/s41562-023-01799-z,,,,,,Computational modeling using secondary data: All analyses were performed using existing datasets and computational methods.,,How can computational modeling using secondary data and publicly available datasets be used to investigate and replicate findings related to neurological conditions and generative memory?,"The study is a computational analysis using secondary data to model neurological conditions. The main objective is to ensure robust, replicable findings using random samples from datasets. No direct human or clinical data were used. Results are statistically significant, and all code and data are publicly available for replication.","The research goal was to computationally model memory using secondary data; the approach involved simulations on datasets like MNIST, Shapes3D, and ROC Stories, and the principal finding is that the results are robust and replicable using the provided code.",
Repeated random sampling: Multiple random samples were taken from datasets to test the robustness of results.,,,,,,,,,,,,,,,
Statistical analysis: Statistical parameters such as means,standard deviations,"and confidence intervals were reported.""","The research is reproducible. All code for data collection and simulations, along with replication instructions and a list of Python libraries used, is available at https://github.com/ellie-as/generative-memory. The datasets used are publicly available and covered by the Creative Commons Attribution 4.0 License.","The study used computational modeling with secondary data, ensuring robust results by taking repeated random samples from datasets.",,,,,,,,,,,
No outliers were excluded,and sample sizes were chosen to achieve statistically significant results; however,no specific quantitative results or p-values are provided.,,,,,,,,,,,,,
"The findings are fully replicable using code and data available on GitHub.""","No primary outcomes, results, or measured effects (including statistical values) are explicitly stated in the provided context.",,,,,,,,,,,,,,
"The study is a computational analysis using secondary data; details on outcomes or effects are not included.""","Randomization in the usual sense does not apply, as only secondary data was used.",,,,,,,,,,,,,,
No modelling related to sex,gender,race,ethnicity,or other socially relevant groupings.,,,,,,,,,,,
Population data on neurological conditions discussed but not directly used.,,,,,,,,,,,,,,,
"No outlier exclusions; subset restrictions are stated in Methods.""","The study is fully computational, using only secondary data, with no direct modeling of sex, gender, race, or ethnicity.",,,,,,,,,,,,,,
Findings are robust and replicable; all code and data are openly available on GitHub.,,,,,,,,,,,,,,,
No outliers were excluded; sample sizes ensured statistical significance.,,,,,,,,,,,,,,,
"No specific recommendations are provided.""","The study does not use data relating directly to neurological conditions, though it discusses their relation to the model.",,,,,,,,,,,,,,
No modeling was performed related to sex,gender,race,ethnicity,or other socially relevant groupings.,,,,,,,,,,,
"Future work could involve applying the model to primary data or broader demographic variables.""",,,"This computational study used secondary data and robust statistical methods to analyze neurological conditions in relation to the model, with all code and data openly available for replication.",,,,,,,,,,,,
FAIR Digital Twins for Data-Intensive Research,"Schultes Erik, Roos Marco, Bonino da Silva Santos Luiz Olavo, Guizzardi Giancarlo, Bouwman Jildau, Hankemeier Thomas, Baak Arie, Mons Barend",2022,reference-manager,10.3389/fdata.2022.883341,,,,,,"Use of Knowlets: Knowlets are used to represent machine awareness of context, enabling rapid rationalization of real-world observations using curated knowledge.",,"How can we transition from traditional, human-driven scientific research to a machine-assisted environment where precisely defined real-world semantics and globally unique and persistent identifiers enable fully AI-ready, FAIR data and services?","The paper argues that precise definition of scientific concepts and real-world semantics is essential for fully AI-ready scholarly communication. Using ontological analysis and tools like nanopublications, the authors propose reducing ambiguity for both humans and machines. The main barrier is conceptual, not technological, with implications for more effective FAIR data environments.",The paper’s main objective is to enable fully AI-ready FAIR Digital Twins by clarifying foundational concepts; its key method is ontological analysis to define precise real-world semantics; the principal finding is that precise semantic definitions are essential for both machine and human scientific communication.,No information available
Filtering by Semantic Types: The study filters concepts (e.g.,“gene”) and annotated relations from curated databases to focus on relevant data.,,,,,,,,,,,,,,
"Organ-on-a-chip Technology: This technique is used to explore biological mechanisms by perfusing patient plasma through lung tissue organoids.""","The article and supplementary material include the original contributions. There is no explicit mention of source code availability. For further details, inquiries can be directed to the corresponding authors.","The main barrier to realizing FAIR Digital Twins (FDTs) is conceptual, not technological; further clarification of foundational concepts is needed.",,,,,,,,,,,,,
Using Knowlets and curated relationships,68 genes were identified as co-occurring with morbid obesity,type 2 diabetes,and early onset Alzheimer’s disease.,,,,,,,,,,,,
"Application of knowledge graphs rationalized dexamethasone’s mechanism in COVID-19 patients; statistical significance (p-values) not reported.""",Primary outcomes include identification of 68 genes with non-specified co-occurrence in articles on three diseases.,,,,,,,,,,,,,,
Dexamethasone's mechanism in severe COVID-19 patients was rationalized as reducing prostaglandin E2 overproduction.,,,,,,,,,,,,,,,
Perfusion of patient plasma through lung organoids showed increased prostaglandin levels,indicating inducing precursors.,,,,,,,,,,,,,,
"No explicit statistical values provided.""","Machines struggle with near sameness, conceptual drift, homonyms, and synonyms.",,,,,,,,,,,,,,
Machines have limited ability to independently determine if different identifiers (GUPRIs) refer to the same concept.,,,,,,,,,,,,,,,
The main barrier is conceptual,not technological.,,,,,,,,,,,,,,
Further clarification of foundational concepts is needed.,,,,,,,,,,,,,,,
"Human bottlenecks in manual queries and algorithms limit scalability.""","The main barrier to realizing FAIR Digital Twins (FDTs) is conceptual, not technological.",,,,,,,,,,,,,,
Precise communication of scientific meaning is essential for effective machine-assisted FAIR science.,,,,,,,,,,,,,,,
"Publishing scientific assertions in machine-readable formats (like nanopublications) is recommended.
Stepwise",modular construction of FDTs enables scalable,"machine-assisted knowledge discovery.""","The main barrier to FAIR Digital Twins (FDTs) is conceptual, not technological; foundational concepts need further clarification.",,,,,,,,,,,,
There is a need to reduce ambiguity in scientific communication,especially regarding concept definitions and agreements.,,,,,,,,,,,,,,
"Developing Knowlets to support machine learning and knowledge discovery requires addressing limitations of fixed ontologies and static knowledge graphs.""","Future research should focus on clarifying foundational concepts for FAIR Digital Twins (FDTs), addressing conceptual—not just technological—barriers, and exploring autonomous management of semantic challenges like “near-sameness” and “conceptual drift.” Further investigation into privacy, identity protection, and collective FDTs is also recommended.",,"The discussion concludes that transitioning to machine-assisted FAIR science requires clarifying foundational concepts and increasing precision in scientific communication to reduce ambiguity, enabling machines to better interpret and process scientific data and thus improve both machine and human understanding.",,,,,,,,,,,,
Learning temporal granularity with quadruplet networks for temporal knowledge graph completion,"Geng Rushan, Luo Cuicui",2025,reference-manager,10.1038/s41598-025-00446-z,,,,,,"Utilized five publicly available Temporal Knowledge Graph (TKG) datasets, enriched with temporal annotations, for model validation.",,How can leveraging fine-grained temporal information and advanced feature aggregation mechanisms improve the accuracy and adaptability of Temporal Knowledge Graph Completion models across diverse datasets?,"The paper addresses challenges in Temporal Knowledge Graph Completion by proposing a model that uses triaffine mechanisms and CNNs for feature aggregation and dissemination. Experiments on five datasets show improved performance, especially in capturing temporal granularity, with evaluation based on Mean Reciprocal Rank and Hits@N metrics.","The research goal is to improve Temporal Knowledge Graph Completion by leveraging temporal granularity; the approach uses triaffine transformations, CNNs, and a gating mechanism for feature aggregation and dissemination; results show enhanced model accuracy on multiple benchmark datasets.","Keywords: Timestamps mapping, Triaffine, Dynamic convolutional neural networks, Temporal knowledge graph, Temporal knowledge graph completion."
Employed evaluation protocols using Mean Reciprocal Rank (MRR) and Hits@N metrics under a time-wise filtering paradigm.,,,,,,,,,,,,,,,
Implemented the model in PyTorch,using Adam optimizer,grid search for hyperparameters,and regularization techniques like label smoothing,batch normalization,,"and dropout.""",,,,,,,,"The proposed LTGQ model achieved the highest MRR scores on ICEWS05-15 (69.4%), GDELT (44.9%), and ICEWS14 (64.1%), outperforming all baselines.","The research provides detailed experimental settings and evaluation protocols, including datasets and metrics. The code was written by R.G., but there is no explicit mention of the source code being publicly available. For materials, correspondence should be addressed to C.L. No direct code link is provided."
LTGQ also led in Hits@1,Hits@3,and Hits@10 across these datasets,indicating superior temporal link prediction performance.,,,,,,,,,,,,
"No p-values or explicit statistical significance values are reported in the context.""
On ICEWS05-15
On ICEWS14
On GDELT
On ICEWS05-15","Primary outcomes are evaluated using Mean Reciprocal Rank (MRR) and Hits@N (N=1,3,10).
the proposed model achieves a 1.1% higher MRR and 1.0% higher Hits@1 than TeAST.
MRR and Hits@1 are higher than TeAST
the model outperforms TuckERTNT by 6.8% in MRR and 6.9% in Hits@1.
GDELT","but Hits@10 is slightly lower.
and ICEWS14",LTGQ achieves the highest MRR (69.4%,44.9%,,64.1%) and Hits@1 (61.4%,,,,,,,,56.4%) among compared methods.,35.2%
Ablation studies show that removing DTM,QFM,or DM modules reduces performance on ICEWS14 and YAGO11k.,,,,,,,,,,,,,
On YAGO11k and Wikidata12k,"the model shows marked performance improvements (exact values not specified).""",,The proposed method (LTGQ) achieves the best or second-best performance on all benchmark datasets for temporal link prediction.,,,,,,,,,,,,
The model’s aggregation and dissemination modules,using triaffine transformations and CNNs,enhance feature fusion and adaptability.,,,,,,,,,,,,,
The approach is robust across diverse datasets and temporal granularities.,,,,,,,,,,,,,,,
"Recommendation: Apply LTGQ for improved temporal knowledge graph completion tasks.""",,,,"The proposed model effectively captures fine-grained temporal information and complex interactions among entities, relations, and timestamps, leading to improved performance in temporal knowledge graph completion tasks compared to existing methods.",,"The objectives of the study are to improve Temporal Knowledge Graph Completion (TKGC) by leveraging the structured and granular nature of temporal information, addressing semantic fragmentation, and enhancing model accuracy in predicting future events by effectively correlating entities, relations, and timestamps at finer temporal granularity.",,,,,,,,,
Leveraging Knowledge Graphs for Big Data Integration: the XI Pipeline,Cudré-Mauroux Philippe,2020,reference-manager,10.3233/sw-190371,,,,,,Human attention (crowdsourcing or manual inspection) is used to ensure high-quality results in data integration.,,"How can Knowledge Graphs be leveraged to effectively integrate heterogeneous enterprise data, and what processes and techniques can be used to semi-automatically map diverse content onto a Knowledge Graph for improved data integration?","The paper presents the XI Pipeline, an end-to-end process for semi-automatically mapping diverse data onto Knowledge Graphs. Using techniques like Entity Linking, Co-Reference Resolution, and Relation Extraction (with neural networks), the study demonstrates effective integration across domains. Knowledge Graphs are powerful but integration remains complex and resource-intensive.","The research goal is to integrate heterogeneous data into Knowledge Graphs using the XI Pipeline, which employs entity linking and relation extraction techniques; the principal finding is that Knowledge Graphs are powerful for integration, but the process remains complex and resource-intensive.",
Entity Linking: Identifying and connecting textual mentions from input data to corresponding entities in a Knowledge Graph.,,,,,,,,,,,,,,,
"Relation Extraction using Distant Supervision and a neural architecture (Aggregated Piecewise Convolutional Neural Network) to identify relationships between entities.""",,"Knowledge Graphs are powerful and flexible for integrating heterogeneous data, but mapping input data onto them is complex and time-consuming.",,,,,,,,,,,,,
The XI Pipeline enables semi-automatic integration of various data types (publications,social content,cloud infrastructure) into Knowledge Graphs.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""","Introduced the XI Pipeline, an end-to-end process to semi-automatically map content onto a Knowledge Graph.",,,,,,,,,,,,,,
Developed a new neural architecture (Aggregated Piecewise Convolutional Neural Network) for effective relation extraction.,,,,,,,,,,,,,,,
Deployed ScienceWise to help physicists track relevant articles,with automatic notifications.,,,,,,,,,,,,,,
Deployed Guider for integrating cloud infrastructure data,enabling job auditing,compliance,SLO extraction,"and job ranking at Microsoft.""",,Automated techniques cannot fully understand the meaning of arbitrary input data.,,,,,,,,,
Human attention is still necessary for high-quality results; subjectivity and ambiguity remain issues.,,,,,,,,,,,,,,,
Integration quality depends on the Knowledge Graph,which may contain errors,inconsistencies,or missing data.,,,,,,,,,,,,
Generic integration platforms are impractical; approaches must be specialized for each use-case.,,,,,,,,,,,,,,,
Some input data mentions (e.g.,"noun phrases) cannot be resolved by current methods.""","Knowledge Graphs are powerful for integrating diverse data, but mapping input data to them is complex and resource-intensive.",,,,,,,,,,,,,
Human involvement (crowdsourcing/manual review) remains essential for high-quality integration,as automated methods are not yet sufficient.,,,,,,,,,,,,,,
Using fine-grained entity types early in the process greatly benefits downstream tasks.,,,,,,,,,,,,,,,
"The integration quality depends on the Knowledge Graph’s accuracy and completeness; errors or missing data must be addressed first.""","The integration process for mapping input data onto a Knowledge Graph is highly complex and time-consuming, especially for unstructured and semi-structured data.",,,,,,,,,,,,,,
Classical NLP and entity linking techniques are not applicable to certain data types,such as log data without sentences,requiring customized approaches.,,,,,,,,,,,,,
"Relation extraction remains a challenging task due to the many explicit and implicit ways relationships can be expressed between entities.""","Suggested future research directions include: improving automated entity linking and co-reference resolution techniques, addressing errors and missing data in Knowledge Graphs, exploring fine-grained entity types, and developing composable software artifacts tailored to specific data modalities instead of generic integration platforms. Human involvement remains important for quality assurance.",,"Knowledge Graphs are powerful tools for integrating diverse data, but mapping input data onto a Knowledge Graph is complex and resource-intensive, requiring advanced techniques for entity linking, co-reference resolution, and relation extraction.","The objectives of the study are to design, build, and deploy systems to integrate publications, social content, and cloud infrastructure data using the XI Pipeline, and to provide observations and recommendations for future Big Data Integration efforts based on these experiences.",,,,,,,,,,,
Pattern-Based Design Applied to Cultural Heritage Knowledge Graphs,"Carriero Valentina Anita, Gangemi Aldo, Mancinelli Maria Letizia, Nuzzolese Andrea Giovanni, Presutti Valentina, Veninata Chiara",2020,reference-manager,,,,,,,"Requirements engineering: Collecting and structuring requirements as user stories from the customer team, prioritizing and managing dependencies.",,"How can a modular, root-thematic-foundations architectural pattern be designed and implemented to effectively address requirements and competency questions in large ontology networks, such as ArCo, for modeling cultural heritage domains?","The paper presents the design of ArCo, a knowledge graph for Italian Cultural Heritage, using the eXtreme Design (XD) methodology. Requirements were gathered as user stories. ArCo outperforms existing ontologies in terminological coverage (0.72 vs. 0.07 for EDM, 0.2 for CIDOC CRM). ArCo is well-documented and impactful.","The paper's research goal is to design ArCo, a knowledge graph for Italian Cultural Heritage, using the eXtreme Design methodology; the key approach is a modular, pattern-based ontology design; the principal finding is that ArCo provides expressive, high-quality data and serves as a reference for knowledge graph engineering.",
Reusing existing ontologies and patterns: Applying the eXtreme Design methodology to reuse Ontology Design Patterns (ODPs) and existing ontologies for modeling.,,,,,,,,,,,,,,,
Test-driven design: Using test data and tools (e.g.,"TESTaLOD) to validate ontology development against requirements.""","The research is reproducible: all test cases (inference, error provocation, competency question verification) are publicly available on GitHub. Configuration files for ontology alignment are on FigShare. TESTaLOD, the evaluation tool, is available as a web application. No explicit source code link is provided.","The ArCo knowledge graph grew significantly from v0.1 to v0.5 (∼133M more triples), then stabilized from v0.5 to v1.0 (∼3K more triples), with individuals decreasing from ∼22.5M to ∼20M due to data cleansing.",,,,,,,,,,,,
Module metrics show high quality: appropriateness = 1,encapsulation = 0.96–1,coupling = 0,and atomic size ≈ 4.85–6.63 across modules.,,,,,,,,,,,,
"No statistical significance (p-values) is reported in the provided context.""","Relationship Richness: ArCo v0.1 (0.43), v0.5 (0.34), v1.0 (0.44), CIDOC-CRM (0.74), EDM (0.84)",,,,,,,,,,,,,,
Inheritance Richness: ArCo v0.1 (1.1),v0.5 (2.9),v1.0 (2.48),CIDOC-CRM (1.17),EDM (0.32),,,,,,,,,,,
Terminological Coverage: ArCo v0.1 (0.09),v0.5 (0.37),v1.0 (0.56),CIDOC-CRM (0.18),EDM (0.07),,,,,,,,,,,
Number of axioms: ArCo v0.1 (715),v0.5 (9,564),v1.0 (13,792),,CIDOC-CRM (3,,,,,,,,EDM (299),503)
Number of logical axioms: ArCo v0.1 (180),v0.5 (2,210),v1.0 (3,416),,CIDOC-CRM (830),,,,,,,,,EDM (130)
Number of classes: ArCo v0.1 (54),v0.5 (329),v1.0 (340),CIDOC-CRM (84),EDM (41),,,,,,,,,,,
Number of object properties: ArCo v0.1 (38),v0.5 (332),v1.0 (616),CIDOC-CRM (275),EDM (51),,,,,,,,,,,
Number of datatype properties: ArCo v0.1 (8),v0.5 (153),v1.0 (154),CIDOC-CRM (12),EDM (12),,,,,,,,,,,
Number of annotation assertions: ArCo v0.1 (429),v0.5 (6,357),v1.0 (8,734),,CIDOC-CRM (2,,,,,,,,EDM (125),589)
Module metrics for ArCo modules:,,,,,,,,,,,,,,,
"Atomic size: 4.85–6.63
Appropriateness: 1",,,,,,,,,,,,,,,
"Encapsulation: 0.96–1
Coupling: 0""",Lack of well-documented and high-quality ontology design patterns and supporting tools.,,,,,,,,,,,,,,
Difficulty and lack of automation in reusing existing ontologies,especially large or poorly documented ones.,,,,,,,,,,,,,,
No clear guidelines or catalogues for designing ontology network architectures.,,,,,,,,,,,,,,,
Unclear methods for gathering requirements from an evolving,diverse community.,,,,,,,,,,,,,,
"Lack of proper tool support for systematic testing.""",The eXtreme Design methodology supports reuse of Ontology Design Patterns but faces challenges due to limited high-quality patterns and tools.,,,,,,,,,,,,,,
Modular ontology design,as in ArCo,enhances flexibility,reusability,and maintainability,,with modules showing high appropriateness,,,,,,,,and low coupling.,encapsulation
Community involvement broadens requirements collection.,,,,,,,,,,,,,,,
Recommendation: Adopt modular,"pattern-based approaches and improve tools for pattern selection and reuse.""","Lack of well-documented, high-quality Ontology Design Patterns (ODPs) and supporting tools for ODP-driven ontology engineering.",,,,,,,,,,,,,
Absence of automatic procedures for detecting,annotating,and reusing ODPs in knowledge graphs.,,,,,,,,,,,,,
"Need for improved expressiveness in annotation languages like OPLa to capture more attributes and relations of patterns.""","Future research should address the lack of well-documented, high-quality Ontology Design Patterns (ODPs), develop tools for ODP-driven ontology engineering, provide guidelines and studies on architectural patterns for ontology networks, and create methodologies and tools for collecting requirements from diverse, evolving communities.",,"The discussion highlights that the eXtreme Design methodology, especially its pattern reuse and test-driven design, enabled flexible, modular, and reusable ontology development, but also revealed challenges due to limited high-quality patterns and tools, and difficulties in reusing existing ontologies.",,,,,,,,,,,,
"Platform Development for Proof-of-Concept of Smartphone-based Continuous Complex Positioning
Explanation:","Seo Seonghun, Yoo JaeJun, Lee Yangkoo, Park Kyunghyun, Kim Yonghyun, Han Jiwoo, Yoon Daesub",2024,reference-manager,10.1109/ictc62082.2024.10827363,,,,,,,,,,,"Keywords—seamless positioning, smartphones, GNSS, Wi-Fi, BLE, IMU"
Seamless positioning: continuous and uninterrupted location tracking.,,,,,,,,,,,,,,,
GNSS: Global Navigation Satellite System.,,,,,,,,,,,,,,,
Wi-Fi and BLE: wireless communication technologies.,,,,,,,,,,,,,,,
IMU: Inertial Measurement Unit,"a sensor for motion tracking.""",The research goal is to develop a smartphone-based continuous complex positioning platform; the approach involves integrating multiple sensors and wireless signals for seamless indoor-outdoor positioning; the principal finding is the successful architectural design and implementation of a flexible platform enabling accurate real-time positioning research.,"The paper presents a smartphone-based Proof of Concept platform for continuous complex positioning across indoor and outdoor environments. Using on-device and server-based processing of multiple sensors and signals, the platform enables seamless, accurate positioning. The study concludes with ongoing efforts to further integrate diverse positioning techniques for real-time solutions.",,,How can a smartphone-based continuous complex positioning platform be architected and implemented to seamlessly integrate diverse positioning techniques and sensors for accurate real-time indoor and outdoor localization?,,,,,,,,,"Hybrid positioning combining Long-Term Evolution (LTE) and Wi-Fi signals for emergency rescue localization, meeting E911 standards."
Integration of GNSS pseudoranges with WLAN Received Signal Strength Indicators (RSSIs) using particle filters and Gaussian process modeling for enhanced localization.,,,,,,,,,,,,,,,
"Smartphone-based continuous complex positioning platform leveraging on-device and server-based processing of diverse sensor and signal measurements.""",,The paper presents a smartphone-based continuous complex positioning platform integrating on-device and server-based positioning with support server communication for seamless indoor-outdoor localization.,,,,,,,,,,,,,
The platform enables flexible addition and integration of various positioning techniques,aiming for more accurate and precise real-time positioning.,,,,,,,,,,,,,,
"No quantitative results
Results: The platform enables seamless integration of various positioning techniques (GNSS","primary findings
Wi-Fi","or statistical significance (p-values) are explicitly provided.""
BLE","Primary outcome: Development and implementation of a smartphone-based continuous complex positioning Proof of Concept (PoC) platform.
IMU) for real-time positioning across indoor and outdoor environments.",,,,,,,,,,,,
"Measured effects/statistical values: No statistical values or quantitative results reported.""",Limited computing resources and asynchronous data acquisition on smartphones may hinder some positioning methods.,,,,,,,,,,,,,,
BLE beacon-based approaches may be limited if line-of-sight installation is not feasible.,,,,,,,,,,,,,,,
No current solution for correcting sensor error accumulation in indoor environments.,,,,,,,,,,,,,,,
"Need for further research to develop correction data mechanisms for indoor positioning errors.""","The study presents a smartphone-based continuous complex positioning platform architecture, enabling seamless integration of diverse positioning techniques.",,,,,,,,,,,,,,
The platform supports flexible addition and modification of methods,aiming for accurate real-time positioning.,,,,,,,,,,,,,,
On-device processing is essential for techniques sensitive to time delays.,,,,,,,,,,,,,,,
"Future work targets efficient integration for improved precision.""",Lack of effective correction data mechanisms for mitigating errors in relative positioning technologies in indoor environments.,,,,,,,,,,,,,,
"Challenges in integrating and efficiently combining diverse positioning methods on smartphones due to limited computing resources and asynchronous data acquisition.
Need for seamless",accurate,"and real-time positioning solutions that work across both indoor and outdoor environments.""",Future research should focus on efficiently combining and integrating various positioning methods through the proposed platform to develop more accurate and precise real-time positioning solutions. There is a need to address challenges related to limited computing resources and asynchronous data acquisition on smartphones.,,,"The paper presents and implements a flexible smartphone-based continuous complex positioning platform, aiming to integrate various positioning techniques for more accurate and precise real-time positioning across indoor and outdoor environments.",,,,,,,,,"The objectives of the study are to develop and present a smartphone-based Proof of Concept (PoC) platform that enables seamless, continuous, and complex positioning across both indoor and outdoor environments by integrating various sensors and wireless signals for more accurate and precise real-time positioning solutions."
PRIVAFRAME: A Frame-Based Knowledge Graph for Sensitive Personal Data,"Gambarelli Gaia, Gangemi Aldo",2022,reference-manager,10.3390/bdcc6030090,,,,,,Manual annotation: The authors constructed and manually labeled a corpus of personal data categories (PDCs) for model evaluation.,,"How can a knowledge graph-based, context-aware model like PRIVAFRAME improve the identification and fine-grained analysis of complex sensitive personal data categories in text, compared to traditional rule-based and keyword-based approaches?","The paper introduces PRIVAFRAME, a novel knowledge graph-based, logical-symbolic approach for identifying sensitive personal data categories (PDCs) using the Data Privacy Vocabulary. The methodology includes constructing a manually labeled corpus and evaluating PRIVAFRAME against transformer-based models. Results highlight improved precision in entity-level sensitive data identification and suggest hybrid approaches for future SID tasks.","The research goal is to improve sensitive information detection (SID) by introducing PRIVAFRAME, a novel frame-based knowledge graph approach, which outperforms deep learning models in accuracy for fine-grained personal data categories, demonstrating promising results and potential for hybrid SID solutions.",
Knowledge graph (PRIVAFRAME): A novel logical-symbolic,frame-based knowledge graph was developed using a top-down approach to represent PDCs.,,,,,,,,,,,,,,
"Comparative experiment: The effectiveness of PRIVAFRAME was compared with a transformer-based deep learning approach.""","The research is reproducible. The dataset and Python source code are available at https://github.com/Gaia-G/SPeDaC-corpora and https://github.com/Gaia-G/PRIVAFRAME, but access requires signing an agreement for ethical research purposes. The test set is included in the PRIVAFRAME repository.","The model achieved high identification accuracy for some PDCs: CRIMINAL (100%), DISABILITY (95%), NAME (91%), PERSONAL POSSESSION (95%), and RELATIONSHIP (93%).",,,,,,,,,,,,,
Critical categories with low or zero identification included POLITICAL AFFILIATION,PROFESSIONAL CERTIFICATION,PROFESSIONAL EVALUATION,and REFERENCE (0%).,,,,,,,,,,,,
"Overall model precision was 60%; no p-values or statistical significance were reported.""",The model achieved an overall precision of 60%.,,,,,,,,,,,,,,
Excellent identification accuracy (+90%) for categories like DISABILITY,NAME,PERSONAL POSSESSION,RELATIONSHIP,CRIMINAL,,HAIR COLOR,SKIN TONE.,,,,,,SCHOOL,PRESCRIPTION & DRUG TEST RESULT,INCOME BRACKET
Critical performance (−55%) for AGE,PHYSICAL TRAITS,POLITICAL AFFILIATION,PRIVACY PREFERENCE,PROFESSIONAL CERTIFICATION,,PROFESSIONAL EVALUATION,,,,,,,,REFERENCE.,RACE
True positive rates ranged from 0% (e.g.,POLITICAL AFFILIATION) to 100% (e.g.,CRIMINAL).,,,,,,,,,,,,,
"False positives varied widely by category (e.g.
Some PDCs (e.g.","0 for FETISH
PROFESSIONAL CERTIFICATION","746 for PERSONAL POSSESSION).""
PROFESSIONAL EVALUATION","FRED rarely reconstructs complete compositional frames for some PDCs, especially TATTOO and PIERCING.
REFERENCE) are often mislabeled or confused with others.",,,,,,,,,,,,
Structurally complex and varied sentences reduce identification accuracy.,,,,,,,,,,,,,,,
FRED sometimes misses frame extraction for PDCs like GENDER and RELIGION.,,,,,,,,,,,,,,,
Limited LUs (lexical units) reduce accuracy for some PDCs.,,,,,,,,,,,,,,,
Overly broad compositional frames (e.g.,Earnings\_and\_Losses) cause false positives.,,,,,,,,,,,,,,
High false positives for categories like AGE,CREDIT & SALARY,DEMOGRAPHIC,JOB,PERSONAL POSSESSION.,,,,,,,,,,,
BERT-based model struggles with fine-grained PDCs and depends heavily on training data size.,,,,,,,,,,,,,,,
Some categories (e.g.,DISABILITY,OFFSPRING,INCOME BRACKET,PHYSICAL HEALTH) are not recognized by the BERT-based model.,,,,,,,,,,,
Generic PDCs like PERSONAL POSSESSION have high false positives; specificity reduces errors.,,,,,,,,,,,,,,,
"Suggestion to increase sample sentences and refine modeling strategies.""
Some PDCs (e.g.","The knowledge graph approach is more accurate than deep learning models for identifying fine-grained sensitive personal data categories (PDCs), especially with limited labeled data.
DISABILITY",NAME) are reliably identified,while others (e.g.,POLITICAL AFFILIATION,,PHYSICAL TRAITS) remain critical and require improved modeling.,,,,,,,,,
"Increasing the number of training sentences and refining compositional frames or rules are recommended to enhance identification accuracy.""",Lack of a common benchmark for the Sensitive Information Detection (SID) task across domains and languages.,,,,,,,,,,,,,,
Difficulty in identifying fine-grained personal data categories (PDCs),especially with limited labeled data for deep learning models.,,,,,,,,,,,,,,
"Need for ontological extension of PRIVAFRAME to include thematic roles and sensitivity variables for more nuanced detection.""","Future research should address FRED’s missed frame extraction for certain PDCs (e.g., GENDER, RELIGION), expand lexical units for others (e.g., ETHNICITY, FAMILY), and improve modeling for complex PDCs (e.g., CREDIT & SALARY). Developing a common benchmark is also recommended due to variability in language, domain, and techniques.",,"PRIVAFRAME, a novel frame-based approach for sensitive information detection, enables granular identification of personal data categories and outperformed transformer models in accuracy, supporting its integration as a logical-symbolic layer in hybrid systems for privacy protection.","The objectives of the study are to contribute novel resources and methods for Sensitive Information Detection (SID), focusing on broader sensitive personal data identifiable through linguistic-textual analysis, excluding basic personal data, and to propose a knowledge graph-based approach for identifying such data.",,,,,,,,,,,
FedMIA: An Effective Membership Inference Attack Exploiting 'All for One' Principle in Federated Learning,"Zhu Gongxi, Li Donghao, Gu Hanlin, Yao Yuan, Fan Lixin, Han Yuxing",2025,reference-manager,,,,,,,Developed and evaluated FedMIA-I (using model loss measurement) and FedMIA-II (using Grad-Cosine measurement) for membership inference attacks.,,"How effective is the proposed FedMIA membership inference attack in federated learning settings, and how robust is it against various defense methods and configurations?","The paper introduces FedMIA, a novel Membership Inference Attack (MIA) method for federated learning. Using a one-tailed likelihood-ratio hypothesis test and leveraging cross-client updates, FedMIA outperforms six baseline MIAs and remains effective against common defenses. The study highlights the urgent need for new privacy defenses in federated learning.","The research goal is to assess federated learning's vulnerability to membership inference; the approach introduces FedMIA, leveraging non-target client updates and a likelihood-ratio test; results show FedMIA is highly effective across settings and robust to common defenses, exposing urgent needs for new privacy protections.",
Compared FedMIA methods against six baseline attack methods and tested robustness against six defense techniques,including Gradient Perturbation,Gradient Sparsification,MixUp,Data Augmentation,,and Data Sampling.,,,,,,,,,
"Used AUC and TPR@FPR as evaluation metrics to assess attack effectiveness and utility loss.""",,"FedMIA, a new Membership Inference Attack (MIA), is highly effective across federated learning setups, robust to common defenses, Non-IID data, and varying client configurations.",,,,,,,,,,,,,
Conventional defenses (perturbation,sparsification,mixup) are ineffective against FedMIA,as attackers exploit cross-client information.,,,,,,,,,,,,
Larger hypervolume (HV) values indicate better privacy-utility trade-offs; however,"statistical significance (p-values) are not reported.""","Primary outcomes measured: Attack effectiveness (TPR@FPR=0.1%), privacy-utility tradeoff (hypervolume, HV), and visual leakage (generated images).",,,,,,,,,,,,,
FedMIA-I and FedMIA-II outperform baseline attacks in TPR@FPR=0.1% under various data distributions.,,,,,,,,,,,,,,,
TPR@FPR=0.1% for FedMIA-II reaches up to 83.14 (IID,AlexNet),78.55 (β=10),66.98 (β=1),55.09 (β=0.1).,,,,,,,,,,,
Hypervolume (HV): Lower values indicate more effective attacks. For AlexNet,FedMIA-II achieves HV=0.2588 (Mixup defense); for ResNet,HV=0.2969 (Perturb defense).,,,,,,,,,,,,,
Defense methods reduce attack effectiveness but may increase utility loss.,,,,,,,,,,,,,,,
Generated images from embeddings visually resemble original training images,"indicating privacy leakage.""","Conventional federated learning privacy defenses (perturbation, sparsification, mixup) are ineffective against FedMIA.",,,,,,,,,,,,,
Secure aggregation methods (MPC/HE) block FedMIA but have high computational and communication costs,limiting practical deployment.,,,,,,,,,,,,,,
"There is an urgent need for new defenses specifically targeting cross-client information leakage.""","FedMIA is a highly effective membership inference attack in federated learning, exploiting updates from non-target clients.",,,,,,,,,,,,,,
Conventional privacy defenses (perturbation,sparsification,mixup) are ineffective against FedMIA.,,,,,,,,,,,,,
Secure aggregation can block FedMIA but is impractical due to high costs.,,,,,,,,,,,,,,,
"There is an urgent need for new defenses specifically targeting cross-client information leakage.""",The need for more effective defense methods that balance privacy protection and model utility in federated learning.,,,,,,,,,,,,,,
Limited robustness of current defenses against strong membership inference attacks,especially under severe Non-IID data distributions.,,,,,,,,,,,,,,
Further exploration required on the impact of different federated learning settings (e.g.,client numbers,communication rounds,data volume,"local epochs) on privacy leakage.""",,"There is an urgent need for future research to develop membership inference attack (MIA) defenses that specifically prevent attackers from exploiting information in non-target client updates, as current federated learning privacy defenses are ineffective against FedMIA and secure aggregation methods are impractical due to high costs.",,,,,,,"The objectives of the study are to comprehensively compare the proposed FedMIA-I and FedMIA-II membership inference attack methods against six baseline attacks, evaluate their effectiveness under various Non-IID settings and defense strategies, and analyze the tradeoff between model utility loss and attack effectiveness using metrics like TPR@FPR and AUC.","The paper demonstrates that FedMIA, a new membership inference attack using non-target client updates and a one-tailed likelihood-ratio test, is highly effective and robust against common federated learning defenses, highlighting the urgent need for new privacy protections.",
Personalized and privacy-preserving federated graph neural network,"Liu Yanjun, Li Hongwei, Hao Meng",2024,reference-manager,10.3389/fphy.2024.1383276,,,,,,"Node classification tasks were performed on three graph-structured datasets (Cora, Pubmed, Citeseer) to evaluate model accuracy.",,"How can a personalized federated graph neural network framework effectively address graph data heterogeneity and privacy challenges by leveraging graph similarity estimation, attention-based model aggregation, and function encryption techniques?","The paper aims to address data heterogeneity and privacy issues in federated graph neural networks. It proposes an attention aggregation-based function encryption algorithm (PFGNN), evaluated on Cora, Pubmed, and Citeseer datasets. PFGNN outperforms traditional methods in accuracy and provides strong privacy guarantees through function-hiding MIFE encryption.","The research goal is to improve federated graph neural network performance; the approach uses a function encryption optimization algorithm with attentive aggregation (PFGNN); results show PFGNN achieves higher accuracy and efficiency than traditional methods, with a 5.4% average accuracy improvement over FedAvg in node classification tasks.",No information available
A federated learning framework (PFGNN) using graph similarity strategy,attentive aggregation scheme,and function encryption (specifically,function-hiding MIFE) was implemented and tested.,,,,,,,,,,,,
"Comparative experiments were conducted between PFGNN
,PFGNN achieved the highest average accuracy (0.8891) across Cora","Centralized ML
Pubmed","and FedAvg using GraphSAGE for local model training.""
and Citeseer datasets","The research implements PFGNN in Python and uses gmpy2 for the Paillier function encryption system. There is no explicit mention of the availability or sharing of the source code for the project.
outperforming Centralized ML (0.7905) and FedAvg (0.8486)",with a 5.4% improvement over FedAvg.,,,,,,,,,,,
PFGNN improved accuracy by 7.38% on average over FedAvg under different labels and graphs (Cora: 7.14%,Pubmed: 6.03%,Citeseer: 9.04%).,,,,,,,,,,,,,
"Function-Hiding MIFE provides computational privacy guarantees
Compared to FedAvg","protecting client weights and model parameters from adversaries; statistical significance (p-values) not reported.""
PFGNN improved average accuracy by 5.4%.","PFGNN achieved the highest average accuracy on Cora (0.9213), Pubmed (0.9315), and Citeseer (0.8145), outperforming Centralized ML and FedAvg.",,,,,,,,,,,,,
Under different labels and graphs,PFGNN outperformed FedAvg with improvements: Cora 7.14%,Pubmed 6.03%,Citeseer 9.04%.,,,,,,,,,,,,
Time overhead for encryption/decryption was lower in PFGNN than Hybrid,e.g.,"for 3 clients: Enc (PFGNN) 1.883 vs. Enc (Hybrid) 4.145; Dec (PFGNN) 2.034 vs. Dec (Hybrid) 11.654.""",,"PFGNN achieves higher accuracy than FedAvg and centralized ML, with an average improvement of 5.4% over FedAvg.",,,,,,,,,,,
Function-hiding MIFE in PFGNN provides strong computational privacy,protecting individual client data during aggregation.,,,,,,,,,,,,,,
PFGNN reduces computational overhead to O(N) compared to O(N²) in traditional schemes,with efficient encryption/decryption times.,,,,,,,,,,,,,,
Recommendation: Use PFGNN for secure,efficient,"and accurate federated learning on graph data.""",Addressing the heterogeneity of graph data in federated graph neural network optimization remains a major challenge.,,,,,,,,,,,,
Improving the effectiveness and security of Function-Hiding Multi-Input Function Encryption (MIFE) in federated learning scenarios.,,,,,,,,,,,,,,,
"Reducing communication overhead while maintaining model accuracy and privacy in distributed training frameworks.""",,,"The PFGNN framework outperforms FedAvg and centralized models in classification accuracy across multiple datasets, achieves better results under different labels and graph structures, and significantly reduces communication overhead by eliminating direct client-to-client communication.","The objectives of the study are to evaluate the performance of the PFGNN federated learning framework, focusing on privacy protection, accuracy, and efficiency. The study aims to demonstrate secure aggregation, improved classification accuracy, and effective model updating using attentive aggregation and function encryption.",,,,,,,,,,,
TIMEBENCH: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models,"Chu Zheng, Chen Jingchang, Chen Qianglong, Yu Weijiang, Wang Haotian, Liu Ming, Qin Bing",2024,reference-manager,,,,,,,"Multi-Select Task Format: Models select all possible correct answers from provided options, addressing limitations of traditional multiple-choice formats.",,"How can a comprehensive and hierarchical benchmark be designed to evaluate and quantify the temporal reasoning abilities of large language models across symbolic, commonsense, and event-based scenarios using diverse datasets and task formats?","TIMEBENCH introduces a comprehensive, hierarchical benchmark to evaluate large language models’ temporal reasoning in complex scenarios. It categorizes tasks into symbolic, commonsense, and event temporal reasoning, using diverse formats and 10 datasets. Results highlight challenges in time expression understanding. The benchmark enables systematic, human-aligned model assessment.","The research goal is to comprehensively evaluate large language models' temporal reasoning using the hierarchical TIMEBENCH benchmark; the approach categorizes tasks into symbolic, commonsense, and event temporal reasoning across diverse formats; results provide a normalized, multispectral assessment of model performance relative to humans.",No information available
Data Filtration and Sampling: For datasets like TimeQA and DurationQA,contexts and questions are filtered and sampled to ensure manageable length and relevance.,,,,,,,,,,,,,,
Hierarchical Benchmark Design: TIMEBENCH categorizes tasks into symbolic,commonsense,"and event temporal reasoning to comprehensively evaluate temporal reasoning abilities.""","The research provides data availability at GitHub (exact link not specified). There is no explicit mention of source code availability for the project. Reproducibility is supported by detailed descriptions of evaluation metrics, task formats, and sampling procedures, but source code details are not provided.","TIMEBENCH comprises 3 categories, 10 tasks, and 15 subtasks, totaling 19,000 instances, with detailed statistics in Figure 7 and Table 7.",,,,,,,,,,,
LLaMA2-Base70b achieved the highest normalized average score (64.4),with human performance set at 100; other models scored lower.,,,,,,,,,,,,,,
"No explicit p-values or statistical significance measures are provided in the context.""","Primary outcomes are measured using the overall score S, which combines BLEU-4, METEOR, ROUGE-L, CIDER/10, and MATCH.",,,,,,,,,,,,,,
Model results are normalized to human performance (100 points).,,,,,,,,,,,,,,,
For SituatedGen (Table 8),top model scores: LLaMA2† 70b: 74.92,LLaMA2† 13b: 64.81,Baichuan2† 13b: 64.91.,,,,,,,,,,,,
For TimeBench (Table 9),LLaMA2-Base70b achieves Avg. 64.4,Baichuan2-Base13b Avg. 51.3,GPT-3.5 + FS CoT Avg. 56.6.,,,,,,,,,,,,
Effects of prompting strategies (CoT,FS,FS CoT) are reported for each model and task.,,,,,,,,,,,,,
Statistical values include accuracy (Acc),exact match (EM),"and F1 scores for each subtask.""","Excessively long contexts in TimeQA were reduced, possibly affecting data representativeness.",,,,,,,,,,,,
Chain-of-thought prompting does not consistently improve performance.,,,,,,,,,,,,,,,
Open-source models show inferior temporal reasoning due to limited abstract time understanding and temporal commonsense.,,,,,,,,,,,,,,,
LLMs have uneven mastery of commonsense,especially in event order and typical events.,,,,,,,,,,,,,,
"Significant gap remains between model and human performance.""
CoT prompting yields mixed results: a 10.8% improvement in symbolic reasoning","Chain-of-thought (CoT) reasoning is not consistently effective; it often leads to performance declines, with an overall decrease of 7.4% in zero-shot settings.
but a 15.2% decline in commonsense reasoning.",,,,,,,,,,,,,,
Recommendation: CoT prompting should be applied selectively,"as its effectiveness varies by task.""",There is a substantial gap between state-of-the-art large language models (LLMs) and human performance in temporal reasoning.,,,,,,,,,,,,,
LLMs struggle with abstract time understanding,temporal relations modeling,and lack temporal commonsense.,,,,,,,,,,,,,
"Chain-of-thought prompting does not consistently improve temporal reasoning performance
Co-Design, Development, and Evaluation of a Health Monitoring Tool Using Smartwatch Data: A Proof-of-Concept Study","indicating a need for better reasoning techniques.""
Bajaj Ruhi Kiran, Meiring Rebecca Mary, Beltran Fernando","Future research should address the substantial gap between state-of-the-art large language models and human performance in temporal reasoning, improve models’ abstract time understanding, temporal relations modeling, and temporal commonsense, and further analyze the inconsistent effects of chain-of-thought prompting across different temporal reasoning tasks.
2023",reference-manager,"TIMEBENCH is a comprehensive, hierarchical benchmark designed to evaluate large language models’ temporal reasoning abilities across symbolic, commonsense, and event-based tasks using diverse formats and multi-select questions, with results normalized for comparison to human performance.
10.3390/fi15030111",,"The objectives of the study are to introduce TIMEBENCH, a comprehensive and hierarchical benchmark designed to quantify and evaluate the temporal reasoning abilities of large language models (LLMs) across symbolic, commonsense, and event temporal reasoning using diverse and challenging task formats.",,,,,,,,,Keywords: Electronic Medical Record (EMR); smartwatch; machine learning; anomaly detection; health monitoring; co-design; design science; diffusion of innovation
Explanation: These keywords refer to digital health records,wearable devices,artificial intelligence methods,identifying unusual patterns,tracking health,,collaborative design,"The study aimed to design and evaluate a web-based application integrating smartwatch data with healthcare systems. Using surveys and prototype testing, it found heart rate was the most valuable parameter. Participants preferred visual summaries and alert features. The prototype improved communication, informed decisions, and supported evidence-based practice.",,,"How can a web-based application prototype that integrates smartwatch data with electronic medical records (EMR) be designed, developed, and evaluated to support healthcare professionals in monitoring health trends, detecting anomalies, and communicating concerns to clients/patients?","Design Science Research (DSR) approach: A problem-focused method involving designing, building, and evaluating new systems to solve practical problems.",,"The research goal was to develop and evaluate a web-based application for clinicians to monitor smartwatch data; using a co-design and Design Science Research approach, the prototype enabled visualization and anomaly detection of health parameters, with results showing heart rate as the most valuable metric and positive usability feedback.","and the spread of new ideas or technologies.""",research methodology
Co-design: Engaging healthcare professionals through surveys and collaborative activities to gather needs and preferences for the application.,,,,,,,,,,,,,,,
Prototype evaluation using MAUQ: Healthcare providers evaluated the prototype with the mHealth App Usability Questionnaire,assessing usability,interface,"and usefulness.""",,,"The clustering-based local outlier algorithm outperformed neighbor-based (knn) and classification-based (iforest) algorithms in anomaly detection, offering faster calculation and better detection of local anomalies.",,,,,,,,,
"Usability scores were high: ease of use averaged 4.5–5
60% of participants preferred an anomaly alert feature; summary reports were favored weekly","interface and satisfaction averaged 4–4.4
monthly","with 7–10 participants rating items ≥4.
or upon anomaly detection. No p-values reported.""","Primary outcomes focused on prototype objectives: improved communication, patient/client empowerment, behavior change, and informed decision-making.",,,,,,,,,,,,
Heart rate was the most valuable parameter for data visualization and anomaly detection.,,,,,,,,,,,,,,,
Preferred report formats: charts and bar graphs.,,,,,,,,,,,,,,,
Usability was evaluated using the mHealth App Usability Questionnaire (MAUQ) with a 5-point Likert scale.,,,,,,,,,,,,,,,
"No specific statistical values or quantitative results are provided in the context.""",The machine learning algorithm used is only an example; results need more thorough clinical validation.,,,,,,,,,,,,,,
Small sample size limits generalizability; user requirements may not represent all target groups.,,,,,,,,,,,,,,,
Patient perspectives and privacy concerns regarding smartwatch data are significant challenges.,,,,,,,,,,,,,,,
"Study did not directly involve patients; only healthcare professionals participated.""","The prototype was rated highly for ease of use, interface, and satisfaction, with average scores mostly above 4 out of 5.",,,,,,,,,,,,,,
Key recommendations include expanding compatibility to all smartwatches,adding features like exercise time tracking,heart rate variability,and personalized anomaly detection.,,,,,,,,,,,,
Limitations include a small sample size and the need for broader clinical validation and patient privacy considerations.,,,,,,,,,,,,,,,
Future work should involve larger,"more diverse participant groups and integrate patient perspectives in the co-design process.""","Need for further research on effective co-design approaches involving healthcare providers, developers, and researchers for successful implementation.",,,,,,,,,,,,,
Limited evaluation of clustering-based local outlier algorithms in detecting local anomalies in smartwatch data; further validation is needed.,,,,,,,,,,,,,,,
"Exploration of methods to enhance the usability and adoption of machine learning-based health monitoring applications.""","Future research should include larger sample sizes for broader representation, involve patients and healthy individuals in co-design, address privacy concerns, and conduct thorough clinical validation of the machine learning algorithm. Further development should also focus on integrating clinically relevant parameters like blood pressure and blood sugar.",,"The prototype web application for smartwatch data visualization and anomaly detection was found to be easy to use, well-organized, and useful for healthcare professionals, with suggestions for further personalization, integration of additional health parameters, and enhanced communication features.","The objectives of the study were to analyze health trends (anomaly detection), enable healthcare professionals (HCPs) to communicate areas of concern to clients/patients, improve communication, empower patients/clients, support behavior change, and inform decision-making through integrating smartwatch data with electronic medical records (EMR).",,,,,,,,,,,
Temporal reasoning for timeline summarisation in social media,"Song Jiayu, Akhter Mahmud, Atzil-Slonim Dana, Liakata Maria",2025,reference-manager,,,,,,,Knowledge Distillation (KD): Temporal reasoning knowledge is transferred from a teacher model (LLaMA-3 or Phi) to a smaller student model (Phi) for timeline summarization.,,How can temporal reasoning knowledge be effectively transferred to large language models to improve the generation of accurate and comprehensive mental health-related timeline summaries from social media data?,"The paper investigates enhancing mental health timeline summarization by combining temporal reasoning with large language models (LLMs). Using knowledge distillation and fine-tuning, the L-Phi model outperformed others in factual consistency and usefulness, reducing hallucinations. Human evaluators preferred L-Phi summaries, highlighting the benefit of distilling knowledge from larger models.","The research goal is to improve mental health timeline summarization by combining temporal reasoning with knowledge distillation; the approach fine-tunes a teacher model on temporal reasoning, distills this knowledge into a student model, and results show L-Phi achieves the highest factual consistency and usefulness, reducing hallucinations in summaries.",
Fine-tuning: Models are fine-tuned on temporal reasoning (NarrativeReason) and timeline summarization datasets.,,,,,,,,,,,,,,,
"Human Evaluation: Clinical psychology graduate students evaluate generated summaries for factual consistency and usefulness.""",,"The fine-tuned L-Phi model showed the highest factual consistency (3.83) and usefulness (general: 3.48, diagnosis: 3.62) in human evaluations, outperforming other models.",,,,,,,,,,,,,
L-Phi significantly reduced hallucinations and errors compared to baseline models,especially in factual consistency and diagnosis aspects.,,,,,,,,,,,,,,
"No significant improvement was observed for Moments of Change (MoC); p-values or statistical significance are not reported.""","Fine-tuning on a single dataset (Phitemp, Phitl) worsened hallucination issues and reduced performance (FC: .141, .184; EA: .895, .966).",,,,,,,,,,,,,,
Combining datasets in Phijoint failed to improve results (FC: .238; EA: .941).,,,,,,,,,,,,,,,
L-Phi and P-Phi models achieved the best automatic evaluation results (e.g.,L-PhiNST\&PRT FC: .424,EA: .971; P-PhiNST\&PRT FC: .397,EA: .969).,,,,,,,,,,,,
Human evaluation showed L-Phi had the highest scores for factual consistency (3.83/5) and general usefulness (3.48/5),outperforming other models.,,,,,,,,,,,,,,
"Fine-tuned models significantly reduced hallucinations but did not show significant improvement in Moments of Change (MoC).""","LLM-generated clinical summaries may contain factual inaccuracies (""""hallucinations"""") and biases, potentially leading to serious errors in mental health decision-making.",,,,,,,,,,,,,,
Most temporal reasoning datasets involve only pairs of events,limiting the ability to model complex event sequences.,,,,,,,,,,,,,,
The timeline summarization dataset contains inconsistencies between events across timelines and summaries,so it was only used for training.,,,,,,,,,,,,,,
The fine-tuned model did not show significant improvement in detecting Moments of Change (MOC).,,,,,,,,,,,,,,,
The study is limited to 30 sampled TalkLife timelines for evaluation,"which may affect generalizability.""",The fine-tuned L-Phi model significantly reduces hallucinations and improves factual consistency and usefulness in mental health timeline summarization.,,,,,,,,,,,,,
Knowledge distillation from a larger LLM (LLaMA-3) to a smaller model (Phi) is beneficial.,,,,,,,,,,,,,,,
L-Phi outperforms other models,especially in factual consistency and diagnosis usefulness.,,,,,,,,,,,,,,
"Fine-tuning on a single dataset or combining datasets directly can worsen performance.""",Fine-tuning on a single dataset does not improve model performance and increases hallucination issues in timeline summarization.,,,,,,,,,,,,,,
Incorporating temporal reasoning information can interfere with the LLM’s ability to handle timeline summarization effectively.,,,,,,,,,,,,,,,
Jointly training on multiple data types fails to integrate them effectively,"leading to worse performance than in-context learning.""",,,"The fine-tuned L-Phi model, using knowledge distillation from a larger LLM, significantly improves factual consistency and general usefulness in mental health timeline summarization, effectively reducing hallucinations, though it shows no significant improvement in capturing Moments of Change (MOC).",,"The objectives of the study are to evaluate and improve timeline summarization for mental health using large language models (LLMs), specifically by applying knowledge distillation and fine-tuning strategies to enhance factual consistency and usefulness in generated summaries, as assessed by both automatic and human evaluation.",,,,,,,,,
Recommendation System Based on Temporal Knowledge Graph Path Reasoning,"Ren Haoyuan, Cui Liangzhong",2023,reference-manager,10.1145/3630138.3630436,,,,,,Embedding-based method: Represents entities and their relationships in the Knowledge Graph using embeddings to enrich semantic information and explore user preferences.,,"How can temporal knowledge graph path reasoning, incorporating time information from user-item interactions, improve the accuracy and interpretability of recommendation systems compared to existing static knowledge graph-based methods?","The paper aims to improve recommendation systems by integrating temporal information into knowledge graph path reasoning. Using a graph neural network combined with LSTM and reinforcement learning, the model encodes time-aware relationships and user behaviors. Experiments on three datasets show superior performance over existing static and temporal knowledge graph-based methods.","The paper's main objective is to improve recommendation accuracy by proposing a temporal knowledge graph path reasoning model that combines relationship-aware graph neural networks and reinforcement learning; experiments on three datasets show the approach outperforms existing methods in NDCG@10, Recall@10, and Precision@10.",
Path-based method: Utilizes path connections in the Knowledge Graph to generate recommendation candidates and improve interpretability.,,,,,,,,,,,,,,,
"Unified reinforcement learning method: Integrates semantic representation and connectivity
Quantitative results: For example","using a policy network trained with reinforcement learning to optimize recommendations.""
on the cloth dataset",NDCG@10 improved from 2.770% (TPRec) to 2.935% (ours),"The proposed model outperforms TPRec by 1%-7% on NDCG@10, Recall@10, and Precision@10 across cloth, phone, and beauty datasets.
a 5.928% relative gain.",,,,,,,,,,,,
The model consistently produces fewer invalid users than PGPR and TPRec,"demonstrating better guidance for reinforcement learning agents; no p-values are reported.""","The primary outcomes are NDCG@10, Recall@10, and Precision@10 on cloth, phone, and beauty datasets.",,,,,,,,,,,,,
The proposed model outperformed TPRec by 1%-7% across all metrics.,,,,,,,,,,,,,,,
On the cloth dataset: NDCG 2.935,Recall 4.949,Precision 0.798.,,,,,,,,,,,,,
On the phone dataset: NDCG 5.467,Recall 9.311,Precision 1.419.,,,,,,,,,,,,,
On the beauty dataset: NDCG 5.883,Recall 9.025,Precision 1.881.,,,,,,,,,,,,,
"The model had fewer invalid users compared to PGPR and TPRec.""",,"The proposed temporal knowledge graph path reasoning model outperforms existing methods (PGPR, ADAC, TPRec) on all datasets and metrics.",,,,,,,,,,,,,
It achieves 1%-7% improvement over TPRec in NDCG,Recall,and Precision at K=10.,,,,,,,,,,,,,
Time-aware rewards guide reinforcement learning agents more effectively,reducing invalid users.,,,,,,,,,,,,,,
"Recommendation: Incorporate temporal information for higher-quality inference paths and improved recommendation performance.""","Most current knowledge graph reasoning (KGR) methods treat user-item interactions as static, ignoring time information, which limits modeling of user behavior patterns.",,,,,,,,,,,,,,
Existing methods using time information mainly enhance data representation,not reasoning,missing deeper inference from timestamps.,,,,,,,,,,,,,
"Embedding-based and path-based methods each use only part of the knowledge graph information; a unified approach integrating both is needed.""",,,"The paper proposes a time-aware path reasoning model on knowledge graphs for recommendation, combining relationship-aware graph neural networks and LSTM with reinforcement learning to improve recommendation accuracy by incorporating temporal information, and demonstrates its effectiveness through experiments on real e-commerce data.",,,,,,,,,,,,
Aggregation and exploration of heterogeneous data collected from diverse information sources,Teraoka Teruhiko,2011,reference-manager,10.1145/2030066.2030076,,,,,,"Aggregation and organization of heterogeneous personal and public data using time, location, and people as key viewpoints.",,"How can heterogeneous personal and public data be effectively organized, aggregated, and explored to support both individual recall and innovative social uses in ubiquitous computing environments?","The paper aims to aggregate and organize heterogeneous personal and public data for personal recall and social use. Using temporal zooming user interfaces and digest views, the study explores methods for summarizing and visualizing data. Findings highlight challenges in schema matching, permissions, and interface design, concluding with implications for innovative social services.",The research goal is to organize and aggregate heterogeneous personal and public data; the approach uses temporal zooming user interfaces and digest views for data exploration; the principal finding is that this method supports personal recall and enables new social services by analyzing aggregated behavioral data.,"Personal data, heterogeneous data, exploration, aggregation, community memory"
Visualization techniques,including summaries (digests of daily life) and landmarks (cues for remembering experiences),to help users explore large datasets.,,,,,,,,,,,,,
"Development and use of a prototype system for data organization and social use studies.""",,"Aggregating heterogeneous personal data using time, location, and people viewpoints enables effective organization, summaries, and landmarks for personal recall and exploration.",,,,,,,,,,,,,
Safe collection and sharing of personal data are essential for creating new social services and analyzing collective behavior; public summaries for groups and events are planned.,,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the context.""",Aggregated heterogeneous personal data from diverse sources for personal and social use.,,,,,,,,,,,,,,
Studied data organization by time,location,and people.,,,,,,,,,,,,,
Developed digest views (yearly,monthly,daily) and visualized summaries and landmarks.,,,,,,,,,,,,,
Identified privacy concerns as data types increased.,,,,,,,,,,,,,,,
"No statistical values or measured effects reported.""",The study is ongoing; results and conclusions may not be final.,,,,,,,,,,,,,,
Increasing types of aggregated data raises concerns about information leaks and privacy invasion.,,,,,,,,,,,,,,,
Management of permission and authorization regarding privacy is identified as important future work.,,,,,,,,,,,,,,,
Public summaries for groups,communities,areas,"and events are planned for future work.""",Easy and safe personal use of heterogeneous data is essential for effective social use.,,,,,,,,,,,
Interactive visualization and temporal zooming interfaces help users explore and recall experiences from large,diverse datasets.,,,,,,,,,,,,,,
Key challenges include schema matching,access permissions,and unified information architecture.,,,,,,,,,,,,,
Future work should focus on public summaries,efficient network management,"and activity recommendations.""",Safe and effective daily collection and sharing of diverse personal data remains insufficient and needs further research.,,,,,,,,,,,,
Efficient management of network structures that mediate relationships among people and data attributes is an ongoing research area.,,,,,,,,,,,,,,,
Creating public summaries for groups,communities,areas,"and events using shared heterogeneous data is a planned future direction.""","Future research should address predicting traffic jams, detecting popular trends, and monitoring public health using heterogeneous data. Further investigation is needed into efficient management of data-mediated networks and creating public summaries for groups, communities, areas, and events. These are important topics for future work.",,,,,,,,,,"The objectives of the study are to develop methods for organizing heterogeneous data, introduce a prototype system for this purpose, and conduct an ongoing study on the social uses of such data, including aggregation for personal recall and creating new social services through data sharing and analysis.","The study presents ongoing research on organizing and exploring heterogeneous data from diverse sources, highlighting that easy and safe personal data use enables effective social use, with interactive visual interfaces and efficient network management being key areas of focus."
A literature review of current technologies on health data integration for patient-centered health management,"Peng Cong, Goswami Prashant, Bai Guohua",2019,reference-manager,10.1177/1460458219892387,,,,,,Snowballing sampling method: A recursive process of identifying relevant articles from references (backward snowballing) and citations (forward snowballing).,,"What are the existing approaches for integrating and collaboratively utilizing heterogeneous health data, how can these approaches be classified, and what are the main challenges affecting health data integration and utilization?","The paper aims to review methods and technologies for integrating and utilizing health data from multiple sources for patient healthcare management. Using a snowballing literature review, 32 peer-reviewed articles (2008–2018) were analyzed. Most approaches achieve structural interoperability, with varying integration effort. Security and privacy remain underexplored.","The research goal was to review and classify approaches for integrating heterogeneous health data, using a snowballing literature review method, and found that most studies focus on General Healthcare Information Management, with interoperability standards being crucial for successful integration.",
Inclusion and exclusion criteria: Applied to select articles based on language,publication date,relevance,and quality.,,,,,,,,,,,,
"Data extraction form: Used to consistently extract metadata and synthesized data from each included article for analysis.""",,"Out of 32 included articles, 22 achieved structural interoperability and 10 achieved semantic interoperability; none achieved only foundational interoperability.",,,,,,,,,,,,,
Most integration approaches required high (18 articles) or medium (14 articles) human effort; no low-effort approaches were identified.,,,,,,,,,,,,,,,
"No statistical significance (p-values) or quantitative results were reported.""",32 articles were included in the final review.,,,,,,,,,,,,,,
Most studies targeted General Healthcare Information Management,followed by Health/Lifestyle Self-Management and Chronic Disease Management.,,,,,,,,,,,,,,
22 articles achieved Structural interoperability; 10 achieved Semantic interoperability; none achieved only Foundational interoperability.,,,,,,,,,,,,,,,
Integration effort: High (most studies),Medium (some),Low (none reported).,,,,,,,,,,,,,
"No statistical values reported.""",Possible neglect of relevant articles due to exclusion criteria.,,,,,,,,,,,,,,
Only peer-reviewed articles included; gray literature excluded.,,,,,,,,,,,,,,,
Non-English-language articles excluded.,,,,,,,,,,,,,,,
Vague and overlapping definitions in classifications may cause misunderstanding.,,,,,,,,,,,,,,,
"Security and privacy issues not discussed due to lack of coverage in included articles.""","Most studies focus on integrating health data for General Healthcare Information Management, followed by Health/Lifestyle Self-Management and Chronic Disease Management.",,,,,,,,,,,,,,
Interoperability standards are crucial for effective health data exchange and easier integration.,,,,,,,,,,,,,,,
"Security and privacy issues are largely unaddressed and need future attention.""",Security and privacy issues in health data integration are largely unaddressed and need future research.,,,,,,,,,,,,,,
Exclusion of non-English and non-peer-reviewed literature limits understanding; future studies should include broader sources.,,,,,,,,,,,,,,,
Integration and utilization approach classifications are vague and overlapping,"requiring clearer definitions and frameworks.""",,,"The review identified and classified existing approaches for integrating and utilizing heterogeneous health data, highlighted main challenges, and noted that security and privacy issues remain largely unaddressed due to the early stage of research in this area.",,"The objectives of the study were to analyze the target problems addressed by included articles, classify them into categories, and investigate characteristics of the approaches such as interoperability standard conformance, interoperability level, and integration effort.",,,,,,,,,
"Healthcare knowledge graph construction: A systematic review of the state-of-the-art, open issues, and opportunities","Abu-Salih Bilal, AL-Qurishi Muhammad, Alweshah Mohammed, AL-Smadi Mohammad, Alfayez Reem, Saadeh Heba",2023,reference-manager,10.1186/s40537-023-00774-9,,,,,,"In-depth analysis of state-of-the-art Knowledge Graph (KG) construction methodologies, discussing their strengths and weaknesses.",,"What are the recent methodologies for constructing knowledge graphs in healthcare domains, and how do these approaches address challenges related to data integration, complexity reduction, and application in areas such as drug discovery, disease modeling, and drug repurposing?","This paper reviews recent methodologies for constructing Knowledge Graphs (KGs) in healthcare, focusing on drugs, diseases, and biomedicine. It highlights KGs' effectiveness in integrating diverse data but notes limitations in data sources, construction transparency, and evaluation. The study calls for better documentation and sharing to advance the field.","The paper's main objective is to review knowledge graph (KG) construction methods in healthcare, using a comparative analysis approach, and concludes that while KGs effectively integrate diverse healthcare data, current methods have limitations and open issues that suggest directions for future research.","Automatic extraction, schema-based, schema-free, hybrid, EMR, CRF, Bio-BERT, COVID-19, PubMed, Clinical Trials, DrugBank, SIDER, MalaCards, UMLS, KEGG, SemRep, SemMedDB, knowledge extraction, entity-level, relation-level, MinHash lookup, Skip-gram, kmeans++, Jaccard, depression, autism spectrum disorder."
Taxonomy and multi-perspective analysis of KG construction in healthcare,including schema-based and schema-free approaches.,,,,,,,,,,,,,,
Case studies evaluating KG construction effectiveness using real-world healthcare data and various evaluation measures (e.g.,P,R,F1,AUC,,"Accuracy).""",,,,,,,,"Knowledge Graph (KG) technology is widely used in healthcare for integrating diverse data sources, reducing complexity, and enabling flexible, unified data frameworks.","Most studies failed to adequately disclose internal KG construction mechanisms, with poor or limited discussion on methodology, ontology design, and extraction tasks. Many KGs are not publicly available, and source code is not provided, making replication difficult. No information on source code availability is explicitly stated."
Many studies focus on limited data sources and lack detailed evaluation; quantitative results include KG sizes (e.g.,#n: 12,473–millions; #e: 10,232–77,844,,574),,,,,,,,,but statistical significance (p-values) is not reported.
Main limitations include insufficient discussion on KG construction mechanisms,limited evaluation (mainly on embedding components),"and lack of real-life case studies.""","Primary outcomes were measured using P (Precision), R (Recall), F1, AUC (Area Under Curve), AUPRC (Area Under Precision-Recall Curve), and Accuracy.",,,,,,,,,,,,
Evaluations were mainly conducted via case studies and embedding components.,,,,,,,,,,,,,,,
Some studies reported observed bias in training data and insufficient evaluation of the constructed knowledge graphs.,,,,,,,,,,,,,,,
"No detailed statistical values or measured effects are explicitly provided.""",Limited data sources used for KG construction.,,,,,,,,,,,,,,
Lack of detailed discussion on KG construction mechanisms.,,,,,,,,,,,,,,,
Many KGs are not publicly available,hindering replication and reuse.,,,,,,,,,,,,,,
Insufficient evaluation,often limited to embedded components.,,,,,,,,,,,,,,
Limited discussion on KG statistics and validation.,,,,,,,,,,,,,,,
Some KGs are context-specific (e.g.,Chinese context),reducing generalizability.,,,,,,,,,,,,,
Lack of comparative studies and real-life case study evaluations.,,,,,,,,,,,,,,,
Systems often limited in functionality (e.g.,"answering only one intention per question).""","Knowledge Graph (KG) technology is effective for integrating diverse healthcare data, reducing complexity, and enabling flexible, unified frameworks.",,,,,,,,,,,,,
Most existing KGs rely on limited data sources and lack detailed construction and evaluation mechanisms.,,,,,,,,,,,,,,,
There is insufficient discussion on ontology design,KG statistics,and real-world validation.,,,,,,,,,,,,,
"Future research should address these gaps and improve evaluation methods.""","Limited diversity and integration of KG data sources, often relying on a small set of biomedical publications.",,,,,,,,,,,,,,
Insufficient discussion and transparency regarding the mechanisms used for KG construction and integration.,,,,,,,,,,,,,,,
"Evaluation methods focus mainly on embedded components rather than the overall effectiveness and quality of the resultant KG.""","Future research should address limited data sources, insufficient disclosure of KG construction mechanisms, and lack of real-life case studies. Detailed explanations of construction steps and public sharing of KGs are recommended to improve replicability, transparency, and adherence to FAIR principles in healthcare knowledge graph research.",,"The discussion highlights the effectiveness and popularity of knowledge graph (KG) technology in healthcare, while also identifying key limitations in data sources, evaluation methods, and integration mechanisms that present opportunities for future research.",,,,,,,,,,,,
User Behavior Enriched Temporal Knowledge Graphs for Sequential Recommendation,"Hu Hengchang, Guo Wei, Liu Xu, Liu Yong, Tang Ruiming, Zhang Rui, Kan Min-Yen",2024,reference-manager,10.1145/3616855.3635762,,,,,,"Construction of temporal knowledge graphs (KGs) from user behavior, incorporating time-aware properties and interest transition relations.",,"How can incorporating time-aware, behavior-centric knowledge from a Temporal Knowledge Graph (TKG) improve the identification of relevant information and enhance the performance of sequential recommendation models compared to traditional item-centric and user-centric knowledge distillation methods?","The paper investigates how popularity-based statistics and knowledge graphs (KGs) can improve sequential recommendation (SR) through knowledge distillation. Using experiments on four public datasets, the study finds that their proposed KEN and TKG-SRec frameworks outperform traditional methods, especially in modeling sequential relevance and handling KG complexity, enhancing recommendation accuracy.","The research goal is to improve sequential recommendation by introducing Temporal Knowledge Graphs (TKGs); the approach uses a two-phase TKG-SRec framework with a Knowledge Evolution Network (KEN) to learn dynamic entity embeddings, and the principal finding is that TKG-SRec outperforms state-of-the-art baselines by 5% on average.",
Two-phase TKG-SRec framework: (1) entity embedding learning (pretraining on static knowledge,refinement with temporal knowledge),(2) sequential modeling with dynamic entity embeddings.,,,,,,,,,,,,,
Evaluation using ranking-based metrics: Hit Ratio@k (HR@k),Normalized Discounted Cumulative Gain@k (NDCG@k),"and Mean Reciprocal Rank@k (MRR@k).""",,"The proposed KEN model produces less related embeddings for distant sequence items, improving the correlation between item order distance and embedding similarity, especially as KG size increases.",,,,,,,,,,,
TKG-SRec achieves the best NDCG@5 (0.0459,+6.9%) and NDCG@10 (0.0617,+5.0%) with statistical significance (A-value < 0.05) compared to baselines.,,,,,,,,,,,,,
Static encoder (S.K.) outperforms TransE and RESCAL in KG modeling,"but relying solely on static knowledge significantly decreases performance in sequential tasks.""","Primary outcomes were measured using HR@5, HR@10, NDCG@5, and NDCG@10 across four datasets (ML-100K, ML-1M, Amazon, LastFM).",,,,,,,,,,,,,
"TKG-SRec achieved the best performance on all datasets and metrics
Example results: LastFM NDCG@5 = 0.0459† (+6.9%)","with statistically significant improvements over the best baseline (†
NDCG@10 = 0.0617† (+5.0%).""","𝐴-value<0.05).
Overly large numbers of graph convolution layers (𝐴 and 𝐴′ = 4) degrade performance by mixing too much neighbor information.",,,,,,,,,,,,,
Increasing hidden layer size (𝐴) beyond 64 causes overfitting and reduced performance.,,,,,,,,,,,,,,,
Future work suggests incorporating more factual temporal data and advanced time modeling methods.,,,,,,,,,,,,,,,
"Time-aware & dynamic KGs in sequential recommendation remain underexplored.""","The proposed static encoder outperforms TransE and RESCAL, demonstrating effective graph learning and reliable entity embeddings.",,,,,,,,,,,,,,
Incorporating structure dynamics in temporal knowledge yields greater performance gains (↑8.2%) than entity dynamics (↑3.2%),except on ML-1M.,,,,,,,,,,,,,,
TKG-SRec surpasses all baselines,"confirming its superior dynamic knowledge distillation for sequential recommendation tasks.""","Incorporating additional factual data (e.g., product release dates) as temporal knowledge beyond behavior statistics.",,,,,,,,,,,,,
Adopting more sophisticated time modeling methods,such as Time2Vec,to better capture periodicity and improve effectiveness.,,,,,,,,,,,,,
"Addressing limitations of current item-centric and user-centric knowledge filtering approaches in sequential recommendation tasks.""",,,"TKG-SRec, which integrates dynamic temporal knowledge graphs with sequential recommendation, significantly outperforms state-of-the-art methods by 5% on average, demonstrating that filtered temporal knowledge effectively enhances entity embeddings for sequential recommendation tasks.","The study has two main objectives: (1) entity classification, which aims to classify the popularity of entities in the next time frame, and (2) relation prediction, which guides the training of temporal knowledge evolution for improved sequential recommendation.",,,,,,,,,,,
A Temporal Knowledge Graph Generation Dataset Supervised Distantly by Large Language Models,"Zhu Jun, Fu Yan, Zhou Junlin, Chen Duanbing",2025,reference-manager,10.1038/s41597-025-05062-0,,,,,,Construction of temporal quadruples by combining triplets using discovered relation patterns through statistics and human annotation.,,"How can document-level temporal relation extraction be systematically achieved to construct temporal knowledge graphs by associating temporal information with entity relations, and what methods and datasets enable effective extraction and reasoning of such temporal relations from documents?","The paper aims to automate temporal relation extraction from documents using a large language model (LLM)-based framework. By redefining relations and constructing the Tem-DocRED dataset, the study demonstrates that LLMs outperform rule-based and traditional methods in temporal reasoning, though with higher training complexity. The approach enables richer temporal knowledge graphs.","The paper's main objective is to extract document-level temporal relations using LLMs; it introduces a novel dataset (Tem-DocRED) and a seq-to-seq framework for generating time-aware quadruples, concluding that LLMs enable more effective temporal knowledge graph construction than traditional methods.","Keywords or tags for this research include: temporal knowledge graph (TKG), TKG reasoning, TKG question answering, DocTRE, Re-DocRED, Tem-DocRED, relation extraction, large language models (LLMs), entity annotations, event participation, administrative entity, dataset construction, and relation definitions."
Utilization of Large Language Models (LLMs) to generate timestamps and evaluate candidate quadruples by transforming them into natural language sentences and scoring their correctness.,,,,,,,,,,,,,,,
"Implementation of a filtering mechanism to remove errors and redundancies based on LLM-generated scores.""","The research used LLaMA-Factory to fine-tune open-source LLMs, with all training settings detailed in the code. However, the actual source code is not provided in the context. No further reproducibility details or code links are given.",GLM-4-9B achieved the highest F1 score (22.90) and recall (22.51) among LLMs tested on Tem-DocRED without entity annotations.,,,,,,,,,,,,,
Llama-3-8B had the highest precision (25.57) but lower recall (19.49) and F1 (22.12) compared to GLM-4-9B.,,,,,,,,,,,,,,,
"No p-values or statistical significance measures are reported.""",Primary outcomes:,,,,,,,,,,,,,,
All relations in Re-DocRED were redefined based on original descriptions; relations that could not be transformed were removed.,,,,,,,,,,,,,,,
Tem-DocRED contains fewer facts per document compared to Re-DocRED.,,,,,,,,,,,,,,,
Table 4 provides precise statistics: Re-DocRED (Train: 3,053 documents,59,359 entities,85,,932 triples),095 entities,,,908 triples).,,10,43,124 documents,Tem-DocRED (Train: 2
Results and measured effects:,,,,,,,,,,,,,,,
Average number of facts per document: Re-DocRED (Train: 28.1),Tem-DocRED (Train: 5.1).,,,,,,,,,,,,,,
Average number of entities per document: Re-DocRED (Train: 19.4),Tem-DocRED (Train: 20.2).,,,,,,,,,,,,,,
Average number of sentences per document: Re-DocRED (Train: 7.9),Tem-DocRED (Train: 8.0).,,,,,,,,,,,,,,
"No explicit statistical significance values are provided.""","Fine-tuning LLMs requires significant GPU resources, limiting accessibility in low-resource settings.",,,,,,,,,,,,,,
Errors in entity linking or timestamp extraction can degrade downstream performance.,,,,,,,,,,,,,,,
Framework struggles with precise temporal granularity due to sparse annotations.,,,,,,,,,,,,,,,
LLM-based systems have higher training complexity and slower inference than non-LLM baselines.,,,,,,,,,,,,,,,
Some patterns are not suitable for certain triples,causing redundancy.,,,,,,,,,,,,,,
"Human annotation is needed to eliminate errors in the test set.""",LLM-based methods show strong comprehension and generation abilities for temporal knowledge graph (TKG) generation but require significant computational resources.,,,,,,,,,,,,,,
Errors in entity linking or timestamp extraction negatively affect downstream performance.,,,,,,,,,,,,,,,
LLMs struggle with precise temporal granularity due to sparse annotations,causing information redundancy.,,,,,,,,,,,,,,
"Rule-based and traditional neural network methods are less resource-intensive but less effective for complex relations.""",Lack of effective methods for automatic construction of temporal knowledge graphs (TKGs) from documents.,,,,,,,,,,,,,,
Insufficient benchmark datasets for document-level temporal relation extraction (DocTRE),addressed by the new Tem-DocRED dataset.,,,,,,,,,,,,,,
"Current LLM-based frameworks face high computational complexity and slower inference compared to non-LLM baselines.""","Future research should address the lack of annotated datasets for document-level temporal relation extraction (DocTRE), develop new methods for DocTRE, and explore generating temporal knowledge graphs, as current approaches cannot extract facts with temporal information to construct such graphs.",,"The Tem-DocRED dataset redefines temporal relations for improved event representation, filters and constructs quadruples using content-based scoring, and provides detailed data records and structure to support downstream temporal knowledge graph tasks.","The objectives are to research tasks such as TKG (Temporal Knowledge Graph) reasoning and TKG question answering using the proposed concept and the DocTRE dataset, and to redefine relations for constructing new temporal relations in datasets like Tem-DocRED.",,,,,,,,,,,
Memoro: Using Large Language Models to Realize a Concise Interface for Real-Time Memory Augmentation,"Zulfikar Wazeer Deen, Chan Samantha, Maes Pattie",2024,reference-manager,10.1145/3613904.3642450,,,,,,"Within-subject user study: 20 participants experienced four different conditions to evaluate interaction, usability, and user experience with Memoro.",,"How can a memory assistant system be designed and evaluated to support users in conversational recall tasks by providing concise, context-aware memory cues during interactions?","The paper introduces Memoro, an audio-based wearable assistant using large language models to help users retrieve personal information through concise suggestions. In a study with 20 participants, Memoro improved recall confidence and conversational quality. Query Mode responses were 85% shorter than baseline, and most participants preferred Memoro.","The research goal was to design a minimally disruptive wearable memory assistant using LLMs; the approach involved developing Memoro with Query and Queryless modes for concise, context-aware memory retrieval; results showed Memoro improved recall confidence, reduced disruption, and was preferred by most participants, especially for concise responses.",
Thematic analysis: Open-ended feedback was coded and analyzed using Braun and Clarke’s method to generate and review themes.,,,,,,,,,,,,,,,
Technical evaluation: System response accuracy,conciseness,and processing times were measured using statistical tests (Shapiro-Wilk,Friedman,"Wilcoxon signed-rank).""",,,,,,,,,,,"Query Mode responses were significantly shorter than Baseline (85% reduction, 115.4 to 16.6 characters, p<.001); Query Mode also reduced average query time by 15% (3.4s to 2.9s, p=.03)."
System conditions significantly improved confidence (p<.001),relevance (p<.001),and reduced difficulty (p<.001) in recalling information compared to No System.,,,,,,,,,,,,,
"Query Mode was rated significantly more useful than Baseline (p<.01); no significant differences in helpfulness between conditions (p=.119).""","Query Mode responses were significantly shorter than Baseline (85% reduction, from 115.4 to 16.6 characters, 𝐴<.001); Queryless Mode had similar response length to Query Mode.",,,,,,,,,,,,,,
Query time reduced by 15% in Query Mode (3.4s to 2.9s,𝐴=.03).,,,,,,,,,,,,,,
System usability: Query Mode (80.0,𝐴𝐴=11.8),Queryless (77.1,𝐴𝐴=8.1),Baseline (68.75,,𝐴𝐴=15.15); significant difference between Baseline and Query Mode (𝐴=.015).,,,,,,,,,
Significant differences in rated appropriateness of response lengths between conditions (𝜋2 = 26,𝐴<.01).,,,,,,,,,,,,,,
No significant difference in helpfulness (𝜋2 = 4.25,𝐴=.119).,,,,,,,,,,,,,,
Usefulness: Query Mode rated higher than Baseline (𝐴=5.50 vs 4.30,𝐴<.01); no significant difference between Baseline and Queryless (𝐴=.0358),or Query and Queryless (𝐴=.233).,,,,,,,,,,,,,
Task load (NASA-TLX): lower in system conditions; Baseline (9.34,𝐴𝐴=7.19),Query (8.51,𝐴𝐴=9.93),Queryless (8.68,,𝐴𝐴=11.4),,,,,,,"The study was conducted in a laboratory setting, which may limit generalizability.","𝐴𝐴=7.06).""",No System (10.0
The fictional person introductions may not reflect real-world memory demands.,,,,,,,,,,,,,,,
Self-reported measures were used,which can introduce bias.,,,,,,,,,,,,,,
"Further research is suggested to explore real-world scenarios and long-term use.""","Query Mode produced significantly shorter and more concise responses than Baseline, with an 85% reduction in mean response length (115.4 to 16.6 characters).",,,,,,,,,,,,,,
Query Mode reduced average query time by 15% (3.4s to 2.9s).,,,,,,,,,,,,,,,
Users preferred Queryless Mode overall; Baseline was least preferred due to redundancy and lack of adaptability.,,,,,,,,,,,,,,,
Recommendations include prioritizing concise,"adaptive responses and minimizing conversation disruption in memory augmentation systems.""","Need for technical improvements in Memoro’s design, as current technical issues remain unaddressed.",,,,,,,,,,,,,
Necessity for longitudinal and real-world studies with diverse and specific populations (e.g.,elderly) to assess Memoro’s usefulness outside lab settings.,,,,,,,,,,,,,,
Exploration of alternative information presentation methods,"such as providing clues or familiar voices instead of direct answers.""","Future research should address issues not covered in the current Memoro design, such as privacy controls, social acceptability, and bystander considerations. Suggested directions include longitudinal and field studies in natural settings, studies with diverse and specific populations (e.g., elderly), and exploring alternative information presentation methods.",,"Most participants preferred at least one system condition over No System, with Query Mode providing significantly shorter and more concise responses than Baseline, and users valuing system integration, appropriateness, and improved conversational support.",,,,,,,,,,,
"Ontology-Driven Architecture for Managing Environmental, Social, and Governance Metrics
Explanation:
ESG: Environmental","Yu Mingqin, Rabhi Fethi A., Bandara Madhushi
Social","2024
and Governance",reference-manager,10.3390/electronics13091719,,,,,,,,,,,ESG; ontology; ontology-driven architecture; ESG metrics; design science method
Ontology: a structured framework for organizing information,,,,,,,,,,,,,,,
Ontology-driven architecture: system design based on ontology,,,,,,,,,,,,,,,
ESG metrics: measurements for ESG factors,,,,,,,,,,,,,,,
"Design science method: research approach for creating and evaluating artifacts""","The research goal is to develop a knowledge graph-based solution (ESGMKG) for ESG metric management using a combination ontology development approach within the Design Science Research methodology; results show the ontology-driven ESGMKG enables unified, flexible ESG reporting and effective querying of ESG metrics information.","The paper aims to develop an ESG Metrics Knowledge Graph (ESGMKG) and its ontology to improve ESG metric management and reporting. Using the Design Science Research (DSR) methodology and a combination ontology development approach, the study demonstrates that ESGMKG effectively answers key ESG queries, supporting comprehensive and structured ESG data management.",,"How can a knowledge graph-based solution, underpinned by a well-defined ontology, improve the management, assessment, and reporting of ESG metrics to address the challenges of data interoperability, personalization, and consistency across multiple ESG reporting frameworks?",,"Design Science Research (DSR) Methodology: A structured six-step approach for creating and evaluating IT artifacts, specifically applied to develop and assess the ESG Metrics Knowledge Graph (ESGMKG).",,,,,,,,,
Combination Ontology Development Process: Utilizes both top–down and bottom–up approaches to design the ESGMKG ontology,ensuring comprehensive domain representation.,,,,,,,,,,,,,,
Prototype Implementation and Validation: The ESGMKG is implemented on the Stardog Enterprise Knowledge Graph platform,"validated through usage scenarios and competency questions using SPARQL queries.""",The research is reproducible through its use of the Design Science Research (DSR) methodology and implementation using semantic web standards (RDF/OWL). The ontology and queries are executed on the Stardog platform. The source code for the project is not provided in the context.,The ESG Metrics Knowledge Graph (ESGMKG) was developed using the Design Science Research (DSR) methodology to address gaps in ESG metric management and reporting.,,,,,,,,,,,,
The Stardog platform implementation successfully answered all competency questions using SPARQL queries,demonstrating the system’s effectiveness.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) were reported.""",The primary outcome is the development of the ESG Metrics Knowledge Graph (ESGMKG) and its ontology.,,,,,,,,,,,,,,
The ESGMKG enables unified representation and querying of ESG metrics-related information.,,,,,,,,,,,,,,,
Experimental findings show the ontology-driven approach can accurately represent and manage ESG data for improved reporting.,,,,,,,,,,,,,,,
"No statistical values are provided.""","Limited depth and utility of the ontology; further research needed to enrich ESG model representation, data quality, and metric ranking.",,,,,,,,,,,,,,
Prototype tested only in selected scenarios; broader applicability not established.,,,,,,,,,,,,,,,
Lack of standardized and contextualized ESG indicators and metrics.,,,,,,,,,,,,,,,
High subjectivity and variability in ESG metrics.,,,,,,,,,,,,,,,
Challenges in aligning with diverse ESG reporting frameworks.,,,,,,,,,,,,,,,
Complex data management and integration with financial reporting.,,,,,,,,,,,,,,,
Evolving regulatory requirements across jurisdictions.,,,,,,,,,,,,,,,
"Difficulty achieving consistency and comparability in ESG reporting.""","The ESG Metrics Knowledge Graph (ESGMKG) effectively supports ESG metric management and assessment using a structured, ontology-based approach.",,,,,,,,,,,,,,
Current limitations include a narrow evaluation scope; future work will expand use cases and integrate natural language processing for automatic knowledge extraction.,,,,,,,,,,,,,,,
"Incorporating advanced technologies like large language models is recommended to enhance analysis and reduce maintenance effort.""","Enriching the ontology’s depth and utility in ESG model representation, information access, data quality, and ESG metric ranking.",,,,,,,,,,,,,,
Expanding evaluation to include more competency questions for different organizations and datasets.,,,,,,,,,,,,,,,
"Incorporating natural language processing and large language models for automatic knowledge extraction and enhanced analysis capabilities.""","Future research should focus on enriching the ontology for ESG model representation, improving information access and processing, enhancing data quality and ESG metric ranking, expanding use cases, integrating natural language processing for automatic knowledge extraction, and leveraging large language models to improve analysis and reduce maintenance.",,"The paper presents the design, development, and demonstration of an ESG Metrics Knowledge Graph (ESGMKG) using a combined ontology approach to improve ESG metric management and assessment, and evaluates its effectiveness through real-world competency questions.","The objective is to develop a knowledge graph-based solution (ESGMKG) for ESG metric management and assessment that maintains links between ESG reporting categories and selected metrics, enabling organizations to satisfy multiple reporting frameworks and improve consistency and comparability in ESG reporting.",,,,,,,,,,,
TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and Heterogeneous Graphs,"Gastinger Julia, Huang Shenyang, Galkin Mikhail, Loghmani Erfan, Parviz Ali, Poursafaei Farimah, Danovitch Jacob, Rossi Emanuele, Koutis Ioannis, Stuckenschmidt Heiner, Rabbany Reihaneh, Rabusseau Guillaume",2024,reference-manager,,,,,,,Data was collected from online public sources using Python scripts and software APIs.,,"What are the characteristics, collection processes, ethical considerations, and potential uses of the TGB 2.0 datasets released for benchmarking temporal knowledge graphs (TKGs) and temporal heterogeneous graphs (THGs)?","The paper introduces the TGB 2.0 benchmark datasets for temporal graph research, aiming to support academic benchmarking of methods. Data was collected from public sources and anonymized; no preprocessing was done. All datasets were tested and benchmarked. The datasets are public, intended for research, and regularly updated based on feedback.","The paper's main objective is to introduce TGB 2.0, a benchmark for evaluating multi-relational temporal graphs; the key method is providing new, large, diverse datasets and an automated evaluation pipeline; the principal finding is that existing methods struggle with scalability, and simple baselines remain competitive.",
Sampling strategies included random selection (e.g.,thgl-myket users over two weeks) and filtering by criteria such as Wiki page ID or node degree.,,,,,,,,,,,,,,
Datasets were split chronologically into training (70%),validation (15%),"and test (15%) sets.""",The research is reproducible. Mining scripts and preprocessing code are available on the project Github. Dataset and project documentation are at https://tgb.complexdatalab.com/. Main dataset files are at https://huggingface.co/datasets/andrewsleader/TGB/tree/main. Metadata is at https://object-arbutus.cloud.computecanada.ca/tgb/tgb2\_croissant.json.,"TGB 2.0 introduces four new TKG and four new THG datasets, which are significantly larger and more diverse than previous datasets, focusing on dynamic link property prediction.",,,,,,,,,,,
Experimental results show that both TKG and THG methods struggle with large-scale datasets due to long runtimes or memory limits; heuristic methods achieve competitive results.,,,,,,,,,,,,,,,
No statistical significance (p-values) is reported; Hits@10 and MRR metrics are used,"showing rankings are closely but not fully matched.""",All datasets were tested and benchmarked; results are in Section 5.,,,,,,,,,,,,,
Hits@10 (H@10) and Mean Reciprocal Rank (MRR) were reported (see Table 10).,,,,,,,,,,,,,,,
MRR per relation for the top 10 relations in three TKG datasets is shown in Figure 4.,,,,,,,,,,,,,,,
No single method showed superior performance across all four datasets.,,,,,,,,,,,,,,,
The RecB heuristic performed competitively,especially on tkgl-smallpedia and tkgl-icews.,,,,,,,,,,,,,,
Edgebank had low performance except on the large tkgl-wikidata dataset,where it was the only scalable method.,,,,,,,,,,,,,,
Methods achieved higher MRRs on datasets with high Recurrency Degrees and Consecutiveness values (tkgl-smallpedia,tkgl-wikidata).,,,,,,,,,,,,,,
"Exact statistical values for MRR per relation are visualized in Figure 4 but not explicitly listed in the text.""",Only continuous-time setting for THG datasets considered; many methods use discrete settings.,,,,,,,,,,,,,,
Datasets cover only five domains; biological and citation networks not included.,,,,,,,,,,,,,,,
Potential data privacy concerns due to sensitive information.,,,,,,,,,,,,,,,
Some datasets (tkgl-smallpedia,"tkgl-wikidata) may contain errors from crowd-sourced sources.""","The datasets are anonymized, publicly released, and intended for benchmarking temporal graph research methods.",,,,,,,,,,,,,
No ethical review or data protection impact analysis was conducted; datasets are for research use only.,,,,,,,,,,,,,,,
No preprocessing or cleaning was performed; raw data is from public sources or provided by Myket.,,,,,,,,,,,,,,,
"Datasets can be used for additional tasks like user churn prediction and studying knowledge changes over time.""","Scalability: Both TKG and THG methods struggle with large-scale datasets, indicating a need for more scalable approaches.",,,,,,,,,,,,,,
Discrete vs. Continuous-Time: Current work focuses only on continuous-time THG datasets; future work should include discretized versions for comparative analysis.,,,,,,,,,,,,,,,
"Dataset Diversity: The dataset collection lacks domains like biological and citation networks; expanding to more domains is a future direction.""","Future research should explore adding discretized versions of datasets for comparing discrete and continuous methods, expanding dataset domains (e.g., biological and citation networks), improving scalability for large datasets, and enhancing anonymization to address data privacy concerns. There is also room to improve current methods beyond simple baselines.",,"The datasets are publicly released for benchmarking temporal graph methods, are anonymized to protect privacy, have limitations in domain coverage and time settings, and will be expanded and updated based on community feedback to enhance diversity and comprehensiveness.","The objective of the study is to curate TGB 2.0 for realistic, reproducible, and robust evaluation of temporal multi-relational graphs. Specifically, the four TKG and four THG datasets are designed for the dynamic link property prediction task.",,,,,,,,,,,
Question Answering Over Temporal Knowledge Graphs,"Saxena Apoorv, Chakrabarti Soumen, Talukdar Partha",2021,reference-manager,,,,,,,"Creation of question templates for different reasoning structures, followed by human and machine paraphrasing to increase linguistic diversity.",,"How can large-scale, diverse temporal question answering datasets and enhanced temporal knowledge graph embedding models improve the performance of knowledge graph question answering systems on complex temporal reasoning tasks?","The paper introduces a large dataset for temporal knowledge graph question answering (KGQA), focusing on both simple and complex reasoning. Using human and machine-generated paraphrases, the study categorizes questions, applies temporal KG embeddings, and proposes CRONKGQA. Results show temporal embeddings improve performance, supporting the dataset’s value for temporal reasoning research.","The research goal is to create a large, diverse temporal knowledge graph question answering dataset using human and machine-generated paraphrases; the approach involves slot-filling templates and paraphrasing, and the principal finding is a dataset with 654 templates and strict train-test entity separation for robust temporal reasoning evaluation.",
Use of temporal knowledge graph (KG) embeddings and scoring functions (e.g.,TComplEx) for answer prediction.,,,,,,,,,,,,,,
Evaluation of multiple models (e.g.,BERT,T5,CRONKGQA) on a large,"automatically generated dataset with strict train/test entity separation.""",,"The research is reproducible; the authors state, """"we have released our code as well."""" This indicates that the source code for the project is available.",,,,,,,,,"CRONKGQA outperforms all baselines, achieving Hits@1 of 0.647 overall, 0.987 on simple questions, and 0.699 for entity answers."
Temporal KG embeddings (TComplEx) significantly improve performance over non-temporal embeddings (ComplEx),especially for simple questions.,,,,,,,,,,,,,,
"Model performance increases with larger training dataset size; before/after questions remain most challenging (best Hits@1: 0.288).""",CRONKGQA achieves the highest Hits@1 (0.647) and Hits@10 (0.884) overall on the CRONQUESTIONS dataset.,,,,,,,,,,,,,,
CRONKGQA outperforms other methods on both simple and complex reasoning tasks,especially on time join questions (Hits@1: 0.988).,,,,,,,,,,,,,,
Temporal KG embeddings (TComplEx) improve Hits@1 for CRONKGQA from 0.29 (ComplEx) to 0.987 (TComplEx) on simple questions.,,,,,,,,,,,,,,,
Increasing training dataset size improves model performance,especially for T-EaE-add.,,,,,,,,,,,,,,
Before/after questions are most challenging; best Hits@1 is 0.288.,,,,,,,,,,,,,,,
"BERT-based models perform significantly worse (Hits@1: 0.07–0.081) than KG embedding-based models.""",CRONKGQA achieves high accuracy on simple temporal reasoning but struggles with complex reasoning questions.,,,,,,,,,,,,,,
Question/answer pairs are generated automatically,resulting in an artificial question distribution with lower-than-natural linguistic variety.,,,,,,,,,,,,,,
"The dataset leaves ample scope to improve complex Temporal KGQA.""","The proposed CRONKGQA method significantly outperforms all baselines, especially on entity-type answers (Hits@1: 0.987, Hits@10: 0.992).",,,,,,,,,,,,,,
Temporal KG embeddings provide clear advantages over non-temporal embeddings.,,,,,,,,,,,,,,,
Larger training dataset size improves model performance.,,,,,,,,,,,,,,,
"The dataset enables robust evaluation of temporal reasoning in QA models.""","Lack of broad-coverage datasets for Temporal KGQA, limiting progress in the field.",,,,,,,,,,,,,,
State-of-the-art KGQA methods perform poorly on large,complex Temporal KGQA datasets like CRONQUESTIONS.,,,,,,,,,,,,,,
"Significant further improvements are possible in transformer-based approaches for Temporal KGQA.""","CRONQUESTIONS reveals that while CRONKGQA achieves high accuracy on simple temporal reasoning, it struggles with complex reasoning. Future research should focus on improving performance on complex Temporal KGQA tasks, addressing current limitations, and exploring more advanced reasoning capabilities.",,"The discussion concludes that the dataset enables training and evaluation of temporal reasoning models in knowledge graph question answering, despite its artificial question distribution and lower linguistic variety, by providing large-scale, diverse, and well-categorized question-answer pairs.","The objectives of the study are to evaluate how baselines and CRONKGQA perform on the CRONQUESTIONS task, determine if some methods perform better on specific reasoning tasks, assess how training dataset size affects model performance, and examine if temporal KG embeddings offer advantages over non-temporal KG embeddings.",,,,,,,,,,,
Overview of Current Challenges in Multi-Architecture Software Engineering and a Vision for the Future,"Sowiński Piotr, Lacalle Ignacio, Vaño Rafael, Palau Carlos E., Ganzha Maria, Paprzycki Marcin",2024,reference-manager,,,,,,,"Use of knowledge graphs for representing use case logic and requirements, aiding explainability in machine learning.",,"How can a holistic, ontology-based system leveraging AI, autonomy, and knowledge representation accelerate and automate software engineering tasks for multi-architecture applications across all stages of the software lifecycle?","The paper addresses challenges in developing multi-architecture applications, accelerating and automating software engineering (SE) processes, and software modeling. It proposes a comprehensive system architecture using WebAssembly, AI, and knowledge representation to automate SE tasks across the software lifecycle, aiming for a paradigm shift in next-generation software development.","The research goal is to address challenges in multi-architecture applications, software engineering process automation, and software modeling by proposing a unified system architecture using WebAssembly; the approach leverages AI and knowledge representation, and the principal finding is its potential to transform next-generation software development.",
Proposed system architecture integrating WebAssembly,Autonomy Core,and Modular Pluggable Connectors to automate software engineering processes.,,,,,,,,,,,,,
Application of neuro-symbolic fusion,"combining machine learning and symbolic reasoning for autonomous decision-making and explainability.""",No information available,"The study highlights the importance of open, shared APIs and a sustainable ecosystem of pluggable modules for multi-architecture software engineering.",,,,,,,,,,,,
It identifies significant technical challenges in integrating neuro-symbolic AI with knowledge graphs and dynamic knowledge graphs,requiring further research.,,,,,,,,,,,,,,
No quantitative results,conclusions,"or statistical significance (p-values) are explicitly provided.""",,Precise technological and methodological foundations for the Autonomy Core are not yet established; further studies are needed.,,,,,,,,,,,
Many technical challenges in integrating neuro-symbolic AI with knowledge graphs remain unsolved.,,,,,,,,,,,,,,,
The guiding scenario highlights areas in need of improvement in software engineering for heterogeneous,"evolving platforms.""","The study addresses challenges in developing multi-architecture applications, accelerating and automating software engineering processes, and software modeling.",,,,,,,,,,,,,
"It proposes an integrated system architecture using WebAssembly to accelerate software engineering tasks across the entire software lifecycle.
Leveraging AI",autonomy,"and knowledge representation may enable a paradigm shift in next-generation software development.""","Need for further studies to establish the precise technological and methodological base for the Autonomy Core, especially its integration with the WT.",,,,,,,,,,,,
Technical challenges remain in connecting neuro-symbolic AI with knowledge graphs and dynamic knowledge graphs.,,,,,,,,,,,,,,,
Sustaining an ecosystem of interdependent,pluggable modules through an open,"shared API.""",Further research is needed to establish the precise technological and methodological foundations for the Autonomy Core. Many technical challenges remain in connecting neuro-symbolic AI with knowledge graphs and dynamic knowledge graphs. These areas require additional exploration to address current limitations.,,,"The proposed system architecture leverages AI, autonomy, and knowledge representation to accelerate and automate software engineering for multi-architecture applications using WebAssembly, aiming for transparency, explainability, and modularity across all software lifecycle stages.",,,,,,,,,"The objectives are to address challenges in developing multi-architecture applications, accelerating and automating software engineering processes, and software modeling by proposing a system architecture that uses WebAssembly and AI to streamline all stages of the software lifecycle for multi-architecture applications."
"Integration of Knowledge and Task Management in an Evolving, Communication-intensive Environment","Hübscher Gerd, Geist Verena, Auer Dagmar, Hübscher Nicole, Küng Josef",2020,reference-manager,10.1145/3428757.3429260,,,,,,"Prototypical implementation and testing of the TEAM model in real-life environments, specifically at patent law firms.",,"How can a flexible, user-centered, knowledge- and process-centric model effectively integrate and support both dynamic knowledge work and well-defined administrative processes within organizations, particularly in communication-intensive scenarios like patent prosecution?","The paper aims to integrate knowledge and task/process management using the TEAM model, tested in Austrian patent law firms. The methodology involved prototypical implementation and user feedback. Results show improved support for both knowledge and administrative work, with users favoring general task and data object types. The TEAM model enables flexible, communication-focused work.","The research goal is to flexibly integrate knowledge and task management; the approach uses the TEAM model with a graph database and layered modeling; results show the system supports both knowledge and administrative work effectively, enabling flexible, user-centered management in real-world patent law firm settings.","Keywords for this research are: knowledge work, business process management, human communication, graph database."
Use of a graph database to integrate knowledge and task/process management,enabling flexible modeling of documents,tasks,and processes.,,,,,,,,,,,,
Layered modeling approach providing flexibility,user guidance,and traceability of communication,task handling,"and data manipulation.""",,,,,,,,,,,"Users found the integrated system intuitive for changing or specifying new data object types, but had difficulties selecting and specifying task types; they preferred using general types over specific ones."
The TEAM model enables flexible management of documents,knowledge,tasks,and processes,supporting both administrative and knowledge work in one integrated model.,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""",Both user groups reported satisfying experiences with information presentation.,,,,,,,,,,,,,,
Users had difficulties selecting or specifying task types before execution.,,,,,,,,,,,,,,,
Changing or specifying new data object types was intuitive within users’ own domains.,,,,,,,,,,,,,,,
Users preferred using general task and data object types over specific ones.,,,,,,,,,,,,,,,
"No statistical values reported.""",Users had difficulty selecting and specifying appropriate task types.,,,,,,,,,,,,,,
Users tended to extend general task types instead of creating specific ones.,,,,,,,,,,,,,,,
Defining types for both user groups required more communication.,,,,,,,,,,,,,,,
Users preferred general over specific task and data object types due to the effort involved.,,,,,,,,,,,,,,,
The resulting graph model is very complex,requiring specialized interfaces to manage complexity.,,,,,,,,,,,,,,
After a certain point,the number of new data objects decreases,but data object relations continue to grow,"increasing complexity.""","The TEAM model enables flexible, integrated management of documents, knowledge, tasks, and processes for both administrative and knowledge work.",,,,,,,,,,,
Users found the system intuitive for managing data object types but needed time to adapt to task execution and type selection.,,,,,,,,,,,,,,,
Specialized user interfaces are recommended to manage model complexity and improve usability.,,,,,,,,,,,,,,,
Users tend to prefer general over specific task and data object types,"suggesting a need for clearer guidance or support in type definition.""","Lack of models to define and handle evolving mental concepts in knowledge work, hindering continuous knowledge-driven processes.",,,,,,,,,,,,,
Insufficient integration of knowledge and task/process management,especially for supporting both dynamic communication-focused and well-structured processes within one system.,,,,,,,,,,,,,,
"Need for user interfaces that effectively manage the complexity of integrated graph models for practical user interaction.""","Future research should address gaps in integrating knowledge and tasks/processes, especially supporting dynamic communication and evolving mental concepts. Further investigation is needed into flexible, data-driven business process modeling that better supports knowledge work, user interaction modeling, and the balance between general and specific task/data object types.",,"The TEAM model enables flexible and integrated management of documents, knowledge, tasks, and processes for both administrative and knowledge work, with user tests supporting its suitability and highlighting improved interaction despite some initial user adaptation challenges.",,,,,,,,,,,,
Deriving Design Knowledge Graph for Complex Sociotechnical Systems Using the AIA Design Thinking,"Shen Tao, Gao Chan, Nagai Yukari, Ou Wei",2021,reference-manager,10.1155/2021/6416061,,,,,,Experiment: Participants were divided into two groups to design ideas for sustainable communities using either a knowledge graph with the AIA design thinking framework or a traditional knowledge graph.,,How can an integrated approach using the AIA design thinking framework and knowledge graph construction technology support idea generation and improve design outcomes for complex sociotechnical systems?,"The paper aims to develop an integrated approach for structuring a design knowledge graph using the AIA design thinking framework to support idea generation in complex sociotechnical systems. Through a controlled experiment with 28 design students, results show improved idea quality and idea space extension compared to traditional methods.",The research goal is to support idea generation for complex sociotechnical systems; the approach integrates a design knowledge graph using the AIA design thinking framework; results show this method improves idea space extension and idea quality compared to traditional knowledge graphs.,
Data Collection: Protocol data (utterances and interviews) and idea outputs were collected and analyzed.,,,,,,,,,,,,,,,
"Statistical Analysis: Paired t-test was used to compare idea space extension and idea quality/quantity between groups.""",,"The AIA design thinking framework significantly improved idea quality (Group A: 6.10 ± 0.81 vs. Group B: 4.90 ± 0.88, p = 0.002) and extension of idea space (Group A: 0.73 ± 0.10 vs. Group B: 0.64 ± 0.06, p = 0.001).",,,,,,,,,,,,,
No significant difference was found in idea quantity between groups (p = 0.336).,,,,,,,,,,,,,,,
"The approach supports deeper thinking in design and contributes to design support systems for complex sociotechnical systems.""",,"Participants were all students with an innovation design background, not practicing professionals, which may limit generalizability.",,,,,,,,,,,,,
The sample size was small (28 participants).,,,,,,,,,,,,,,,
Participants were volunteers and received no additional course credits,which may affect motivation.,,,,,,,,,,,,,,
"No information on long-term effects or real-world application.""","Using the AIA design thinking framework with a knowledge graph significantly increased the extension of idea space (0.73 ± 0.10 vs. 0.64 ± 0.06, p = 0.001).",,,,,,,,,,,,,,
The integrated approach supports better idea generation for complex sociotechnical systems.,,,,,,,,,,,,,,,
"Recommendation: Apply the AIA design thinking framework to enhance design support methods.""",Building a more complete design knowledge database for complex sociotechnical systems.,,,,,,,,,,,,,,
Improving the neural network model to develop a more robust architecture for learning design features.,,,,,,,,,,,,,,,
"Applying the proposed approach to derive more effective design knowledge graphs for broader use by other designers.""",,,"The integrated approach using a knowledge graph with the AIA design thinking framework significantly improves idea quality and the extension of idea space for complex sociotechnical system design, but does not significantly increase idea quantity compared to traditional methods.","The specific aims of the study are to evaluate whether using a knowledge graph with the AIA design thinking framework improves the quality and diversity (extension of idea space) of design ideas for sustainable communities, compared to using a traditional knowledge graph.",,,,,,,,,,,
Data Integration Challenges for Machine Learning in Precision Medicine,"Martínez-García Mireya, Hernández-Lemus Enrique",2022,reference-manager,10.3389/fmed.2021.784455,,,,,,Algorithmic inventory: Creating and maintaining a record of computational methods used in the institution.,,"How can diverse biomedical data sources—including molecular, clinical, social, and environmental information—be effectively integrated, modeled, and managed using AI/ML approaches to advance Precision Medicine while addressing ethical, legal, and privacy challenges?","The paper discusses the importance of a Data Management Plan (DMP) for projects handling large, sensitive datasets. The DMP sets guidelines for data handling, quality control, preservation, and compliance with legal and organizational requirements. The study highlights DMPs as essential for ensuring data integrity, security, and future usability.","The paper's main objective is to discuss challenges in applying large-scale data analytics for Precision Medicine, using a review approach, and concludes that comprehensive data management plans are needed to address technical, computational, ethical, and policy issues for effective AI/ML integration in clinical settings.",
Monitoring and evaluation: Monitoring clinical use and performance of computational tools,and evaluating their safety,efficacy,and fairness.,,,,,,,,,,,,
"Use of metadata: Leveraging metadata for automated labeling or classification tasks in AI/ML for Precision Medicine.""","The context mentions the importance of reproducibility and data sharing but does not provide specific information on the reproducibility of the research or source code for the project. Some software packages and tools are listed with URLs, but no direct source code for the main research is given.",,,Class imbalance can cause machine learning methods to underestimate features of minority classes.,,,,,,,,,,,
Real-life datasets often have multiple challenging features,making it hard for methods to perform well across all issues.,,,,,,,,,,,,,,
Each method for large-scale data analytics has its own assumptions and limitations.,,,,,,,,,,,,,,,
"Combining methods and drawing conclusions can be complex and non-trivial.""","AI/ML approaches are highly valuable for large-scale biomedical and clinical data analysis, crucial for Personalized Medicine.",,,,,,,,,,,,,,
Implementing these methods in medical settings faces technical,computational,mathematical,ethical,legal,,and policy challenges.,,,,,,,,,
"Data-intensive strategies and proactive data management planning are recommended to address these challenges.""",Integrating diverse and heterogeneous data remains a major challenge for effective AI/ML in Precision Medicine.,,,,,,,,,,,,,,
"There is a need to develop standards for data sharing
Addressing technical","collaboration
computational","and large-scale data analytics in biomedical and clinical ecosystems.
ethical",legal,"and policy issues is essential for successful data integration.""",,"Future research should focus on developing effective de-identification methods for sensitive clinical data, revising informed consent procedures due to AI/ML advances, improving data integration strategies for heterogeneous sources, addressing spurious correlations and incidental endogeneity, and standardizing metadata reporting in biomedical and clinical settings.",,,,,,,"The objectives are to integrate multiple, diverse data sources for interpretable models in Precision Medicine, enable complex queries and heterogeneous models, develop high-level intuition and automated reasoning using AI/ML, ensure privacy and data portability, and establish comprehensive data management plans and standards for data handling and reporting.","The discussion highlights the need for adapting informed consent, enhancing patient education, and implementing comprehensive data management plans to address privacy, data use, and governance challenges in large-scale biomedical research involving sensitive data and AI/ML advances.",
Large Language Models Can Learn Temporal Reasoning,"Xiong Siheng, Payani Ali, Kompella Ramana, Fekri Faramarz",2024,reference-manager,,,,,,,"Temporal graph (TG): Uses structured, intuitive representations of temporal relationships to enhance reasoning.",,"How can large language models be improved to perform complex temporal reasoning tasks by integrating temporal graphs, chain-of-thought reasoning, and external knowledge?","The paper proposes TG-LLM, a framework that enhances large language models' temporal reasoning by integrating temporal graphs and intermediate reasoning steps. Using Llama2-13B and ablation studies, the methodology demonstrates that explicit graph structures, chain-of-thought bootstrapping, data augmentation, and external knowledge significantly improve performance. TG-LLM outperforms existing methods.","The research goal is to improve large language models' temporal reasoning by introducing TG-LLM, a framework that translates text to temporal graphs and performs graph-based reasoning; results show TG-LLM outperforms existing methods, especially when using graph augmentation and external knowledge.",
Chain of thought (CoT) bootstrapping with contrastive learning: Improves reasoning by generating and refining intermediate reasoning steps.,,,,,,,,,,,,,,,
"Graph data augmentation and external knowledge integration: Augments training data and incorporates mathematical and commonsense knowledge for better performance.""","No source code for the project is mentioned in the context. The reproducibility relies on public datasets (YAGO11k, TimeQA, TempReason), detailed model descriptions, prompt templates in Appendix B and C, and a semi-automatic data verification pipeline, but no explicit code release is indicated.","The TGQA dataset contains 400 training, 100 validation, and 100 test samples, each with about 30 QA pairs. Q1-type questions are most common, followed by Q7.",,,,,,,,,,,,,
Llama2-13B (1-shot SFT - TG + EK + CoT(bs + aug)) achieved the highest exact match (EM) score of 0.797.,,,,,,,,,,,,,,,
"No statistical significance (p-values) is reported.""",,TG-LLM needs adaptations for temporal commonsense reasoning.,,,,,,,,,,,,,
Explicit in-context integration of commonsense is still an open opportunity.,,,,,,,,,,,,,,,
Improvements mainly come from a new paradigm and better training data; further enhancements could involve simulating environments for feedback.,,,,,,,,,,,,,,,
"Dataset may contain improper or harmful content.""","TG-LLM, a new framework, significantly improves temporal reasoning in language models by using temporal graphs and intermediate reasoning steps.",,,,,,,,,,,,,,
TG-LLM outperforms existing methods across multiple datasets and metrics.,,,,,,,,,,,,,,,
Key enhancements include graph-based reasoning,chain-of-thought bootstrapping,data augmentation,and external knowledge integration.,,,,,,,,,,,,
"Future work should extend TG-LLM to more complex reasoning tasks.""",Extending the TG-LLM framework to more complex applications such as inductive and abductive reasoning.,,,,,,,,,,,,,,
Improving model performance on tasks requiring mathematical,logical,and commonsense knowledge integration.,,,,,,,,,,,,,
Enhancing representation learning for the underlying structure and logic of temporal reasoning,"which is currently underexplored.""","Future research should extend TG-LLM to more complex applications like inductive and abductive reasoning, adapt it for temporal commonsense reasoning, and explore explicit in-context integration of commonsense. Simulating environments to provide feedback and verifying generated temporal graphs using prior knowledge are also suggested directions.",,"TG-LLM, which equips language models with temporal graphs and intermediate reasoning steps, outperforms existing methods in temporal reasoning tasks and shows promise for future extensions to more complex reasoning applications.",,,,,,,,,,,
Developing a proof-of-concept curriculum foundation model for industry 5.0: A primary data survey of built environment academics,"Posillico John J, Edwards David J",2024,reference-manager,10.1177/09504222231224090,,,,,,"Structured questionnaire survey: Developed using job advertisements and industry indices, including closed Likert scale and open-ended questions to collect quantitative and qualitative data.",,"What are the core interpersonal and technical skills and competencies required of a contemporary construction management graduate, and how can these be incorporated into an optimized higher education curriculum?","The study aimed to identify core interpersonal and technical skills needed by construction management graduates for curriculum development. Using a mixed-methods approach with descriptive and inferential statistics, findings show interpersonal skills are rated more important than technical skills. The research proposes a model to optimize higher education curricula.","The research goal was to identify core interpersonal and technical skills for construction management graduates using a mixed-methods approach; results show interpersonal skills are more important than technical ones, leading to a proof-of-concept curriculum model prioritizing communication, teamwork, leadership, and listening/understanding.",
Mixed philosophical stance: Combined postpositivism (for statistical analysis) and interpretivism (for understanding development skills),using abductive reasoning.,,,,,,,,,,,,,,
Descriptive statistics: Used mean,median,variance,"and proportion to summarize and contextualize survey data.""",,,"The survey is reliable with a Cronbach’s alpha of 0.839 (>0.700), indicating strong internal consistency.",,,,,,,,,
Interpersonal skills were rated highly: 93.31% of responses marked them as ‘important’ or ‘very important’; Relative Importance Index (RII) values for these skills ranged from 0.822 to 0.966.,,,,,,,,,,,,,,,
Technical skills like “Creating BIM” and “Digital technologies” had lower RII values (0.617–0.622),"indicating they were considered less important than interpersonal skills. Statistical significance (p-values) not reported.""",,Lack of connectivity between research conducted across the HEI sector.,,,,,,,,,,,,
Prevailing academic individualism and bespoke curriculum development hinder cohesion.,,,,,,,,,,,,,,,
Digital specialists may lack practical construction management knowledge.,,,,,,,,,,,,,,,
"The proof-of-concept model requires further statistical testing and development in future research.""",Interpersonal skills are more important than technical skills for construction management graduates.,,,,,,,,,,,,,,
Many interpersonal skills and competencies outrank technical ones in importance.,,,,,,,,,,,,,,,
Digital skills are ranked lower than traditional skills like workflow,budgeting,and costing.,,,,,,,,,,,,,
"The study recommends prioritizing interpersonal skills in curriculum development for construction management.""","Lack of connectivity and cohesion in research across higher education institutions, leading to fragmented knowledge.",,,,,,,,,,,,,,
Misalignment between construction management education and the core skills actually required by the profession,especially undervaluing interpersonal skills.,,,,,,,,,,,,,,
Overemphasis on digital skills in curricula despite limited perceived importance by academics,"suggesting a need to reassess curriculum priorities.""",Future research should explore how practicing academics view and integrate specific interpersonal skills as foundational in construction management curricula. Action research and longitudinal case studies are recommended to validate findings and assess if applying these theories improves graduate employability in the industry.,,"Academics strongly prioritize interpersonal skills—especially teamwork, communication, leadership, and listening/understanding—over technical or digital skills for construction management graduates, despite current curricula emphasizing digital competencies.",,Objectives:,,,,,,,,,
Investigate the core interpersonal and technical skills and competencies required of a contemporary construction management graduate.,,,,,,,,,,,,,,,
Formulate a foundational set of core interpersonal and technical skills for construction management curricula.,,,,,,,,,,,,,,,
"Stimulate pathways for industry and academic informed curriculum development.""",,,,,,,,,,,,,,,
Personalized Diabetes Management with Digital Twins: A Patient-Centric Knowledge Graph Approach,"Sarani Rad Fatemeh, Hendawi Rasha, Yang Xinyi, Li Juan",2024,reference-manager,10.3390/jpm14040359,,,,,,"Comprehensive data collection and integration from sources like electronic health records, wearable devices, mobile health applications, and patient-generated data.",,"How can a comprehensive digital twin framework, integrating ontology development, data collection, and personal health knowledge graph construction, be developed to enable personalized diabetes management through dynamic, patient-centered virtual representations?","The paper introduces a novel approach to personalized diabetes management using digital twins and patient-centric knowledge graphs (PHKGs). By integrating diverse healthcare data and standardized ontologies, the methodology enables comprehensive, adaptable, and patient-centered care. The findings highlight improved personalization and knowledge sharing in diabetes management.","The research goal was to improve personalized diabetes management using digital twins and patient-centric knowledge graphs; the approach integrated diverse healthcare data with standardized ontologies, and results showed highly precise glucose prediction and control, highlighting adaptability, patient-centricity, and potential for broader healthcare applications.",
Development and use of standardized health ontologies to ensure interoperability and support digital twin models.,,,,,,,,,,,,,,,
"Construction of patient-centric knowledge graphs (PHKGs) and generation of digital twins for personalized diabetes management.""",,"The study introduces a novel personalized diabetes management approach using digital twins and patient-centric knowledge graphs (PHKGs), emphasizing interoperability and adaptability.",,,,,,,,,,,,,
The PHKG integrates diverse health data,enabling personalized meal recommendations and holistic patient understanding.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""",,,The study introduces a novel approach to personalized diabetes management using digital twins and patient-centric knowledge graphs (PHKGs).,,,,,,,,,,,,
Emphasizes integrating diverse healthcare data sources and ensuring interoperability via standardized ontologies.,,,,,,,,,,,,,,,
Highlights strengths: patient-centricity,knowledge sharing,and adaptability to new data.,,,,,,,,,,,,,
Recommends adopting this methodology for enhanced,"individualized diabetes care.""","Existing diabetes management approaches are fragmented, often focusing on isolated aspects rather than integrating genetic, lifestyle, and environmental factors.",,,,,,,,,,,,,
There is a need for improved data integration and interoperability across diverse healthcare data sources using standardized ontologies.,,,,,,,,,,,,,,,
"Future research should focus on expanding and adapting patient-centric knowledge graphs (PHKGs) as new patient data and knowledge become available.""",,,"The paper introduces a novel, patient-centric digital twin framework using personal health knowledge graphs (PHKGs) for personalized diabetes management, emphasizing holistic data integration, interoperability, adaptability, and demonstrating its effectiveness in real-world applications like glucose prediction and insulin optimization.","The objective of this research is to develop digital twins for personalized diabetes management. This includes creating adaptive virtual representations of patient health, integrating diverse data sources, building a robust ontology, and enabling precise, individualized care and outcome prediction through advanced machine learning techniques.",,,,,,,,,,,
A SysML-based Integration Framework for the Engineering of Mechatronic Systems,"Chami Mohammad, Seemüller Holger, Voos Holger",2023,reference-manager,,,,,,,"V-model: A process model divided into four phases—requirements, system design, domain-specific design, and integration—used to structure interdisciplinary engineering and validation.",,"How can an integrated framework based on SysML and the V-model improve interdisciplinary collaboration, data integration, and knowledge-sharing in the engineering process of mechatronic systems?","The paper aims to address the complexity and interdisciplinary challenges in mechatronic system engineering by proposing a SysML-based integration framework. Using the V-model process and SysML for system design, the approach improves collaboration, manages complexity, and supports model-based engineering. The framework enhances communication and understanding among engineers.","The paper's main objective is to improve collaboration in mechatronic engineering by integrating domain-specific models using a SysML-based framework, with the key method being the unification of models under a distributed SysML system model, and the principal finding is enhanced knowledge-sharing, traceability, and interdisciplinary communication.",
SysML-based modeling: Use of SysML (Systems Modeling Language) to create detailed,interdisciplinary system models for mechatronic products.,,,,,,,,,,,,,,
Multiagent-based architecture: Implementation of agents (software assistants) to manage,monitor,"and coordinate engineering tasks and data exchange across domains.""",,"The new SysML-based approach unifies interdisciplinary models into a distributed overall data model, improving knowledge-sharing, traceability, navigation, and transition between design phases.",,,,,,,,,,,
The approach was exemplified with a quadrotor UAV project,demonstrating integration of mechanical,electrical,and IT models.,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are provided.""",,Lack of interoperability between specialized tools complicates interdisciplinary collaboration.,,,,,,,,,,,,,
The V-model is a theoretical construct without tool support and is essentially sequential,leading to long durations.,,,,,,,,,,,,,,
The approach requires further development,including architecture improvement,agent goal identification,and communication protocol creation.,,,,,,,,,,,,
"Methodologies for integrating the approach into an overall process model are still needed.""","The new approach links distributed models into a unified SysML-based data model, improving knowledge-sharing, traceability, and navigation.",,,,,,,,,,,,,,
"Communication and collaboration in mechatronic system development are enhanced.
Agents support",but do not replace,human engineers.,,,,,,,,,,,,,
Future work includes refining the architecture,defining agent goals,developing communication protocols,"and integrating methodologies like SYSMOD or Pahl\&Beitz.""","The architecture for realization needs improvement and clarification, including reconsideration and partitioning of use cases among different agent types.",,,,,,,,,,,
Development of agent goals and a first communication protocol is required,followed by practical examination through a prototype.,,,,,,,,,,,,,,
Methodologies must be established to integrate the approach into an overall process model,"potentially extending the V-model with methods like SYSMOD or Pahl\&Beitz.""","Future research should improve and specify the architecture for implementation, reconsider and partition use cases among agent types, define agent goals, and develop a communication protocol. Methodologies for integrating the approach into an overall process model should be established, possibly extending the V-model with methods like SYSMOD or Pahl\&Beitz.",,"The proposed approach links distributed, discipline-specific models into a unified SysML-based data model, improving knowledge-sharing, traceability, navigation, and collaboration throughout the V-model process for mechatronic system development.",,"The objectives of the study are to improve collaboration and communication in engineering by integrating multiple tool-specific model data into an overall system model using SysML, addressing knowledge-sharing, traceability, navigation, and supporting the transition between domain-independent and domain-specific phases of the V-model.",,,,,,,,,
"KNOW–A Real-World Ontology for Knowledge Capture with Large Language Models
Open-source","Bendiken Arto
collaborative","2024
and iterative development: The ontology is developed publicly on GitHub","reference-manager
emphasizing community contributions and real-world use cases.",,,,,,,"Comparative analysis: The study compares and contrasts previous ontologies, specifically Schema.org and Cyc, to highlight differences and similarities.",,"How can the KNOW ontology be designed and implemented to capture and represent everyday knowledge for augmenting large language models in real-world generative AI use cases, while prioritizing developer experience, interoperability, and pragmatic, commonsense modeling of social relationships and roles?","The paper introduces KNOW, an ontology designed to capture and represent everyday knowledge for enhancing large language models in generative AI, especially personal AI assistants. Using Semantic Web standards (RDF, OWL) and prioritizing developer experience, KNOW emphasizes commonsense modeling, open-source development, and interoperability. The initiative aims for broad applicability and polyglot programming support.","The research goal is to create KNOW, an ontology for representing everyday knowledge to support generative AI; the approach uses a pragmatic, developer-friendly, open-source RDF/OWL framework; results show KNOW enables interoperable AI by capturing commonsense knowledge with a flat class hierarchy and broad language support.",
Code generation for SDKs: The ontology is designed for code generation,"enabling software development kits (SDKs) in multiple programming languages for direct use of ontology concepts.""","The research is highly reproducible. The source code and ontology are openly developed on GitHub (github.com/KnowOntology), with documentation and downloads available at know.dev. Open-source SDKs are provided for 12 major programming languages, supporting direct use and extension by the community.","The KNOW ontology focuses on pragmatic, commonsense modeling of everyday knowledge, especially social relationships, differing from Schema.org and Cyc in scope, hierarchy, and openness.",,,,,,,,,,,,
Open-source SDKs for 12 major programming languages enable direct use of ontology concepts and semantic interoperability.,,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""",,Limited context window and poor scaling in LLMs.,,,,,,,,,,,,,
Lack of introspectability and justifiability in LLMs.,,,,,,,,,,,,,,,
Propensity for hallucinations in LLMs.,,,,,,,,,,,,,,,
Implicit and static nature of LLM knowledge,making updates difficult.,,,,,,,,,,,,,,
Practitioners’ general lack of awareness of knowledge graphs and ontologies.,,,,,,,,,,,,,,,
"Much remains to be done; the ontology is a first draft.""","KNOW is an ontology designed to capture everyday knowledge for augmenting LLMs in generative AI, with a focus on commonsense, pragmatic modeling and developer experience.",,,,,,,,,,,,,,
The initiative aims to promote interoperability and dissemination of semantic technology in AI through open-source,collaborative development.,,,,,,,,,,,,,,
"Current ad-hoc approaches to knowledge representation are unsustainable; adopting structured ontologies like KNOW is recommended for future-proofing AI systems.""","Lack of awareness and adoption of ontologies and semantic technologies among LLM practitioners, leading to ad-hoc and non-interoperable knowledge capture.",,,,,,,,,,,,,,
Need to expand language support and interoperability,aiming to cover at least the top 20 most popular programming languages.,,,,,,,,,,,,,,
Ongoing requirement for open-source,collaborative,"and iterative development to extend the ontology’s coverage and applicability.""",,,,"The KNOW ontology provides a commonsense, everyday framework focused on human universals and social relationships to support neuro-symbolic AI, differing from Schema.org in scope, structure, and use cases, and is open source for broad, real-world application and ongoing community-driven development.",,,,,,,,,"The objectives are to create an open-source, commonsense ontology (KNOW) for neuro-symbolic AI, focusing on human universals (spacetime, social relationships, basic needs, emotions, daily activities), enable data interchange via JSON and RDF serializations, and extend support to at least the top 20 programming languages."
DEPSRAG: Towards Agentic Reasoning and Planning for Software Dependency Management,"Alhanahnah Mohannad, Boshmaf Yazan",2024,reference-manager,,,,,,,Retrieval-Augmented Generation (RAG): Uses a large language model (LLM) to answer questions by retrieving relevant data from databases and providing source references for validation.,,"How can a multi-agent, retrieval-augmented generation system like DEPSRAG be designed and orchestrated to automate software dependency analysis, identify critical and vulnerable packages, and generate comprehensive, validated reports such as SBOMs while addressing the challenges of complex dependency structures and LLM limitations?","The paper aims to improve software supply chain security by introducing DEPSRAG, a multi-agent system for generating Software Bill of Materials (SBOM) and managing dependencies. Using orchestrated agents and retrieval-augmented generation, DEPSRAG constructs hierarchical dependency graphs, identifies risky dependencies, and recommends secure updates, exceeding regulatory SBOM requirements.","The research goal is to improve software dependency management and security; the approach uses a multi-agent Retrieval-Augmented Generation (RAG) system called DEPSRAG to analyze and recommend safe dependencies; results show DEPSRAG generates comprehensive dependency graphs, supports SBOM generation, and suggests non-vulnerable package versions.",
Multi-Agent Orchestration: Employs multiple agents communicating via message exchange,managed by the Langroid framework,to delegate and complete sub-tasks.,,,,,,,,,,,,,
Dependency Graph Construction and Analysis: Constructs software dependency graphs,identifies nodes with the highest in-degree,"and queries vulnerability databases for risk assessment.""",,"The top five packages with the highest in-degree (most dependencies relying on them) are: typing-extensions (9), anyio (7), opentelemetry-api (6), deprecated (4), and idna (4).",,,,,,,,,,,
No known vulnerabilities were reported for these high in-degree packages at this time.,,,,,,,,,,,,,,,
"No p-values or statistical significance data are provided.""",,LLM responses are limited to pre-training knowledge and cannot address private or post-training information.,,,,,,,,,,,,,
LLM cannot verify the correctness of its responses.,,,,,,,,,,,,,,,
Existing security tools may miss issues like circular dependencies or version conflicts.,,,,,,,,,,,,,,,
"Updating dependencies can be cumbersome and may introduce incompatibilities.""","DEPSRAG generates comprehensive Software Bill of Materials (SBOM), documenting all direct and transitive dependencies, exceeding regulatory requirements.",,,,,,,,,,,,,,
It provides recommendations for non-vulnerable package versions,reducing compatibility issues during dependency updates.,,,,,,,,,,,,,,
Multi-agent orchestration and Critic-Agent evaluation improve answer quality and task management.,,,,,,,,,,,,,,,
Advanced dependency analysis addresses gaps in current tools,"supporting better vulnerability management.""",Lack of advanced tools for analyzing and addressing security and maintainability concerns in open-source and third-party libraries.,,,,,,,,,,,,,
Challenges in decomposing complex,hierarchical software dependency tasks for effective RAG application and risk assessment.,,,,,,,,,,,,,,
Need for improved orchestration mechanisms to prevent issues like infinite loops,deadlocks,"and instruction deviation in multi-agent systems.""","Future research should evaluate different Critic-Agent settings to optimize feedback effectiveness, develop mechanisms for SBOM (Software Bill of Materials) generation and vulnerability management, and enhance dependency resolution to recommend non-vulnerable, compatible package versions. Efficient orchestration and prevention of feedback loops are also identified as important areas for further investigation.",,,"The integration of CriticAgent in DEPSRAG significantly improves answer accuracy—raising precision from 13.3% to 40%—by enabling iterative feedback and validation, while the system’s structured dependency graph supports comprehensive SBOM generation and effective vulnerability management.",,,,,,,,,"The objectives of the study are to: (1) identify risky dependencies in software packages, (2) evaluate different Critic-Agent settings for feedback optimization, (3) generate a Software Bill of Materials (SBOM) with full dependency chains, and (4) provide recommendations for safe dependency resolution."
Blockchain-secure patient Digital Twin in healthcare using smart contracts,"Amofa Sandro, Xia Qi, Xia Hu, Obiri Isaac Amankona, Adjei-Arthur Bonsu, Yang Jingcong, Gao Jianbin",2024,reference-manager,10.1371/journal.pone.0286120,,,,,,"Multi-receiver Identity-Based Signcryption (mIBSC): Uses four algorithms (Setup, Extract, Signcrypt, Designcrypt) to securely share data with multiple receivers.",,"How can a blockchain-secure patient digital twin using smart contracts ensure secure, private, and efficient personal health data sharing and access control in healthcare systems?","The paper investigates digital twin technology in healthcare, aiming to enhance secure patient data sharing. It introduces a mathematical model for patient Digital Twins and proposes a novel Multi-receiver Identity-Based Signcryption (mIBSC) scheme. Using blockchain and smart contracts, the study demonstrates improved security, privacy, and efficient data management.","The paper's main objective is to enable secure, privacy-preserving patient data sharing in healthcare using a blockchain-secure digital twin; it employs smart contracts and multi-receiver identity-based signcryption (mIBSC), and demonstrates reduced communication/computation costs and improved data integrity compared to existing frameworks.",No information available
Digital Twin Modeling: Creates evolving,data-driven models of patients using Electronic Medical Records and other sources for analysis and prediction.,,,,,,,,,,,,,,
"Smart Contracts: Automates and secures data access transactions within the digital twin system after authentication and policy checks.""",The research is open access and the dataset can be requested by emailing 202124080119@std.uestc.edu.cn. There is no explicit mention of source code availability for the project. Reproducibility is partially supported through data access upon request.,"The proposed system maintains static ciphertext size regardless of the number of receivers, reducing communication and computation costs compared to others where ciphertext size grows linearly.",,,,,,,,,,,,,
It provides a system model enabling secure digital twin access to data/services,with verification and digital twin authorship tracking.,,,,,,,,,,,,,,
Experimental results demonstrate lower signcryption and unsigncryption costs,"and improved efficiency over existing frameworks; specific p-values or statistical significance are not reported.""",,Some protocols lack verification.,,,,,,,,,,,,
Ciphertext sizes in some protocols grow linearly with the number of receivers.,,,,,,,,,,,,,,,
Not applicable to health data sharing if there is no system model.,,,,,,,,,,,,,,,
Some works do not specify a model for data sharing.,,,,,,,,,,,,,,,
Some do not provide experimental results.,,,,,,,,,,,,,,,
Some have unclear blockchain implementation.,,,,,,,,,,,,,,,
Some store raw data off-chain.,,,,,,,,,,,,,,,
Some works are not applicable to healthcare.,,,,,,,,,,,,,,,
Some lack data confidentiality.,,,,,,,,,,,,,,,
Some do not describe security parameters for file storage.,,,,,,,,,,,,,,,
Some use symmetric encryption without one-to-many sharing support.,,,,,,,,,,,,,,,
Some do not provide integrity controls for off-chain storage.,,,,,,,,,,,,,,,
Some focus only on individual patient data sharing.,,,,,,,,,,,,,,,
Some use anonymity instead of identity for digital twin creation.,,,,,,,,,,,,,,,
"Some do not protect data integrity.""","The proposed system provides a secure, blockchain-based digital twin model for healthcare, supporting one-to-many data sharing and service access.",,,,,,,,,,,,,,
It maintains static ciphertext size regardless of the number of receivers,reducing communication and computation costs.,,,,,,,,,,,,,,
The scheme ensures data confidentiality,integrity,and authorship tracking.,,,,,,,,,,,,,
Recommendations include adopting this model for secure,"efficient digital twin operations in healthcare.""",Lack of a comprehensive system model for secure health data sharing and digital twin operations.,,,,,,,,,,,,,
Absence of protocol verification and integrity controls in existing solutions.,,,,,,,,,,,,,,,
"Need for enhanced digital twin autonomy and support for complex operations like data and service contracts.""","Future research should address protocol verification, develop system models for health data sharing, and explore digital twin operations like data and service contracts. Additional investigation is needed into efficient ciphertext size management and digital twin authorship tracking for patient data completeness and progress.",,"The discussion concludes that the proposed system model enables secure and efficient patient Digital Twin operations using smart contracts, reducing communication and computation costs, supporting authorship tracking, and addressing research gaps in healthcare digital twin technologies.",,,,,,,,,,,,
Building and Using a Knowledge Graph to Combat Human Trafficking,"Szekely Pedro, Knoblock Craig A., Slepicka Jason, Philpot Andrew, Singh Amandeep, Yin Chengye, Kapoor Dipsy, Natarajan Prem, Marcu Daniel, Knight Kevin, Stallard David, Karunamoorthy Subessware S., Bojanapalli Rajagopal, Minton Steven, Amanatullah Brian, Hughes Todd, Tamayo Mike, Flynt David, Artiss Rachel, Chang Shih-Fu, Chen Tao, Hiebel Gerald, Ferreira Lidia",2023,reference-manager,,,,,,,Data acquisition using Apache Nutch to crawl and extract information from relevant web pages at scale.,,"How can Semantic Web technologies be used to build scalable, high-quality knowledge graphs that integrate heterogeneous data sources to support combating human trafficking?","The paper presents the DIG system, which uses Semantic Web technologies to build a knowledge graph for combating human trafficking. Using a common ontology and tools like Karma, it integrates diverse data sources, enabling large-scale, provenance-rich analysis. The approach is scalable and adaptable to other domains beyond human trafficking.","The research goal is to rapidly build knowledge graphs for domains like human trafficking using Semantic Web technologies; the approach integrates diverse data sources into a common ontology with tools like Karma; the principal finding is that this method enables scalable, flexible knowledge graph construction for complex problem-solving.",
Data integration and merging using Karma to convert datasets into JSON-LD,link similar records,and merge documents into a unified knowledge graph.,,,,,,,,,,,,,
"Use of Semantic Web technologies
,The DIG system enables rapid construction of large knowledge graphs for domains like human trafficking
As of July 2015","including common ontology
integrating diverse data sources using Semantic Web technologies.
the system processed 60 million ads (162","URIs
000 new ads/day)","and RDF
with 1.4 billion RDF subjects and 222 million feature objects; rebuilding the graph takes under 19 hours.","to represent and publish data.""",,"The research describes Karma, which can generate JSON-LD and n-triples, and mentions modifications for Hadoop and AWS. However, there is no explicit mention of source code availability or detailed reproducibility instructions.",,,,,,,,,
"No statistical significance (p-values) or quantitative evaluation results are reported.""",,"No agreement on APIs or schemas among over 15 organizations, leading to incompatible data formats and attribute names.",,,,,,,,,,,,,
Provenance challenges: Difficult to trace and resolve conflicting extractions from multiple sources.,,,,,,,,,,,,,,,
Scale: Managing and processing over 1.4 billion nodes from 50 million+ web pages is challenging.,,,,,,,,,,,,,,,
"Need for further research to refine tools and leverage ontological axioms for richer queries.""",The DIG system effectively builds knowledge graphs using Semantic Web technologies to combat human trafficking and other domains.,,,,,,,,,,,,,,
DIG enables integration of diverse data sources and extraction tools,supporting large-scale,flexible,and rapid application development.,,,,,,,,,,,,
Provenance tracking is essential for data reliability and law enforcement needs.,,,,,,,,,,,,,,,
Future work includes improving usability,richer queries,"and expanding deployment.""","Lack of agreement on APIs or schemas among consortium organizations, leading to integration challenges.",,,,,,,,,,,,
Need for efficient techniques to leverage ontological axioms for richer queries and user interface facets.,,,,,,,,,,,,,,,
"Scalability issues in rebuilding and incrementally updating a knowledge graph with over 1.4 billion nodes.""","Future research should focus on refining tools and technology for faster application development, leveraging ontological axioms for richer queries and user interface facets, and developing efficient techniques for forward-chaining inferences and explicit representation in the knowledge graph.",,"The DIG system uses Semantic Web technologies to build knowledge graphs for combating human trafficking and other domains, and future work will focus on refining tools, enabling richer queries, and improving application development efficiency.",,,,,,,,,,,,
Capturing Semantic Relationships in Electronic Health Records Using Knowledge Graphs: An Implementation Using MIMIC III Dataset and GraphDB,"Aldughayfiq Bader, Ashfaq Farzeen, Jhanjhi N. Z., Humayun Mamoona",2023,reference-manager,10.3390/healthcare11121762,,,,,,Ontology development using Protégé to define entities and relationships in EHR data.,,"How can the development and application of knowledge graphs using semantic technologies improve the analysis, integration, and clinical utility of heterogeneous electronic health record (EHR) data, particularly from the MIMIC III dataset, to enhance patient outcomes and healthcare quality?","The paper investigates whether knowledge graphs built from the MIMIC III dataset using GraphDB can effectively capture semantic relationships in electronic health records (EHRs). Using ontology mapping, RDF conversion, and SPARQL queries, the study finds that knowledge graphs improve EHR analysis and patient outcomes, though research gaps remain.","The research goal is to improve EHR data analysis by creating a knowledge graph from the MIMIC III dataset using ontology mapping and GraphDB; the approach enables efficient, accurate analysis, and the principal finding is that knowledge graphs effectively capture semantic relationships, supporting better clinical decision-making and patient outcomes.",No information available
Data mapping from CSV files to RDF format using Ontotext Refine,enabling graph representation.,,,,,,,,,,,,,,
"Querying and analyzing the knowledge graph with SPARQL to extract trends and patterns.""","The research uses the publicly available MIMIC-III dataset and tools like Protégé and Ontotext Refine for ontology development and RDF mapping. However, there is no explicit mention of source code availability for the project. Reproducibility is supported by dataset and tool access, but not by code release.","Knowledge graphs created from the MIMIC III dataset effectively capture semantic relationships within electronic health records (EHRs), enabling more efficient and accurate data analysis.",,,,,,,,,,,,,
The implementation provides valuable insights into patient outcomes and potential risk factors,supporting decision-making and improved patient outcomes.,,,,,,,,,,,,,,
"No specific quantitative results or statistical significance (p-values) are provided in the context.""",,"EHRs are disorganized, fragmented, and hard to analyze due to varying data sources and formats.",,,,,,,,,,,,,
Lack of standardization in data items,language,and formats complicates data integration and analysis.,,,,,,,,,,,,,
Existing ontologies for MIMIC III may lack detail,coverage,or suitability for specific research questions.,,,,,,,,,,,,,
"Semantic technologies may be limited for certain analyses.
Reproducing ontology",mapping,and analysis work is challenging.,,,,,,,,,,,,,
Existing work often focuses on specific use cases.,,,,,,,,,,,,,,,
Knowledge graph research faces scalability,interoperability,clinical validity,privacy,and security issues.,,,,,,,,,,,
Many models are limited to small datasets or specific domains and are not widely adopted in practice.,,,,,,,,,,,,,,,
"Need for more rigorous evaluation in real-world clinical settings.""","Knowledge graphs enable standardized, interoperable representation and integration of EHR data, improving analysis efficiency and effectiveness.",,,,,,,,,,,,,,
They support evidence-based knowledge graph development,enhancing patient outcomes and reducing healthcare costs.,,,,,,,,,,,,,,
The study demonstrates knowledge graphs’ ability to capture complex relationships in EHRs,aiding clinical decision-making.,,,,,,,,,,,,,,
Further research is recommended to address scalability,interoperability,clinical validity,"and privacy issues.""",Need for a more standardized and interoperable approach to representing and integrating EHR data.,,,,,,,,,,,
"Limitations in existing ontologies for the MIMIC III dataset regarding detail
Scale limitations","coverage
interoperability difficulties","and suitability for specific research questions.
and clinical validity issues restrict current knowledge graph research using EHR data.""","Future research should focus on linking the ontology and knowledge graph with standard external vocabularies (like SNOMED CT and LOINC) for interoperability, integrating data from different EHR databases, and applying machine learning algorithms to detect new risk factors and forecast patient outcomes.",,,"The study demonstrates that knowledge graphs built from the MIMIC-III dataset enable efficient, accurate analysis of electronic health records, improve usability and query performance compared to traditional databases, and have potential to support decision-making and enhance patient outcomes in healthcare.",,,,,,,,,The objectives of the study are to:
Create a standardized and interoperable approach for representing and integrating EHR data.,,,,,,,,,,,,,,,
Enable more efficient and effective data analysis for clinical decision-making.,,,,,,,,,,,,,,,
Support evidence-based knowledge graph development to improve patient outcomes and reduce costs.,,,,,,,,,,,,,,,
Address research gaps in scalable,interoperable,"and clinically valid EHR knowledge graphs.""",,,,,,,,,,,,,
BEYOND INFORMATION SILOS — AN OMNIPRESENT APPROACH TO SOFTWARE EVOLUTION,"RILLING JUERGEN, WITTE RENÉ, SCHUEGERL PHILIPP, CHARLAND PHILIPPE",2008,reference-manager,10.1142/s1793351x08000567,,,,,,"Unified ontological representation: Used to model and integrate process, tasks, and resources information for software maintenance.",,"How can an integrated, ontology-based and context-sensitive knowledge management environment support software maintenance and evolution by providing dynamic guidance and automated reasoning across diverse knowledge resources and processes?","The paper investigates how ontology-based approaches support program comprehension strategies—bottom-up, top-down, and as-needed—during software evolution. Using case studies, it shows that ontologies help programmers identify, specify, and verify concepts and relationships in code, thereby improving understanding and accelerating maintenance tasks.",The research goal is to improve software maintenance by integrating semantic traceability links and contextual knowledge; the approach uses a unified process model and ontology-based queries; results show enriched knowledge bases and improved guidance for maintenance tasks in case studies involving component substitution and bug fixing.,
Pre-defined queries: Employed at various context levels to retrieve relevant knowledge and support context-sensitive guidance.,,,,,,,,,,,,,,,
Automatic ontology population: Populated the knowledge base with process models,source code,"and historical data to enhance tool support.""","The research describes methods for ontology population using static code analysis and text mining, with implementation based on the Eclipse JDT compiler. However, there is no explicit mention of publicly available source code for the project. Reproducibility details beyond methodology are not provided.",Tool acceptance increased from 71% in the first case study to 94% in the second case study.,,,,,,,,,,,
Perceived tool usefulness reached 82%,and 65% of students indicated contextual queries and additional knowledge helped with their tasks.,,,,,,,,,,,,,,
"No statistical significance (p-values) was reported; further studies with more participants are planned to claim significance.""",,Limited knowledge base (KB) and process/context-sensitivity in the first case study.,,,,,,,,,,,,,
Only sub-ontologies for process,tools,and techniques were populated in the first case study.,,,,,,,,,,,,,
"Additional larger user studies are needed to further evaluate the approach due to the complexity of the software evolution domain.""",Enriching the knowledge base (KB) with additional contextual queries and historical data increased tool acceptance from 71% to 94%.,,,,,,,,,,,,,,
Perceived tool usefulness and availability of contextual queries were key factors for improved acceptance.,,,,,,,,,,,,,,,
The unified ontological approach provided context-sensitive guidance and benefits to maintainers.,,,,,,,,,,,,,,,
"Further user studies with more participants are recommended to validate results.""","Integration of additional clients and resources (e.g., mailing lists, blogs) to broaden data collection and exchange.",,,,,,,,,,,,,,
Investigation of alternative query languages (e.g.,iSPARQLh,SPARQL-DL) and rule languages (e.g.,SWRLi) for more complex queries.,,,,,,,,,,,,
"Conducting more user studies with larger participant groups to validate the results' significance.""","Future research should focus on integrating additional clients and resources (e.g., mailing lists, blogs), exploring other query languages (such as iSPARQLh, SPARQL-DL), using rule languages like SWRLi for complex queries, and conducting larger user studies to validate the approach's significance.",,"The discussion concludes that enriching the knowledge base and providing context-sensitive, ontology-driven guidance supports more effective and targeted software maintenance and impact analysis, enabling maintainers to access relevant artifacts, documentation, and expertise for specific modification tasks.","The objectives of the study were to provide users with a context-sensitive, ubiquitous knowledge environment for software maintenance by integrating process, tasks, and resource information using a unified ontological representation, and to evaluate the benefits and acceptance of this approach through two case studies.",,,,,,,,,,,
Knowledge-Enhanced Program Repair for Data Science Code,"Ouyang Shuyin, Zhang Jie M., Sun Zeyu, Merono Penuela Albert",2025,reference-manager,,,,,,,"Structured prompt construction: Organizing information (problem description, incorrect code, error messages, API knowledge, bug knowledge) into a structured prompt for LLM-based code repair.",,"How effective is DSrepair in repairing LLM-generated data science code compared to state-of-the-art program repair approaches, and what factors influence its performance, cost, and ability to uniquely fix bugs that other baselines cannot address?","This paper introduces DSrepair, an LLM-based program repair method for data science code using knowledge graph-based RAG and enriched bug information. Through empirical studies with four LLMs and five baselines, DSrepair achieves significantly higher fix rates and stability, demonstrating superior effectiveness and generalizability.","The paper's main objective is to develop DSrepair, an LLM-based program repair approach for data science code using knowledge graph-based retrieval and enriched bug information; the key method involves structured prompts with API and bug knowledge; the principal finding is that DSrepair significantly outperforms baselines in fix rate, uniqueness, and cost efficiency.",
Overlap analysis: Comparing the buggy code fix rate and unique bug fixes between DSrepair and baseline methods.,,,,,,,,,,,,,,,
Ablation study: Systematically removing key prompt components (API knowledge,"bug knowledge) to analyze their contribution to DSrepair’s performance.""",,"DSrepair significantly outperforms all baselines in repairing data science code, achieving mean Fix Rates of 101.80 ± 6.71 (GPT-3.5-turbo), 142.90 ± 6.44 (GPT-4o-mini), 163.67 ± 1.25 (DeepSeek-Coder), and 137.80 ± 4.60 (Codestral).",,,,,,,,,,,,
DSrepair’s performance is most effective when using full expressions of invoked API in retrieval,outperforming variants with less API knowledge richness.,,,,,,,,,,,,,,
Despite LLM non-determinism,"DSrepair shows greater stability and consistently superior results across multiple trials; small standard deviations indicate statistical reliability.""",,Threats to internal validity due to prompt design implementation.,,,,,,,,,,,,
Threats to external validity from dataset and LLM selection.,,,,,,,,,,,,,,,
Results may be influenced by the inherent randomness (non-determinism) of LLMs.,,,,,,,,,,,,,,,
Study is focused on data science code,"which may limit generalizability.""","DSrepair outperforms baselines, fixing about 55% more unique buggy code snippets and achieving higher mean Fix Rates across all tested LLMs.",,,,,,,,,,,,,
DSrepair is more cost-efficient,using up to 34.24% fewer tokens per code task than the second-best baseline.,,,,,,,,,,,,,,
Using full API expressions in prompts yields the best bug-fixing performance.,,,,,,,,,,,,,,,
"DSrepair’s results are stable despite LLM non-determinism.""",Limited generalizability due to reliance on the DS-1000 dataset and selected LLMs; results may not extend to other datasets or models.,,,,,,,,,,,,,,
The influence of prompt design and API knowledge richness on DSrepair’s performance requires further exploration.,,,,,,,,,,,,,,,
"The impact of LLM non-determinism on experimental outcomes remains a challenge for consistent evaluation.""",,,"DSrepair significantly outperforms all baselines in repairing data science code, uniquely fixes about 55% of bugs that baselines cannot, uses fewer tokens and less cost, and achieves the best results when using full API expressions, with consistent performance despite LLM non-determinism.","The objectives of the study are to evaluate DSrepair’s effectiveness in repairing LLM-generated data science code, compare its bug fixes with baselines, assess its cost, analyze the impact of prompt design and knowledge retrieval approaches, examine the effect of API knowledge richness, and study LLM non-determinism on results.",,,,,,,,,,,
Big data fusion with knowledge graph: a comprehensive overview,"Liu Jia, Lan Ruotian, Du Yajun, Yuan Xipeng, Xu Huan, Li Tianrui, Huang Wei, Zhang Pengfei",2025,reference-manager,10.1007/s10489-025-06549-4,,,,,,Machine learning methods are used to extract features from data and fuse them with embedded knowledge graphs at the feature level.,,"How can knowledge graph-based data fusion methods effectively integrate multi-source heterogeneous data to address challenges such as data quality, privacy, and application-specific requirements in real-world scenarios?","This paper systematically reviews data fusion methods using knowledge graphs, categorizing them by data types: raw data fusion, raw data with knowledge graph, and knowledge graph fusion. It highlights frameworks, applications, and future directions, concluding that knowledge graphs enhance data fusion's effectiveness, especially for heterogeneous and dynamic data.",The research goal is to systematically review and categorize data fusion methods using knowledge graphs; the approach involves classifying methods by data type and analyzing frameworks and examples; the principal finding is a theoretical and practical guide for multi-source heterogeneous data fusion in intelligent systems.,
Construction of heterogeneous knowledge graphs by defining relations between data entities and existing knowledge graph entities,mainly for textual data.,,,,,,,,,,,,,,
Combination of deep learning techniques (e.g.,word embedding,named entity recognition,"relation extraction) with knowledge graph construction for feature learning.""","The research uses 5 public datasets, providing links to each. There is no mention of source code for the project. The reproducibility relies on the availability of these datasets, but no explicit project code is provided.",,"The paper systematically reviews and categorizes data fusion methods with knowledge graphs into three types: fusion of raw data, fusion of raw data with knowledge graphs, and fusion between knowledge graphs.",,,,,,,,,
It highlights challenges in fusing multi-source heterogeneous data and emphasizes privacy and security issues in knowledge graph-based data fusion.,,,,,,,,,,,,,,,
No quantitative results,statistical significance,"or p-values are provided.""",,No associated data is available for the manuscript.,,,,,,,,,,,
Lack of research on textual data in multi-source heterogeneous data fusion.,,,,,,,,,,,,,,,
Ongoing challenges with data privacy,security,and secure data sharing.,,,,,,,,,,,,,
Need for unified data description frameworks without sharing data.,,,,,,,,,,,,,,,
"Further research needed on dynamic evolutionary data fusion.""",The study systematically reviews and categorizes data fusion methods with knowledge graphs by data types.,,,,,,,,,,,,,,
Key future research directions include dynamic evolutionary data fusion and privacy-secure data fusion.,,,,,,,,,,,,,,,
Knowledge graph-based data fusion faces challenges from multi-source heterogeneity,dynamic evolution,"and privacy/security issues.""","Fusing multi-source heterogeneous data, especially integrating textual data with numerical and image data, remains underexplored.",,,,,,,,,,,,
Fusion of privacy-secure data is challenging; there is an urgent need to protect user privacy and data security during data fusion and machine learning.,,,,,,,,,,,,,,,
"Dynamic evolutionary data fusion requires further research to handle frequently updated raw data and knowledge graphs efficiently.""","Future research should address fusing dynamically evolving data with knowledge graphs, especially considering high real-time requirements and high training time costs. Additional investigation is needed into fusing multi-source heterogeneous data, including textual data, and ensuring privacy and security in data fusion processes.",,"The paper systematically reviews and categorizes data fusion methods with knowledge graphs, discusses practical applications and future research directions, and highlights the importance of knowledge graphs in enhancing big data fusion across heterogeneous, dynamic, and privacy-sensitive data sources.",,,,,,,,,,,,
nekCRF: A next generation high-order reactive low Mach flow solver for direct numerical simulations,"Kerkemeier Stefan, Frouzakis Christos E., Tomboulidis Ananias G., Fischer Paul, Bode Mathis",2024,reference-manager,,,,,,,Low Mach number formulation: The study uses equations for reacting flow based on the low Mach number formulation to model thermally driven flows.,,"How can the newly developed open-source solver nekCRF be optimized and evaluated for efficient, scalable direct numerical simulations of reactive low Mach flows with complex geometries on GPU-accelerated supercomputers?","The paper introduces nekCRF, an open-source, GPU-optimized successor to LAVp for thermochemistry simulations. Built on nekRS using the spectral element method, it targets high computational efficiency on modern supercomputers. Key findings highlight memory bandwidth as a main performance limiter and communication overhead as a scalability constraint.","The research goal is to assess and optimize the thermochemistry solver's performance; the approach involves analyzing kernel computational and communication efficiency on HPC systems; the principal finding is that memory bandwidth and communication overhead, especially in gather/scatter and inner product operations, limit scalability and performance.",
Coupling strategy for advection,diffusion,and production: The algorithm from reference \[14] is employed to couple these terms efficiently.,,,,,,,,,,,,,
Computational and communication efficiency analysis: The study evaluates computational efficiency of local compute kernels and communication efficiency,"including strategies to mitigate global synchronization overhead.""","At the time of writing, the code has not been made publicly available. However, after a testing phase, it will be released as open-source under the BSD license and can be accessed on GitHub: https://github.com/nekCRF.",,,,"Performance results are influenced by specific implementation details, hardware setup, and problem size.",,,,,,,,,
Strong scaling is limited by communication overhead,especially for small chemistry models.,,,,,,,,,,,,,,
Gather/scatter kernels suffer from latency and irregular memory access,limiting bandwidth.,,,,,,,,,,,,,,
Inner products in the linear solver require global synchronization,restricting scalability.,,,,,,,,,,,,,,
"Results may not generalize to other systems or problem sizes.""","nekCRF achieves up to 22× speedup per compute unit over LAVp, but this advantage decreases with increased communication overhead.",,,,,,,,,,,,,,
Most performance bottlenecks are due to global memory bandwidth limitations and communication inefficiencies,especially in gather/scatter kernels.,,,,,,,,,,,,,,
Optimizations like using a smaller perturbation factor and relaxed convergence criteria significantly improve thermochemistry solver speed.,,,,,,,,,,,,,,,
"Recommendations include further optimizing communication patterns and kernel memory access to enhance scalability and efficiency.""","Most top kernels are limited by global memory bandwidth, with additional factors preventing ideal performance.",,,,,,,,,,,,,,
Gather/scatter operations suffer from irregular memory access patterns,leading to inefficient memory bandwidth usage.,,,,,,,,,,,,,,
Optimizing the reaction rate kernel is difficult due to large working sets,irregular execution orders,"and complex data access patterns.""",,,,"The discussion concludes that while the thermochemistry solver achieves high computational efficiency and strong scaling on GPU-based systems, communication overhead—especially from global synchronization—limits scalability, and the solver's competitive advantage decreases as communication costs rise with increased node counts.",,,,,,,,,"The objectives are to assess kernel performance by focusing on the top five most time-consuming kernels of the thermochemistry solver, identify the dominant performance limiter, and present a figure of merit (FOM). The evaluation uses data collected on 50 compute nodes under high local workload conditions."
Privacy-Preserving Synthetically Augmented Knowledge Graphs with Semantic Utility,"Bellomarini Luigi, Catalano Costanza, Coletta Andrea, Iezzi Michela, Samarati Pierangela",2024,reference-manager,,,,,,,"(k,x)-isomorphism anonymization: A structural anonymization method ensuring each subgraph of x vertices has k−1 structurally indistinguishable counterparts with diverse sensitive attributes.",,"How can knowledge graphs be anonymized to prevent re-identification attacks that exploit derived knowledge, while preserving business semantics, data utility, and sensitive attribute diversity?","This paper introduces a new structural anonymization method for knowledge graphs (KGs) called (k,x)-isomorphism anonymization, aiming to prevent re-identification while preserving data utility. Two algorithms, KLONE and KGUARD, are proposed and evaluated, showing improved privacy protection with controlled utility loss on real-world datasets.","The research goal is to prevent re-identification in knowledge graphs (KGs) while preserving utility; the approach introduces (k,x)-isomorphism anonymization and two algorithms (KLONE, KGUARD) to generate synthetic KGs; results show improved privacy and utility compared to existing methods.",
Semantic utility metric: Uses a Jaccard-based similarity index to maximize the usefulness of synthetic knowledge graphs for specific tasks.,,,,,,,,,,,,,,,
KLONE and KGUARD algorithms: Two anonymization algorithms implementing (k,"x)-isomorphism anonymization while optimizing semantic utility.""",,,,,Simple de-identification is not foolproof; the knowledge graph (KG) remains exposed to background knowledge attacks.,,,,,,,,,
Existing structural anonymization methods do not consider derived knowledge,risking re-identification.,,,,,,,,,,,,,,
Differential privacy is not directly suitable due to challenges with highly correlated and network data.,,,,,,,,,,,,,,,
Few anonymization solutions exist specifically for KGs,often focusing only on sequential publishing or node attributes.,,,,,,,,,,,,,,
"Ensuring both privacy and utility (business semantics) in KGs is a nontrivial challenge.""","The proposed (k,x)-isomorphism anonymization effectively protects against re-identification attacks, achieving δ-anonymity = 1.0 across all tested datasets.",,,,,,,,,,,,,,
Both KLONE and KGUARD algorithms preserve high utility,with utility U close to 0,but KGUARD introduces significantly less node overhead and redundant structures.,,,,,,,,,,,,,
Compared to state-of-the-art methods,these approaches provide stronger privacy guarantees,especially when derived edges are present.,,,,,,,,,,,,,
Recommendation: Use KGUARD for better balance between privacy,utility,"and data overhead.""","Existing anonymization methods for knowledge graphs (KGs) often neglect derived knowledge, leading to privacy leaks.",,,,,,,,,,,,
Differential privacy struggles with highly correlated and network data,making it unsuitable for KGs without severely degrading data utility.,,,,,,,,,,,,,,
"There is a lack of anonymization solutions that ensure privacy while maintaining the utility of financial KGs.""","Few anonymization solutions exist specifically for knowledge graphs (KGs), mainly focusing on sequential data publishing and node attributes. Future research should address privacy for stakeholders in KG financial data while maintaining utility, and develop anonymization methods that consider the implications of derived knowledge.",,"The paper introduces new privacy requirements and two anonymization algorithms, KGUARD and KLONE, for Knowledge Graphs, demonstrating their effectiveness in protecting sensitive data while preserving utility, with KGUARD achieving minimal changes and high fidelity and KLONE offering consistent anonymization with more redundancy.","The objectives of the study are to develop anonymization approaches for knowledge graphs (KGs) that: (R.1) protect entities from re-identification attacks exploiting reasoning, (R.2) preserve the knowledge encoded in the KG for downstream tasks, and (R.3) ensure diversity for sensitive attributes.",,,,,,,,,,,
A Human Digital-Twin-Based Framework Driving Human Centricity towards Industry 5.0,"Modoni Gianfranco E., Sacco Marco",2023,reference-manager,10.3390/s23136054,,,,,,Implementation and evaluation of a lightweight prototype based on the Human-CENTRO architecture in real assembly scenarios.,,"How can the Human-CENTRO reference architecture enable and validate a human-centric model of collaborative intelligence (CI) interaction between humans and machines in manufacturing, in alignment with Industry 5.0 principles?","The paper introduces Human-CENTRO, a reference architecture enabling collaborative intelligence (CI) interactions between humans and machines in manufacturing, aligned with Industry 5.0. Using a real factory case study and quantitative surveys, the framework showed improved user confidence, guidance, and system scalability. Human-CENTRO supports human-centric, scalable, and adaptable applications.","The research goal was to implement a human-centric CI interaction model in manufacturing using the Human-CENTRO reference architecture; the approach involved deploying and validating an AR-based application, which resulted in improved user confidence, clearer guidance, fewer errors, reduced assembly time, and demonstrated system scalability.",
Comparative user study with two groups (experienced and average users) performing assembly tasks using both AR application and traditional paper instructions.,,,,,,,,,,,,,,,
Quantitative analysis of system performance,including scalability tests (up to 500 KB/s telemetry,"500 connected resources).""",,,,,,,,,,,,,Generalizability of Human-CENTRO is still under evaluation; further experiments in different industrial fields are needed.
Current assessment is based on a single case study.,,,,,,,,,,,,,,,
"No explicit mention of other limitations or self-reported problems in the provided context.""","Human-CENTRO, a reference architecture for human-machine CI (Collaborative Intelligence) interaction, was developed and validated in a real factory case study.",,,,,,,,,,,,,,
The AR application outperformed traditional paper instructions: 74% felt more confident,80% found it clearer for component selection,and 90% received better assembly guidance.,,,,,,,,,,,,,
The system demonstrated high scalability,supporting up to 500 connected resources and 500 KB/s telemetry throughput.,,,,,,,,,,,,,,
Human-CENTRO is recommended for broader adoption in various fields requiring human-machine collaboration,"with further validation in other industries planned.""",Further assess Human-CENTRO in different industrial fields to explore its generalizability.,,,,,,,,,,,,,
Address the lack of significant human-centered practical cases in current research by implementing and validating more real-world scenarios.,,,,,,,,,,,,,,,
"Guide strategic future developments for human-centric digital twin (DT)-based platforms managing human–machine collaboration.""","Future research should: (i) further assess Human-CENTRO in different industrial fields to test its generalizability; (ii) adapt and evaluate Human-CENTRO in a digital factory within the AI REGIO network; (iii) extend the PT model, especially internal interactions; (iv) adopt data sovereignty mechanisms for secure data exchange.",,"The study introduced Human-CENTRO, a reference architecture enabling collaborative intelligence (CI) interactions between humans and machines, validated its effectiveness and scalability in a real case study, and highlighted its potential for broader application and future evaluation in various industrial fields.","The objectives of the study were to investigate the potential of the Human-CENTRO architecture in real motivational scenarios, assess its capability to support real scenarios, and guarantee personalized, up-to-date information for workers to enhance guidance, safety, and error prevention during assembly tasks.",,,,,,,,,,,
Personal Health Knowledge Graphs for Patients,"Rastogi Nidhi, Zaki Mohammed J.",2023,reference-manager,,,,,,,Literature review and critique of Knowledge Graph (KG) approaches for extracting personal context from patient data.,,"How can Personal Health Knowledge Graphs (PHKGs) be effectively generated, represented, and integrated with existing knowledge bases to provide personalized health recommendations while addressing challenges related to data heterogeneity, scalability, validation, and patient-specific requirements?","The paper reviews and critiques approaches for extracting personal context from patient data using small-sized Personal Health Knowledge Graphs (PHKGs). It discusses methodologies like inferring preferences from general KGs and dynamic PHKG creation. Key findings highlight challenges in scalability, representation, and integration. The study concludes more research is needed for effective personal health recommendations.",The research goal is to critique existing literature and discuss challenges in designing personal health knowledge graphs (PHKGs) for patients; the approach is a literature review; the principal finding is that current methods are limited and several research questions remain for effective PHKG development and use.,"Personal Health Knowledge Graphs, Knowledge Graphs, Diabetes."
Aggregation and integration of heterogeneous data sources (e.g.,sensors,web data) into personalized health knowledge graphs (PHKG).,,,,,,,,,,,,,
"Construction of PHKGs using patient queries and preferences to dynamically generate personalized summaries.""",,,,"Brute force approaches do not address creating relationships, identifying accurate classes, or removing irrelevant entities.",,,,,,,,,,,
Lack of standard model for representing PHKG.,,,,,,,,,,,,,,,
Scalability and graph structure issues for individual PHKGs remain unaddressed.,,,,,,,,,,,,,,,
Static personalized summaries become unmanageable over time.,,,,,,,,,,,,,,,
Device memory and computing resource limitations need further analysis.,,,,,,,,,,,,,,,
Data from social media is unreliable and inconsistent for health information.,,,,,,,,,,,,,,,
No patient-side recommendation input feedback in some approaches.,,,,,,,,,,,,,,,
Integration and entity linking with general knowledge bases is lacking.,,,,,,,,,,,,,,,
Challenges in defining scope,validation,ingestion,processing,"and scalability due to heterogeneous data sources.""",,"PHKGs (Personal Health Knowledge Graphs) offer personalized health recommendations but face challenges in dynamic updating, scalability, and integration with general KGs.",,,,,,,,,
Collaboration between patients and healthcare professionals is recommended for effective PHKG construction.,,,,,,,,,,,,,,,
Device limitations and data heterogeneity present scalability and processing challenges.,,,,,,,,,,,,,,,
"No standard PHKG representation exists; further research is needed.""",Lack of standard models for representing and scaling Personal Health Knowledge Graphs (PHKGs).,,,,,,,,,,,,,,
Challenges in dynamically updating and managing PHKGs with heterogeneous,real-time patient data.,,,,,,,,,,,,,,
"Need for effective validation mechanisms to ensure PHKGs accurately capture personal patient information.""","Future research should address: dynamic creation and updating of PHKGs, accurate relationship and class identification, removal of irrelevant entities, scalability, graph structure modeling, validation mechanisms, handling heterogeneous data sources, device resource limitations, and collaborative approaches between patients and healthcare professionals.",,,,,,,,,,,,,,
A semantics-enabled approach for personalised Data Lake exploration,"Bianchini Devis, De Antonellis Valeria, Garda Massimiliano",2023,reference-manager,10.1007/s10115-023-02014-1,,,,,,Top-n precision and Top-n recall metrics: Used to assess the effectiveness of personalized indicator suggestions by measuring the relevance of retrieved results.,,"How can the PERSEUS approach enable personalized, context-aware exploration of multi-dimensional aggregated data in Semantic Data Lakes by leveraging user profiles, preferences, and semantic metadata to support decision making and data visualization across diverse application domains?","The paper proposes PERSEUS, a computer-aided approach for personalized exploration of multi-dimensional data in Data Lakes. Using semantic metadata catalogs, Multi-Dimensional Ontology, and user profiles, PERSEUS enables interactive data exploration. Validated in the Brescia Smart Living project, results show improved semantic annotation and relevant indicator suggestions.","The paper's main objective is to enable personalized exploration of Data Lakes using the PERSEUS approach, which formalizes semantic metadata catalog construction, models indicators with a Multi-Dimensional Ontology, and incorporates user preferences; results show effective support for domain experts, efficient query response, and improved personalized indicator suggestions.",
Semantic annotation with DL-DIVER tool: Supports domain experts in annotating data source attributes for building a semantic metadata catalog.,,,,,,,,,,,,,,,
Experimental evaluation in Spark environment: Conducted experiments on system performance,including response times and search space reduction,"using real data.""",,,,,,,,,,,,,Limited effectiveness of WordNet for lexical enrichment when attributes are abbreviations or acronyms.
High percentage of empty results when performing concept searches without lexical enrichment.,,,,,,,,,,,,,,,
Need for domain experts to introduce new concepts from scratch due to limited useful concepts in plain searches.,,,,,,,,,,,,,,,
"Only binary preferences are supported; more advanced preference models are not used for simplicity.""","PERSEUS enables personalized, context-aware exploration of indicators in Data Lakes using user profiles and preferences.",,,,,,,,,,,,,,
The approach effectively reduces the search space and improves the relevance of suggested indicators,as shown by Top-n precision and recall metrics.,,,,,,,,,,,,,,
Binary preferences are intuitive and user-friendly for specifying interests during exploration.,,,,,,,,,,,,,,,
The system supports diverse user goals and roles,"enhancing usability for a broad audience.""",Enhance the DL-DIVER tool for Semantic Data Lake construction by increasing interactivity and introducing a quality metric for feedback.,,,,,,,,,,,,,
Improve support for indicator modeling by developing metrics to assess mapping strength,such as aggregation power.,,,,,,,,,,,,,,
"Advance preference-based indicators exploration by enabling preference propagation across different exploration contexts.""",Future research should improve the DL-DIVER tool for better interactivity and introduce a quality metric for semantic layer management. Further work is needed on mapping metrics and preference propagation across exploration contexts. Adapting PERSEUS for new domains like MICS is also suggested.,,"The discussion concludes that the semantics-enabled approach allows for personalized, multi-dimensional exploration and aggregation of Data Lake data through formalized indicators and dimensions, supporting efficient and flexible analysis using the DL-DIVER tool.","The objectives of the study are to define a Multi-Dimensional Ontology (MDO) for modeling indicators and analysis dimensions with personalization aspects, and to demonstrate the effectiveness of personalized recommender systems for suggesting relevant indicators in data exploration, using metrics like Top-n precision and Top-n recall.",,,,,,,,,,,
Human Digital Twin: Systematic Literature Review and Concept Disambiguation for Industry 5.0,"Gaffineta Ben, Al Haj Ali Jana, Naudet Yannick, Panetto Hervé",2024,reference-manager,,,,,,"Many papers do not provide explicit definitions of HDTs, leading to varied interpretations.",,"The research ensures reproducibility by following a defined protocol based on established guidelines, detailing search strategies, inclusion/exclusion criteria, and categorization methods. Eight databases were systematically searched. There is no mention of source code for the project.",What is a general definition for HDTs that applies to all application areas?,"The paper conducts a systematic literature review to define Human Digital Twins (HDTs), analyze their conceptual understanding, application areas, and development challenges. Using a rigorous protocol across eight databases, 90 primary studies were analyzed. Key findings highlight diverse definitions and applications, with implications for clearer taxonomy and future research directions.","The paper's research goal is to define a general concept of Human Digital Twins (HDTs) across domains, using a systematic literature review approach, and its principal finding is the identification of divergent definitions and the proposal of a coherent, general definition while summarizing key research challenges and directions.","The keywords or tags for this research are: “Human Digital Twin”, “Patient Digital Twin”, “Worker Digital Twin”, “Operator Digital Twin”, “Student Digital Twin”, “Driver Digital Twin”, “Citizen Digital Twin”, “Human Digital Shadow”, “Customer Digital Twin”, “Consumer Digital Twin”, “Elderly Digital Twin”, “Bio Digital Twin”, “Human Digital Clone”, “Human Virtual Twin”."
There is a lack of consensus on the nature of the twinned entity.,,,,,,,,,,,,,,,
Not all HDTs are conceptualized with all characteristics of a full Digital Twin (DT).,,,,,,,,,,,,,,,
Limited literature and novelty of the HDT concept.,,,,,,,,,,,,,,,
Exclusion of non-English and inaccessible documents may limit comprehensiveness.,,,,,,,,,,,,,,,
"Only primary studies are analyzed; reviews are excluded from detailed analysis.""",There is no consensus on the definition of HDTs; different backgrounds lead to varied interpretations.,,,,,,,,,,,,,,
"The scope of the """"""""twinned entity"""""""" and the understanding of Digital Twins (DTs) differ across application areas.",,,,,,,,,,,,,,,
The study highlights the need for a general,coherent definition and further research into essential components.,,,,,,,,,,,,,,
"Recommendations include clarifying conceptual aspects and addressing research challenges for HDT development.""",Divergence in conceptual interpretation: There is a lack of consensus on the definition and essential components of HDTs across different domains.,,,,,,,,,,,,,,
"Limited focus on conceptual aspects: Most reviews emphasize enabling technologies rather than foundational conceptual frameworks for HDTs.
Need for a general","coherent definition: Further research is required to establish a unified definition of HDTs applicable to all application areas.""","Future research should focus on integrating specialized HDTs into a holistic system, developing standardized frameworks and architectures, addressing data gaps and quality issues, and exploring the use of generative AI to create synthetic data for robust modeling across diverse contexts.",,"The discussion highlights diverse and evolving definitions of HDTs across application areas, identifies conflicting interpretations, and proposes a general definition while summarizing research challenges and future directions for HDT development.",,"The objectives of the study are to: (i) summarize the literature on the concept of HDT, (ii) identify conflicting interpretations of the concept, (iii) propose a general coherent definition, and (iv) summarize research challenges and provide research directions.",,,,,,,,,
Evaluating Ontology-Based PD Monitoring and Alerting in Personal Health Knowledge Graphs and Graph Neural Networks,"Zafeiropoulos Nikolaos, Bitilis Pavlos, Tsekouras George E., Kotis Konstantinos",2024,reference-manager,10.3390/info15020100,,,,,Sustaining relevance of Wear4PDmove amid evolving technologies and healthcare practices.,,,"How can the Wear4PDmove ontology semantically integrate heterogeneous health data to support advanced reasoning, event recognition, and personalized decision-making for Parkinson’s disease monitoring and care?","The paper aims to improve Parkinson’s Disease (PD) monitoring and alerting by integrating wearable sensor data, advanced semantic analysis, and Graph Neural Networks (GNNs). Using the Wear4PDmove ontology and a Patient Health Knowledge Graph (PHKG), the study demonstrates enhanced, patient-centric PD care and offers insights for future research.","The research goal is to enhance PD monitoring and care using advanced semantic data analysis and wearable technology; the approach integrates the Wear4PDmove ontology, PHKG, and PHGNNs for nuanced movement data analysis; results show improved patient-centric monitoring, interoperability, and insights for personalized healthcare.","Tags: Parkinson’s Disease, Knowledge Graph, Ontology, Semantic Web, Wearables, Machine Learning, Graph Neural Network, RDF, OWL, Sensor Data, Linked Data, Neuroinformatics, Explainable AI, Data Integration, Semantic Interoperability, Drug Repurposing, Feature Selection, MRI, Personal Healthcare Record."
Compatibility and alignment challenges with third-party ontologies and evolving standards.,,,,,,,,,,,,,,,
Scalability concerns with large datasets and expanding PD-related information.,,,,,,,,,,,,,,,
Adaptability to diverse healthcare settings.,,,,,,,,,,,,,,,
Difficulty integrating advanced algorithms for high-level event recognition.,,,,,,,,,,,,,,,
Overfitting risk in deeper GNN architectures (GAT,GCN).,,,,,,,,,,,,,,
Sensitivity of GNNs to graph structure variations.,,,,,,,,,,,,,,,
"Scalability challenges in GNNs when handling large-scale graphs or datasets.""","Developed a robust PHKG integrating movement data, clinical records, and health information for comprehensive PD monitoring.",,,,,,,,,,,,,,
"Extended the Wear4PDmove ontology and implemented PHGNNs for nuanced analysis of complex medical data.
Enabled real-time",patient-centric PD monitoring using wearable technology and semantic data enrichment.,,,,,,,,,,,,,,
"Recommended further research on ontology limitations and future advancements.""",Sustaining the relevance of the Wear4PDmove ontology with evolving technologies and healthcare practices.,,,,,,,,,,,,,,
Addressing compatibility and alignment challenges with third-party ontologies and evolving standards.,,,,,,,,,,,,,,,
"Managing scalability concerns when handling extensive datasets and expanding PD-related information coverage.""","Future research should focus on sustaining the relevance of the Wear4PDmove ontology, addressing compatibility with third-party ontologies, managing scalability, adapting to diverse healthcare settings, integrating advanced event recognition algorithms, developing advanced semantic data analysis, and creating practical, patient-centric PD monitoring solutions.",,"The study presents advancements in knowledge-based PD monitoring using wearable sensors, semantic data analysis, and GNNs, highlights the Wear4PDmove ontology's role in comprehensive PD care, and discusses ongoing challenges such as scalability, compatibility, and adaptability for future research.",Objectives:,,,,,,,,,,,
Retrieve and analyze all activities performed by patients to monitor behavior and disease/symptom progress.,,,,,,,,,,,,,,,
Identify which patient performs a specific activity (e.g.,Sketching Activity) and assess their performance for personalized treatment.,,,,,,,,,,,,,,
Retrieve and analyze recorded observations for specific patients to monitor condition progression and adjust treatment.,,,,,,,,,,,,,,,
Retrieve patient Personal Health Records (PHR) to access medical history and relevant information for personalized care.,,,,,,,,,,,,,,,
Check if all patients have associated PHRs by retrieving patients and their respective PHRs.,,,,,,,,,,,,,,,
"Identify patients exhibiting rigidity symptoms and determine the severity level of their rigidity.""",,,,,,,,,,,,,,,
Data Integration for Heterogenous Datasets,Hendler James,2014,reference-manager,10.1089/big.2014.0068,,,,,"Datasets are often not designed to work together, causing formatting and terminology issues.",,"The research demonstrates reproducibility through publicly available demos and technologies at http://logd.tw.rpi.edu. Details of the approaches are referenced in Ding et al.17. However, no explicit source code repository is mentioned in the context.","How can the challenges of discovering, integrating, and exploring increasingly heterogeneous and mixed-quality open data—particularly in the context of open government data—be addressed to enable effective data analysis and knowledge extraction?","The paper examines challenges and solutions in integrating heterogeneous datasets, focusing on the """"broad data"""" area where data variety limits analysis. Using technical overviews and examples, it highlights metadata, annotation, and faceted search as key methods. The study concludes that improved discovery and integration tools are essential for effective analytics.","The research goal is to address challenges in integrating heterogeneous datasets; the approach involves technical overviews of data discovery, integration, and combining structured and unstructured data; the principal finding is that increasing data variety, not just volume or velocity, drives the need for new integration techniques.",
Data dictionaries are complex and expert-oriented,making understanding difficult.,,,,,,,,,,,,,,
"Differences in units (e.g.
Missing","currencies
improperly entered","inflation adjustment) complicate integration.
or """"""""dirty data"""""""" are common.",,,,,,,,,,,,,
Correlations may be spurious; causality cannot be assumed.,,,,,,,,,,,,,,,
Some problems require domain expertise to resolve.,,,,,,,,,,,,,,,
Integration of structured and unstructured data remains difficult.,,,,,,,,,,,,,,,
"Ad hoc integration can be hindered by missing or unknown information.""
New technologies","Integrating heterogeneous data is increasingly important as analysts rely on external sources.
including machine-readable annotations and language processing",are improving integration of structured and unstructured data.,,,,,,,,,,,,,
Rapid data screening and visualization can reveal valuable insights,but deeper analytics are still needed.,,,,,,,,,,,,,,
"There are significant business opportunities in broad data integration.""","Integration of heterogeneous datasets remains challenging due to differences in formatting, terminology, and metadata standards.",,,,,,,,,,,,,,
Most research has focused on volume and velocity,with less attention to the challenge of variety in data.,,,,,,,,,,,,,,
"Future directions include improving integration of structured and unstructured data using new language processing technologies.""","Future research should focus on developing more domain-specific metadata, improving integration of heterogeneous datasets, enhancing machine-readable annotations, and advancing knowledge base population from large text corpora. There is also a need to address challenges in data variety, not just volume and velocity.",,"The article highlights the growing importance and challenges of integrating heterogeneous data, emphasizing that new technologies and approaches are enabling rapid data discovery and integration, which can provide valuable insights and business opportunities as the variety of available data increases.","The objectives of the study are to explore emerging themes in data discovery, data integration, linked data, and the combination of structured and unstructured data, with a focus on addressing challenges related to the variety of heterogeneous data rather than just the scale of data.",,,,,,,,,,,
Docs2KG: Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models,"Sun Qiang, Luo Yuanyi, Zhang Wenxiao, Li Sirui, Li Jichunyang, Niu Kai, Kong Xiangrui, Liu Wei",2024,reference-manager,,,,,,No explicit limitations or shortcomings are stated in the provided context.,,"Docs2KG’s reproducibility is partially supported. The project’s source code is not explicitly provided in the context. However, it uses the open-source library Markdownify (source: https://github.com/matthewwithanm/python-markdownify). No direct link or repository for Docs2KG itself is mentioned.","How can a unified knowledge graph framework, such as Docs2KG, effectively extract, integrate, and represent multimodal information from heterogeneous unstructured documents to enable meaningful data-driven analysis and support Retrieval Augmented Generation (RAG) applications?","The paper proposes Docs2KG, a system that integrates heterogeneous unstructured documents (PDF, Excel, etc.) into a unified Knowledge Graph. Using dual-path data processing, it extracts and merges multimodal data. The system enables dynamic, source-referenced queries, improving information retrieval and reducing hallucination in knowledge applications.","The research goal is to unify multimodal unstructured data extraction using the Docs2KG framework, which integrates deep learning-based layout analysis and structured parsing; the principal finding is that Docs2KG dynamically constructs knowledge graphs from diverse document types, enabling effective semantic and structural queries across heterogeneous data.","Unstructured Data, Heterogeneous Data, Knowledge Graph"
"No self-reported problems or suggestions for further research are mentioned.""","Docs2KG effectively integrates heterogeneous, unstructured documents (PDF and Excel) to extract comprehensive population information from 2011 to 2021.",,,,,,,,,,,,,,
Meaningful insights cannot be derived from individual files alone; integration is essential.,,,,,,,,,,,,,,,
The dual-path strategy maximizes document type coverage and reduces time,effort,and resource requirements.,,,,,,,,,,,,,
Docs2KG is adaptable and supports knowledge-grounded retrieval,"reducing outdated knowledge and hallucination risks.""","Extraction of multimodal data (including tables, texts, images, and figures) from diverse formats remains challenging.",,,,,,,,,,,,,
Integrating modality-specific information extraction models into a unified framework is an open problem.,,,,,,,,,,,,,,,
"Achieving meaningful semantic representation of data with references to the source is still unresolved.""","Future research should address the challenges of extracting multimodal data from diverse formats, integrating modality-specific extraction models into a unified framework, and ensuring meaningful semantic representation with source references. Further exploration is needed to improve dynamic integration, human-in-the-loop updates, and handling of heterogeneous unstructured data.",,"The Docs2KG framework enables dynamic integration and querying of multimodal, heterogeneous unstructured documents into a unified knowledge graph, allowing efficient retrieval and analysis of population information from 2011 to 2021 that cannot be achieved using individual files alone.","The objectives are to extract multimodal data (tables, texts, images, figures) from diverse formats, integrate modality-specific extraction models into a unified framework, and represent data meaningfully with source references. The study proposes using Knowledge Graphs via the Docs2KG system to achieve these aims.",,,,,,,,,,,
TimelineKGQA: A Comprehensive Question-Answer Pair Generator for Temporal Knowledge Graphs,"Sun Qiang, Li Sirui, Huynh Du, Reynolds Mark, Liu Wei",2025,reference-manager,10.1145/3701716.3715308,,,,,,,,,,,"knowledge graph, temporal knowledge graph, question answering"
"These are the keywords or tags for this research. A """"""""knowledge graph"""""""" is a structured representation of facts; a """"""""temporal knowledge graph"""""""" includes time information; """"""""question answering"""""""" refers to systems that answer questions using these graphs.""","The paper’s main objective is to create a comprehensive temporal question categorization framework and a universal TKGQA dataset generator; using TimelineKGQA, it systematically generates diverse, complex QA pairs, and empirically shows that question difficulty aligns with its categorization, addressing key limitations of existing datasets.","The paper introduces a comprehensive framework for categorizing temporal questions and a universal dataset generator, TimelineKGQA, for temporal knowledge graph question answering (TKGQA). Using this framework, the authors generate diverse benchmark datasets, demonstrate alignment between question complexity and model performance, and provide tools to advance TKGQA research.",,"How can a comprehensive, multi-dimensional categorization framework for temporal question answering be developed to address the limitations of existing TKGQA datasets and enable the generation of diverse, complex temporal QA pairs from any temporal knowledge graph?",,,Existing datasets lack comprehensive coverage of temporal question types and operations.,,,,,,,,The research is reproducible. The source code for TimelineKGQA is available as an open-source Python package at https://github.com/PascalSun/TimelineKGQA.
Absence of a systematic,multi-dimensional temporal complexity framework in prior work.,,,,,,,,,,,,,,
CronQuestions is dominated by simple,structurally similar templates,lacking diversity in temporal relation complexity.,,,,,,,,,,,,,
"Medium.Temporal category and duration comparison/time range inference questions are missing.""",The study introduces a comprehensive temporal question categorization framework and a universal TKGQA dataset generator addressing existing dataset limitations.,,,,,,,,,,,,,,
The framework systematically classifies question complexity and identifies four key temporal capabilities: TCR,TPR,TSO,and TAO.,,,,,,,,,,,,
Empirical evaluation confirms that question difficulty aligns with the proposed complexity categorization.,,,,,,,,,,,,,,,
"The work enables development and evaluation of advanced TKGQA solutions and supports broad application in private domains.""","Existing datasets lack comprehensive coverage of all temporal question complexity dimensions, especially aggregation and multi-dimensional temporal operations.",,,,,,,,,,,,,,
There is no systematic,unified framework for categorizing and characterizing temporal complexity in TKGQA datasets.,,,,,,,,,,,,,,
Effective methods for generating temporal QA pairs in private domains,especially using Large Language Models,"remain largely unexplored.""","Future research should focus on: (1) developing comprehensive datasets that cover all dimensions of temporal question complexity, including aggregation and ordinal types; (2) establishing a systematic multi-dimensional framework for temporal complexity; and (3) creating effective temporal QA pair generation methods for private domains.",,,"We introduce a comprehensive temporal question categorization framework and a universal TKGQA dataset generator that systematically classifies question complexity, demonstrates empirical alignment between question difficulty and complexity, and provides a foundation for developing and evaluating advanced TKGQA solutions.",,,,,,,,,"The objectives are to develop a comprehensive temporal question categorization framework addressing complexity across multiple dimensions, and to create a universal TKGQA dataset generator (TimelineKGQA) that produces diverse, natural, and complex question-answer pairs for temporal knowledge graphs, overcoming limitations of existing template-based datasets."
Multimodal sensor fusion in the latent representation space,"Piechocki Robert J., Wang Xiaoyang, Bocus Mohammud J.",2023,reference-manager,10.1038/s41598-022-24754-w,,,,,Performance depends heavily on high-quality training data.,,,"How can a novel multimodal sensor fusion method, inspired by the Bayesian brain hypothesis and implemented via Multimodal Variational Autoencoder (M-VAE), effectively estimate latent causes and reconstruct signals from subsampled, lossy, or noisy data for applications such as human activity recognition in E-Health?","The paper introduces a novel multimodal sensor fusion method for tasks like human activity recognition, using a self-supervised generative model in latent space. The method excels under subsampled and noisy data, outperforming existing techniques in classification and reconstruction. It enables robust fusion, but relies on high-quality training data.","The research goal is to develop a novel sensor fusion method (SFLR) for human activity recognition, using latent representation fusion and compressed sensing; results show SFLR outperforms baselines in classification and reconstruction tasks, especially under subsampled or noisy data conditions.",No information available
"Learning the manifold of p(z) is crucial; poor learning affects estimation.
In stage 2",z may fall into local minima,leading to sub-optimal solutions.,,,,,,,,,,,,,
"This local minima issue is common in gradient descent optimization.""","The study introduces a novel multimodal sensor fusion method (SFLR) that achieves superior performance in classification and reconstruction tasks, especially for human activity recognition (HAR).",,,,,,,,,,,,,,
SFLR is robust to subsampled,lost,or noisy data and can leverage strong modalities to recover weak ones.,,,,,,,,,,,,,
The method shows significant reduction in reconstruction error and improved F1-macro scores compared to other models.,,,,,,,,,,,,,,,
"Continued research in this area is recommended due to its societal benefits
,,,The proposed sensor fusion method effectively enables classification and reconstruction from radar signals for human activity recognition","particularly in healthcare and potential military applications.""
performs well under lossy and noisy data conditions","No explicit research gaps or future directions are identified in the provided context.
and can leverage strong modalities to aid weaker ones","though its success depends on high-quality training data and optimization challenges remain.""","The objectives of the study are to present a novel method for multimodal sensor fusion that estimates the causes of observed data, enables fusion under lossy data conditions, and leverages strong modalities to aid weaker ones, particularly for classification and reconstruction tasks in applications like human activity recognition.",,,,,,,,,,,
"Agents in Software Engineering: Survey, Landscape, and Vision","Wang Yanlin, Zhong Wanjun, Huang Yanxian, Shi Ensheng, Yang Min, Chen Jiachi, Li Hui, Ma Yuchi, Wang Qianxiang, Zheng Zibin",2024,reference-manager,,,,,,"Lack of research on tree/graph-based, visual, and auditory input modalities.",,,"What are the key components, challenges, and opportunities in integrating LLM-based agents with software engineering, and how do retrieval and reasoning actions—such as search engine ranking and chain-of-thought—impact the effectiveness of these agents?","The paper analyzes how Large Language Model (LLM)-based agents are applied in Software Engineering (SE). It proposes a conceptual framework with perception, memory, and action modules, reviews 115 papers, identifies current challenges, and suggests future research opportunities to improve agent efficiency and integration with SE technologies.","The paper's research goal is to survey and analyze how LLM-based agents are combined with software engineering (SE), using a framework with perception, memory, and action modules, and its principal finding is a summary of current challenges and future research opportunities in integrating LLM-based agents with SE.",No information available
LLM-based agents struggle with multi-role and complex task performance.,,,,,,,,,,,,,,,
Absence of an authoritative,recognized code-related knowledge base for retrieval.,,,,,,,,,,,,,,
"Multi-agent collaboration requires significant computing resources and communication overhead.""","The study proposes a framework for LLM-based agents in software engineering, consisting of perception, memory, and action modules.",,,,,,,,,,,,,,
Key challenges include inefficient multi-agent collaboration due to high resource and communication costs.,,,,,,,,,,,,,,,
"Future research should focus on improving collaboration efficiency and integrating advanced SE technologies into agents.""","Limited research on LLM-based agents using tree/graph, visual, and auditory input modalities.",,,,,,,,,,,,,,
Need for enabling agents to effectively adopt new roles and manage multi-role performance in complex software engineering tasks.,,,,,,,,,,,,,,,
Lack of an authoritative,"recognized knowledge base containing rich code-related knowledge for external retrieval in software engineering.""","Future research should explore tree/graph-based, visual, and auditory input modalities for LLM-based agents in software engineering. There is also a need to improve agents' multi-role abilities, enhance multi-agent collaboration, integrate advanced SE technologies, and develop better knowledge retrieval mechanisms.",,"The discussion summarizes a framework for LLM-based agents in software engineering with perception, memory, and action modules, analyzes current challenges such as multi-agent collaboration efficiency, and highlights opportunities for integrating SE technologies and improving agent collaboration in future research.",,"The objectives are to present a general conceptual framework for LLM-based agents in Software Engineering (SE), introduce the perception, memory, and action modules, detail their internal and external actions, and discuss challenges and opportunities for improving multi-agent collaboration and integrating advanced SE technologies into agents.",,,,,,,,,
Quality and Relevance Metrics for Selection of Multimodal Pretraining Data,"Rao Roshan, Rao Sudha, Nouri Elnaz, Dey Debadeepta, Celikyilmaz Asli, Dolan Bill",2020,reference-manager,10.1109/cvprw50498.2020.00486,,,,,"Difficulty disentangling the effects of quality and relatedness metrics, as they are correlated in the datasets.",,,How do the quality and relatedness of image-text pairs in pretraining datasets impact the performance of models on downstream visuolinguistic tasks?,"The paper investigates how pretraining dataset quality and relatedness affect performance on visuolinguistic tasks. Using normalized scoring and correlation analysis, the study finds that higher quality datasets improve downstream results more than relatedness. The conclusion suggests prioritizing quality when selecting data for pretraining models.","The research goal is to maximize pretraining utility for downstream tasks using simple, inexpensive metrics; the approach involves applying these metrics to filter data for quality and relatedness; results show that higher-quality data improves performance, and future work aims to extend these methods to large paired video and ASR datasets.",
Current methodology limits further investigation of relationships between metrics and downstream performance.,,,,,,,,,,,,,,,
Results may not generalize to domains with much larger data,"such as paired video and ASR data.""","Simple, inexpensive metrics can effectively maximize pretraining utility for many downstream tasks.",,,,,,,,,,,,,
Quality of pretraining data correlates more strongly with performance than relatedness.,,,,,,,,,,,,,,,
Applying these metrics can help filter large datasets,especially in domains like paired video and ASR.,,,,,,,,,,,,,,
"Further research is needed to refine metrics and disentangle quality from relatedness.""","Need to apply proposed metrics to extremely large datasets, such as paired video and ASR data, where full pretraining is infeasible.",,,,,,,,,,,,,,
Difficulty in disentangling the effects of quality and relatedness metrics on downstream performance.,,,,,,,,,,,,,,,
"Limitations in current methodology for further investigating relationships between metrics and downstream task performance.""","Future research should apply the proposed metrics to domains with vast data, such as paired video and automatic speech recognition (ASR), to filter for highly related segments. Further investigation is needed to disentangle the effects of quality and relatedness due to their correlation in current analyses.",,"Simple, easily calculated metrics for relatedness and quality of pretraining data strongly correlate with improved performance on downstream visuolinguistic tasks, suggesting these metrics can guide dataset selection for better model outcomes.","The objectives of the study are to evaluate how simple, inexpensive metrics of relatedness and quality in image-text datasets affect the utility of pretraining for large models across various downstream visuolinguistic tasks, and to determine if these metrics can help maximize pretraining effectiveness.",,,,,,,,,,,
Development of a proof-of-concept prototype amid limited face-to-face interactions: A case study of an engineering two-student team,"Medina Uzcátegui Luis, Mardones Fernández José, Pailapán Neicuán Alex, Cárdenas Villegas Miguel",2023,reference-manager,10.1177/03064190231200397,,,,,"Small sample size (case study of only two students), limiting generalizability.",,,How do teamwork attributes and prior PBL experience influence the effectiveness of a two-student engineering team developing a proof-of-concept prototype under limited in-person interaction during the COVID-19 crisis?,"The paper qualitatively examines a two-student team’s effectiveness in developing a proof-of-concept prototype during COVID-19, using student perspectives and a questionnaire based on 11 teamwork attributes. Key findings highlight the importance of shared goals, commitment, and independence. Conclusions stress caution in generalizing results and suggest further research.","The research goal was to qualitatively assess teamwork effectiveness in a two-student engineering team with prior PBL experience; using a questionnaire on teamwork attributes, the study found improved shared goals and commitment in the small team, but noted differences from larger teams and called for further research on team size impact.","Engineering education, online learning, autonomous learning, project-based learning, digital teamwork"
Qualitative approach; lacks quantitative data.,,,,,,,,,,,,,,,
Differences in team size may influence results; further research needed.,,,,,,,,,,,,,,,
Limited face-to-face and hands-on activities due to project constraints.,,,,,,,,,,,,,,,
Reliance on self-reported perceptions.,,,,,,,,,,,,,,,
Need for interviews to complement survey data.,,,,,,,,,,,,,,,
"Prior PBL experience may bias results.""","Shared goals, values, commitment, constructive feedback, and accountability strongly influenced teamwork effectiveness in the current project.",,,,,,,,,,,,,,
Independence and self-pacing were crucial due to limited face-to-face interaction.,,,,,,,,,,,,,,,
Prior PBL experience helped mitigate project limitations.,,,,,,,,,,,,,,,
Further research is recommended on team size,self-regulation skills,"and teamwork without PBL experience.""",Further research is needed to explore the impact of team size on teamwork dynamics.,,,,,,,,,,,,
Studies should investigate the development of self-regulation skills in similar contexts.,,,,,,,,,,,,,,,
"Future research should examine teamwork dynamics when students lack experience in a PBL (Project-Based Learning) course.""","Future research should explore the impact of team size on teamwork dynamics, investigate the development of self-regulation skills in similar contexts, examine teamwork when students lack PBL experience, and use interviews to complement survey data for a more comprehensive understanding of effective teamwork strategies.",,"The study concludes that shared goals, commitment, feedback, accountability, independence, and self-pacing were key to effective teamwork in a remote, two-student project, while differences from previous larger teams highlight the need for further research on team size and dynamics in engineering education.","The objective of the study was to compare students’ perceptions of teamwork in the current project with their previous experience, focusing on assessing the efficiency of teamwork using 11 specific attributes. The aim was to evaluate teamwork efficiency regardless of project outcomes.",,,,,,,,,,,
ZEP: A Temporal Knowledge Graph Architecture for Agent Memory,"Rasmussen Preston, Paliychuk Pavlo, Beauvais Travis, Ryan Jack, Chalef Daniel",2025,reference-manager,,,,,,Results with Graphiti and Zep represent only initial advances; further research is needed.,,,"How can knowledge graphs be leveraged to enhance LLM-agent memory retrieval systems, specifically through the integration of multiple search methods, reranking, and context construction, and what are the implications for accuracy, scalability, and evaluation within business-oriented and production environments?","The paper investigates Zep, a graph-based memory retrieval system for LLM agents. Using a multi-step search, rerank, and construct methodology, Zep outperforms existing baselines and MemGPT on Deep Memory Retrieval benchmarks. The study highlights Zep’s accuracy, scalability, and the need for better memory benchmarks and ontology integration.","The paper’s research goal is to improve LLM memory using a graph-based approach (Zep) that combines semantic and episodic memory; the key method is a configurable graph search API retrieving relevant nodes and edges, and the principal finding is that Zep achieves state-of-the-art accuracy and latency on memory benchmarks.",
Limited and inadequate memory benchmarks,especially for business applications.,,,,,,,,,,,,,,
Existing benchmarks do not assess Zep’s ability to process conversation history with structured business data.,,,,,,,,,,,,,,,
Scalability in terms of cost and latency is insufficiently addressed in current literature.,,,,,,,,,,,,,,,
DMR evaluation is small-scale and poorly represents real-world enterprise use cases.,,,,,,,,,,,,,,,
Some question types in LongMemEvals are not uniformly distributed.,,,,,,,,,,,,,,,
"Additional development may be needed to improve less capable models’ understanding of temporal data.""","Graphiti and Zep show strong initial results in graph-based memory, but further research is needed, especially integrating GraphRAG and domain-specific ontologies.",,,,,,,,,,,,,,
Zep marginally outperforms MemGPT and full-conversation baselines in Deep Memory Retrieval,but current benchmarks are limited in complexity and business relevance.,,,,,,,,,,,,,,
There is a need for more robust,business-focused memory benchmarks and better evaluation of production scalability,including cost and latency.,,,,,,,,,,,,,
"Fine-tuned models and ontologies could further improve knowledge extraction and system performance.""","Limited robust and complex memory benchmarks, especially for business applications; need for new benchmarks to evaluate systems like Zep.",,,,,,,,,,,,,,
Insufficient research on production system scalability (cost and latency) for LLM memory and RAG systems.,,,,,,,,,,,,,,,
"Underexplored use of domain-specific ontologies and fine-tuned models for knowledge extraction in graph-based memory frameworks.""","Future research should explore integrating other GraphRAG approaches into Zep, fine-tuning models for Graphiti prompts, leveraging domain-specific ontologies, developing robust memory benchmarks (especially for business applications), evaluating Zep’s RAG capabilities, and addressing scalability regarding cost and latency in production systems.",,"Zep outperforms or matches baseline and prior memory retrieval methods on standard benchmarks, but current benchmarks inadequately assess complex, real-world memory needs, highlighting the need for more challenging and representative evaluations.",,,,,,,,,,,,
Knowledge-enhanced Multi-perspective Video Representation Learning for Scene Recognition,"Yu Xuzheng, Jiang Chen, Zhang Wei, Gan Tian, Chao Linlin, Zhao Jianan, Cheng Yuan, Guo Qingpei, Chu Wei",2024,reference-manager,,,,,,Difficulty distinguishing useful from useless information due to diversity and discreteness of multi-perspective data.,,,"How can a two-stream framework that integrates temporal and knowledge-enhanced non-temporal information, along with knowledge-enhanced feature fusion and self-distillation, improve the effectiveness and efficiency of video scene recognition?",,The research goal is to improve video scene recognition by effectively fusing multi-perspective information using knowledge enhancement; the approach introduces a knowledge-enhanced model balancing efficiency and performance; results show the proposed model outperforms baselines on the Koubei dataset in F1 score (0.735) and RP@90% (0.561).,
"Challenge in leveraging external knowledge because of conflicts between domain specificity and generality.
Lack of general",effective methods for fusing multi-perspective information with knowledge.,,,,,,,,,,,,,,
Knowledge-enhanced models are usually large-scale and computationally inefficient.,,,,,,,,,,,,,,,
"Future work needed on utilizing knowledge graphs and edge information for better reasoning.""","The proposed model outperforms all baselines on the Koubei dataset, achieving the highest F1 score (0.735) and RP@90% (0.561).",,,,,,,,,,,,,,
Knowledge enhancement and hierarchical structure significantly improve video scene recognition.,,,,,,,,,,,,,,,
The model balances performance and efficiency by using both temporal and non-temporal modules during training.,,,,,,,,,,,,,,,
"Recommendation: Incorporate knowledge-enhanced learning and hierarchical structures for better video scene recognition.""","Effectively fusing multi-perspective information (global vs local, temporal vs non-temporal, visual vs textural) with knowledge in video understanding remains a challenge.",,,,,,,,,,,,,,
Balancing computational efficiency with the additional time required for knowledge-enhanced models is unresolved.,,,,,,,,,,,,,,,
Future work aims to better utilize knowledge graphs,"especially leveraging edge information for improved reasoning in video understanding.""","Future research should focus on better utilization of knowledge graphs, specifically leveraging edge information to assist reasoning and enhance video understanding. Addressing the balance between model efficiency and the computational cost of knowledge integration is also suggested as a future direction.",,"The proposed knowledge-enhanced model effectively improves video scene recognition by integrating multi-perspective information while balancing efficiency, as demonstrated by superior performance on a real-world dataset.",,"The objectives of the study are to improve video scene recognition by introducing knowledge enhancement, effectively fusing multi-perspective information (global/local, temporal/non-temporal, visual/textual), and balancing model efficiency with the benefits of knowledge-enhanced methods.",,,,,,,,,
Are Knowledge Graphs Ready for the Real World? Challenges and Perspective,"Chaves-Fraga David, Corcho Oscar, Dimou Anastasia, Vidal Maria-Esther, Iglesias-Molina Ana, Van Assche Dylan",2024,reference-manager,,,,,,Lack of standard access control mechanisms in KG systems.,,,"Are knowledge graphs ready for real-world applications, and what are the main social and technical challenges—including privacy, quality, governance, and stakeholder engagement—that must be addressed to enable their effective construction, maintenance, and use across diverse contexts?",,"The paper's main objective is to enable semantic orchestration of digital twins using the SMOL language; its key method is semantic lifting of program states to RDF for semantic reflection; the principal finding is that this approach maintains a coherent semantic view and type safety, demonstrated in geological process simulation.","Keywords and phrases: access control and privacy, federated query processing, intelligent knowledge graph management, programming paradigms for knowledge graphs, semantic data integration."
Security and privacy awareness/methodological issues in knowledge representation.,,,,,,,,,,,,,,,
Difficulty generalizing security/privacy approaches to KGs.,,,,,,,,,,,,,,,
Challenges specific to federations of KGs,including semantic approaches for access control.,,,,,,,,,,,,,,
No generalizations of k-anonymity,l-diversity,t-closeness for KGs.,,,,,,,,,,,,,
High complexity of anonymization algorithms.,,,,,,,,,,,,,,,
Underdeveloped differential privacy for KGs.,,,,,,,,,,,,,,,
Data quality issues from source data,construction pipeline,and ontologies.,,,,,,,,,,,,,
"Validation and quality assessment challenges due to missing/inaccurate/redundant data.""",There is a need to create template project specification files for knowledge graph (KG)-based application projects to standardize project structure and dependencies.,,,,,,,,,,,,,,
Identifying both coarse-grained and fine-grained dependencies among project artifacts is essential for effective KG-based application development.,,,,,,,,,,,,,,,
"Further work is recommended to address challenges in access control and to promote adoption of Linked Enterprise Data (LED) in enterprises.""","Lack of standard access control mechanisms and policy languages for federations of knowledge graphs, especially regarding security and privacy.",,,,,,,,,,,,,,
Persistent data quality issues throughout the knowledge graph lifecycle,including validation,traceability,and quality assessment.,,,,,,,,,,,,
Need for practical guidelines,educational programs,"and standardized competencies to promote knowledge engineering in both academia and industry.""","Future research should address epistemic challenges in defining trustworthy knowledge engineering systems, bridging disciplinary gaps, and understanding the impact of Large Language Models (LLMs). Additional areas include migration from legacy tools, metadata management, privacy, policy, quality assurance, and developing educational materials for diverse audiences and roles.",,,"The seminar concluded that advancing knowledge graph ecosystems requires addressing technical and social challenges, integrating human expertise, ensuring quality assessment, and standardizing education to prepare for the convergence of knowledge graphs with emerging technologies like generative AI.",,,,,,,,,Objectives:
Develop an abstract framework for general access control in federated data management,adaptable to various use cases and technical setups.,,,,,,,,,,,,,,
Create template project specification files for KG-based application projects.,,,,,,,,,,,,,,,
Identify fine-grained dependencies among artefacts in knowledge graph creation and application development.,,,,,,,,,,,,,,,
"Define core educational content on knowledge in action.""",,,,,,,,,,,,,,,
"Integrating Manifold Knowledge for Global Entity Linking with
Heterogeneous Graphs""","Chen Zhibin, Wu Yuting, Feng Yansong, Zhao Dongyan",2022,reference-manager,10.1162/dint\_a\_00116,,,,,Residual connection with K = 2 drops both in-domain and cross-domain performance.,,,How can heterogeneous graph neural networks be leveraged to improve global entity linking by modeling and integrating diverse types of information within documents?,,"The research goal is to improve global entity linking; the approach uses HEGEL, a heterogeneous graph neural network that models interactions among diverse information sources; results show HEGEL achieves state-of-the-art performance in entity linking tasks.",Keywords: Entity linking; Heterogeneous graph; Graph neural network; Entity disambiguation; Knowledge base
Residual connection with K ≥ 3 improves in-domain performance but not to the level of K = 2.,,,,,,,,,,,,,,,
Error types include Topic Errors,Similar Entity Errors,Related Entity Errors,and Dataset Annotation Errors.,,,,,,,,,,,,
Similar and related entity errors may require more information to resolve.,,,,,,,,,,,,,,,
Dataset annotation errors occur,especially in CWEB.,,,,,,,,,,,,,,
"The heterogeneous structure across propagation steps may be too different for the same network layer to handle correctly.""","HEGEL, a novel graph-based global entity linking method, effectively models interactions among heterogeneous information sources.",,,,,,,,,,,,,,
HEGEL achieves state-of-the-art performance on standard entity linking benchmarks.,,,,,,,,,,,,,,,
Ablation studies show that both keyword and neighbor node information are crucial for capturing topical coherence.,,,,,,,,,,,,,,,
"Recommendation: Further research should explore additional heterogeneous information and improve error handling.""","Addressing Topic Errors, which are the main challenge for current global methods, especially in cross-domain scenarios.",,,,,,,,,,,,,,
Improving disambiguation of similar entities by introducing more information to distinguish candidates with very similar semantics.,,,,,,,,,,,,,,,
"Enhancing handling of heterogeneous structures in different propagation steps within heterogeneous graph neural networks (HGNN).""","Future research should address topic errors, similar entity errors, and related entity errors, as these are major challenges for HEGEL. Introducing more information to help disambiguate similar entities and improving handling of heterogeneous graph structures across propagation steps are recommended directions for further investigation.",,"HEGEL, a novel graph-based global entity linking method, effectively models interactions among heterogeneous information sources using a heterogeneous graph neural network, achieving state-of-the-art performance on standard benchmarks.","The objectives of the study are to present HEGEL, a novel graph-based global entity linking method that models and utilizes interactions among heterogeneous types of information from different sources, aiming to achieve state-of-the-art performance in the entity linking (EL) task.",,,,,,,,,,,
Personal Knowledge Graphs,"Balog Krisztian, Kenter Tom",2019,reference-manager,10.1145/3341981.3344241,,,,,"Lack of large, open datasets for PKGs, making evaluation difficult.",,,"How can personal knowledge graphs be effectively defined, represented, implemented, and integrated with external sources, considering their unique properties, challenges, and the need for structured, user-centric knowledge distinct from general knowledge graphs?",,"The research goal is to define personal knowledge graphs (PKGs), distinguish them from general knowledge graphs, and outline a research agenda; the approach involves identifying key properties, challenges, and open questions; the principal finding is that PKGs require new methods for evaluation, implementation, and representation due to their personal and dynamic nature.","Personal knowledge graphs, knowledge representation, personal information management"
High costs and privacy concerns in generating real user interaction data.,,,,,,,,,,,,,,,
Implementation challenges: interaction with external services,access control,storage location,multi-device support,offline functionality,,security,,,,,,,,,and privacy.
Need for user involvement in integrating and updating information.,,,,,,,,,,,,,,,
"Potential for conflicting facts or relations requiring human intervention.""",The study defines personal knowledge graphs (PKGs) and distinguishes them from general knowledge graphs.,,,,,,,,,,,,,,
Key challenges include lack of large open PKG datasets,privacy concerns,and technical implementation issues.,,,,,,,,,,,,,
A research agenda is proposed,highlighting open problems and specific research questions for PKGs.,,,,,,,,,,,,,,
"The study aims to inspire and coordinate future research on PKGs.""","Lack of large, open datasets for PKGs, making evaluation difficult due to privacy and cost concerns.",,,,,,,,,,,,,,
Open challenges in implementing PKGs,including access control,storage location,device synchronization,and ensuring security and privacy.,,,,,,,,,,,
Need for continuous integration of external knowledge sources with PKGs,"potentially involving user intervention.""","Future research should address the lack of large, open datasets for PKGs, implementation challenges (such as access control, storage, security, and privacy), entity linking methods specific to PKGs, and continuous integration of external knowledge sources with user involvement to resolve conflicting information.",,"The paper defines personal knowledge graphs (PKGs), distinguishes their unique challenges from general knowledge graphs, and proposes a research agenda addressing integration, evaluation, and implementation issues specific to PKGs.",,The study aims to (1) define the concept of a personal knowledge graph (PKG); (2) relate PKG tasks and challenges to existing knowledge graph (KG) work; and (3) propose a research agenda for PKGs by formulating specific research questions.,,,,,,,,,
Incorporation of fragmented visuo-olfactory episodic memory into dreams and its association with memory performance,"Plailly J., Villalba M., Vallat R., Nicolas A., Ruby P.",2019,reference-manager,10.1038/s41598-019-51497-y,,,,,"Low statistical power due to small sample sizes (participants or dreams), limiting interpretation and generalization.",,,"To what extent do dreams incorporate elements of recently learned multisensory episodes, and how does the method of scoring learning-related content in dream reports affect the observed relationship between dreaming and memory consolidation?",,"The research goal was to examine how elements from a learning phase are incorporated into dreams; the approach involved analyzing dream reports for learning-related content using strict and liberal scoring; results showed recent learning episodes were incorporated into dreams, supporting a link between dreaming and memory consolidation.",
Possible bias from longer dream reports in some groups,affecting results.,,,,,,,,,,,,,,
Learning-related elements in pre-experiment dreams not measured.,,,,,,,,,,,,,,,
Dream reports are subjective and may not reflect actual dream content.,,,,,,,,,,,,,,,
Repeated awakenings may alter sleep and memory consolidation.,,,,,,,,,,,,,,,
Moderate inter-scorer agreement (kappa = 0.68) due to subjective scoring criteria.,,,,,,,,,,,,,,,
Most studies used poorly described criteria for scoring task incorporation into dreams.,,,,,,,,,,,,,,,
"Lack of social dimension in memory tasks; suggested for future research.""","The study suggests learning-related dream reports may aid memory performance, but methodological limitations prevent firm conclusions.",,,,,,,,,,,,,,
Differences in dream report length may influence observed effects; future studies should control for this.,,,,,,,,,,,,,,,
Incorporation of recent memories into dreams is biased toward recent experiences.,,,,,,,,,,,,,,,
"Recommendations include using social memory tasks and improving dream scoring methods.""",The need to further test whether longer dream reports in the D+Learn group are due to better sampling of dreaming activity.,,,,,,,,,,,,,,
The lack of measurement of learning-related elements in dreams before the experiment,making it hard to assess incorporation beyond chance.,,,,,,,,,,,,,,
The possibility that learning-related dream reports may act as reminders and induce cerebral reprocessing,"which requires further investigation.""",Future research should:,,,,,,,,,,,,,
Further test if longer dream reports in the D+Learn group are due to better sampling of dreaming activity.,,,,,,,,,,,,,,,
Measure learning-related elements in dreams before experiments.,,,,,,,,,,,,,,,
Develop methods to recognize and score transformed or associated memories in dream reports.,,,,,,,,,,,,,,,
"Address limitations of subjective dream reporting and sampling.""",,"The study found that dreams rarely replay complete episodic memories but may incorporate elements from recent learning, and only the visual component of memory performance significantly improved in participants whose dreams included learning-related content, with several methodological limitations noted.","The objectives of the study were: (1) to test whether recalling a dream related to a recent experience is associated with improved memory performance, especially when memory encoding is not explicitly required and odors are included; (2) to discuss the plausibility of linking dreaming of a task with improved memory.",,,,,,,,,,,,
Blind Multimodal Quality Assessment of Low-Light Images,"Wang Miaohui, Xu Zhuowei, Xu Mai, Lin Weisi",2024,reference-manager,10.1007/s11263-024-02239-9,,,,,Subjectivity and variability in verbal descriptions hinder standardized assessment.,,"The datasets are available upon reasonable request and at https://charwill.github.io/bmqa.html. There is no mention of source code availability. For Xiang2020TMM, the authors used their own re-implementation due to unavailable official code. No further information on code reproducibility is provided.","How can QSD-based text descriptions, guided by human visual perception principles, be used to assess and analyze the relationship between image quality attributes (such as luminance and content) and subjective quality scores in low-light images?",,"The research goal is to develop a verbal description paradigm for low-light image quality, using quality semantic description (QSD) principles; the approach collects and analyzes 3,600 human-generated text descriptions focusing on perceptual attributes, and the results show strong correlations between text features (like luminance) and image quality scores.",
Human language has limited expressiveness for visual attributes.,,,,,,,,,,,,,,,
Only some QSD-based text features are considered; more attributes (e.g.,sharpness,composition,aesthetic) need study.,,,,,,,,,,,,
Multimodality-driven assessment needs verification on more benchmark datasets and full-reference tasks.,,,,,,,,,,,,,,,
"Image-text modality interaction requires further exploration.""","The proposed BMQA method outperforms 25 representative BIQA methods, especially on low-light images, with higher PLCC and SRCC and lower RMSE.",,,,,,,,,,,,,,
Text features like ‘blur’,‘noisy’,and ‘dull’ indicate poor visual experience,while ‘vivid’ indicates good quality.,,,,,,,,,,,,
Increasing the number of observed objects in images correlates with higher quality scores.,,,,,,,,,,,,,,,
"Recommendations include leveraging both image and text modalities for more accurate quality assessment.""",Limited exploration of multimodality-based quality assessment in user-perceived quality of experience (QoE).,,,,,,,,,,,,,,
Need for further investigation into the applicability and generalization of BMQAimage−only in single-image modality scenarios.,,,,,,,,,,,,,,,
Insufficient research on the contribution of each training stage (self-supervised pre-training,self-supervised training,"supervised training).""","Future research should: (1) explore how multimodal data collectively determines visual quality experience using more sensors and multi-sensor data; (2) investigate more complex quality attributes (e.g., sharpness, composition, aesthetics); (3) enhance image-text interaction; (4) validate multimodality-driven assessment on diverse benchmarks and full-reference tasks.",,,"The discussion concludes that QSD-based text descriptions effectively capture human visual perception attributes—such as luminance, content, and color—which show strong correlations with image quality scores and provide valuable supplementary information for assessing low-light image quality.",,,,,,,,,"The objectives are to develop a tractable verbal description paradigm for low-light image quality assessment, establish a multimodal quality assessment database, design quality-aware principles for benchmark construction, and investigate multimodal learning frameworks to improve blind image quality assessment using both subjective and objective perspectives."
QuanCrypt-FL: Quantized Homomorphic Encryption with Pruning for Secure Federated Learning,"Mia Md Jueal, Amini M. Hadi",2024,reference-manager,,,,,,"Strict privacy laws (GDPR, CCPA) complicate large-scale data collection, slowing research.",,,How can quantized homomorphic encryption with pruning (QuanCrypt-FL) enhance security and efficiency in federated learning by mitigating inference attacks such as gradient inversion and membership inference while maintaining model accuracy and reducing communication overhead?,,"The paper's main objective is to enhance federated learning security and efficiency by introducing QuanCrypt-FL, which combines homomorphic encryption, low-bit quantization with mean-based dynamic layerwise clipping, and unstructured pruning; results show improved model accuracy, storage, and robustness against attacks compared to previous privacy-preserving methods.","Federated Learning, Homomorphic Encryption, Quantization, Pruning, Gradient Inversion, Security."
Differential Privacy (DP) introduces noise,degrading model performance and making it impractical for real-time applications.,,,,,,,,,,,,,,
Federated Learning (FL) is vulnerable to gradient inversion attacks (GIA) and membership inference attacks (MIA),risking sensitive data exposure.,,,,,,,,,,,,,,
"Honest-but-curious servers can infer private information from exchanged gradients.""",QuanCrypt-FL achieves nearly identical accuracy to Vanilla-FL (less than 1% loss) while providing enhanced privacy.,,,,,,,,,,,,,,
QuanCrypt-FL consistently outperforms BatchCrypt in both accuracy and scalability,especially as the number of clients increases.,,,,,,,,,,,,,,
BatchCrypt suffers from significant accuracy loss (>10%) and scalability issues.,,,,,,,,,,,,,,,
QuanCrypt-FL is recommended for secure,efficient,"and scalable federated learning.""",Existing techniques have not fully addressed vulnerabilities from compromised client models during Homomorphic Encryption (HE) implementation.,,,,,,,,,,,,
Challenges remain in managing numerical overflows and maintaining model performance when applying low-bit quantization in Federated Learning (FL).,,,,,,,,,,,,,,,
Further research is needed to optimize the balance between privacy protection (e.g.,pruning,quantization) and model utility in real-time,"high-precision applications.""",,,,,,,,,,,,"QuanCrypt-FL improves efficiency, scalability, and security in federated learning by using layer-wise homomorphic encryption with quantization and pruning, resulting in faster training and inference, reduced computational overhead, and stronger defenses against gradient inversion attacks, while maintaining a balance between pruning rate and model utility."
Personal Knowledge Graphs: Use Cases in e-learning Platforms,Ilkou Eleni,2022,reference-manager,10.1145/3487553.3524196,,,,,"No gold-standards or baseline metrics exist for evaluating collaborative search and SaL, affecting evaluation consistency.",,"The research aims to make the implemented solution available to the community, considering users’ privacy. There is no explicit mention of source code availability. Evaluation involves human participants, but reproducibility may be limited due to privacy constraints and lack of standard benchmarks.","How can Personal Knowledge Graphs (PKGs) be deployed to enhance personalization and collaborative features in e-learning environments, while addressing challenges related to knowledge acquisition, entity recognition, user privacy, and technical implementation?",,"The research goal is to enhance e-learning with personalized features using Semantic Web technologies; the approach combines qualitative and quantitative analysis, interdisciplinary literature review, and user studies; results show the implemented solution supports personalization while addressing privacy and evaluation challenges.",
Existing benchmarks like EdNet do not fit SaL activities,limiting direct comparison.,,,,,,,,,,,,,,
PKGs are highly time-dependent; information may change based on computation time.,,,,,,,,,,,,,,,
"User data retention must balance privacy regulations and team objectives.""","The research is in early stages, focusing on user modeling in e-learning platforms using Personal Knowledge Graphs (PKGs).",,,,,,,,,,,,,,
There is a lack of standard evaluation metrics for collaborative search and Social and Learning (SaL) activities.,,,,,,,,,,,,,,,
Recommendations include developing privacy-respecting user profiling and time-constrained PKG computation.,,,,,,,,,,,,,,,
Future work involves interdisciplinary analysis,formalizing research questions,"and user-centered evaluations.""","Addressing privacy concerns in personalized knowledge graphs (PKGs), including user consent, data access rights, and content credibility.",,,,,,,,,,,,
Developing evaluation metrics and gold standards for collaborative search and learning systems,as current benchmarks are lacking.,,,,,,,,,,,,,,
Improving knowledge acquisition,entity recognition,linking,and scalability of PKGs,"especially when integrating with external knowledge graphs and handling large user bases.""",,"Future research should address the lack of gold-standards and baseline metrics for evaluating collaborative search and SaL. Further investigation is needed into privacy, PKG maintenance, content credibility, semantic personalized recommendations, user-centric visualizations, and the impact of PKGs in knowledge-building spaces and human-computer interaction.",,,,,,,"The objectives are to analyze and deploy PKGs (pocket-sized knowledge graphs) for personalized web searches and enhanced e-learning environments, focusing on collaborative learning. The study aims to formalize research questions, develop and evaluate solutions, and improve personalisation and explainability in educational systems using SW (Semantic Web) technologies.","The discussion concludes that while the research on personal knowledge graphs in e-learning is in early stages, challenges remain in evaluation metrics and privacy, but future work aims to address these by formalizing research questions, involving human participants, and making solutions available to the community.",
Integrate Any Omics: Towards genome-wide data integration for patient stratification,"Ma Shihao, Zeng Andy G.X., Haibe-Kains Benjamin, Goldenberg Anna, Dick John E., Wang Bo",2024,reference-manager,,,,,,"Most methods require complete data across samples, which is often not available due to experimental or financial constraints.",,,"How can IntegrAO, an integrative framework, effectively address the challenges of incomplete and heterogeneous multi-omics data to improve patient stratification and facilitate the practical application of precision medicine in clinical settings?",,"The research goal is to address challenges in multi-omics analysis; the approach is IntegrAO, an integrative framework for handling incomplete, heterogeneous data; the principal finding is that IntegrAO identifies distinct subtypes and regulatory patterns, enhancing survival prediction in patient cohorts.",No information available
Excluding samples with missing data reduces sample sizes,especially when integrating multiple omics layers.,,,,,,,,,,,,,,
Imputing missing values can introduce bias and uncertainty.,,,,,,,,,,,,,,,
Joint imputation approaches require large sample sizes and may introduce bias if imputed values are inaccurate.,,,,,,,,,,,,,,,
Optimization masking techniques may require at least one common data view across samples,which is not always feasible.,,,,,,,,,,,,,,
Optimization masking can increase computational complexity and cause inaccuracies in clustering as graph size or number increases.,,,,,,,,,,,,,,,
Difficulty in accurately classifying new patients into molecular subtypes when their omics data is incomplete,"limiting clinical application.""","IntegrAO effectively identifies clinically and biologically distinct subtypes, outperforming methods like NEMO and MSNE in survival differentiation and clinical enrichment.",,,,,,,,,,,,,
IntegrAO subtypes show significant prognostic value beyond established clinical factors (p-value=0.01425) and greater drug sensitivity differentiation (47/122 drugs,ANOVA p<0.05).,,,,,,,,,,,,,,
IntegrAO robustly integrates incomplete,heterogeneous multi-omics data,enabling accurate new patient classification.,,,,,,,,,,,,,
"Recommendation: Use IntegrAO for holistic patient stratification and translational applications requiring robust integration of partial multi-omics data.""","Develop computational techniques that directly model heterogeneous multi-omics datasets with missing data, avoiding sample exclusion or biased imputation.",,,,,,,,,,,,,,
Transform the graph fusion process into an end-to-end deep neural network for improved scalability and flexibility with large biomedical datasets.,,,,,,,,,,,,,,,
Incorporate diverse data types (e.g.,histopathology images,clinical notes,"sensor data) and conduct thorough evaluations to enhance model interpretability and applicability.""","Key future research directions include: transforming the graph fusion process into an end-to-end deep neural network for scalability, incorporating diverse data types (e.g., histopathology images, clinical notes, sensor data), expanding applications beyond cancer, and improving model interpretability and evaluation across biomedical domains.",,,,,,,,,,No information available,"IntegrAO is a robust framework for integrating incomplete and heterogeneous multi-omics data, demonstrating resilience to missing data and effectiveness in patient characterization, thus supporting comprehensive precision oncology applications."
Building a Knowledge Base of Bridge Maintenance Using Knowledge Graph,"Zhang Yang, Liu Jia, Hou Kepeng",2023,reference-manager,10.1155/2023/6047489,,,,,Data sparsity in the relational database is shown as a limitation.,,All data generated or analyzed during this study are included in the published article. No information about the availability of source code for the project is provided.,How can a comprehensive bridge maintenance knowledge graph (BMKG) be constructed using a hybrid ontology-based approach to effectively manage heterogeneous and discrete knowledge in the bridge maintenance domain?,,"The research goal is to build a bridge maintenance knowledge base using a hybrid knowledge graph method; the approach combines ontology for organization and a graph database for storage, and the principal finding is that this method improves efficiency, generality, and quality in managing bridge maintenance knowledge.",
The structure and relationships in the knowledge representation may be complex.,,,,,,,,,,,,,,,
No explicit mention of other limitations,self-reported problems,"or suggestions for further research found in the provided context.""",The proposed hybrid method using ontology and graph databases effectively manages bridge maintenance knowledge.,,,,,,,,,,,,
Rule and ontology reasoning improve knowledge graph completeness,efficiency,and quality.,,,,,,,,,,,,,
The BMKG system covers comprehensive bridge maintenance knowledge and is more suitable than relational databases.,,,,,,,,,,,,,,,
"Future work should focus on automatic knowledge extraction to speed up construction.""",Manual extraction of bridge maintenance knowledge is time-consuming; future work should focus on automatic knowledge extraction methods.,,,,,,,,,,,,,,
Existing defect classifications are too broad; more detailed subindicator concepts are needed for accurate representation.,,,,,,,,,,,,,,,
"There are few research results on knowledge graphs in bridge engineering; further studies are needed to expand applications in this domain.""","Future research should focus on developing automatic knowledge extraction methods to accelerate the construction of the bridge maintenance knowledge graph, as manual extraction is time-consuming. No other specific future research directions or gaps are mentioned in the provided context.",,"A hybrid method for building a bridge maintenance knowledge graph (BMKG) was proposed, demonstrating improved efficiency, quality, and comprehensiveness over existing methods, with future work focusing on automating knowledge extraction.","The study aims to design and construct a bridge maintenance knowledge graph (BMKG) using a hybrid approach, establish a bridge maintenance domain ontology (BMDO), and define its components—class, instance, relation, axiom, and function—to formalize and structure bridge maintenance knowledge.",,,,,,,,,,,
Multi-behavior Session-based Recommendation via Graph Reinforcement Learning,"Qin Shuo, Feng Lin, Xu Lingxiao, Deng Bowen, Li Siwen, Yang Fancheng",2023,reference-manager,,,,,,"The recommendation is limited to a specific target behavior, reducing generalizability to other behavior types.",,"The context provides detailed implementation settings (e.g., optimizer, batch size, hyperparameters) for reproducibility. However, there is no mention of the availability or location of source code for the project.",,,"The research goal is to improve multi-behavior session-based recommendation; the approach is MB-GRL, which combines graph neural networks and reinforcement learning; the principal finding is that MB-GRL outperforms all baselines on multiple datasets and behavior types, especially in distinguishing different user behavior tendencies.",
Focusing only on certain behaviors (e.g.,sharing) may not capture comprehensive user preferences.,,,,,,,,,,,,,,
Performance decreases if the discount factor or reward ratio is set too high,causing instability.,,,,,,,,,,,,,,
"Short session sequences limit the effectiveness of large discount factors.""","MB-GRL outperforms all baseline models on every metric for various behaviors, confirming its suitability for multi-behavior session-based recommendation (MBSBR) tasks.",,,,,,,,,,,,,,
The model effectively distinguishes different behavior types,leveraging the BF-RL layer to assign appropriate rewards.,,,,,,,,,,,,,,
Considering long-term rewards (using an appropriate discount factor) improves recommendation performance,but excessively high values can decrease effectiveness.,,,,,,,,,,,,,,
"Recommendations: Use MB-GRL with carefully tuned discount factors and reward ratios for optimal multi-behavior recommendation results.""",No explicit research gaps or future directions are identified in the provided context.,,,,,,,,,,,,,,
"The context focuses on experimental setup
,,,MB-GRL outperforms all baselines on every metric for various behaviors","model comparisons
effectively distinguishes different behavior types","and evaluation metrics.
and achieves superior performance among multi-behavioral models","confirming its suitability for the multi-behavior session-based recommendation (MBSBR) task.""",,,,,,,,,,,,
Time-R1: Towards Comprehensive Temporal Reasoning in LLMs,"Liu Zijia, Han Peixuan, Yu Haofei, Li Haoru, You Jiaxuan",2025,reference-manager,,,,,,Evaluation limited to Time-Bench; broader validation on external benchmarks and diverse datasets needed.,,The research is reproducible: the Time-Bench dataset and models will be released to the research community. No explicit mention of source code release is provided in the context.,"How can structured prompting and reinforcement learning be leveraged to enhance large language models’ temporal reasoning abilities, particularly for future event prediction and scenario generation, using datasets constructed from publicly available news articles?",,"The paper’s main objective is to advance temporal reasoning research by introducing Time-Bench, a large-scale dataset, and training Time-R1 using a three-stage reinforcement learning approach; results show strong performance for smaller models, with evidence that the method could scale to larger models for even greater gains.",
Focus on a 3B model due to resource constraints; larger models may yield greater gains but were not tested.,,,,,,,,,,,,,,,
"Generalizability and robustness could be further strengthened by applying the framework to larger models and datasets.""","The three-stage RL framework, especially with dynamic reward mechanisms, significantly improves both accuracy and logical consistency in temporal reasoning tasks.",,,,,,,,,,,,,,
Specialized RL fine-tuning enables smaller models (3B) to outperform or match larger baselines,offering cost-effective solutions.,,,,,,,,,,,,,,
Applying this RL approach to larger models could yield even greater performance gains.,,,,,,,,,,,,,,,
"Broader evaluation on diverse benchmarks is recommended to further validate robustness.""",Broader validation is needed: The model's effectiveness and generalization should be tested on more external temporal reasoning benchmarks and diverse datasets.,,,,,,,,,,,,,,
Scalability to larger models: Applying the three-stage RL framework to larger foundation models could yield greater performance gains.,,,,,,,,,,,,,,,
"Enhanced temporal reasoning integration: Future work should focus on improving scalability and integrating more advanced temporal reasoning capabilities.""","Future research should validate model effectiveness and generalization on more external temporal reasoning benchmarks and diverse datasets. Applying the three-stage RL framework to larger models could yield greater performance gains. Comprehensive approaches for complex, creative future-oriented reasoning and robust safeguards for generative temporal models are also needed.",,"The discussion highlights that penalty mechanisms for repetition and inconsistency improve response conciseness and logical coherence, while dynamic reward strategies and curriculum learning enhance the model’s temporal reasoning accuracy and consistency across tasks.",The primary goal is to establish a robust foundation for temporal comprehension in large language models by fine-tuning them to interpret fundamental temporal relationships between events and their times using historical news data. The study aims to improve accuracy on temporally-focused and logic-based subtasks.,,,,,,,,,,,
Data Harmonization for Heterogeneous Datasets: A Systematic Literature Review,"Kumar Ganesh, Basri Shuib, Imam Abdullahi Abubakar, Khowaja Sunder Ali, Capretz Luiz Fernando, Balogun Abdullateef Oluwagbemiga",2021,reference-manager,10.3390/app11178275,,,,,"Limited data availability and domain context due to broad use of """"harmonization"""" keyword.",,"The research follows a systematic literature review process with clear protocols for study selection, data extraction, and quality assessment. No information about the availability of source code for the project is provided.",,,"The research goal is to systematically review methods for harmonizing heterogeneous big textual data; the approach is a three-phase Systematic Literature Review (SLR); the principal finding is that various ML, DL, and NLP techniques address data heterogeneity, harmonization, and performance in large-scale textual datasets.","Keywords or tags for this research include: Data Integration, Data Fusion, Data Harmonization, Heterogeneous Data, Heterogeneity, Textual Data, Text Data, Text Preprocessing, Preprocessing, Techniques, Algorithm. These terms relate to combining and processing different types of text data using various methods and algorithms."
Biases in article selection,SLRs,and surveys.,,,,,,,,,,,,,
Some relevant articles were unavailable (publication phase 2015–2020).,,,,,,,,,,,,,,,
Minor limitation: only studies in English were selected.,,,,,,,,,,,,,,,
"Suggestions for further research include updates in medical health models and emotion recognition.""",The study highlights the importance of addressing heterogeneous data and data harmonization in large textual datasets.,,,,,,,,,,,,,,
Core techniques from Machine Learning (ML),Deep Learning (DL),and Natural Language Processing (NLP) are essential for processing and harmonizing industrial textual data.,,,,,,,,,,,,,
Performance measurement techniques guide analysts in selecting effective models and tools for real-time applications.,,,,,,,,,,,,,,,
"Recommendations include using efficient algorithms and harmonization models to manage disparate data domains.""","Data availability and domain context limitations, especially due to the broad use of """"harmonization"""" across applications.",,,,,,,,,,,,,,
Need to update FHIR-based EHR models using NLP (Natural Language Processing) and DL (Deep Learning) techniques on SSU (Structured,Semi-Structured,Unstructured) data.,,,,,,,,,,,,,
"Further research on real-time fusion methods for emotion recognition from textual data.""","Future research should address data availability and context limitations, reduce selection biases, and include non-English studies. Suggested directions include updating FHIR-based EHR models with NLP and DL on SSU data, exploring emotion and sentiment analysis, and applying real-time fusion methods for emotion recognition from textual data.",,"The discussion concludes that the study reviews research on heterogeneous textual data, data harmonization approaches, core techniques (including ML, DL, and NLP), and performance measurement methods, mainly in medical and healthcare domains, to address integration, representation, and evaluation challenges in large, disparate textual datasets.",The objectives of the study are to: (1) identify domains focused on heterogeneous data; (2) examine how data harmonization addresses heterogeneity; (3) review techniques for harmonizing large textual datasets; and (4) determine efficient deep learning algorithms for large sequential datasets.,,,,,,,,,,,
Neural spatio-temporal patterns of information processing related to cognitive conflict and correct or false recognitions,"Janik Romuald A., Podolak Igor T., Struski Łukasz, Ceglarek Anna, Lewandowska Koryna, Sikora-Wachowicz Barbara, Marek Tadeusz, Fafrowicz Magdalena",2022,reference-manager,10.1038/s41598-022-09141-9,,,,,Requires a large dataset for mean signal analysis to achieve statistical relevance.,,,,,,
Mean signals computed for individual regions may miss broader contextual information.,,,,,,,,,,,,,,,
Shapley analysis is more powerful but harder to interpret neurocognitively.,,,,,,,,,,,,,,,
"Decision tree forests in Shapley analysis may oversimplify models without enough examples.""","The Shapley analysis can identify important brain activation patterns for distinguishing events, potentially revealing patterns missed by mean signal analysis.",,,,,,,,,,,,,,
Mean signal analysis is easier to interpret but may overlook context from other brain regions.,,,,,,,,,,,,,,,
Both methods have limitations related to data size and interpretability.,,,,,,,,,,,,,,,
"Trial-by-trial analysis is suggested for future research.""",,"Future research should use simultaneous EEG–fMRI to investigate mechanisms of late brain responses. Trial-by-trial studies using machine learning are also suggested. Further exploration of spatio-temporal processing differences and late activations in short-term memory, especially using surface-based data, is recommended.",,"The discussion highlights that Shapley analysis provides a more comprehensive but less interpretable approach than mean signal analysis for identifying important brain regions, with each method having distinct limitations and strengths, and suggests future work for trial-by-trial analysis.",,,,,,,,,,,
A comprehensive survey of personal knowledge graphs,"Chakraborty Prantika, Sanyal Debarshi Kumar",2023,reference-manager,10.1002/widm.1513,,,,,"Bias in conversations: algorithmic bias, people-centric bias, and their combination.",,"No source code is provided. Data sharing is not applicable as no new data were created or analyzed. Therefore, the reproducibility of the research is limited.",,,"The paper's main objective is to survey literature on Personal Knowledge Graphs (PKGs), categorizing their applications, discussing construction methods from user data, and highlighting limitations, with the key finding that PKGs serve as personalized information databases with diverse applications and future improvement opportunities.",
Maintenance of temporal information: challenges in updating or removing outdated data in PKGs.,,,,,,,,,,,,,,,
Privacy and security issues: risk of unauthorized access to personal or sensitive data.,,,,,,,,,,,,,,,
"Limitations in sharing and expanding PKGs for collaborative domains.""","PKGs serve as personalized information databases, useful for conversational agents and recommender systems.",,,,,,,,,,,,,,
Methods for constructing PKGs from user utterances and stored data were discussed.,,,,,,,,,,,,,,,
Privacy and security are key limitations; user control over PKG access is essential.,,,,,,,,,,,,,,,
"Future work should address privacy by integrating solutions like Solid PODS.""","Privacy and security issues in PKGs, including user control over sensitive data and protection from malicious access.",,,,,,,,,,,,,,
Sparse literature on PKGs,indicating a need for more comprehensive research in this emerging area.,,,,,,,,,,,,,,
"Integration of privacy solutions like Solid PODS into PKGs as a future direction.""","Future research should address privacy and security issues in PKGs, such as user control over data access and protection of sensitive information. Integrating concepts like Solid PODS is suggested. There is also a need for more research due to sparse literature, especially in health, finance, and education domains.",,"The discussion highlights advancements and challenges in personal knowledge graph construction and conversational AI, emphasizing privacy concerns, the need for user control over personal data, and the importance of integrating secure data management solutions.","The objectives of the study are to categorize PKGs by application domain and construction process, enumerate limitations of existing works, identify possible future research directions, and provide the first survey of PKGs and their diverse aspects.",,,,,,,,,,,
Future Research Directions of Software Engineering and Knowledge Engineering,Xu Haiping,2015,reference-manager,10.1142/s0218194015500035,,,,,No explicit limitations or shortcomings are stated in the provided context.,,,,,"The paper's main objective is to analyze the intersection of software engineering and knowledge engineering by dividing it into three subareas, using a systematic discussion approach, and concludes that their interplay is increasingly vital for developing efficient intelligent software systems and guiding future research directions.",Keywords: Software engineering; knowledge engineering; knowledge-supported software engineering; intelligent software systems; knowledge as a software.
"No self-reported problems or suggestions for further research limitations are mentioned.""","Software Engineering (SE) and Knowledge Engineering (KE) are increasingly interconnected, especially for developing intelligent software systems.",,,,,,,,,,,,,,
The interplay between SE and KE is essential for efficient and cost-effective intelligent system development.,,,,,,,,,,,,,,,
Emerging technologies like big data analysis and mobile cloud computing will strongly influence future SEKE research.,,,,,,,,,,,,,,,
"Future research should focus on developing intelligent software for mobile/cloud platforms and leveraging big data for predictive capabilities.""","Integration of advanced technologies (like mobile cloud computing, big data analysis, and ambient intelligence) into software engineering and knowledge engineering.",,,,,,,,,,,,,,
Developing architectures that can combine various advanced technologies for intelligent software systems.,,,,,,,,,,,,,,,
Enhancing the interplay between software engineering and knowledge engineering for efficient,"cost-effective intelligent system development.""","Future research should focus on developing intelligent software systems for mobile devices and the cloud, leveraging big data analysis for predictive security, studying slow intelligent systems that learn like humans, and designing scalable architectures that integrate advanced technologies, especially in ambient intelligence environments.",,"Software engineering and knowledge engineering are increasingly interconnected, and their interplay is essential for developing intelligent software systems, with future research influenced by trends like big data analysis and mobile cloud computing.",,"The objectives of the study are to describe the challenges and current trends in the intersection of Software Engineering (SE) and Knowledge Engineering (KE), and to predict future research directions with the most potential for success in three subareas: KSSE, EKaaS, and ISSE.",,,,,,,,,
Temporal Fact Reasoning over Hyper-Relational Knowledge Graphs,"Ding Zifeng, Wu Jingcheng, Wu Jingpei, Xia Yan, Xiong Bo, Tresp Volker",2024,reference-manager,,,,,,No explicit limitations or shortcomings are stated in the provided context.,,"The research is reproducible. All experiments are implemented with PyTorch. The source code for baselines is available via provided GitHub links (e.g., https://github.com/ServiceNow/HypE). Hyperparameter search strategy, hardware details, and dataset construction are described. No explicit link for HypeTKG source code is given.",,,"The research goal is to improve hyper-relational temporal knowledge graph (HTKG) reasoning; the approach introduces HypeTKG, which leverages qualifier matching and time-invariant (TI) knowledge; results show HypeTKG achieves state-of-the-art performance, with ablation studies confirming the effectiveness of qualifier and TI knowledge integration.",
"No self-reported problems or suggestions for further research are mentioned.""","The qualifier matcher significantly improves reasoning by effectively leveraging related qualifiers, even when queries lack explicit qualifiers.",,,,,,,,,,,,,,
Incorporating time-invariant (TI) relational knowledge enables more accurate predictions; omitting it leads to mistakes.,,,,,,,,,,,,,,,
HypeTKG’s performance improves as more qualifiers are used,"indicating a positive correlation between qualifier usage and model accuracy.""",Existing HKG reasoning approaches lack modules specifically for temporal reasoning.,,,,,,,,,,,,,
Previous mainstream benchmark HKGs do not explicitly specify temporal information for each fact.,,,,,,,,,,,,,,,
"There is a need for new benchmark datasets and models to study temporal fact reasoning over hyper-relational knowledge graphs.""","Future research should address qualifier prediction and time prediction in HTKGs, develop HTKG extrapolation methods, and explore inductive learning on HTKGs. These areas remain unaddressed or are limitations of the current study and are important directions for further investigation.",,"HypeTKG enhances temporal knowledge graph reasoning by effectively utilizing additional subject-related qualifiers and time-invariant relational knowledge, enabling improved prediction accuracy even when queries lack explicit qualifiers.","The objectives of the study are to conduct ablation studies to demonstrate the importance of different model components, analyze the effectiveness of qualifier matchers and time-invariant (TI) relational knowledge, and evaluate how varying the proportion of qualifier-augmented facts affects model performance.",,,,,,,,,,,
Enhancing Sequential Recommendation with Graph Contrastive Learning,"Zhang Yixin, Liu Yong, Xu Yonghui, Xiong Hao, Lei Chenyi, He Weiliang, Li-Cui, Miao C.",2022,reference-manager,,"GCL4SR, a new recommendation model, uses a global transition graph and subgraph sampling to improve sequence representation.",,,,,,,,,"The research goal is to improve sequential recommendation; the approach, GCL4SR, uses subgraphs from all users’ sequences to provide both local and global context; results show GCL4SR consistently outperforms state-of-the-art methods on multiple datasets and metrics.",
It consistently outperforms state-of-the-art sequential recommendation methods across all datasets and metrics.,,,,,,,,,,,,,,,
Self-supervised learning objectives and global context enhance performance.,,,,,,,,,,,,,,,
"Future work includes developing new auxiliary objectives and applying GCL4SR to other models.""","Most existing methods only use local context from individual sequences, missing global context information.",,,,,,,,,,,,,,
Current models rely solely on item prediction loss,which may not capture appropriate sequence representations.,,,,,,,,,,,,,,
"There is a need to better integrate global and local context to improve sequential recommendation performance.""","For future research, the study suggests developing novel auxiliary learning objectives to further improve GCL4SR’s performance. Additionally, applying GCL4SR to enhance other sequential recommendation models is recommended as a future direction.",,"GCL4SR, which integrates graph contrastive learning and multi-task learning, achieves superior performance over state-of-the-art sequential recommendation methods by aligning personalized global and local sequence representations using maximum mean discrepancy (MMD) and leveraging graph neural networks for enhanced sequence modeling.","The objectives of the study are to predict the next item a user will interact with based on their interaction sequence, and to assess the performance of different methods using Hit Ratio@K (HR@K) and Normalized Discounted Cumulative Gain@K (N@K) metrics.",,,,,,,,,,,
A Study on Temporal Knowledge Graph Enrichment,Liu Yu,2021,reference-manager,,,,,,"Previous methods mainly focus on """"isA"""" relation extraction, limiting direct application to temporal instances and patterns.",,,,,"The research goal is to enrich temporal knowledge graphs (KGs) by proposing algorithms for temporal knowledge harvesting, completion, and alignment; the approach includes novel modules for entity disambiguation and temporal pattern extraction; results show improved TKG quality and effectiveness over state-of-the-art models on real-world datasets.",
Evaluation of temporal instances is challenging due to sparsity and severe semantic drift.,,,,,,,,,,,,,,,
Temporal knowledge is sparser than traditional knowledge graphs.,,,,,,,,,,,,,,,
Many missing entities,relations,and time spans in current temporal knowledge graphs.,,,,,,,,,,,,,
The number of entities and relations in temporal knowledge graphs is still small,"limiting downstream applications.""","The proposed clustering approach effectively groups relevant contexts, improving the capture of entity evolution.",,,,,,,,,,,,,
The alignment model outperforms baselines in temporal knowledge graph alignment tasks.,,,,,,,,,,,,,,,
Temporal knowledge extraction algorithms enhance accuracy and coverage by leveraging cross-sentence entity disambiguation and advanced pattern extraction.,,,,,,,,,,,,,,,
"Recommendation: Focus on relevant contexts and clustering to improve temporal knowledge graph applications.""","Limited data quality in temporal knowledge graphs (TKGs), including missing entities, relations, and time spans, and a small number of temporal facts.",,,,,,,,,,,,,,
Need for improved temporal knowledge harvesting algorithms that leverage larger existing temporal KGs.,,,,,,,,,,,,,,,
Challenges in temporal knowledge validation and evaluation,"especially without golden labeled data.""","Future research should address: (1) improving temporal knowledge harvesting using larger knowledge bases, (2) combining language models like BERT with text corpora for better temporal knowledge graph (TKG) embeddings and alignment, (3) enhancing efficiency via graph partitioning, and (4) developing robust validation and evaluation methods for temporal knowledge.",,"The thesis proposes a clustering approach for grouping relevant contexts in entity disambiguation, and experimental results validate the superiority of the proposed alignment model.",,,,,,,,,,,
Integrating Functional Status Information into Knowledge Graphs to Support Self-Health Management,"Dragoni Mauro, Bailoni Tania, Donadello Ivan, Martin Jean-Claude, Lindgren Helena",2023,reference-manager,10.1162/dint\_a\_00203,,,,,"Health assessments represent only a snapshot of a person’s status, not capturing long-term behavior change.",,"The research emphasizes reproducibility by planning to publish the Barrier Ontology in open-source repositories like BioPortal and OBO Foundry for community review. However, there is no explicit mention of source code availability for the project. Evaluations confirm consistency and error-free ontology modules.",,,The research goal is to design an AI-enabled system for monitoring and preventing functional decline using motivational coaching; the approach involves building a knowledge graph integrating expert input and literature on behavior change; the principal finding is a refined ontology supporting personalized self-health management.,"Tags: behavior change techniques, theoretical mechanisms of action, ontology, knowledge graph, self-health management, barriers, Transtheoretical Model of change, argumentation, context, utility, technique, strategy, support, attack, physical activity, patient, health barrier, psychological barrier, social barrier."
There is a gap between health goals and infrequent health assessments,limiting quality of care.,,,,,,,,,,,,,,
Ontologies require ongoing updates to add missing concepts and relations.,,,,,,,,,,,,,,,
"Imported ontologies need refinement to retain relevant elements.""","The FuS-KG is complete, concise, consistent, and coherent, especially in its TBox; all relevant health domain concepts are covered.",,,,,,,,,,,,,,
Inclusion of commercial product individuals in the ABox is recommended to improve user engagement for end-user applications.,,,,,,,,,,,,,,,
The FuS-KG demonstrates satisfactory computational efficiency and accuracy.,,,,,,,,,,,,,,,
The new modules (“Enablers”,“Barriers”,"“Arguments”) enhance AI coaching systems for monitoring users’ functional status.""",Expand the Knowledge Base by adding missing concepts and relations using domain experts and data mining techniques like Natural Language Processing and Machine Learning.,,,,,,,,,,,,
Refine the Barrier ontology to improve readability,usability,and relevance by retaining only essential elements for the application domain.,,,,,,,,,,,,,
Integrate the ontology with natural language understanding (NLU) and natural language generation (NLG) components,"and evaluate the system in real-world coaching scenarios.""",Future research should focus on: (1) expanding the knowledge base by adding missing concepts and using data mining techniques; (2) refining the Barrier ontology for better usability; (3) integrating ontology with natural language understanding/generation; and (4) evaluating the system in real-world coaching scenarios.,,"The FuS-KG was found to be complete, concise, consistent, and computationally efficient, though experts noted the need to include commercial product individuals in the ABox to enhance user engagement and support the development of end-user applications.",,"The objectives of the study are to provide conceptual modules for representing users’ functional status information (FSI) and to support the design and development of AI-enabled systems that implement behavior change strategies in patients facing specific barriers, aiming to assist individuals in monitoring and improving their health.",,,,,,,,,
Scoping review of knowledge graph applications in biomedical and healthcare sciences,"Budhdeo Sanjay, Zhang Joe, Abdulle Yusuf, Agapow Paul M, McKechnie Douglas GJ, Archer Matt, Shah Viraj, Forte Eugenia, Noori Ayush, Zitnik Marinka, Ashrafian Hutan, Sharma Nikhil",2023,reference-manager,,,,,,"Manuscript inclusion cut-off date was November 2021, possibly missing recent developments.",,,,,"The research goal was to conduct a scoping review of knowledge graph use in health, using systematic data extraction and categorization; the principal finding is that most future work focuses on improving data (99 counts) and algorithms (59 counts), with less emphasis on clinical validation or application.",No information available
Selection criteria may have excluded relevant studies not using specific keywords or MESH terms.,,,,,,,,,,,,,,,
Limited outside graph validation due to budget,regulatory,expertise,or coordination challenges.,,,,,,,,,,,,
Commercial activity may limit data access; patent screens could provide more insights.,,,,,,,,,,,,,,,
No single best all-purpose graph; performance depends on use-case.,,,,,,,,,,,,,,,
Technical and regulatory challenges in combining clinical and biomedical data.,,,,,,,,,,,,,,,
"Few studies included or suggested external validation.""","Knowledge Graphs (KGs) have diverse potential in biomedicine, mainly used for medical science insights and drug repurposing.",,,,,,,,,,,,,,
"Expanding use cases and disease coverage is recommended.
Improving data",algorithms,and validation methods is needed.,,,,,,,,,,,,,
"External validation will strengthen the robustness and clinical utility of KGs.""","Best practices for knowledge graph (KG) construction, especially regarding graph size, remain unresolved and need further study.",,,,,,,,,,,,,,
Enhancing integration of -omics data and patient data within KGs could advance personalized medicine and disease understanding.,,,,,,,,,,,,,,,
"More external validation of findings from KGs is needed to ensure real-world applicability and robustness.""","The study suggests future research should focus on improving data (99 counts) and algorithms (59 counts), refining graph construction best practices, enhancing integration of -omics and patient data, expanding use cases, increasing external validation, and comparing KGs with alternative data integration techniques. Further framework refinement is also recommended.",,"The most common future directions discussed are improving data (99 counts) and algorithms (59 counts), while validation through clinical trials and other methods is mentioned less frequently (3 and 11 counts, respectively).","The objectives of the study are to summarize planned or suggested next steps in knowledge graph (KG) research, analyze use cases, data and research characteristics, validation methods, and future directions, and to establish and refine categorizations for use cases, disease areas, and analysis methods in biomedical and healthcare KGs.",,,,,,,,,,,
Knowledge sharing and discovery across heterogeneous research infrastructures,"Farshidi Siamak, Liao Xiaofeng, Li Na, Goldfarb Doron, Magagna Barbara, Stocker Markus, Jeffery Keith, Thijsse Peter, Pichot Christian, Petzold Andreas, Zhao Zhiming",2021,reference-manager,10.12688/openreseurope.13677.1,"The ENVRI-KMS system addresses over 62% of identified requirements, making its main components functional.",,,,,,"Reproducibility: The research is only partly reproducible. Sufficient details of the code, methods, and analysis are not provided to allow replication. Information for interpreting output datasets is partly available. A related source code is referenced at https://github.com/xiaofengleo/actris.",,,,"Keywords: Knowledge base, knowledge management, search engine, research infrastructure, software development lifecycle."
The study used a mixed-method approach (design science,surveys,documentation analysis) to capture knowledge management needs.,,,,,,,,,,,,,
Building a community for ongoing assessment and improvement of ENVRI-KMS is recommended.,,,,,,,,,,,,,,,
"Key challenges remain in stakeholder engagement and fully addressing all requirements.""","There is a lack of user-friendly interfaces for general users, especially those without technical knowledge, to search and explore knowledge management systems.",,,,,,,,,,,,,,
No single existing solution satisfies all requirements; combining multiple technologies is necessary for optimal results.,,,,,,,,,,,,,,,
More than one-third (38%) of identified requirements remain unaddressed,"indicating significant gaps for future research.""","Future research should address the partially or unmet requirements, such as contact persons, multilingual queries, source code/API search, SPARQL queries, assessment tools, geolocation and metadata of entities, and Q\&A forums. Further exploration of user interfaces for different user categories and improved performance is also recommended.",,"The discussion concludes that the software tool's rationale and technical description are only partly clear, details for replication are lacking, and information for interpreting outputs is partial, leading reviewers to find the scientific standard insufficient for acceptance.",,"The objectives of the study are to develop the ENVRI-KMS knowledge management system using a design science approach, address knowledge management challenges, ensure research assets are FAIR (Findable, Accessible, Interoperable, Reusable), and meet the functional requirements of 26 research infrastructures.",,,,,,,,,
AeonG: An Efficient Built-in Temporal Support in Graph Databases,"Hou Jiamin, Zhao Zhanhao, Wang Zhouyu, Lu Wei, Jin Guodong, Wen Dong, Du Xiaoyong",2024,reference-manager,10.14778/3648160.3648187,,,,,No explicit limitations or shortcomings are stated in the provided context.,,"The research uses experimental artifacts but does not mention releasing the source code for AeonG or the baseline implementations. There is no explicit information about public availability of source code, so reproducibility is limited.",,,"The research goal is to design AeonG, a graph database with efficient built-in temporal support; the approach uses a hybrid “anchor+delta” storage engine and a native temporal query engine; results show up to 5.73× lower storage consumption and 2.57× lower query latency compared to state-of-the-art systems.",
"No self-reported problems or suggestions for further research are mentioned.
,AeonG significantly reduces storage consumption compared to Clock-G (up to 5.73×) and T-GQL (up to 3.59×) across various workloads.",,,,,,,,,,,,,,,
AeonG achieves lower temporal query latency,outperforming Clock-G by up to 26.16× and T-GQL by up to 33.23×.,,,,,,,,,,,,,,
The performance drop for non-temporal queries in AeonG is minimal (up to 9.74%).,,,,,,,,,,,,,,,
"Recommendation: AeonG is effective for managing temporal graph data with lower storage and query latency overheads.""","Limited exploration of optimizing historical data migration strategies to further reduce query latency, especially for reclaimed data.",,,,,,,,,,,,,,
Need for more comprehensive benchmarks and workloads that reflect diverse real-world temporal graph scenarios.,,,,,,,,,,,,,,,
"Insufficient study on balancing storage efficiency and temporal query performance in large-scale temporal graph systems.""
Gifts, Contexts, Means, and Ends Differing: Informing Task Scenarios to Serve Knowledge Workers’ Needs in Dynamic Complex Settings","Schmitt Ulrich, Gill Grandon",2020,"AeonG significantly reduces storage consumption compared to baseline systems and maintains acceptable non-temporal query performance, with only up to a 9.74% performance drop, while its design ensures efficient handling of both current and historical data.
reference-manager",10.28945/4667,,,,,"Limited solution space when dealing with specialized or unfamiliar topics, especially in multi/inter/trans-disciplinary problems.",,,,,"The research goal is to address attention-consuming rising entropy and enable generative innovation for knowledge workers using a PKMS-DPE approach, which employs cumulative synthesis and scenario writing; results show PKMS-DPE's negentropic and generative features efficiently support knowledge creation and innovation in complex settings.",
Did not address all possible configurations of client and target states.,,,,,,,,,,,,,,,
Current knowledge management systems (KMSs) cannot adequately address interdependent negative impacts of knowledge entropy.,,,,,,,,,,,,,,,
"Existing tools for digital curation may increase disconnected replication and redundancy.
Prototype testing
Testing","completion
completing","and migration to a viable system are still pending and require further research.""
and migrating the PKMS prototype to a viable system is a primary recommendation.",The study demonstrates how addressing rising entropy and leveraging generative innovation can be achieved through PKMS-DPE.,,,,,,,,,,,,
The article proposes a five-scenario heuristic to guide knowledge management system design for increasing complexity.,,,,,,,,,,,,,,,
"Future research should focus on iterative improvement and community network development within PKMS.""","Completion, testing, and migration of the PKMS prototype to a viable system is a primary concern.",,,,,,,,,,,,,,
Addressing user acceptance risks,including willingness to share knowledge and ensuring privacy,confidentiality,and protection.,,,,,,,,,,,,
"Further exploration of “promise engineering” to build trust and sustain the PKMS-DPE approach.""","Future research should focus on testing, completing, and migrating the PKMS prototype to a viable system, analyzing scalability risks, comparing PKMS-DPE with semantic web and AI technologies, verifying RDF-statement and ontology generation, and empirically studying PKMS sustainability and its memetic storage versus document-centric approaches.",,"The discussion concludes that the PKMS-DPE approach offers a flexible, user-centered system for knowledge management, addressing individual and community needs through scenario-based design and iterative development, while highlighting the need for empirical validation of its proposed benefits.","The objectives of the study are to develop and test PKMS (Personal Knowledge Management System) repositories that preserve individual creative resources, enable sharing and interoperability, support knowledge asset creation and diffusion, and facilitate learning, collaboration, and capacity development across diverse contexts and user communities.",,,,,,,,,,,
"iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models
https://github.com/tomasonjo/blogs/blob/master/llm/openaifunction\_constructing\_graph.ipynb
https://python.langchain.com/v0.1/docs/use\_cases/graph/constructing/
https://docs.llamaindex.ai/en/stable/examples/property\_graph/property\_graph\_basic/""","Lairgi Yassir, Moncla Ludovic, Cazabet Rémy, Benabdeslem Khalid, Cléau Pierre",2024,"reference-manager
Precision drops by 10% when using global entities as context, leading to more irrelevant relations.",,,,,,,,"The research provides detailed reproducibility metrics, including schema consistency scores and false discovery rates for entity/relation resolution. It compares results with baseline methods. Source code references are provided:",,,"The research goal is incremental knowledge graph (KG) construction using LLMs; the approach, iText2KG, employs a user-defined blueprint for flexible extraction; results show iText2KG outperforms baselines in schema consistency and precision, effectively handling diverse document types and reducing semantic duplication and unresolved entities.","Keywords: Knowledge Graph Construction, Large Language Models, Natural Language Processing."
Trade-off between graph enrichment and precision; user must choose based on use case.,,,,,,,,,,,,,,,
Annotated dataset for triplet extraction is not exhaustive,requiring manual checks.,,,,,,,,,,,,,,
Future work needed on advanced entity/relation matching metrics.,,,,,,,,,,,,,,,
"Slightly lower schema consistency for websites due to less structured content.""","iText2KG enables flexible, incremental knowledge graph construction using LLMs, outperforming baseline methods in schema consistency and extraction precision.",,,,,,,,,,,,,,
The user-defined blueprint allows adaptation to various document types and structures.,,,,,,,,,,,,,,,
There is a trade-off between graph enrichment and precision depending on entity context.,,,,,,,,,,,,,,,
"Future work should improve entity/relation matching metrics and reduce manual parameter tuning.""",Improve metrics like cosine similarity for entity and relation matching.,,,,,,,,,,,,,,
Remove the need to set a threshold as a hyperparameter in the matching process.,,,,,,,,,,,,,,,
"Integrate entity type as a parameter in the entity and relation matching process.""","Future research should enhance cosine similarity metrics for entity and relation matching, remove the need for threshold hyperparameters, and integrate entity type as a matching parameter. Addressing computational intensity and improving resolution in unstructured, large-scale text scenarios are also recommended directions.",,"The iText2KG method effectively improves knowledge graph construction by enhancing schema and information consistency, achieving high precision in triplet extraction, and demonstrating better entity and relation resolution than baselines, especially for simpler documents, though challenges remain with complex or unstructured data.","The objectives of the study are to improve the signal-to-noise ratio by reducing redundant information, guide knowledge graph construction using a schema, and ensure adaptability across various document types and use cases by extracting structured information with high schema consistency and evaluating entity/relation resolution and triplet extraction precision.",,,,,,,,,,,
Group verifiable secure aggregate federated learning based on secret sharing,"Zhou Sufang, Wang Lin, Chen Liangyi, Wang Yifeng, Yuan Ke",2025,reference-manager,10.1038/s41598-025-94478-0,,,,,"Assumes synchronized updates; real-world network latency may cause asynchronous updates, leading to instability or suboptimal model convergence.",,,,,"The research goal is to enhance secure aggregation in federated learning; the approach introduces the GVSA protocol using lightweight validation and secret sharing; results show GVSA improves security and efficiency over existing methods, though large-scale deployment still faces efficiency and security challenges.",No information available
Scalability challenges as user/device numbers grow,affecting aggregation efficiency and security.,,,,,,,,,,,,,,
Potential privacy risks if users act inquisitively,even when following the protocol.,,,,,,,,,,,,,,
"Malicious servers can introduce bias or leak data.""","The proposed group verifiable secure aggregation (GVSA) scheme addresses privacy, efficiency, and fault tolerance in federated learning.",,,,,,,,,,,,,,
GVSA reduces computational and communication overhead compared to existing methods,making it suitable for large-scale applications.,,,,,,,,,,,,,,
The scheme effectively mitigates the single point of failure risk at the aggregation server.,,,,,,,,,,,,,,,
"GVSA maintains privacy and accuracy even with user dropouts.""",Addressing instability from asynchronous updates due to network latency by developing an adaptive update mechanism.,,,,,,,,,,,,,,
Maintaining aggregation efficiency and security as the number of users and devices increases.,,,,,,,,,,,,,,,
"Enhancing practical deployment of GVSA in real-world environments with variable computational and communication states.""","The study suggests future research should address GVSA’s challenges in practical deployment, especially handling asynchronous updates due to network latency. It recommends developing an adaptive update mechanism for users and exploring ways to maintain aggregation efficiency and security as the number of users and devices increases.",,"The GVSA protocol enables secure and efficient aggregation in federated learning by using lightweight validation and secret sharing to tolerate user dropouts and prevent malicious server attacks, but further optimization is needed for large-scale deployment.","The objectives of the study are to design a protocol that enables secure model aggregation without exposing users’ local model information, defending against inference and tampering attacks, and achieving confidentiality, privacy, and correctness in federated learning systems.",,,,,,,,,,,
Enabling building digital twin: Ontology-based information management framework for multi-source data integration,"Xie X, Moretti N, Merino J, Chang J Y, Pauwels P, Parlikad A K",2022,reference-manager,10.1088/1755-1315/1101/9/092010,,,,,The proposed Foundation Data Model (FDM) does not include building product or sensor observation ontologies.,,No information available,,,The research goal is to enable effective information management for built assets by integrating heterogeneous data; the approach merges BOT and BRICK ontologies into a Foundation Data Model and compares data warehouse and mediator architectures; results show both architectures are preferable in different asset management scenarios.,No information available
Inconsistencies and redundancies between BOT and BRICK ontologies must be handled.,,,,,,,,,,,,,,,
The mediator approach may result in inconsistency and redundancy issues for data stored in separate sources.,,,,,,,,,,,,,,,
Aggregating disparate data remains challenging due to trust,privacy,ownership,"and curation issues.""",Merging BOT and BRICK ontologies creates a feasible Foundation Data Model (FDM) for building data integration.,,,,,,,,,,,
The data warehouse and mediator approaches are each preferable depending on the specific asset management service.,,,,,,,,,,,,,,,
Knowledge reuse is essential for efficient and high-quality ontology modeling.,,,,,,,,,,,,,,,
"Future work should focus on evaluating the quality of integrated data.""","Need to address inconsistencies and redundancies when merging complementary ontologies (e.g., BOT and BRICK) for a complete Foundation Data Model (FDM).",,,,,,,,,,,,,,
Lack of evaluation methods for the quality of integrated data; future studies will focus on this.,,,,,,,,,,,,,,,
Challenges in integrating heterogeneous,"multi-source data while maintaining autonomy and minimizing redundancy.""",Future research should focus on evaluating the quality of integrated data to fully realize the benefits of data integration. This addresses the limitation that large volumes of data do not always yield high-quality analytical results.,,"The discussion demonstrates how digital twin technologies enable integrated monitoring and evaluation of indoor temperature and lighting power density in Seminar Room 3, supporting energy efficiency and comfort through effective data integration and ontology use.",,"The study aims to review practical implementations of an Information Management Framework (IMF) for building digital twins, focusing on ontology-based processes to extract, transform, and integrate data using a common data model and reference data libraries, and to compare integration architectures for asset management services.",,,,,,,,,
Towards a neurodevelopmental cognitive perspective of temporal processing,"Buzi Giulia, Eustache Francis, Droit-Volet Sylvie, Desaunay Pierre, Hinault Thomas",2024,reference-manager,10.1038/s42003-024-06641-4,,,,,Gap in studies on time perception between nine years and adolescence.,,,,,"The paper's main objective is to clarify the development of brain and cognitive mechanisms underlying temporal cognition across the lifespan using a lifespan approach, with the principal finding that evaluating temporal perception can aid in detecting neuropsychiatric and neurodegenerative disorders and inform new treatments.",
Scarcity of findings on passage of time in adolescents.,,,,,,,,,,,,,,,
Need for further longitudinal studies to outline neurodevelopmental trajectory.,,,,,,,,,,,,,,,
Complex temporal tasks may still show differences at this stage.,,,,,,,,,,,,,,,
Influence of exogenous factors (e.g.,substance abuse,"negative experiences).""",Increasing anti-correlation between specific brain regions into adulthood is a hallmark of mature executive functions.,,,,,,,,,,,,
Enhanced performance in timing tasks from childhood to adulthood may be linked to improved connectivity in fronto-striatal-parietal pathways.,,,,,,,,,,,,,,,
"Strengthened connections in the salience and executive control networks improve timing abilities by integrating internal and external cues.""",,"Future research should address gaps in time perception studies between ages nine and adolescence, conduct longitudinal studies on the neurodevelopmental trajectory of temporal aspects, explore the impact of neuropsychiatric conditions, and develop cognitive assessments for temporal performance across the lifespan.",,,,"The objectives are to specify healthy development trajectories of brain and cognitive systems supporting timing abilities, clarify the evolution of temporal mechanisms (Rhythm, Durations processing, Passage of Time, Mental time travel), and promote assessment and understanding of temporal cognition in relation to neuropsychiatric conditions across the lifespan.",,,,,,,,,
Large Language Models and Medical Knowledge Grounding for Diagnosis Prediction,"Gao Yanjun, Li Ruizhe, Croxford Emma, Tesch Samuel, To Daniel, Caskey John, W. Patterson Brian, M. Churpek Matthew, Miller Timothy, Dligach Dmitriy, Afshar Majid",2023,reference-manager,10.1101/2023.11.24.23298641,,,,,"Linguistic Quality was not considered, as it was deemed less relevant in the clinical setting.",,"The research uses validated human evaluation methods (construct and content validity), detailed scoring metrics, and public (MIMIC-III) and private datasets. The evaluation framework and survey are in Supplementary Materials. No explicit mention of source code availability is provided.",,,"The research goal is to improve diagnosis summarization from daily EHR progress notes using a novel graph model, DR.KNOWS, which retrieves relevant knowledge paths from UMLS KG; results show that integrating these knowledge paths enhances ChatGPT’s diagnosis summarization performance, as measured by CUI prediction and human evaluation metrics.",
The evaluation process is not certified by peer review.,,,,,,,,,,,,,,,
Diagnostic reasoning is not a definitive process,"which may affect evaluation consistency.""","The study developed and validated a human evaluation framework for assessing clinical diagnostic reasoning outputs from LLMs, focusing on diagnostic safety and content quality.",,,,,,,,,,,,,
Integrating knowledge paths from DR.KNOWS into ChatGPT improved diagnosis summarization from daily progress notes.,,,,,,,,,,,,,,,
"Recommendations include prioritizing content-focused evaluation criteria over linguistic quality in clinical settings.""","Addressing the omission of diagnoses by minimizing aleatoric uncertainty, which occurs when evidence is present but not captured by the model.",,,,,,,,,,,,,,
Improving DR.KNOWS by enhancing knowledge path selection,incorporating probabilistic modeling,and refining embedding quality.,,,,,,,,,,,,,
"Exploring graph-prompting and instruction tuning on open-source language models to better utilize relevant knowledge paths.""","Future research should focus on reducing aleatoric uncertainty, especially where evidence exists but the model fails to use it. Enhancing clinical narrative embedding and improving DR.KNOWS, possibly with Bayesian networks, are recommended to address limitations in knowledge graph integration for diagnostic prediction.",,,"The objectives of the study are to identify and define key aspects for human evaluation in biomedical NLP tasks, develop and validate a human evaluation framework for clinical diagnostic reasoning, and assess the impact of integrating knowledge graph pathways into LLM-based diagnosis summarization.",,,,,,,,,,,
Finding Long-COVID: temporal topic modeling of electronic health records from the N3C and RECOVER programs,"O’Neil Shawn T., Madlock-Brown Charisse, Wilkins Kenneth J., McGrath Brenda M., Davis Hannah E., Assaf Gina S., Wei Hannah, Zareie Parya, French Evan T., Loomba Johanna, McMurry Julie A., Zhou Andrea, Chute Christopher G., Moffitt Richard A., Pfaff Emily R., Yoo Yun Jae, Leese Peter, Chew Robert F., Lieberman Michael, Haendel Melissa A.",2024,reference-manager,10.1038/s41746-024-01286-3,,,,,"LDA does not model temporal relationships between terms, so topics may mix risk factors and outcomes.",,"Analysis code is available at https://github.com/oneilsh/lda\_pasc. No explicit mention of data availability is provided; analyses used data accessed through the NCATS N3C Data Enclave, which may have access restrictions. Thus, only the source code is openly available for reproducibility.",,,"The research goal was to analyze sex, life-stage, and wave-specific health contrasts in PASC using N3C data; the approach involved statistical modeling of 5,400 contrasts across 68 topics; results showed most effects were small, with strong effects mainly in the PASC cohort but less coherent topics.",
LDA and its online variant can have suboptimal convergence,causing topic variation across runs.,,,,,,,,,,,,,,
PASC cohort had larger confidence intervals due to smaller size.,,,,,,,,,,,,,,,
Some topics had low specificity and coherence,making interpretation difficult.,,,,,,,,,,,,,,
Paradoxical findings (e.g.,reduced hypothyroidism in PASC patients) may reflect underdiagnosis or misclassification.,,,,,,,,,,,,,,
Most contrasts had small odds ratios,limiting effect size interpretation.,,,,,,,,,,,,,,
Study design choices (e.g.,topic selection,data filtering) may affect generalizability.,,,,,,,,,,,,,
Diagnoses for COVID-19 were removed before topic modeling,"possibly influencing topic composition.""","Of 5,400 contrasts, 314 were significant after correction, representing 68 topics; most had small odds ratios (ORs), with only 30 contrasts across 9 topics having OR ≥2.",,,,,,,,,,,,,
Strongest effects were seen in the PASC cohort,but topic coherence was lower for these.,,,,,,,,,,,,,,
Some paradoxical findings were observed,such as decreased known PASC outcomes in certain groups.,,,,,,,,,,,,,,
"Interpretation of diffuse topics is challenging due to low specificity and coherence.""","Limited coherence and interpretability of topics with the strongest increases in the PASC cohort, indicating a need for improved topic modeling methods.",,,,,,,,,,,,,,
Larger confidence intervals in the PASC cohort due to smaller sample size,highlighting the need for larger,more balanced datasets.,,,,,,,,,,,,,
Many significant contrasts have small odds ratios,"suggesting further research is needed to identify clinically meaningful differences.""","Future research should assess factors like acute disease severity, compare different cohorts, analyze inter-topic patterns for sub-phenotype-specific risk factors, and use time-series techniques to examine topic distributions over time. Investigating unique pediatric outcomes and non-specific phenotypes in COVID cohorts is also recommended.",,,,,,,,,,,,,
Continuous multimodal data supply chain and expandable clinical decision support for oncology,"Chang Jee Suk, Kim Hyunwook, Baek Eun Sil, Choi Jeong Eun, Lim Joon Seok, Kim Jin Sung, Shin Sang Joon",2025,reference-manager,10.1038/s41746-025-01508-2,,,,,Further evaluations in different institutional and national settings are needed to fully validate the system’s utility and generalizability.,,"The research is partially reproducible. Data access requires application and approval via the Severance Data Portal, with strict confidentiality policies. Custom code/scripts are not publicly available but may be requested from the corresponding author, subject to IRB approval and institutional permissions. Key software versions and analysis parameters are provided.",,,"The research goal was to establish a comprehensive oncology data supply chain by integrating clinical, genomic, and imaging data; the approach involved building a persistent, flexible, and expandable infrastructure; the principal finding is that this system can accelerate clinical decision support and AI application development for risk stratification and diagnosis.",
Additional methods such as user trials,interviews,and heuristic evaluations are required for comprehensive assessment.,,,,,,,,,,,,,
ETL operations present unique challenges,"especially with multiple ETL-related complications.""",Data consolidation and a continuous multimodal data supply chain can enhance clinical decision support and generate visual patient timelines.,,,,,,,,,,,,,
The system improves efficiency and quality of care for complex cancer cases but was developed for a single institution and may need adaptation elsewhere.,,,,,,,,,,,,,,,
Integration with national interoperability standards (like FHIR) is planned.,,,,,,,,,,,,,,,
"Further research is needed on tumor auto-segmentation.""",Limited generalizability: The method is based on a single institution's experience; large-scale adjustments are needed for broader implementation.,,,,,,,,,,,,,,
Lack of interoperability: The system was not developed with HL7 and FHIR standards in mind; future work will address compliance as standards evolve.,,,,,,,,,,,,,,,
"Need for further research on tumor auto-segmentation.""","Future research should address tumor auto-segmentation, evaluate multimodal versus single-modal data for outcome prediction, expand to additional cancer types, ensure interoperability (HL7, FHIR), improve data sharing, enhance NLP models, and capture quality of life and toxicity outcomes beyond survival and recurrence.",,"The developed data warehouse enabled rapid clinical hypothesis testing and visualization of survival by tumor stage, with high user satisfaction reported for its integration with electronic medical records and decision support systems.","The objectives of the study are to create a comprehensive cancer data library by standardizing terminology and classification, integrate preexisting registries from diverse cancer groups, and develop automated, high-quality data management systems to support multi-institutional collaboration and meet specific end-user needs.",,,,,,,,,,,
Privacy-Preserving Federated Graph Neural Network Learning on Non-IID Graph Data,"Zhang Kainan, Cai Zhipeng, Seo Daehee",2023,reference-manager,10.1155/2023/8545101,,,,,Improvement from federated learning diminishes as subgraph density increases.,,,,,"The research goal is to improve privacy-preserving graph learning using federated learning; the approach applies federated DeepWalk and federated GAT frameworks on Cora and CiteSeer datasets; results show federated methods outperform local training, with global improvements up to +18.8 in classification accuracy.",
Low-degree shared nodes cannot comprehensively transmit local information.,,,,,,,,,,,,,,,
Only specific embedding alignment methods were explored; more advanced techniques could improve results.,,,,,,,,,,,,,,,
"Further research needed on optimal public node composition to balance privacy and data availability.""",The proposed federated learning framework with embedding alignment improves data usability and privacy over local training.,,,,,,,,,,,,,,
The improvement effect is greater on sparser datasets (e.g.,CiteSeer) and diminishes as subgraph density increases.,,,,,,,,,,,,,,
Alignment precision positively correlates with classification accuracy,confirming method effectiveness.,,,,,,,,,,,,,,
"Future work should explore advanced alignment techniques and optimal public node selection for better privacy-data balance.""",Explore advanced embedding alignment technologies for more accurate information integration.,,,,,,,,,,,,,,
Investigate the role and selection of shared public nodes to balance privacy protection and data availability.,,,,,,,,,,,,,,,
"Study the application of federated graph neural networks to practical problems and real-world scenarios.""","Future research should explore advanced embedding alignment technologies for more accurate information integration. Investigating the role and selection of shared public nodes is recommended, especially to balance privacy protection and data availability. Further studies should also examine practical applications and suitable scenarios for federated GNN frameworks.",,"The proposed federated learning framework improves data usability and privacy over local training, with experimental results showing its effectiveness, though the benefit decreases with denser subgraphs, and future work should optimize public node selection for better privacy and data availability balance.",,,,,,,,,,,,
The order of multisensory associative sequences is reinstated as context feature during successful recognition,"Maack Marike Christiane, Ostrowski Jan, Rose Michael",2025,reference-manager,,"The study reveals that the brain actively integrates temporal and sensory information during multisensory memory formation, with modality sequence serving as a key contextual feature.",,,,,,The raw EEG and behavioral data are available at https://www.fdr.uni-hamburg.de/record/17120. There is no mention of source code availability.,,,"The research goal was to investigate neural mechanisms of multisensory memory retrieval using an explicit learning paradigm; the approach involved behavioral and EEG measures with modality sequence as a context feature, and the principal finding is that encoding modality sequence directly affects memory retrieval, cognitive control, and learning processes.",
"Neural mechanisms for context-feature retrieval extend beyond unimodal tasks to complex multisensory episodes.
Theta",alpha,and beta brain oscillations play distinct roles in encoding and retrieving multisensory memories.,,,,,,,,,,,,,
"Recognizing modality sequence as context can inform educational and rehabilitative strategies by optimizing learning and memory through multimodal integration.""",,,,"The study demonstrates that the brain encodes and retrieves the sequence of sensory modalities as a contextual feature during multisensory memory, highlighting the importance of temporal and sensory integration in episodic memory processes.",,,,,,,,,,,
"Auto-tuning for HPC storage stack: an optimization perspective
Large","Liu Zhangyu, Wang Jinqiu, Wu Huijun, Ma Qingzhen, Peng Lin, Tang Zhanyong
sparse","2024
and complex parameter search space with sensitive and interacting parameters.",reference-manager,10.1007/s42514-024-00198-8,,,,,"Users lack time or expertise to explore complex I/O systems, often leading to poor performance.",,,,,"The research goal is to survey auto-tuning technology in HPC I/O; the approach classifies and analyzes tuning methods (heuristic, rule-based, simulation, machine learning, hybrid); the principal finding is that auto-tuning significantly improves performance with minimal human intervention and adapts to diverse systems and applications.",
Existing research often focuses only on specific layers,missing cross-layer optimization.,,,,,,,,,,,,,,
Parameter importance varies and depends on hardware,software,and workload.,,,,,,,,,,,,,
Heuristic-based approaches are time-consuming and application-specific.,,,,,,,,,,,,,,,
Rule-based approaches require deep system knowledge,are labor-intensive,and may not capture system dynamics.,,,,,,,,,,,,,
Simulation-based approaches depend heavily on model quality and may struggle with complex systems.,,,,,,,,,,,,,,,
Machine learning-based approaches require long exploration periods and may not perform well in limited time.,,,,,,,,,,,,,,,
Hybrid approaches need more time/resources for verification and lack knowledge sharing between algorithms.,,,,,,,,,,,,,,,
"Experimental comparisons are needed to confirm theoretical strengths and weaknesses.""",Auto-tuning significantly improves HPC I/O performance with minimal human intervention and adapts to different systems and applications.,,,,,,,,,,,,,,
Simulation-based approaches reduce optimization time but depend on model accuracy.,,,,,,,,,,,,,,,
Future research should focus on abstracting applications into I/O patterns and integrating tuning with other technologies.,,,,,,,,,,,,,,,
"Experimental comparisons are needed to validate theoretical strengths and weaknesses of tuning approaches.""
The large","The complexity of the I/O path requires more comprehensive consideration of I/O component settings and their interactions.
sparse",and multidimensional search space for parameter tuning,with complex cross-layer dependencies,remains difficult to model and optimize.,,,,,,,,,,,
"There is a lack of experimental comparisons to validate the strengths and weaknesses of different auto-tuning approaches.""","Future research should focus on: experimental comparisons of tuning approaches to validate strengths and weaknesses; abstracting applications into I/O patterns for targeted tuning; integrating tuning with other technologies like resource allocation; addressing large, sparse search spaces; and improving parameter importance identification and modeling cross-layer interactions.",,"The survey reviews and categorizes auto-tuning approaches in HPC I/O, highlights their strengths and weaknesses, and suggests future research directions to address increasing complexity and performance demands in high-performance computing environments.","The objectives of the study are to improve resource utilization by tuning storage stack parameters to achieve higher goals, such as increasing I/O bandwidth or reducing I/O time, and to ensure consistency and stability of performance results, including both program and tuning algorithm performance.",,,,,,,,,,,
Multi-task heterogeneous graph learning on electronic health records,"Chan Tsai Hor, Yin Guosheng, Bae Kyongtae, Yu Lequan",2024,reference-manager,10.1016/j.neunet.2024.106644,,,,,"EHR data presents challenges of heterogeneity, sparsity, and complexity, making feature representation learning difficult.",,,,,"The research goal is to improve predictive performance on EHR tasks using a unified multi-task graph learning approach (MulT-EHR); the method employs shared-weight multi-task learning with robust hyperparameter tuning, and results show MulT-EHR outperforms all compared single- and multi-task baselines across multiple benchmarks.",
Performance decreases when the regularization parameter λ is too large.,,,,,,,,,,,,,,,
Deeper GNNs suffer from the over-smoothing problem,slightly compromising performance.,,,,,,,,,,,,,,
"Optimal dropout rate tuning is essential for best performance.""",MulT-EHR outperforms existing methods on length of stay and drug recommendation tasks across MIMIC-III and MIMIC-IV datasets.,,,,,,,,,,,,,,
The framework provides interpretable results,identifying key medical entities influencing predictions.,,,,,,,,,,,,,,
Ablation studies confirm the robustness of MulT-EHR to component and hyperparameter changes.,,,,,,,,,,,,,,,
"MulT-EHR can potentially generalize to other graph-based domains.""","Difficulty in feature representation learning due to EHR data heterogeneity, sparsity, and complexity.",,,,,,,,,,,,,,
Need for further interpretability of node embeddings to better explain key medical entities and events.,,,,,,,,,,,,,,,
Potential to generalize the proposed framework to other domains using graph representation learning,"such as recommendation systems and molecular chemistry.""",,,"The proposed method demonstrates superior and robust performance across tasks and datasets, with ablation and qualitative analyses confirming the benefits of its components, interpretability, and generalizability to other domains using graph representation learning.",,"The objectives are to evaluate the effect of the regularization parameter λ on different task losses, analyze the robustness of performance to λ, and assess how λ impacts the feature space and predictive performance in the context of multi-task learning on EHR data.",,,,,,,,,
AI-CTO: Knowledge graph for automated and dependable software stack solution,"Xu Xiaoyun, Wu Jingzheng, Yang Mutian, Luo Tianyue, Meng Qianru, Li Weiheng, Wu Yanjun",2021,reference-manager,10.3233/jifs-200899,,,,,No attributes for relations in the software knowledge graph; more information could be included in the future.,,,,,The research goal is to automatically suggest effective software stack solutions using a knowledge graph; the approach combines graph structure and semantic information with an SVM model for prediction; results show AI-CTO outperforms baselines in recommending valuable software stacks.,
Only the name is used for embedding,while various other information could be utilized.,,,,,,,,,,,,,,
The selection space does not include full-stack tools,"so the upper-bound is higher in real-world situations.""","AI-CTO effectively suggests software stack solutions by leveraging a software knowledge graph and outperforms all baseline methods in correctness (Hits@20: 0.95, Hits@50: 0.84, Hits@100: 0.75).",,,,,,,,,,,,,
Solutions recommended by AI-CTO are widely used by real companies (top20: 49,top50: 94,top100: 157).,,,,,,,,,,,,,
Future work includes adding more software features (e.g.,"code features) and improving the description encoder.""","Include more features for software, such as code features of open-source software.",,,,,,,,,,,,,
Expand the description encoder to utilize various types of information beyond software descriptions and names.,,,,,,,,,,,,,,,
"Add attributes for relations in the software knowledge graph to enrich relational information.""","Future research directions include: (1) incorporating more software features, such as code features of open-source software; (2) utilizing additional information beyond software descriptions in the description encoder; and (3) adding attributes for relations in the software knowledge graph to enrich relational information.",,"AI-CTO effectively suggests software stack solutions by combining software knowledge graph and description embeddings, outperforming baseline methods in accuracy, and future work will explore adding more features and relation attributes.","The objectives of the study are to evaluate whether AI-CTO can find effective software stack solutions (RQ1) and whether these solutions are used by enough real users (RQ2), by comparing AI-CTO’s performance to baseline methods using real-world data from stackshare.",,,,,,,,,,,
Temporal Knowledge Graph Question Answering: A Survey,"Su Miao, Li Zixuan, Chen Zhuo, Bai Long, Jin Xiaolong, Guo Jiafeng",2025,reference-manager,,,,,,Finer-grained granularities are underexplored.,,,,,"The paper’s main objective is to provide a comprehensive survey of Temporal Knowledge Graph Question Answering (TKGQA), systematically categorizing temporal question types and TKGQA methods, and concludes by identifying research gaps and future directions to stimulate further innovation in the field.",
Implicit questions receive more attention than explicit ones.,,,,,,,,,,,,,,,
Start/End and Equal questions are less studied due to limited dataset representation.,,,,,,,,,,,,,,,
Most complex temporal constraint compositions lack sufficient research.,,,,,,,,,,,,,,,
LLMs struggle with temporal expressions and symbolic temporal reasoning,"especially multi-step tasks.""","Both semantic parsing-based (SP-based) and temporal knowledge graph embedding-based (TKGE-based) methods are effective for TKGQA, with no clear overall superiority.",,,,,,,,,,,,,
SP-based methods handle a wider range of question types and benchmarks due to their flexibility.,,,,,,,,,,,,,,,
Key challenges remain in LLMs’ temporal understanding and reasoning.,,,,,,,,,,,,,,,
"Future work should expand question types and improve model robustness.""","Expand question types in datasets, including more combinations, finer time granularity, posed time, future prediction, and common sense temporal questions.",,,,,,,,,,,,,,
Improve LLMs’ understanding of temporal expressions and symbolic temporal reasoning,especially for complex,multi-step questions.,,,,,,,,,,,,,
"Address the lack of attention to the most complex temporal constraint compositions in current methods.""","Future research should explore more diverse temporal question types, finer time granularity, and questions considering the posed time or predicting the future. Improving model robustness, multi-agent collaboration, diverse data generation, and developing multi-modal TKGQA systems are also recommended. Addressing LLMs' temporal reasoning limitations is crucial.",,"The discussion summarizes that TKGQA methods are categorized into Semantic Parsing-based and TKG Embedding-based approaches, with increasing attention to complex temporal questions, but highlights the need for further research on more diverse and complex temporal constraints.",,,,,,,,,,,,
The Roadmap toward Personalized Medicine: Challenges and Opportunities,"Cinti Caterina, Trivella Maria Giovanna, Joulie Michael, Ayoub Hussein, Frenzel Monika",2024,reference-manager,10.3390/jpm14060546,,,,,Limited or short-term financial resources impede infrastructure and data integration.,,,,,"The paper’s main objective is to identify key tasks and facilitators for implementing personalized medicine (PM) through expert interviews and PM examples, using a semi-structured data collection approach; principal findings highlight eight major facilitators and stress the importance of stakeholder collaboration, education, resources, and ethical considerations.",
Complex and fragmented regulations across jurisdictions hinder implementation.,,,,,,,,,,,,,,,
Shortage of highly qualified personnel,especially in data science.,,,,,,,,,,,,,,
Health inequalities and digital divide affect access.,,,,,,,,,,,,,,,
Cultural differences and public awareness challenges.,,,,,,,,,,,,,,,
"Need for further research on disparities in PM access.""","Collaborative, multidisciplinary efforts and stakeholder engagement are essential for successful personalized medicine (PM) implementation.",,,,,,,,,,,,,,
Sustainable investment in research,innovation,and healthcare infrastructure is crucial.,,,,,,,,,,,,,
"Addressing ethical considerations and health inequalities is necessary to ensure equitable PM access.
Education",training,"and regulatory frameworks are recommended to support PM integration.""","Addressing disparities in access to personalized medicine, ensuring equitable availability regardless of socioeconomic status.",,,,,,,,,,,,
Harmonizing complex and fragmented regulations and legislation across jurisdictions to facilitate data sharing,privacy,and ethical implementation.,,,,,,,,,,,,,
Enhancing interdisciplinary education,training,and collaboration among stakeholders,"with a focus on ethical and legal issues.""","Future research should address gaps in interdisciplinary collaboration, integration of PM topics in education, ethical and legal issues, financial resource limitations, regulatory harmonization, digital divide, health inequalities, and workforce shortages. Additional studies are needed to ensure equitable access and effective implementation of personalized medicine.",,,,,,,,,,The objectives of the study are to stratify populations to identify who benefits most from a given treatment and to decrease the number of patients treated unnecessarily.,"The discussion highlights that successful personalized medicine implementation requires addressing digital, cultural, and workforce challenges, ensuring equitable access, and fostering collaboration among stakeholders, with strategies tailored to diverse populations and healthcare systems."
Lifelong learning on evolving graphs under the constraints of imbalanced classes and new classes,"Galke Lukas, Vagliano Iacopo, Franke Benedikt, Zielke Tobias, Hoffmann Marcel, Scherp Ansgar",2023,reference-manager,10.1016/j.neunet.2023.04.022,,"Parameter reuse allows high accuracy with limited history size. Adding unlabeled data after training does not improve inductively pre-trained models. Weighted binary cross-entropy in gDOC is crucial for unseen class detection in imbalanced graphs, while the risk reduction technique is not helpful. Omitting old data may improve out-of-distribution detection.",,,,Comparison of transductive and inductive learning: Models are trained on labeled data only (inductive) versus including unlabeled data (transductive) to assess accuracy changes.,,"Does adding unlabeled data to the graph after initial training improve the accuracy of inductively pre-trained models compared to transductive models, and how do different train-test splits affect this in static graph datasets for vertex classification?","The paper investigates graph neural network (GNN) performance on static and dynamic scientific publication datasets. Using repeated experiments, it compares pre-training, model types, and history sizes. Pre-trained GNNs outperform others in accuracy and stability. Unlimited history size yields the best results. gDOC outperforms DOC in new class detection.","The research goal is to improve unseen class detection in dynamic graphs; the approach introduces gDOC, a class-weighted extension of DOC, and results show gDOC consistently outperforms DOC in both MCC and Open F1 Macro scores, especially on imbalanced and challenging datasets.",
Use of static and dynamic graph datasets: Experiments utilize both static (e.g.,Cora,Citeseer,Pubmed) and dynamic (e.g.,PharmaBio,,DBLP-easy,,,,,,,,,DBLP-hard) datasets.
"Application of the tdiﬀk measure: This technique determines history sizes for training by analyzing time differences in k-neighborhoods
On DBLP-hard","enabling comparable temporal partitions across datasets.""
gDOC achieves much higher F1 scores (up to 0.16) than DOC (as low as 0.01)",demonstrating the necessity of class-weighted binary cross-entropy for reasonable performance.,"The gDOC method consistently outperforms the baseline DOC in unseen class detection on both DBLP-easy and DBLP-hard, with higher MCC (up to 0.10 vs. 0.07) and Open F1 Macro scores (up to 0.35 vs. 0.33).",,,,,,,,,,,,
"Higher detection thresholds (τ = 0.75) yield better results
On DBLP-hard","while risk reduction does not improve performance; statistical significance is not explicitly reported (no p-values provided).""
DOC F1 scores: 0.01 (history size 1)","gDOC outperforms DOC in both MCC (correct detection of new classes) and Open F1 Macro (overall OOD detection + classification) on DBLP-easy and DBLP-hard.
0.12 (unlimited); gDOC: 0.13 (history size 1)",0.16 (history size ≥6).,,,,,,,,,,,,
High threshold (τ = 0.75) yields better results.,,,,,,,,,,,,,,,
Risk reduction does not improve performance.,,,,,,,,,,,,,,,
Highest MCC on DBLP-hard: 0.09 (warm restarts,"small history size).""","Only a portion of labeled data is used for training in subsequent tasks, which may limit generalizability.",,,,,,,,,,,,,
Labeled data is sampled uniformly at random without class stratification,possibly causing class imbalance.,,,,,,,,,,,,,,
The assumption that old data (labels) do not change may not hold in all domains.,,,,,,,,,,,,,,,
The approach is limited to methods providing crisp decisions,not OOD (out-of-distribution) scores.,,,,,,,,,,,,,,
Further research is needed on adapting out-of-distribution approaches and retrieval/similarity components for graphs.,,,,,,,,,,,,,,,
"The effect of omitting old training data on OOD detection requires further analysis.""",Weighted binary cross-entropy in gDOC is essential for detecting unseen classes in imbalanced graph data.,,,,,,,,,,,,,,
The risk reduction technique does not improve performance on imbalanced datasets.,,,,,,,,,,,,,,,
gDOC consistently outperforms plain DOC,especially with class imbalance.,,,,,,,,,,,,,,
"Incremental training and parameter reuse maintain high accuracy without full retraining.""","Explore and adapt more out-of-distribution (OOD) approaches to graphs, such as using the IsoMax loss function.",,,,,,,,,,,,,,
Adapt ideas from the L2AC framework to graphs,integrating explicit retrieval and similarity components.,,,,,,,,,,,,,,
"Analyze why omitting old training data helps detect OOD examples and investigate alternative data removal strategies.""","Suggested future research directions include: adapting more out-of-distribution (OOD) approaches to graphs (e.g., IsoMax loss), integrating retrieval and similarity components (e.g., from L2AC), analyzing why omitting old training data aids OOD detection, exploring alternative vertex selection strategies, and investigating when to shrink GNN output layers.",,,"The objectives are to analyze the effect of adding unlabeled data after (pre-)training, compare inductively pre-trained models to transductively trained models, and assess whether adding unlabeled data improves performance. The study also aims to evaluate in-distribution accuracy and unseen class detection using specific measures.",,,,,,,,,,,
Managing polyglot systems metadata with hypergraphs,"Hewasinghage Moditha, Abelló Alberto, Varga Jovan, Zimányi Esteban",2021,reference-manager,10.1016/j.datak.2021.101896,,"The implementation retrieves all directly related data from the main document but must join external collections using a row-nested loop join, impacting access frequency. Algorithms 3–5 identify storage needs and access patterns, enabling cost-based schema design and query performance estimation for NoSQL systems. New insight: the approach supports multiple data store types, including graph and key–value stores.",,,,"Row-nested loop join: Used to join data across different collections outside the data store, with index and collection access frequencies calculated.",,"How can a hypergraph-based formalization of the SOS model provide a unified conceptual framework for representing, managing, and querying metadata across heterogeneous NoSQL systems in polyglot environments?","This paper formalizes the SOS model using a hypergraph-based approach to create a unified conceptual model for metadata in NoSQL polyglot systems. Using RDF exemplars and logical definitions, the methodology enables expressive, semantically flexible metadata management. Results show improved storage statistics, query access analysis, and practical feasibility in real-world systems.","The research goal is to formally represent and manage heterogeneous, semistructured metadata in polyglot systems using a hypergraph-based extension of the SOS Model; the approach enables expressive, flexible meta-representation, and the results show improved evaluation of design alternatives in document stores regarding storage, query cost, and access patterns.",
Hypergraph-based approach: Represents heterogeneous,semistructured data and manages metadata in polyglot systems.,,,,,,,,,,,,,,
"SOS model: Provides a high-level meta-layer to generalize and manage data models of heterogeneous NoSQL systems.""",The research is reproducible. The implementation of the catalog is available at https://git.io/vxyHO. No further source code details are provided in the context.,"The approach enables calculation of storage sizes and access frequencies for collections and indexes; for example, index on 𝐴\_𝐴𝐴 is accessed 5 times, and the collection 75 times.",,,,,,,,,,,,,
Relative access frequencies for Record,Track,and their indexes are 0.059,0.87,0.012,,and 0.059,,,,,,,,,respectively.
"The method supports cost-based schema design for NoSQL systems by predicting query performance using calculated storage metadata and access patterns; no explicit p-values reported.""",The index on 𝐴\_𝐴𝐴 is accessed 5 times; the collection is accessed 75 times (5 ∗ 15).,,,,,,,,,,,,,,
Relative access frequencies: Record index 0.059 (5/(5+1+5+75)),Record collection 0.87 (75/(5+1+5+75)),Track index 0.012 (1/(5+1+5+75)),Track collection 0.059 (5/(5+1+5+75)).,,,,,,,,,,,,
"The approach enables identification of storage requirements and physical access patterns for queries.""",The approach currently only generates projection queries; selection queries are considered separately.,,,,,,,,,,,,,,
The model covers RDBMS,wide-column,document,graph,and key–value stores,,but complex structures in key–value store values are not addressed.,,,,,,,,,
Multiple tables/collections may answer the same query,"requiring a greedy selection strategy.""","The proposed algorithms identify storage requirements and physical access patterns for queries, enabling estimation of query performance for different datastore designs.",,,,,,,,,,,,,
The formalized data design approach supports cost-based schema design for NoSQL systems,moving beyond rule-based methods.,,,,,,,,,,,,,,
The model is applicable to RDBMS,wide-column,document,graph,and key–value stores.,,,,,,,,,,,
Including statistical and storage metadata allows for accurate storage size and access frequency calculations,"aiding in data design decisions.""","Need for extending the formalization to support cost-based schema design for NoSQL systems, moving beyond rule-based approaches.",,,,,,,,,,,,,
Incorporating storage metadata and statistical calculations to predict query performance for alternative NoSQL designs.,,,,,,,,,,,,,,,
Further evaluation and application of the hypergraph-based meta-representation in managing heterogeneous,"semistructured data and metadata in polyglot systems.""",Future research should focus on extending the formalization of data design to support data design decisions and optimization in NoSQL systems. Developing a cost estimator using storage metadata from the metamodel could enable prediction of query performance and support cost-based schema design for NoSQL systems.,,,,"The objectives of the study are to address metadata management in polyglot systems by formalizing the SOS Model using a hypergraph-based representation, enabling a common conceptual model for metadata, and supporting the design and evaluation of data stores regarding storage requirements and query access patterns.",,,,,,,,,
The temporal compression of events during episodic future thinking,"Jeunehomme Olivier, Leroy Nathan, D'Argembeau Arnaud",2020,reference-manager,10.1016/j.cognition.2020.104416,,"Verbal reports were segmented into """"experience units"""" using verbal cues and silences. Robust statistical methods were used due to data violations. The study found that both memory and imagination compress events similarly, with higher density of experience units leading to longer duration estimates. Compression rates vary by event type and context.",,,,"Memory and future simulation tasks: Participants either remembered experienced events or mentally simulated future events, involving both action sequences and spatial displacements.",,"How do the rates and mechanisms of temporal compression differ between episodic memories and future event simulations, and how are these differences influenced by the nature of events and related to duration judgments?","The study investigated whether real-world events are structured and compressed similarly when imagining the future and remembering the past. Using verbal reports and robust statistical methods, results showed both processes use temporal compression, but past events have a higher density of experience units. Compression rates vary by event type.","The research goal was to examine whether real-world events are temporally structured and compressed similarly when imagining the future and remembering the past; using memory and future simulation tasks, results showed both processes involve temporal compression via discrete experience units, but compression rates differ by event type and temporal orientation.",
"Verbal report analysis: Researchers identified and counted """"""""experience units"""""""" (discrete moments) in participants' verbal descriptions using verbal cues and pauses.
Robust statistical methods: Data were analyzed using robust statistics (e.g.",robust two-way repeated measures ANOVA,"robust multilevel regression) due to violations of classical assumptions.""",,"Past events were described with a higher density of experience units than future events (Q = 11.99, p < .001, ξ = 0.65).",,,,,,,,,,,
Events involving specific actions had a higher density of experience units than spatial displacements (Q = 26.66,p < .001,ξ = 0.79).,,,,,,,,,,,,,
Temporal compression operates similarly for past and future events,"but compression rates vary by event type and temporal orientation.""","Past events were described with a higher density of experience units than future events (Q = 11.99, p < .001, ξ = 0.65).",,,,,,,,,,,,,
Actions had a higher density of experience units than spatial displacements (Q = 26.66,p < .001,ξ = 0.79).,,,,,,,,,,,,,
No significant interaction between temporal direction and event type (Q = 2.26,p = .133).,,,,,,,,,,,,,,
Density of experience units significantly predicted compression rates for memories (b = −0.06,SE = 0.009,df = 24.74,t = 6.34,p < .001) and future thoughts (b = −0.14,,SE = 0.02,,,,,,,p < .001).,t = 8.48,df = 24.74
The negative relationship between density of experience units and event compression rates was stronger for future than past events (interaction: b = −0.08,SE = 0.02,df = 154.38,t = 5.09,p < .001).,,,,,,,,,,,
Estimated durations were higher for future than past events (Q = 5.06,p = .025,ξ = 0.21).,,,,,,,,,,,,,
Estimated durations were higher for actions than spatial displacements (Q = 16.35,p < .001,ξ = 0.42).,,,,,,,,,,,,,
No significant interaction between temporal orientation and event type for estimated durations (Q = 0.001,p = .966).,,,,,,,,,,,,,,
Duration estimates increased with density of experience units for both past (b = 0.04,SE = 0.01,df = 17.06,t = 4.38,p < .001) and future (b = 0.08,,SE = 0.02,,,,,,,p = .001) events.,t = 3.63,df = 26.84
No significant interaction between density of experience units and temporal orientation for duration estimates (b = 0.018,SE = 0.013,df = 240.69,t = 1.41,"p = .16).""",,The study does not determine whether event compression rates can be flexibly changed based on goals or task context; this is suggested for future research.,,,,,,,,,
The sample consisted only of thirty-two young adults,"which may limit generalizability.""","Temporal compression occurs similarly when remembering the past and imagining the future, but compression rates differ by event type and temporal orientation.",,,,,,,,,,,,,
Past events and action-based events are described with higher density of experience units than future or spatial displacement events.,,,,,,,,,,,,,,,
Duration estimates increase with the density of experience units for both past and future events.,,,,,,,,,,,,,,,
"Future research should examine if event compression rates can be flexibly adjusted based on goals and context.""",Investigate whether event compression rates can be flexibly modulated based on goals and task context.,,,,,,,,,,,,,,
Examine if the nature of events affects compression rates similarly when remembering the past and imagining the future.,,,,,,,,,,,,,,,
"Assess how future event compression impacts duration judgments.""","Future research should examine whether event compression rates can be flexibly adjusted based on goals and task context. Further investigation is also needed into how the nature of events affects compression rates in both memory and future simulation, and how event compression influences duration judgments.","The study was a within-subjects experimental design with repeated measures. Tasks and event sets were counterbalanced across participants. The study involved memory and future simulation tasks, but there is no mention of randomization, blinding, control groups, or multi-site implementation.",,The objectives of the study were to investigate possible differences between retrospective and expected duration judgments and to determine the extent to which duration estimates are influenced by the density of recalled or imagined moments of experience.,,,,,,,,,,,
Intelligent bug fixing with software bug knowledge graph,Zhou Cheng,2018,reference-manager,10.1145/3236024.3275428,,"The implementation uses a Question Answering (QA) system built on a bug knowledge graph to help developers fix bugs. Entities are recognized, templates generated, and answers retrieved from the graph. Automatic bug classification and reasoning over historical fixes improve understanding and resolution efficiency. No new insights beyond these are explicitly stated.",,,,Construction and use of a bug knowledge graph to organize and manage complex bug information for intelligent bug fixing.,,"How can a software bug knowledge graph, constructed from multi-source software data, be used to enable intelligent bug fixing through collaborative search, recommendation, and question answering systems for developers?","The paper aims to improve bug understanding and fixing by leveraging the complex, multi-dimensional knowledge found in open source code, bug reports, and Q\&A documents. Using a knowledge graph-based Question Answering System, the study enhances bug classification and recommends fixes. The approach addresses challenges of heterogeneous, noisy, and expanding bug data.",The research goal is to enable intelligent bug fixing by constructing a software bug knowledge graph; the approach involves automatic bug classification and a question answering system leveraging semantic associations; results show that this method effectively guides developers in understanding and resolving bugs.,
"Automatic classification of software bugs into """"""""what",""" """"""""how",""" and """"""""why"""""""" categories using bug reports and program context.",,,,,,,,,,,,,
Implementation of a Question Answering (QA) system that recognizes entities,generates templates,"and searches the knowledge graph to assist developers.""","The research uses open source projects’ source code, bug reports, and Q\&A documents to construct a bug knowledge graph. It selects code data related to bugs from platforms like GitHub. No explicit mention of the availability of the project’s source code is provided.",The QA system uses a bug knowledge graph to help developers efficiently understand and fix bugs by recommending bug locations and solutions.,,,,,,,,,,,
Existing studies often ignore the structural features and semantic connections between bug reports and source code.,,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are provided in the context.""",Proposed a semi-supervised NER (Named Entity Recognition) method called BNER based on the CRF (Conditional Random Field) model.,,,,,,,,,,,,,,
Empirical study on Mozilla and Eclipse projects showed the baseline corpus is useful.,,,,,,,,,,,,,,,
Using unsupervised word embeddings as features had the greatest impact on BNER.,,,,,,,,,,,,,,,
"Approach effective for cross-projects NER.""",Some types of bug data are artificially missed during data acquisition for specific research purposes.,,,,,,,,,,,,,,
Overall structural features and internal semantic connections between bug reports and source code are often ignored.,,,,,,,,,,,,,,,
Bug-related data resources are heterogeneous,unbalanced,and contain noise.,,,,,,,,,,,,,
"Information overload due to expanding and updating bug data is unavoidable.""","Open source software resources contain extensive, complex bug knowledge with rich semantic associations.",,,,,,,,,,,,,,
The proposed framework uses knowledge graphs for bug classification,search,and recommendation,improving bug understanding and fixing efficiency.,,,,,,,,,,,,
The BNER method with word embeddings enhances bug-specific entity recognition,"effective even across different projects.""",Need for more effective search and recommendation techniques based on knowledge graphs for bug fixing.,,,,,,,,,,,,,
Development and implementation of a bug-fix knowledge QA system to assist developers in understanding,locating,and resolving bugs.,,,,,,,,,,,,,
"Improvement in automatic classification of bugs using bug knowledge graphs for better bug understanding and resolution.""",,,,"The objective is to implement a Question Answering (QA) system to help developers intelligently fix bugs by recognizing entities in a bug knowledge graph, generating templates, searching for answers, and recommending bug locations and solutions to improve bug understanding and fixing efficiency.",,,,,,,,,,,
Adaptive Feature Fusion Networks for Origin-Destination Passenger Flow Prediction in Metro Systems,"Xu Yuhang, Lyu Yan, Xiong Guangwei, Wang Shuyu, Wu Weiwei, Cui Helei, Luo Junzhou",2023,reference-manager,10.1109/tits.2023.3239101,,"The Implementation Insights highlight that AFFN and multi-task AFFN effectively predict metro passenger flows, capturing complex patterns using advanced neural networks. Although their prediction time is longer than smaller models, it remains within milliseconds, supporting real-time use. AFFN is scalable and maintains high accuracy even in complex metro systems.",,,,Direct Estimation Approaches: Use location-specific sample surveys to count passengers and collect trip information from a random sample.,,"How can the integration of external factor-based attention modules and enhanced multi-graph convolution gated recurrent units improve the accuracy of real-time origin-destination flow prediction in metro systems, considering periodic patterns, external influences, and incomplete or sparse OD matrices?","The paper proposes an external factor-based attention module and an Enhanced Multi-Graph Convolution Gated Recurrent Unit (EMGC-GRU) to improve metro origin-destination passenger flow prediction. Using deep learning and aggregated estimation approaches, the AFFN model outperforms baselines in accuracy, demonstrating effective spatial-temporal modeling and practical implications for metro systems.","The research goal is to improve metro origin-destination flow prediction; the approach is an Adaptive Feature Fusion Network (AFFN) using spatial-temporal modeling and external factor-based attention; results show AFFN outperforms baselines, and multi-task learning with inflow/outflow prediction further boosts accuracy.",
Dis-Aggregated Estimation Approaches: Model-based methods involving model specification,calibration,and validation,using either revealed or stated preference data.,,,,,,,,,,,,
"Aggregated Estimation Approaches: Use aggregate travel demand and traffic counts to calibrate demand models and estimate origin-destination flows.""",,"AFFN achieved the best prediction performance (lowest MAE, MAPE, and RMSE) on both Nanjing and Xi’an datasets compared to all baseline methods.",,,,,,,,,,,,,
Multi-task AFFN outperformed AFFN in OD and IO prediction on Nanjing,and in OD prediction on Xi’an,showing IO prediction helps OD accuracy.,,,,,,,,,,,,,
Integrating graph attention,periodic flows,and external factors significantly improved prediction accuracy; self-attention on periodic flows helped,"but full external factor integration (AFFN) performed best. No explicit p-values reported.""","AFFN achieved the best OD flow prediction performance (lowest MAE, MAPE, RMSE) on both Nanjing and Xi’an datasets.",,,,,,,,,,,
Multi-task AFFN outperformed single-task AFFN in OD and IO prediction on Nanjing,and in OD prediction on Xi’an.,,,,,,,,,,,,,,
AFFN variants without graph attention,periodic flow,or external factors performed worse.,,,,,,,,,,,,,
IO flow prediction errors (MAPE) were much lower than OD flow prediction errors for all methods.,,,,,,,,,,,,,,,
"AFFN maintained stable and reliable performance across different metro system scales.""",Estimation is affected by non-negligible error due to difficulty in obtaining the precise assignment matrix.,,,,,,,,,,,,,,
Limited survey budget restricts data volume,reducing estimation accuracy.,,,,,,,,,,,,,,
Demand estimation methods struggle to capture complex spatial and temporal dependencies.,,,,,,,,,,,,,,,
Existing models fail to capture the impact of external factors on periodic patterns.,,,,,,,,,,,,,,,
"OD matrices are incomplete and sparse due to unfinished trips.""",AFFN outperforms all baseline and state-of-the-art models in OD and IO passenger flow prediction accuracy on both Nanjing and Xi’an datasets.,,,,,,,,,,,,,,
Integrating IO prediction as a sub-task improves OD prediction due to strong correlation and data completeness.,,,,,,,,,,,,,,,
The external factor-based attention module and EMGC-GRU enhance prediction by capturing spatial,temporal,and external influences.,,,,,,,,,,,,,
"Aggregated estimation approaches can improve demand estimation but are limited by assignment matrix errors and data constraints.""","Difficulty in accurately estimating the assignment matrix, leading to non-negligible errors in aggregated estimation approaches.",,,,,,,,,,,,,,
Challenges due to incomplete and sparse OD matrices,which hinder accurate prediction.,,,,,,,,,,,,,,
"Limited data volume and model representational capability restrict the effectiveness of demand estimation methods.""","Future research directions include: 1) extending from one-step to multi-step prediction models, 2) predicting more detailed passenger flows by integrating local trip data (e.g., movements, waiting times), 3) testing the model in more complex metro systems, and 4) incorporating non-metro trips like bus and taxi data.","Direct Estimation Approaches: location-specific sample surveys, random sampling, observational, non-controlled, not real-time.",,,,,,,,,,,,,
Dis-Aggregated Estimation Approaches: model-based,includes model specification,calibration,validation; can use revealed preference (RP) or stated preference (SP) data.,,,,,,,,,,,,
Aggregated Estimation Approaches: uses aggregate data (traffic counts),model calibration,may assume assignment matrix independence or mutual dependence,"observational.""",,,"The objectives are to: 1) evaluate the effectiveness of the proposed AFFN network in modeling spatial-temporal dependencies of OD and IO flows, 2) verify how IO prediction helps OD prediction via multi-task learning, and 3) assess the impact of GAT and external factor-based attention modules.",,,,,,,,,
Causal feature selection using a knowledge graph combining structured knowledge from the biomedical literature and ontologies: a use case studying depression as a risk factor for Alzheimer's disease,"Malec Scott Alexander, Taneja Sanya B, Albert Steven M, Shaaban C. Elizabeth, Karim Helmet T, Levine Art S, Munro Paul Wesley, Callahan Tiffany J, Boyce Richard David",2022,reference-manager,10.1101/2022.07.18.500549,,"The implementation combines machine and human strategies: machines extract and link large-scale knowledge, while humans assess variable relevance. Ontology-grounded knowledge graphs help remove errors and provide explanations. The approach enables discovery of novel biomarkers and supports causal feature selection, but the small, non-systematic corpus limits comprehensiveness.",,,,"Machine reading systems (SemRep, EIDOS, REACH in the INDRA ecosystem) were used to extract triples from biomedical literature.",,How can a biomedical knowledge graph be used to identify and select confounders for estimating the total effect of depression on Alzheimer's disease risk from observational data?,"The study aimed to estimate depression's effect on Alzheimer's disease risk using observational data. Researchers built a knowledge graph from PubMed articles using machine reading systems, then identified confounders through graph searches. Results showed ontology-grounded graphs improved accuracy, but search depth and terminology mapping limited recall. Future work is needed.","The research goal was to identify confounding, colliding, and mediating variables between depression and Alzheimer’s disease using a knowledge graph approach, which combined machine reading and ontology grounding; results showed KGs efficiently automate reasoning and generate mechanistic hypotheses, though further research is needed for definitive conclusions.",
Forward-chaining was applied to infer implied triples based on logical properties like transitivity.,,,,,,,,,,,,,,,
Ontology-grounded knowledge graphs (KG) were constructed to organize,prune,"and explain relationships.""",The research data are publicly available at the project's Zenodo repository (https://doi.org/10.5281/zenodo.6785307) with no restrictions. There is no explicit mention of the availability of source code for the project.,The study demonstrated that ontology-grounded Knowledge Graphs (KGs) can help identify confounders and provide mechanistic explanations linking depression and Alzheimer’s disease (AD).,,,,,,,,,,,
The KG search identified several confounders,including genes,enzymes,and drug exposures,but recall was limited by search depth constraints.,,,,,,,,,,,
"No statistical significance (p-values) or quantitative results were reported in the provided context.""",The primary outcome was the identification of confounders for the relationship between depression and Alzheimer’s disease (AD) using a Knowledge Graph (KG) search.,,,,,,,,,,,,,,
The KG search successfully identified confounders,including conditions and phenotypes,some of which matched those reported in the literature (e.g.,homocysteine,APOEɛ4 carrier status).,,,,,,,,,,,
The KG search only produced output for phenotypes as colliders and mediators; genes,enzymes,and drug exposures were also identified as potential confounders.,,,,,,,,,,,,,
Limiting the search depth may have resulted in lower recall (fewer confounders found) than a deeper search.,,,,,,,,,,,,,,,
"No explicit statistical values were reported.""",Self-reported data may not align with objective measures.,,,,,,,,,,,,,,
Non-transportability of causal relationships due to population differences.,,,,,,,,,,,,,,,
Small literature corpus limits recall and representativeness.,,,,,,,,,,,,,,,
Risk of latent and unmeasured confounding remains.,,,,,,,,,,,,,,,
Search procedure constrained by max search length,limiting recall.,,,,,,,,,,,,,,
Imperfect terminology mapping leads to information loss.,,,,,,,,,,,,,,,
Confounder database is not comprehensive or systematic.,,,,,,,,,,,,,,,
Potential for confounder misclassification and omitted variable bias.,,,,,,,,,,,,,,,
Possible overadjustment or collider-stratification bias.,,,,,,,,,,,,,,,
"Bias from incompleteness and inaccuracies in machine reading/ontologies.
Selection bias",measurement error bias,"and missing data may affect results.""",Knowledge graphs (KG) can help identify potential confounders and mechanistic links in observational studies of depression and Alzheimer’s disease (AD).,,,,,,,,,,,,
Ontology-grounded KGs improve accuracy and provide mechanistic explanations but are limited by terminology mapping and search depth.,,,,,,,,,,,,,,,
Combining machine and human strategies enhances variable identification and study rigor.,,,,,,,,,,,,,,,
"Future work should improve KG search for causal feature selection and standardize reporting to increase reproducibility.""","The search depth threshold may have limited recall, missing relevant confounders, genes, enzymes, and drug exposures.",,,,,,,,,,,,,,
Imperfect terminology mapping led to information loss,with many concepts lost during translation between systems.,,,,,,,,,,,,,,
"Future work is needed to enhance KG search for causal feature selection and extend methods to other content areas.""","Future research should enhance knowledge graph (KG) search for causal feature selection, address terminology mapping issues, use larger and more representative literature samples, standardize reporting of causal diagrams, and iteratively build knowledge of confounders to improve rigor and reproducibility in observational studies.","Observational study; non-randomized; may include retrospective data collection; focus on estimating causal effects; uses methods like matching, stratification, restriction, or statistical adjustment to address confounding bias; sensitivity analyses recommended; systematic review of confounders suggested; reporting of causal directed acyclic graphs (DAGs) encouraged.",,"The objectives of the study are to develop an analytical strategy to reach a definitive conclusion for a specific case, improve the rigor of observational studies, identify potential confounders in studies of depression and Alzheimer’s disease, provide mechanistic hypotheses, and demonstrate combining semantic and causal inference.",,,,,,,,,,,
Towards Top-Down Automated Development in Limited Scopes: A Neuro-Symbolic Framework from Expressibles to Executables,"Gu Jian, Gall Harald C.",2023,reference-manager,10.1145/3611643.3613076,,"The framework uses a top-down pipeline with two key transitions: from intentions to specifications (using semantic sketches) and from specifications to realizations (using code sketches). It addresses semantic gaps and granularity inconsistencies, promoting code reusability and semantic awareness, but may require significant human intervention for complex requirements.",,,,"Mining study on open source software to analyze coding patterns, context, and API dependence.",,"How can a comprehensive code taxonomy and semantic bridging framework be developed and applied to effectively connect program intentions, specifications, and realizations, thereby improving code retrieval, generation, and modular software development?","The paper proposes a semantic-aware framework (SPF) for top-down code generation, aiming to bridge intentions, specifications, and realizations using a semantic pyramid model. The methodology involves building a code taxonomy, semantic bridging, and retrieval-based neural models. SPF enhances code reusability and modularity but faces challenges in human-machine interaction and scope limitations.","The research goal is to overcome neural method limitations in code generation by proposing a semantic-aware, neuro-symbolic framework (Semantic Pyramid Framework) that uses code taxonomy and graph-based representations; the approach enables top-down automated development, and results show improved code reusability and semantic connectivity.",
Construction of a lexical database using frame semantics and knowledge graphs to formally bridge intentions and specifications.,,,,,,,,,,,,,,,
"Design of retrieval-based neural models or inference engines paired with the constructed code taxonomy for code retrieval and generation.""",No information available,The paper proposes a semantic-aware framework (SPF) using a three-layer semantic pyramid and code taxonomy to improve top-down code generation and code reusability.,,,,,,,,,,,,,
The approach emphasizes graph-form representations for connecting intentions to realizations,supporting inspection,requirement elicitation,and multimodal representation.,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""","The primary outcome is the proposal and planned evaluation of a semantic-aware framework (Semantic Pyramid Framework, SPF) for top-down code generation.",,,,,,,,,,,,,,
The framework aims to improve code reusability,support inspection/manipulation,and enable multimodal representation.,,,,,,,,,,,,,
"No statistical values or measured effects are reported in the context.""",The research scope is limited to software with high modularity and low complexity.,,,,,,,,,,,,,,
The framework's applicability to real-world,complex,and diverse development scenarios remains untested.,,,,,,,,,,,,,
"Further research is needed to study the framework's behavior and impact on more complex and highly modular software or libraries.""",Building a code taxonomy and a semantic-aware framework (SPF) helps overcome neural method limitations and supports top-down code generation.,,,,,,,,,,,,,,
SPF enhances code reusability and connectivity,especially using graph-based representations.,,,,,,,,,,,,,,
The framework is best suited for highly modular,low-complexity software in limited scopes.,,,,,,,,,,,,,,
Further evolution and targeted application areas,like WebUI and mobile development,"are recommended.""",Study the framework's behavior and impact on building complex and highly modular software or libraries for real development needs.,,,,,,,,,,,,
Develop solutions for semantic associations between program sketches and functional descriptions,focusing on graph representations of multiple granularity or modality.,,,,,,,,,,,,,,
Construct a code corpus of templatized,"composable subprograms to promote code reusability and support code variants.""",Future research should focus on: studying the framework’s behavior and impact on complex and highly modular software; building code taxonomies in limited scopes; exploring semantic associations using graph representations; constructing lexical databases with frame semantics; and developing retrieval-based neural models for broader applicability.,No information available,,,"The objectives are to gain experience in building a code taxonomy, develop an effective methodology, construct and evaluate code taxonomies, create frameworks for bridging intentions and specifications, build a code corpus for reusability, design retrieval-based neural models, and study the framework's impact on modular software development.",,,,,,,,,
Building Contextual Knowledge Graphs for Personalized Learning Recommendations Using Text Mining and Semantic Graph Completion,"Abu-Rasheed Hasan, Dornhöfer Mareike, Weber Christian, Kismihók Gábor, Buchmann Ulrike, Fathi Madjid",2023,reference-manager,10.1109/icalt58122.2023.00016,,"The implementation uses a text-mining pipeline (TMP) to extract semantic relations and build a knowledge graph (KG) from hierarchical learning object (LO) data. The KG increases LO connectivity, enabling better contextualization. TMP performance depends on LO metadata quality. Multilingualism introduces repetitive content concerns. Future work aims to improve robustness against sparse data.",,,,"Quantitative evaluation using network metrics (e.g., Average Degree Centrality, Clustering Coefficient, Betweenness Centrality) to assess KG structure.",,"How can a semantic approach for knowledge graph completion, using a text mining pipeline for relation extraction, enhance the contextual representation and connectivity of learning objects for personalized learning systems?","The paper aims to enhance contextual representation of learning objects (LOs) for personalized learning by using a text mining pipeline (TMP) and semantic similarity calculations. Using expert-curated OER data, the methodology involves semantic relation extraction and knowledge graph (KG) construction. Results show improved LO connectivity and meaningful semantic relations, supporting better contextual learning.","The research goal is to enhance personalized learning by improving the contextual representation of learning objects (LOs) using a semantic knowledge graph (KG) constructed via a text-mining pipeline (TMP); results show the KG increases semantic connectivity among LOs, supporting richer learning contexts.",
Qualitative evaluation with domain experts to validate relation extraction and contextualization of learning recommendations.,,,,,,,,,,,,,,,
"Comparison of hierarchical data model and KG using selected graph quality metrics relevant to TEL and VET.""",,"The Knowledge Graph (KG) doubled the average degree centrality (ADC) compared to the hierarchical model, indicating increased connectivity among learning objects (LOs).",,,,,,,,,,,,,
The semantic relation extraction using the text mining pipeline (TMP) effectively connected LOs in similar learning contexts,validated both quantitatively and by domain experts.,,,,,,,,,,,,,,
"No explicit p-values or statistical significance values are provided in the context.""
Results:","Primary outcomes were measured using graph quality metrics: Average Degree Centrality, Clustering Coefficient (number of communities and average modularity score), Weakly Connected Components, and Betweenness Centrality.",,,,,,,,,,,,,,
Average Degree Centrality: Hierarchical model = 1.079,KG = 2.262 (increased),,,,,,,,,,,,,,
Clustering Coefficient (Number of communities): Hierarchical = 253,KG = 541 (increased),,,,,,,,,,,,,,
Clustering Coefficient (Average modularity score): Hierarchical = 0.779,KG = 0.636 (decreased),,,,,,,,,,,,,,
Weakly Connected Components: Hierarchical = 63,KG = 35 (decreased),,,,,,,,,,,,,,
Betweenness Centrality: Hierarchical = 1.57,KG = 15.1 (increased; about 10 times higher),,,,,,,,,,,,,,
Effects: The Knowledge Graph (KG) structure improved connectedness,community formation,"and the ability to represent complex learning relationships compared to the hierarchical model.""",Dependency of the TMP on the volume and quality of LO’s textual metadata.,,,,,,,,,,,,
Multilingualism raised concerns about repetitive content in the KG-based RS.,,,,,,,,,,,,,,,
Further development needed to handle identical content in multiple languages.,,,,,,,,,,,,,,,
Robustness against textual data sparsity needs improvement.,,,,,,,,,,,,,,,
"Additional domain-specific features are required for richer contextualization.""",The proposed semantic approach for KG completion improves the contextual representation and connectivity of learning objects (LOs) for personalized learning.,,,,,,,,,,,,,,
The text-mining pipeline (TMP) effectively extracts semantic relations,but its performance depends on the quality and volume of LO metadata.,,,,,,,,,,,,,,
The approach supports multilingualism but may lead to repetitive content; further development is needed to address this.,,,,,,,,,,,,,,,
"Future work should enhance TMP robustness against sparse data and enrich contextualization with domain-specific features.""",The dependency of the text mining pipeline (TMP) on the volume and quality of learning objects’ (LO) textual metadata.,,,,,,,,,,,,,,
Addressing repetitive content and expanding similarity descriptions for multilingual knowledge graphs.,,,,,,,,,,,,,,,
"Increasing TMP robustness against textual data sparsity and enriching LO contextualization with additional domain-specific features.""",Future research should focus on increasing the robustness of the text-mining pipeline (TMP) against sparse textual data and enriching learning objects' (LOs) contextualization with additional domain-specific features. Addressing multilingualism and repetitive content in knowledge graph-based recommender systems is also recommended.,"Systematic literature review; conceptual framework; survey; quantitative and qualitative evaluation; comparison between hierarchical data model and knowledge graph (KG); expert validation; use of network metrics; evaluation of relation extraction; no mention of randomization, blinding, control groups, or experimental design.",,"The objectives of the study are to evaluate the quality of a Knowledge Graph (KG) for learning by using both quantitative and qualitative methods, focusing on metrics that reflect the KG’s role in contextualizing learning, and to assess the effectiveness of semantic relation extraction among learning objects.",,,,,,,,,,,
An Adaptive Framework Embedded With LLM for Knowledge Graph Construction,"Wang Qingwang, Li Chaohui, Liu Yi, Zhu Qiubai, Song Jian, Shen Tao",2025,reference-manager,10.1109/tmm.2025.3557717,,Implementation Insights Summary:,,,,,,,,,
ACKG-LLM integrates multiple large language models (LLMs) and adaptive prompt generation,improving precision,recall,and F1 scores by 1–2%. However,"it incurs higher computational overhead due to multi-stage pipelines. Hybrid strategies with small models and self-checking error correction are suggested for efficiency and accuracy improvements.""",,,"How can the ACKG-LLM framework, with its adaptive prompt generation module, effectively construct knowledge graphs from structured, semi-structured, and unstructured data while reducing manual prompt design costs and improving accuracy and versatility in knowledge graph construction?",,,,,"Information Extraction (IE) Stage: Extracts implicit triples from structured, semi-structured, and unstructured data using large language models (LLMs).",,"The paper proposes the ACKG-LLM framework for constructing knowledge graphs from structured, semi-structured, and unstructured data. Using information extraction, additional relation, and normalization stages, it improves triple extraction. The adaptive prompt generation module increases interpretability and performance by 1–2%. Experiments show superior results over baseline methods.","The research goal is adaptive knowledge graph construction using LLMs; the approach decomposes construction into information extraction, additional relation, and normalization stages with adaptive prompt generation; results show improved automation and accuracy but higher computational overhead compared to other methods."
Additional Relation (AR) Stage: Enhances interpretability and accuracy by generating extra relational semantic information for triples.,,,,,,,,,,,,,,,
"Knowledge Graph Normalization (KGN) Stage: Constructs and normalizes the knowledge graph based on schema constraints using extracted and enhanced triples.""
Compared to SynthIE",ACKG-LLM achieves 1.37% higher precision,"ACKG-LLM outperforms GenIE on REBEL and Wiki-NRE datasets, with higher precision by 4.33%, recall by 3.1%, and F1 score by 3.83%.
1.21% higher recall",and 1.11% higher F1 score.,,,,,,,,,,,,
"Statistical significance (p-values) is not reported.""",The adaptive prompt generation module in ACKG-LLM improves interpretability and reduces manual prompt design time.,,,,,,,,,,,,,,
On the REBEL dataset (exact method): precision +1.04%,recall +1.03%,F1 score +1.01%.,,,,,,,,,,,,,
On the REBEL dataset (partial method): precision +1.39%,recall +1.32%,F1 score +1.36%.,,,,,,,,,,,,,
Triple extraction performance improves by 1–2%.,,,,,,,,,,,,,,,
Qwen2-7B outperforms SynthIE by 1.8% (precision),1.31% (recall),1.59% (F1).,,,,,,,,,,,,,
Mistral-7B-v0.3 outperforms SynthIE by 9.75% (precision),9.91% (recall),"9.78% (F1).""",Large language models relying on prompt engineering still suffer from the illusion problem.,,,,,,,,,,,,
Precision is not high when using multiple large language models.,,,,,,,,,,,,,,,
ACKG-LLM framework incurs high computational and memory overhead.,,,,,,,,,,,,,,,
Generalization is limited in low-resource knowledge graph construction.,,,,,,,,,,,,,,,
Further research needed on hybrid strategies,efficient algorithms,"and adaptive prompt tuning.""","The adaptive prompt generation module in ACKG-LLM improves recall, precision, and F1 scores in knowledge graph construction.",,,,,,,,,,,,
ACKG-LLM incurs higher computational overhead due to its multi-stage,multi-LLM framework.,,,,,,,,,,,,,,
Integrating small models and external models may enhance efficiency and generalization.,,,,,,,,,,,,,,,
"Further research is recommended on efficient algorithms and low-resource knowledge graph construction.""","Integration of multimodal models (e.g., Clip, Chatgpt-4o) to enhance framework generality.",,,,,,,,,,,,,,
Research on efficient search and matching algorithms for improved schema normalization and handling latent semantic information.,,,,,,,,,,,,,,,
"Exploration of hybrid strategies combining small and large models to address high memory usage and improve generalization.""",Suggestions for future research include: integrating models like Clip and Chatgpt-4o to improve generality; exploring fusion of latent semantic information; developing more efficient search and matching algorithms; studying hybrid strategies with small and large models; and enhancing low-resource knowledge graph construction.,,,,,,,,,,,,,,
"Towards Temporal Knowledge Graph Embeddings with Arbitrary Time Precision
The model
New Insights:","Leblay Julien, Chekol Melisachew Wudage, Liu Xin
ToKEi","2020
was implemented in Python3 using PyTorch","reference-manager
built on an existing TransE variant project. Experiments were run on a server with 164GB RAM","10.1145/3340531.3412028
16 CPUs",,"Implementation Insights Summary:
and 8 GPUs. ToKEi was pre-trained on RotatE and TransE",,,,,,,,,showing adaptability to existing techniques.
ToKEi effectively handles hierarchical time granularities,but error accumulation occurs at finer layers. Performance is strong on YAGO11K and Wikidata12K,but lower on ICEWS14 due to its time-point structure,"leading to high precision but poor recall.""",,,"The paper's research goal is to improve temporal knowledge graph embedding; it introduces the ToKEi approach, which uses hierarchical time layers for time prediction, and finds that ToKEi achieves high performance on YAGO11K and Wikidata12K but lower recall on ICEWS14 due to dataset characteristics.",Layer-based performance evaluation: The study assesses each model layer's performance independently to analyze error accumulation across finer granularities.,,,,,,How can hierarchical time granularity be effectively incorporated into temporal knowledge graph embeddings to improve the accuracy of temporal scoping and link prediction across datasets with varying temporal characteristics?,,"The paper investigates temporal scoping and link prediction in knowledge graphs, focusing on predicting time validity of facts at different granularities (century, decade, year, month, day). Using a layer-based approach and datasets like YAGO11K, Wikidata12K, and ICEWS14, the study finds high performance at coarse granularities, with decreasing accuracy for finer time resolutions. The results highlight challenges in precise temporal prediction, especially for datasets with point-based validity intervals."
Time prediction experiments: The study conducts experiments to predict time validity using sample-based and recursive date selection.,,,,,,,,,,,,,,,
"Temporal scoping and node prediction: The study evaluates temporal scoping and temporal node prediction as part of its experimental setup.""","The research is reproducible. The source code for the ToKEi project is available at https://gitlab.com/jleblay/tokei. The implementation uses Python 3 and PyTorch, and builds upon an existing project at https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding.","CENTURY and DECADE layers show high performance in time validity prediction (e.g., CENTURY ROC: .970 YAGO11K, .968 Wikidata12K), but performance drops at finer granularities, especially on ICEWS14 (e.g., DAY recall: .122).",,,,,,,,,,,,,
ToKEi achieves competitive HITS@N and MRR scores,with only slight degradation at finer time layers; it outperforms competitors on YAGO11K down to YEAR and MONTH layers for HITS@3 and HITS@10.,,,,,,,,,,,,,,
"Statistical significance (p-values) is not reported in the context.""
On Wikidata12K
On ICEWS14","ToKEi achieved HITS@1 of .218 (.405), HITS@3 of .355 (.618), HITS@10 of .470 (.779), MR of 723.2 (114.8), and MRR of .307 (.500) on YAGO11K (values in parentheses are for random date selection).
ToKEi had HITS@1 of .244 (.311)
ToKEi had HITS@1 of .210","HITS@3 of .390 (.519)
HITS@3 of .248","HITS@10 of .520 (.669)
HITS@10 of .294","MR of 444.4 (129.1)
MR of 2101.9",,"and MRR of .342 (.442).
and MRR of .236.",,,,,,,,,
For time validity prediction (Table 2),ToKEi achieved ROC scores up to .924,BA up to .822,precision up to .911,and recall up to .691 on YAGO11K.,,,,,,,,,,,
ToKEi generally performed best on valid time model temporal KGs (YAGO11K,Wikidata12K) and lagged behind on ICEWS14,especially in HITS@10 and mean rank.,,,,,,,,,,,,,
In re-ranking error (Table 6),ToKEi had PRE of .316 (.095),NRE of .286 (.622),"and MRD of -225.2 (+383.3) on YAGO11K.""","Lower performance on ICEWS14, attributed to all validity intervals being time points.",,,,,,,,,,,
Poor recall despite high precision on ICEWS14,indicating difficulty distinguishing true positives and negatives.,,,,,,,,,,,,,,
Data skew mainly handled by the first model layer; deeper layers may be less robust.,,,,,,,,,,,,,,,
"No explicit discussion of other limitations or generalizability.""","The model achieves high performance on YAGO11K and Wikidata12K, especially at coarser time granularities, but struggles with finer granularity (YEAR) and on ICEWS14.",,,,,,,,,,,,,,
Data skew is mostly handled by the first model layer; deeper layers perform well due to data characteristics.,,,,,,,,,,,,,,,
Performance on ICEWS14 is lower,mainly due to difficulties distinguishing true positives and negatives at finer time points.,,,,,,,,,,,,,,
Recommendation: Address challenges in fine-grained temporal prediction and improve discrimination between true and false predictions,"especially for datasets like ICEWS14.""","The need to formally redefine ranking metrics for temporal knowledge graph (KG) embeddings, as current metrics do not account for time or multiple valid intervals.",,,,,,,,,,,,,
Difficulty in predicting fine-grained time granularities,especially for datasets like ICEWS14,where recall is low despite high precision.,,,,,,,,,,,,,
"Challenges in discriminating true positives and negatives across margin-based thresholds in temporal scoping tasks.""",,,,,,,,,,,,,,,
Temporal Knowledge Graph Reasoning Based on Evolutional Representation Learning,"Li Zixuan, Jin Xiaolong, Li Wei, Guan Saiping, Guo Jiafeng, Shen Huawei, Wang Yuanzhuo, Cheng Xueqi",2021,reference-manager,10.1145/3404835.3462963,,Implementation Insights Summary:,,,,,,,,,
"The model uses an embedding dimension of 200
New Insights:",relation-aware GCN layers (1 for YAGO,2 for others),and a dropout rate of 0.2. Optimal history lengths vary by dataset (e.g.,ICEWS18: 6). Adam optimizer is used. The time gate recurrent component is crucial for handling long historical sequences and repetitive facts.,,,,,,,,,,,
Performance drops with more neighbors or relations. RE-GCN can predict even without subject entity history,"likely due to static graph and shared initial representations. The time gate recurrent component prevents over-smoothing and vanishing gradients in deep GCNs.""",,The research goal is temporal knowledge graph reasoning; the approach is a novel GCN-based Recurrent Evolution network (RE-GCN) that models KG sequences to learn evolutional representations; results show RE-GCN significantly outperforms baselines in entity and relation prediction and achieves up to 82× speedup in entity prediction.,"The paper proposes RE-GCN, a novel GCN-based recurrent evolution network for temporal knowledge graph reasoning. It models entire KG sequences to learn evolving representations of entities and relations, incorporating both structural and sequential dependencies. Experiments show RE-GCN significantly outperforms baselines in prediction tasks and achieves up to 82x speedup.",,,,,,,,,,"The study conducts a detailed analysis by splitting the validation set based on the number of one-hop neighbors and direct interactions for entity prediction, and the number of relations and occurrence history for relation prediction.",How can the proposed RE-GCN model effectively capture structural dependencies and sequential patterns in temporal knowledge graphs to improve entity and relation prediction at future timestamps?
Performance is evaluated using Hits@3 on each subset.,,,,,,,,,,,,,,,
"Temporal reasoning under the extrapolation setting is applied.""",The research is reproducible. The source code for the project is available at https://github.com/Lee-zix/RE-GCN.,"RE-GCN outperforms all baselines on six benchmarks for temporal knowledge graph reasoning, achieving the highest MRR and Hits@k scores (e.g., MRR up to 39.84% on WIKI, 38.27% on ICE18).",,,,,,,,,,,,,
RE-GCN enables 17 to 82 times speedup in entity prediction compared to RE-NET.,,,,,,,,,,,,,,,
"No statistical significance (p-values) reported.""",Primary outcomes are measured using MRR (Mean Reciprocal Rank) and H@k (Hits at k) for entity and relation prediction tasks.,,,,,,,,,,,,,,
RE-GCN achieves the highest performance among all models:,,,,,,,,,,,,,,,
"On ICE18: MRR = 27.51
On WIKI: MRR = 39.84","H@1 = 17.82
H@3 = 44.43","H@3 = 31.17
H@10 = 53.88",H@10 = 46.55,,,,,,,,,,,,
With ground truth history (w. GT),RE-GCN further improves:,,,,,,,,,,,,,,
"On ICE18: MRR = 30.55
On WIKI: MRR = 51.53","H@1 = 20.00
H@3 = 58.29","H@3 = 34.73
H@10 = 69.53",H@10 = 51.46,,,,,,,,,,,,
On relation prediction (MRR):,,,,,,,,,,,,,,,
RE-GCN: ICE18 = 39.48,WIKI = 95.63,,,,,,,,,,,,,,
RE-GCN w. GT: ICE18 = 40.53,WIKI = 97.92,,,,,,,,,,,,,,
All models perform poorly on GDELT due to high abstraction and noise in entities.,,,,,,,,,,,,,,,
"Ablation studies confirm each component of RE-GCN contributes to performance gains.""","Focus mainly on entity and relation of a query, neglecting structural dependencies among all facts in the knowledge graph at each timestamp.",,,,,,,,,,,,,,
Low efficiency due to encoding history for each query individually.,,,,,,,,,,,,,,,
Ignore static properties of entities,such as entity types.,,,,,,,,,,,,,,
Existing methods only address entity prediction,not relation prediction simultaneously.,,,,,,,,,,,,,,
Poor performance on datasets (e.g.,GDELT) with many abstract concepts,making temporal reasoning difficult and introducing noise.,,,,,,,,,,,,,
Static information missing in some datasets (WIKI,YAGO,GDELT),"so results without static graph constraint are reported.""","RE-GCN effectively models both structural dependencies and sequential patterns in temporal knowledge graphs, improving entity and relation prediction.",,,,,,,,,,,
Incorporating static entity properties further enhances representation quality.,,,,,,,,,,,,,,,
RE-GCN outperforms state-of-the-art baselines on six benchmarks,achieving 17–82× speedup in entity prediction.,,,,,,,,,,,,,,
"Modeling historical information is vital for most datasets.""","Limited exploration of effective and efficient reasoning methods over Temporal Knowledge Graphs (TKGs), especially under the extrapolation setting.",,,,,,,,,,,,,,
Challenges in capturing both structural dependencies and sequential patterns in evolving KGs for accurate entity and relation prediction.,,,,,,,,,,,,,,,
"Need for better integration of static entity properties (like entity types) into temporal reasoning models.""",,,,"The objectives of the study are to propose RE-GCN, a novel GCN-based Recurrent Evolution network, to learn evolutional representations of entities and relations in temporal knowledge graphs for both entity prediction and relation prediction tasks, while modeling structural dependencies, sequential patterns, and static entity properties.",,,,,,,,,,,
"Making knowledge graphs work for smart manufacturing: Research topics, applications and prospects","Wan Yuwei, Liu Ying, Chen Zheyuan, Chen Chong, Li Xinyu, Hu Fu, Packianather Michael",2024,reference-manager,10.1016/j.jmsy.2024.07.009,,"Implementation Insights are summarized as follows: The paper uses keyword co-occurrence and clustering on 210 publications to identify key research areas, application scenarios, and trends for Knowledge Graphs (KGs) in Smart Manufacturing (SM). New insights include the integration of machine learning for improved diagnostics, enhanced explainability, and the synergy between KGs and advanced technologies like LLMs.",,,,"Keyword co-occurrence analysis: Examined keyword distribution across 210 publications to identify prevalent research topics, application scenarios, and trends.",,"What are the key research topics, applications, challenges, solutions, innovative methods, and emerging trends of knowledge graphs in smart manufacturing, and how do different knowledge graph techniques collectively advance the field?","This paper systematically reviews knowledge graphs (KGs) in smart manufacturing (SM), analyzing 210 publications using keyword co-occurrence and clustering. It identifies key research areas, applications, challenges, and emerging trends. The study highlights KGs’ roles in enhancing SM processes and offers a comprehensive framework for future research and development.","The research goal is to identify key research areas, applications, challenges, innovative methods, and trends of knowledge graphs (KGs) in smart manufacturing (SM) using keyword co-occurrence and clustering analysis on 210 publications; the principal finding is the identification of six distinct research clusters in this field.",
Clustering analysis: Grouped keywords into clusters using a co-occurrence matrix to systematically reveal relationships among research areas.,,,,,,,,,,,,,,,
Systematic literature review: Applied exclusion,inclusion,"and retention criteria to select relevant articles from major databases.""",,"Six main research clusters were identified in 210 publications, with key topics including product lifecycle management, maintenance, digital twins, conceptual design, and interoperability.",,,,,,,,,,,
"The most frequent keywords were """"""""ontology"""""""" (14 occurrences","total link strength 33) and """"""""machine learning"""""""" (10 occurrences",total link strength 22).,,,,,,,,,,,,,
"No statistical significance (p-values) or quantitative conclusions were reported.""",Primary outcomes:,,,,,,,,,,,,,,
Six research clusters were identified using keyword co-occurrence and clustering analysis of 210 publications.,,,,,,,,,,,,,,,
Clusters represent domains such as product lifecycle management,engineering design,root cause analysis,cognitive systems,and production failure.,,,,,,,,,,,
"Top keywords include """"""""Ontology"""""""" (14 occurrences
Results:",link strength 33),"""""""""Machine learning"""""""" (10",22),"and """"""""Knowledge management"""""""" (5",,14).,,,,,,,,,
The clustering method grouped keywords by association strength,revealing prevalent research topics and trends in smart manufacturing (SM) knowledge graphs (KGs).,,,,,,,,,,,,,,
Measured effects/statistical values:,,,,,,,,,,,,,,,
"""Ontology"""""""" had the highest occurrence (14) and total link strength (33) among keywords.",,,,,,,,,,,,,,,
Initial search yielded 10,800 publications; after filtering,"267 articles remained; final selection included 59 publications.""",Search term limitations may have led to reduced synonyms and missed relevant publications.,,,,,,,,,,,,
Only English-language articles were included,potentially excluding valuable non-English research.,,,,,,,,,,,,,,
Exclusion of certain publication formats (books,technical reports,dissertations,theses).,,,,,,,,,,,,
"Timeframe limited to publications from 2000 to 2024 (up to 29/02/2024).""","Knowledge Graphs (KGs) play a critical role in addressing complex Smart Manufacturing (SM) challenges, enhancing resilience, efficiency, and intelligence.",,,,,,,,,,,,,,
Key challenges include integrating multi-modal data,incorporating KGs into existing systems,and developing suitable methods.,,,,,,,,,,,,,
"Recommendations highlight focusing on essential research topics and methodologies to make KGs functional in SM.""","Addressing challenges in knowledge graph (KG) construction, integration, and update within smart manufacturing (SM).",,,,,,,,,,,,,,
Enhancing interoperability and semantic unification of diverse manufacturing data using advanced KG techniques.,,,,,,,,,,,,,,,
Developing innovative methods to improve the performance,scalability,"and robustness of KGs in SM.""","Future research should address improving the performance, scalability, and robustness of knowledge graphs (KGs) in smart manufacturing (SM), explore integration with large language models (LLMs), multi-modal KGs, explainable AI, and develop advanced methods for KG reliability, accuracy, and interpretability in evolving manufacturing systems.","The study design is a systematic review. It uses keyword co-occurrence and clustering analysis on 210 selected publications, applies exclusion, inclusion, and retention criteria, and involves manual and snowball searches across multiple databases. The process is comprehensive, structured, and analytical.",,,,,,,,,,,"The objectives of the study are to identify key research areas and advancements in Knowledge Graphs (KGs) within Smart Manufacturing (SM), analyze their applications and impact, examine challenges and solutions, explore innovative methods to enhance KGs, and investigate emerging trends in KGs for SM."
Balancing privacy and performance in federated learning: A systematic literature review on methods and metrics,"Mohammadi Samaneh, Balador Ali, Sinaei Sima, Flammini Francesco",2024,reference-manager,10.1016/j.jpdc.2024.104918,,"Implementation Insights highlight the need to balance privacy, utility, and efficiency in Federated Learning (FL). Encryption and differential privacy are widely used but introduce computational overhead. New insights include adaptive, hybrid privacy methods and context-sensitive metrics, with advances like clustering-based DP and dynamic privacy budgets improving the privacy-utility trade-off.",,,,"Encryption techniques in Federated Learning (FL), such as Homomorphic Encryption (HE) and Secure Multi-Party Computation (SMPC), to protect data confidentiality.",,"What are the main categories of mechanisms for preserving privacy in federated learning, the primary methods and metrics used to assess privacy, and how can privacy requirements be balanced with other performance-related application requirements in federated learning?","The paper systematically reviews privacy-preserving mechanisms in federated learning (FL), focusing on encryption-based, perturbation-based, blockchain-based, and hybrid methods. Using a multi-phase screening and quality assessment, 260 papers were analyzed. Encryption techniques are widely used, especially in healthcare and finance, but present computational challenges. Balancing privacy and performance remains a key issue.","The research goal is to systematically review privacy-preserving mechanisms in federated learning (FL); the approach involves screening, quality assessment, and analysis of 260 papers; the principal finding is that encryption-based methods are prevalent, especially in sensitive sectors, but balancing privacy and computational overhead remains a key challenge.",
Systematic Literature Review (SLR) methodology,including defined research protocol,screening,and quality assessment of selected papers.,,,,,,,,,,,,
"Use of quantitative evaluation and quality criteria (citation rate
18% of methods were encryption-based","methodology relevance
29% perturbation-based","logical findings) to assess included studies.""
16% blockchain-based",and 37% hybrid.,"260 papers from 2019–2023 met quality criteria and were analyzed, with quality scores ranging from 1.00 to 5.00.",,,,,,,,,,,
"No explicit p-values or statistical significance data are provided in the context.""",260 papers were selected for analysis after screening and quality assessment.,,,,,,,,,,,,,,
Quality was measured using three criteria: citation rate (QC1),methodology contribution (QC2),and clarity of findings (QC3),each scored equally; overall scores ranged from 1.00 to 5.00.,,,,,,,,,,,,
"No specific statistical values or measured effects are provided in the context.""","Encryption techniques (HE, SMPC) increase computational load, training time, latency, and communication overhead.",,,,,,,,,,,,,,
Blockchain methods introduce scalability,computational,and communication overheads.,,,,,,,,,,,,,
Selected papers may not fully represent the state-of-the-art due to keyword limitations.,,,,,,,,,,,,,,,
Potential bias in study selection and categorization.,,,,,,,,,,,,,,,
"Construct validity may be affected by search string design.""","Encryption techniques like HE (Homomorphic Encryption) and SMPC (Secure Multi-Party Computation) are widely used in FL (Federated Learning) for data confidentiality, especially in healthcare and finance.",,,,,,,,,,,,,,
These methods increase computational and communication overhead; model compression and efficient algorithms are recommended to address this.,,,,,,,,,,,,,,,
There is a need for adaptive,context-sensitive privacy metrics and hybrid privacy-preserving methods in FL.,,,,,,,,,,,,,,
Ongoing research should focus on balancing privacy,utility,"and system efficiency in real-world FL applications.""","Need for comprehensive, context-sensitive metrics tailored to diverse application scenarios and data sources in federated learning (FL).",,,,,,,,,,,,
Development of adaptive and hybrid privacy-preservation methods that balance privacy,utility,and system overhead.,,,,,,,,,,,,,
Creation of benchmarks reflecting real-world trade-offs between privacy,utility,"and computational/communication overhead in FL implementations.""","Future research should develop adaptive and combined privacy metrics, explore hybrid privacy-preservation methods, and establish benchmarks reflecting the balance between privacy, utility, and overhead in real-world federated learning. There is also a need for efficient encryption algorithms for edge devices and further study on balancing privacy with computational overhead.","Systematic Literature Review (SLR); multi-database search (ACM Digital Library, IEEE Xplorer, Scopus, ScienceDirect); defined research protocol; inclusion and exclusion criteria; quality assessment; data extraction sheets; screening by title, abstract, and full-text; reporting; time range: 2019–2023.",,,,,,,,,,,The objectives of the study are to: (1) provide a comprehensive overview of current categories of privacy-preserving mechanisms in federated learning (FL); (2) collect and analyze different privacy assessment approaches in FL; and (3) examine how privacy requirements can be balanced with other performance-related application requirements in FL.
Knowledge graph-based data integration system for digital twins of built assets,"Ramonell Carlos, Chacón Rolando, Posada Héctor",2023,reference-manager,10.1016/j.autcon.2023.105109,,"The implementation centers on a dynamic knowledge graph integrating BIM, IoT, and process data for digital twins. It uses ontologies for semantic linking, supports varied asset types, and employs a microservice architecture. New insight: unified semantic integration enables adaptable, asset-agnostic digital twin solutions for diverse stakeholders and requirements.",,,,Central knowledge graph: A data structure that organizes and connects information about built assets for unified access and analysis.,,"How can a universally applicable digital twin system be developed to seamlessly integrate virtual models of built assets with diverse data sources, enabling unified utilization and decision-making across varied infrastructure typologies, stages, and stakeholder requirements?","The paper aims to develop a system that integrates virtual models of built assets with diverse data sources using a central knowledge graph, semantic data integration, and a microservice architecture. Tested on ten varied infrastructure assets, the system enables comprehensive digital twins, improving data-driven decision-making in the built environment.","The research goal is to create a system that unifies virtual models of built assets with diverse data sources using a central knowledge graph, semantic integration, and microservice architecture; the approach was validated on ten varied infrastructure assets, demonstrating universal applicability for digital twin solutions.",
Semantic data integration and exploration: Techniques to combine and interpret diverse data sources meaningfully.,,,,,,,,,,,,,,,
Microservice architecture: A system design approach using independent,modular services,"aligned with cloud-based practices.""",,,,The primary outcome is the presentation of a system that combines virtual models of built assets with various data sources for unified utilization.,,,,,,,,,
The system includes a central knowledge graph,semantic data integration and exploration mechanisms,and a microservice architecture.,,,,,,,,,,,,,
"No statistical values or measured effects are reported.""",The data used in the study is confidential.,,,,,,,,,,,,,,
The system is tested only on ten infrastructure assets,which may limit generalizability.,,,,,,,,,,,,,,
Further development is needed for authentication management,data security,and improved interaction workflows.,,,,,,,,,,,,,
The research is ongoing,"with technologies still under development.""","The study demonstrates a system that integrates BIM, IoT, and other data sources using a central knowledge graph and microservice architecture.",,,,,,,,,,,,,
This approach enables unified,adaptable digital twin solutions for diverse built assets and stakeholders.,,,,,,,,,,,,,,
Future work should focus on authentication,data security,"and improving user interaction workflows.""","Creating a universally applicable digital twin system for diverse asset types, stages, and use cases remains a challenge.",,,,,,,,,,,,
Further development is needed in authentication management,data security,and improving interaction workflows with the system.,,,,,,,,,,,,,
Integration of BIM with IoT systems and other data silos,regardless of information type,"requires additional research.""",,,,,,,,,,,,,"The primary objective is to present a system that seamlessly combines virtual models of built assets with various data sources, enabling unified utilization. The goal is to create a universally applicable digital twin system using a central knowledge graph, semantic data integration, and a microservice architecture."
Building and Using Personal Knowledge Graph to Improve Suicidal Ideation Detection on Social Media,"Cao Lei, Zhang Huijun, Feng Ling",2020,reference-manager,,,"Implementation Insights highlight that lower data noise leads to higher detection performance. The knowledge graph (KG)-based method consistently outperforms others, achieving up to 94.53% accuracy. Key properties in information gain are crucial for risk analysis. Limitations include insufficient personal data and challenges with noisy, sparse social media data.",,,,Ablation study: Compared model performance with and without the user knowledge graph to assess its impact.,,"How can a structured personal suicide-oriented knowledge graph, unified with deep neural networks and attention mechanisms, improve the accuracy of suicidal ideation detection on social media by identifying and weighting key personal risk factors?","The study aims to detect suicidal ideation using social media data, achieving over 93% accuracy and F1-measure. It identifies posts, personality, and experience as key indicators, with posted texts and stress levels most significant. Limitations include insufficient data and noise. The research suggests further exploration beyond social media.","The research goal is to improve suicidal ideation detection by integrating a personal suicide-oriented knowledge graph; the approach uses a KG-based method combining multiple data modalities; results show the KG-based method significantly outperforms others, with up to 7.58% higher accuracy and F1-measure compared to methods without the knowledge graph.",
Multi-modal KG-based method: Integrated text,image,social graph,and knowledge graph data using a two-layered attention mechanism.,,,,,,,,,,,,
Comparative evaluation: Benchmarked against CNN,LSTM+Attention,SDM,and Text+History+Graph methods using accuracy,precision,,recall,,,,,,,"Removing the top-x key properties (x=1,3,5,10) in information gain led to consistent declines in accuracy and F1-measure, confirming the importance of these features in suicidal risk analysis.",,"and F1-measure."""
The KG-based method achieved the highest accuracy (up to 95.45%) and F1 (up to 94.31%) across datasets,outperforming other methods.,,,,,,,,,,,,,,
"Detection performance improved as data noise decreased; higher proportions of anti-real posts or few posts reduced model effectiveness. No explicit p-values reported.""","Removing the top-x key properties (x=1,3,5,10) in information gain led to declining accuracy and F1-measure, confirming their effectiveness in suicidal risk analysis.",,,,,,,,,,,,,,
KG-based method consistently outperformed other methods across datasets,with highest accuracy (up to 95.45%) and F1-measure (up to 94.31%).,,,,,,,,,,,,,,
"Data noise (anti-real posts
On Reddit dataset","few posts) reduced detection performance; less noise led to higher accuracy and F1-measure.
KG-based method achieved 65.92% accuracy and 65.93% macro-F1","outperforming other methods.""",Insufficiency of microblog data: Limited personal and personality-related factors included due to resource constraints.,,,,,,,,,,,,
Noise in microblog data: Users may provide false information or have few posts,affecting detection accuracy.,,,,,,,,,,,,,,
Data implicitness and sparsity: Users’ real feelings may be hidden or not directly expressed.,,,,,,,,,,,,,,,
Disparity between informal social media language and medical concepts.,,,,,,,,,,,,,,,
Reliance on social media limits analysis of upbringing and other personal factors.,,,,,,,,,,,,,,,
High training time for the KG-based method.,,,,,,,,,,,,,,,
"Few measurements of indicator contributions related to personal information.""","The KG-based (knowledge graph-based) method achieves over 93% accuracy and F1-measure in suicidal ideation detection, outperforming other methods.",,,,,,,,,,,,,,
Key indicators are posts,personality,and experience,with posted texts,stress level/duration,,images,,,,,,,,,and ruminant thinking most influential.
Data noise (fake or few posts) reduces detection performance.,,,,,,,,,,,,,,,
"Relying solely on social media data is limited; additional data sources are recommended.""","Limited incorporation of important personal factors (e.g., parenting rearing style) due to resource constraints; reliance on social media alone is insufficient.",,,,,,,,,,,,,,
Data noise from inaccurate or sparse user information on social media reduces detection performance.,,,,,,,,,,,,,,,
"Need for methods to assess and filter data quality before model training.""","Suggestions for future research include exploring additional data sources beyond social media to better capture personal factors like parental rearing style, addressing data noise and user identification challenges, and improving alignment between informal social media language and medical knowledge bases for more accurate suicidal ideation detection.",,,"The objectives of the study are to improve suicidal ideation detection accuracy by building a personal suicide-oriented knowledge graph using social media data, identify key indicators among personal factors, and evaluate the effectiveness of this approach through ablation studies and analysis of data noise impact.",,,,,,,,,,,
Causal feature selection using a knowledge graph combining structured knowledge from the biomedical literature and ontologies: a use case studying depression as a risk factor for Alzheimer's disease,"Malec Scott Alexander, Taneja Sanya B, Albert Steven M, Shaaban C. Elizabeth, Karim Helmet T, Levine Art S, Munro Paul Wesley, Callahan Tiffany J, Boyce Richard David",2022,reference-manager,10.1101/2022.07.18.500549,,"The implementation combines machine reading and human expertise to identify confounders for depression and Alzheimer’s disease (AD) using a knowledge graph (KG). Machine tools extract and infer relationships from literature, while humans assess variable relevance. The approach enables mechanistic explanations, but the confounder database is small and not comprehensive.",,,,"Machine reading systems (SemRep, EIDOS, REACH in the INDRA ecosystem) were used to extract structured triples from biomedical literature.",,How can a biomedical knowledge graph be used to identify and select confounders for estimating the total effect of depression on Alzheimer's disease risk from observational data?,"The study aimed to estimate the total effect of depression on Alzheimer’s disease risk using observational data. Using machine reading systems and knowledge graphs, the researchers identified confounders, highlighting the strengths of ontology-grounded resources and machine-human collaboration. Limitations included incomplete terminology mapping and constrained search depth.","The research goal was to identify confounding, colliding, and mediating variables between depression and Alzheimer’s disease using a knowledge graph approach, and the principal finding is that ontology-grounded knowledge graphs efficiently automate reasoning and generate mechanistic hypotheses, though further research is needed for definitive conclusions.",
Forward-chaining was applied to infer implied triples based on logical properties such as transitivity.,,,,,,,,,,,,,,,
"Ontology-grounded knowledge graphs (KGs) were constructed and used for causal feature selection and mechanistic explanation.""","The research is reproducible regarding data, as all supporting data are publicly available via the project's Zenodo repository (https://doi.org/10.5281/zenodo.6785307) with no restrictions. There is no information provided about the availability of source code for the project.","The study demonstrated that ontology-grounded Knowledge Graphs (KGs) can identify confounders linking depression and Alzheimer’s disease, validating some known confounders and suggesting new candidates.",,,,,,,,,,,,,
The KG approach was limited by search depth and terminology mapping,resulting in lower recall and information loss.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) were reported.""",The primary outcome was the identification of confounders for the relationship between depression and Alzheimer’s disease (AD) using a Knowledge Graph (KG) approach.,,,,,,,,,,,,,,
The KG search successfully identified confounders,including genes,enzymes,and drug exposures,but focused analysis on conditions or phenotypes.,,,,,,,,,,,
"No explicit statistical values or measured effects were reported.""",Self-reported data may not align with objective measures.,,,,,,,,,,,,,,
Non-transportability of causal relationships due to population differences.,,,,,,,,,,,,,,,
"Small literature corpus limits recall and representativeness.
Focused only on AD",possibly missing depression-related confounders.,,,,,,,,,,,,,,
Risk of latent and unmeasured confounding remains.,,,,,,,,,,,,,,,
Search depth threshold may have limited results.,,,,,,,,,,,,,,,
"Imperfect terminology mapping led to information loss.
Non-systematic",preliminary scoping strategy.,,,,,,,,,,,,,,
Potential for incomplete or inaccurate confounder identification.,,,,,,,,,,,,,,,
Possible bias from confounder misclassification,omitted variables,or overadjustment.,,,,,,,,,,,,,
Residual bias from machine reading and ontology inaccuracies.,,,,,,,,,,,,,,,
"Selection bias and measurement error bias possible.""",Knowledge graphs (KG) can help identify potential confounders and mechanistic links in studies of depression and Alzheimer’s disease (AD).,,,,,,,,,,,,,,
Ontology-grounded KGs improve accuracy but terminology mapping issues and limited search depth reduce recall.,,,,,,,,,,,,,,,
Combining machine and human strategies enhances variable identification and study rigor.,,,,,,,,,,,,,,,
"Future work should improve KG search and standardize reporting for better reproducibility.""","The search depth threshold may have limited recall, missing some relevant confounders, genes, enzymes, and drug exposures.",,,,,,,,,,,,,,
Imperfect terminology mapping led to information loss,with some concepts lost during translation between source and target terminologies.,,,,,,,,,,,,,,
"Future work is needed to enhance knowledge graph (KG) search for causal feature selection.""","Future research should enhance knowledge graph (KG) search for causal feature selection, improve terminology mapping, use larger and more representative literature corpora, standardize reporting of causal diagrams, and address latent confounding. Expanding computational tools and expert review can increase rigor and reproducibility in observational studies.","Observational study; non-randomized; may include retrospective analysis; systematic review of confounders; use of sensitivity analyses; reporting of causal directed acyclic graphs (DAGs); recommendation for standardized reporting (e.g., STROBE guidelines); manual collection of confounder data; focus on estimating causal effects from observational data.",,"The objectives of the study are to develop an analytical strategy for definitive conclusions in a specific case, improve the rigor of observational studies, identify potential confounders for depression and Alzheimer's disease, provide mechanistic hypotheses for causal relationships, and demonstrate combining semantic and causal inference using knowledge graphs.",,,,,,,,,,,
Knowledge-Based Version Incompatibility Detection for Deep Learning,"Zhao Zhongkai, Kou Bonan, Ibrahim Mohamed Yilmaz, Chen Muhao, Zhang Tianyi",2023,reference-manager,10.1145/3611643.3616364,,"Decide collects package and system library versions using pip or conda, depending on the environment. It detects and reports version issues within 1 minute, referencing relevant Stack Overflow posts. Decide outperforms PyEGo and Watchman in detecting version issues, achieving 92% precision and 65% recall. New insight: knowledge extraction from Stack Overflow is feasible and effective for version incompatibility detection.",,,,Manual identification and filtering of Stack Overflow posts using deep learning-related tags and linguistic patterns to extract version compatibility information.,,How can a pre-trained question-answering model be used to accurately extract and consolidate version compatibility knowledge from online discussions to detect version incompatibility issues in deep learning projects?,"The paper aims to extract and consolidate compatibility knowledge between deep learning (DL) components from Stack Overflow (SO) posts using a pre-trained QA model (UniﬁedQA). The approach achieves fine-grained extraction without finetuning, reaching 84.2% precision and 91.3% recall. Majority vote yields the best consolidation accuracy (95.1%).","The research goal is to detect and explain software version incompatibilities; the approach uses Decide, which extracts version info and consolidates (in)compatibility knowledge from Stack Overflow via a pre-trained QA model; results show Decide detects issues within 1 minute and achieves 95.1% accuracy with majority vote.",
DL stack component recognition by selecting paragraphs mentioning at least two different versioned components and expanding component lists with synonyms.,,,,,,,,,,,,,,,
Evaluation of knowledge consolidation strategies (majority vote,weighted majority vote,"vote by loss) for building a knowledge graph from extracted relations.""","The research is reproducible: the authors created a benchmark of 10 deep learning projects from GitHub, manually reproduced them, and documented version issues. Details and source code are referenced as """"artifact \[46]"""", but the actual source code link is not provided in the context.","The majority vote strategy achieved the highest accuracy (95.1%) for consolidating (in)compatibility relations, followed by vote by loss (93.6%) and weighted majority vote (91.6%), with small variance among them.",,,,,,,,,,,
The sample size of 228 relations is statistically significant at a 95% confidence interval.,,,,,,,,,,,,,,,
"Knowledge consolidation strategy does not significantly influence the overall knowledge extraction pipeline.""",Majority vote strategy achieved the highest accuracy: 95.1%.,,,,,,,,,,,,,,
Vote by loss accuracy: 93.6%.,,,,,,,,,,,,,,,
Weighted majority vote accuracy: 91.6%.,,,,,,,,,,,,,,,
Accuracy variance among strategies is small.,,,,,,,,,,,,,,,
"Data collection pipeline accuracy: 84.9% (Cohen’s Kappa: 0.85).
Knowledge graph: 1",431 nodes,3,124 edges.,,,,,,,,,,,,
Compatible relation example: Python 3.8 and Tensorflow 2.2,"confidence weight: 0.67.""","Only 10 DL projects were used in the benchmark, which may limit generalizability.",,,,,,,,,,,,,
Projects were selected based on having at least 100 stars and a requirements.txt file,possibly introducing selection bias.,,,,,,,,,,,,,,
Manual reproduction and issue documentation may introduce human error.,,,,,,,,,,,,,,,
"Evaluation focused on projects with at least one version issue.""","The proposed approach enables fine-grained extraction of version compatibility knowledge from Stack Overflow posts using a pre-trained QA model, achieving reasonable accuracy without finetuning.",,,,,,,,,,,,,,
"Majority vote is the most accurate knowledge consolidation strategy (95.1%)
The system","but all three tested strategies show similar accuracy.
Decide",efficiently detects and reports version issues in under 1 minute per project without GPU.,,,,,,,,,,,,,
"Knowledge consolidation strategy has minimal impact on overall extraction pipeline accuracy; question template design is more influential.""","Existing techniques cannot detect version issues involving drivers, OS, and hardware.",,,,,,,,,,,,,,
Current methods rely on limited dependency knowledge from PyPI and API documentation,missing undocumented constraints.,,,,,,,,,,,,,,
"There is a need for improved knowledge extraction from free-form Q\&A posts to capture comprehensive and up-to-date version issues.""","Future research should: (1) extend knowledge extraction beyond Stack Overflow to sources like PyPI and GitHub, redesigning consolidation to consider credibility and recency; (2) develop automatic repair strategies using the knowledge graph; (3) improve extraction accuracy by fine-tuning models or using stronger language models like ChatGPT.",,,"The objectives are to evaluate Decide’s effectiveness in detecting version compatibility issues in deep learning projects, assess the accuracy of its extracted knowledge, measure the QA model’s ability to infer compatibility relations, examine the impact of different question templates, and analyze the effect of knowledge consolidation strategies.",,,,,,,,,,,
Microservice extraction based on knowledge graph from monolithic applications,"Li Zhiding, Shang Chenqi, Wu Jianjie, Li Yuan",2022,reference-manager,10.1016/j.infsof.2022.106992,,"The paper introduces a knowledge-graph-based method for microservice extraction, using four entity types and relationships, and a constrained Louvain algorithm. It reduces reliance on human experience, improves team size reduction, cohesion, and coupling, but currently lacks field-level data splitting and comprehensive resource node extraction. Future work targets these limitations.",,,,"Automatic extraction of function nodes by analyzing control classes, identifying business logic and functional components.",,"How can a knowledge-graph-based method, guided by the AKF principle and using a constrained Louvain algorithm, effectively support microservice extraction from monolithic applications while addressing key quality metrics such as cohesion, coupling, code redundancy, and team size reduction?","The paper aims to propose and evaluate a novel microservice extraction method for migrating monolithic applications. Using entity analysis and similarity calculations, the study focuses on cohesion, coupling, and hardware resource considerations. Results show improved microservice quality, performance, and logical business grouping. The approach enhances scalability and resource allocation.","The research goal is to improve microservice extraction from monolithic systems using a knowledge-graph-based approach that considers hardware resources; the method employs a constrained Louvain community detection algorithm, and results show superior performance in team size reduction, cohesion, coupling, and code redundancy compared to three typical methods.",
Construction of a knowledge graph from specification and design artifacts,representing entities and their relationships.,,,,,,,,,,,,,,
"Application of a constrained Louvain community detection algorithm to identify microservice candidates from the knowledge graph.""",The research used two open-source projects: “E-commerce System” (by Macrozheng et al.) and “Cargo Tracking System” (available on SourceForge). No data was used for the research. Supplementary material is available online at doi:10.1016/j.infsof.2022.106992. Source code links are not provided.,"The proposed GD method achieved the highest cohesion (0.60), lowest coupling, and lowest code redundancy rate among four extraction methods, indicating better modularity and maintainability.",,,,,,,,,,,,,
Performance tests (TP50,TP80,TP99,TP999) showed similar results across methods; GD had TP50=6.92,TP80=14.33,,TP99=41.92,,,,,,,,,TP999=52.17.
GD method demonstrated weak reliance on human experience and well-defined software,"with applicability confirmed for both tested projects; statistical significance (p-values) not reported.""","The proposed method yields the lowest coupling value among four strategies, indicating better flexibility and adaptability.",,,,,,,,,,,,,
Experiments show improved performance in team size reduction,cohesion,coupling,and code redundancy rate.,,,,,,,,,,,,
Performance tests (E-comm project): TP50–TP999 for DD: 20.03–67.53; AS: 19.98–66.94; SC: 20.57–69.10; GD: 19.63–66.18.,,,,,,,,,,,,,,,
Performance tests (Cargo project): TP50–TP999 for DD: 6.48–49.54; AS: 6.98–54.42; SC: 6.89–51.67; GD: 6.92–52.17.,,,,,,,,,,,,,,,
The method has low dependency on human experience and considers hardware influence on microservices.,,,,,,,,,,,,,,,
All methods are applicable; DD and AS rely strongly on human experience and well-defined software,while SC and GD are weak in these dependencies.,,,,,,,,,,,,,,
"SC and GD consider hardware; DD and AS do not.""",Only a few parameters were considered in resource node extraction due to experimental environment limitations.,,,,,,,,,,,,,,
The method may be difficult to apply in practical engineering projects because of significant design limitations.,,,,,,,,,,,,,,,
Heavy reliance on human experience in some methods leads to high time costs.,,,,,,,,,,,,,,,
Many monolithic programs lack well-defined interfaces or codes,limiting applicability.,,,,,,,,,,,,,,
Current data splitting is at the table level; field-level splitting is suggested for more accuracy.,,,,,,,,,,,,,,,
More entity types (e.g.,developer and security entities) should be introduced for better system decomposition.,,,,,,,,,,,,,,
"No data was used for the research described in the article.""","The proposed knowledge-graph-based method (GD) achieves the highest cohesion (0.60) and lowest coupling among four extraction methods, indicating better modularity and flexibility.",,,,,,,,,,,,,,
GD reduces code redundancy rate and team size,improving performance and maintainability.,,,,,,,,,,,,,,
GD relies less on human experience and considers hardware resources.,,,,,,,,,,,,,,,
Future work should optimize resource node extraction,"especially for complex cloud scenarios.""","Introduce more entity types, including developer and security level entities, to better decompose large, complex systems.",,,,,,,,,,,,,
Refine data splitting granularity from the table level to the field level for more accurate extraction results.,,,,,,,,,,,,,,,
Further research and optimize the extraction method of resource nodes,considering more complex,"multi-level parameters.""","Future research should focus on optimizing the extraction method of resource nodes by considering more complex parameters, introducing more entity types (such as developer and security entities), and enabling data splitting at the field level for more accurate extraction results.","The study design is a case study using two software projects (“E-commerce System” and “Cargo Tracking System”) to validate the proposed algorithm. The approach involves constructing an undirected weighted graph from design documents, databases, and source code, and analyzing module extraction. No randomization, blinding, or control groups are mentioned.",,,,,,,,,,,"The objectives of the study are to propose and evaluate a novel knowledge-graph-based method for extracting microservices from monolithic applications, focusing on reducing team size, improving cohesion, lowering coupling and code redundancy, and considering the influence of hardware resources on microservice design."
Personal Health Knowledge Graphs for Patients,"Rastogi Nidhi, Zaki Mohammed J.",2020,reference-manager,,,"Implementation Insights highlight that most approaches use brute force to generate Personal Health Knowledge Graphs (PHKGs), often inferring patient preferences from general Knowledge Graphs (KGs). Challenges include static summaries, limited entity updates, device constraints, and lack of scalable, dynamic PHKG creation. New insight: hybrid patient-clinician collaboration is suggested.",,,,Literature review of Knowledge Graph (KG) approaches for extracting personal context from patient data.,,"How can Personal Health Knowledge Graphs (PHKGs) be effectively generated, represented, and integrated with existing knowledge bases to provide personalized health recommendations, while addressing challenges related to data heterogeneity, scalability, validation, and patient-specific requirements?","The paper reviews and critiques methods for extracting personal context from patient data using small-sized Personal Health Knowledge Graphs (PHKGs). It discusses methodologies like summarizing patient data, dynamic PHKG creation, and integrating heterogeneous sources. Key findings highlight challenges in scalability, representation, and validation. The study concludes that further research is needed to address these issues.","The paper's main objective is to review and critique methods for extracting personal context from patient data using small-sized Personalized Health Knowledge Graphs (PHKGs); it evaluates existing approaches and highlights challenges, concluding that further research is needed to improve dynamic, scalable, and accurate PHKG construction for health recommendations.",
Aggregation and integration of heterogeneous data sources (e.g.,environmental sensors,web-based data) into personal health knowledge graphs (PHKG).,,,,,,,,,,,,,
"Construction of personal knowledge bases using text-based life logs from social media platforms.""",No source code for the project is mentioned in the context. The research discusses methods and challenges for constructing personal health knowledge graphs but does not provide reproducibility details or code availability.,"PHKGs (Personal Health Knowledge Graphs) are small, structured graphs summarizing patient-specific data, but current approaches are static, limited in scope, and challenging to update or scale.",,,,,,,,,,,,,
Quantitative results and statistical significance (p-values) are not provided in the context.,,,,,,,,,,,,,,,
Major challenges include integrating heterogeneous data sources,defining relevant entities,ensuring scalability,"and validating PHKG effectiveness for personalized health recommendations.""","No primary outcomes, results, or measured effects (including statistical values) are explicitly stated in the provided context.",,"Brute force approaches do not address creating relationships, identifying accurate classes, or removing irrelevant entities.",,,,,,,,,
Lack of standard model for representing PHKG.,,,,,,,,,,,,,,,
Static personalized summaries with limited,pre-assigned entities are hard to update.,,,,,,,,,,,,,,
Device memory and computing resources may limit PHKG size and span.,,,,,,,,,,,,,,,
Sparse and unreliable data from social media for health information.,,,,,,,,,,,,,,,
"No patient-side recommendation feedback in some approaches.
Scalability",ingestion,and processing issues due to heterogeneous data sources.,,,,,,,,,,,,,
Not all patients know what information to share; privacy concerns.,,,,,,,,,,,,,,,
Need for validation mechanisms to ensure PHKG accuracy.,,,,,,,,,,,,,,,
"Further research needed to define scope and validate PHKG effectiveness.""","PHKGs (Personal Health Knowledge Graphs) offer personalized health recommendations but face challenges in scalability, dynamic updating, and integration with general KGs.",,,,,,,,,,,,,,
Collaboration between patients and healthcare professionals is recommended for effective PHKG construction.,,,,,,,,,,,,,,,
Device limitations and data heterogeneity present significant challenges.,,,,,,,,,,,,,,,
"No standard model exists for PHKG representation; further research is needed.""","Lack of standard models for representing and scaling PHKGs, including graph structure and dynamic creation.",,,,,,,,,,,,,,
Challenges in integrating heterogeneous,personal,and daily life data while ensuring validation and scalability.,,,,,,,,,,,,,
Need for collaborative,"hybrid approaches between patients and healthcare professionals to define PHKG content and privacy.""","Future research should address: dynamic creation and updating of PHKGs, accurate relationship and class identification, removal of irrelevant entities, scalability, graph structure modeling, validation mechanisms, memory and computation requirements, integration with heterogeneous data sources, and hybrid approaches involving both patients and healthcare professionals.",,,,,,,,,,,,,
Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion,"Baek Jinheon, Chandrasekaran Nirupama, Cucerzan Silviu, Herring Allen, Jauhar Sujay Kumar",2024,reference-manager,10.1145/3589334.3645404,,"K-LaMP consistently outperforms baselines in Relatedness, Usefulness, and Ranking, especially as interaction history grows. Its entity-centric knowledge store enables richer personalization than linear histories. Using only “unfamiliar” entities boosts Relatedness and Usefulness. Automatic evaluation metrics correlate well with human judgments. Retrieval relevance is higher for entity-centric stores.",,,,Query Suggestion: Uses the current query and historical queries from the same session to suggest the next query.,,No information available,"The paper introduces a new approach for contextual query suggestion, aiming to generate personalized search queries by considering the user's current search context and personal interests. Using human evaluation, the study finds that traditional metrics are insufficient, highlighting the need for richer context and personalization in query suggestion systems.","The paper’s main objective is to improve personalized query suggestion by leveraging user-specific knowledge; its key method is the K-LaMP framework, which augments large language models with personal entity-centric knowledge; principal finding is K-LaMP significantly outperforms baselines in relatedness, usefulness, and ranking metrics.",
Contextual Query Suggestion: Adds information from the web page clicked as a result of the current query to improve suggestions.,,,,,,,,,,,,,,,
"Contextual Query Suggestion w/ K𝐴: Incorporates related articles previously read by the user for more personalized suggestions.""",,"The K-LaMP framework significantly outperforms all baselines on Relatedness, Usefulness, and Ranking metrics; it ties with Contextual Query Suggestion on Validity.",,,,,,,,,,,,,
Retrieval Relevance scores show entity-centric knowledge stores provide higher quality context than linear search history-based stores.,,,,,,,,,,,,,,,
"No explicit p-values or quantitative statistical significance values are provided.""","The K-LaMP framework significantly outperforms all baselines across Relatedness, Usefulness, and Ranking metrics.",,,,,,,,,,,,,,
On the Validity metric,K-LaMP ties with Contextual Query Suggestion.,,,,,,,,,,,,,,
Main results (Table 1):,,,,,,,,,,,,,,,
Query Suggestion: Validity 1.769,Relatedness 0.962,Usefulness 0.948,Ranking 2.736,,,,,,,,,,,,
Contextual Query Suggestion: Validity 1.966,Relatedness 1.267,Usefulness 1.245,Ranking 2.415,,,,,,,,,,,,
Contextual Query Suggestion w/ K𝐴: Validity 1.822,Relatedness 1.192,Usefulness 1.166,Ranking 2.654,,,,,,,,,,,,
"Retrieval Relevance is higher for the entity-centric knowledge store than for the linear search history-based store.""",Personalizing LLMs for individual users is challenging and expensive.,,,,,,,,,,,,,,
Retrieval-based personalization may provide irrelevant information,leading to poor results.,,,,,,,,,,,,,,
Profile-based personalization raises privacy and scalability concerns.,,,,,,,,,,,,,,,
Validity of query suggestions is unaffected by personal context or interaction history length.,,,,,,,,,,,,,,,
"Human evaluation may not capture all user goals.""","The K-LaMP framework consistently and significantly outperforms all baselines in Relatedness, Usefulness, and Ranking metrics.",,,,,,,,,,,,,,
Entity-centric knowledge stores provide higher retrieval relevance than linear search history-based stores.,,,,,,,,,,,,,,,
Automatic evaluation metrics correlate moderately with human judgment,supporting their use for scalable evaluation.,,,,,,,,,,,,,,
"Recommendation: Use entity-centric knowledge stores and K-LaMP for improved contextual query suggestions.""",No information available,,No information available,,,,,,,,,,,,
Uncovering Values: Detecting Latent Moral Content from Natural Language with Explainable and Non-Trained Methods,"Asprino Luigi, De Giorgis Stefano, Gangemi Aldo, Bulla Luana, Marinucci Ludovica, Mongiovì Misael",2022,reference-manager,,,"The paper presents two novel, non-trained methods—zero-shot learning and a frame-based unsupervised approach—to detect latent moral content in text using Moral Foundation Theory. Key insights include enhanced explainability, domain independence, and future plans to refine models for better handling complex moral values and sentence structures.",,,,"Unsupervised frame-based approach: Uses knowledge graphs for exploring latent moral and semantic content, including disambiguation of lexical units, frame evocation, and knowledge integration.",,"How can unsupervised, frame-based methods leveraging knowledge graphs and Moral Foundation Theory be used to transparently and effectively detect latent moral values in natural language, particularly in Twitter data, without relying on predefined term sets or supervised training?","The paper aims to detect latent moral content in natural language using two domain-independent, unsupervised methods: a frame-based approach using knowledge graphs and a zero-shot model based on Natural Language Inference. Tested on the Moral Foundation Twitter Corpus, both methods achieved strong results without prior training, supporting explainability and versatility.","The research goal is to detect latent moral values in natural language using a frame-based symbolic value detector and a zero-shot learning approach; both unsupervised, domain-independent methods achieve notable results on the Moral Foundation Twitter Corpus without prior training, demonstrating explainability and versatility.",
ValueNet2 integration: Adds a layer formalizing moral and cultural values,aligning them to frames with a foundational ontology backbone.,,,,,,,,,,,,,,
"Evaluation on Twitter dataset: Methods are tested on a benchmark dataset of tweets to assess effectiveness.""",The research uses the Moral Foundation Twitter Corpus and proposes two approaches (zero-shot and heuristic) that do not require training. Source code is referenced at https://github.com/StenDoipanni/MoralDilemmas and http://wit.istc.cnr.it/stlab-tools/fred/demo/. No further reproducibility details are provided.,"The proposed unsupervised, domain-independent, and explainable frame-based approach was evaluated on 6,075 tweets from the Moral Foundation Twitter Corpus, achieving an overall F1 score of 0.44.",,,,,,,,,,,,,
The new architectures improved performance by 10% over the Emotion-zero-shot model and 20% over the Zero-shot model.,,,,,,,,,,,,,,,
"No p-values or statistical significance measures are reported.""","The frame-based approach was evaluated on 6,075 items from the MFTC test set.",,,,,,,,,,,,,,
Overall F1 scores: Random (.11),Zero-shot (.40),Emotion-Zero-shot (.42),Emotion-Zero-shot+ (.41),Frame-based (.39).,,,,,,,,,,,
Frame-based method improved F1 by 10% over Emotion-zero-shot and 20% over Zero-shot models.,,,,,,,,,,,,,,,
"Subversion label had the highest F1 for Frame-based (.65).""","Only Haidt’s Moral Foundation Theory is considered, limiting generalizability to other moral frameworks.",,,,,,,,,,,,,,
The Liberty/Oppression dyad is not included in evaluation metrics due to its absence in the main dataset.,,,,,,,,,,,,,,,
Dataset modifications: “non-moral” labels co-occurring with moral values were removed,potentially affecting results.,,,,,,,,,,,,,,
Zero-shot model performance needs improvement for complex moral values.,,,,,,,,,,,,,,,
Frame-based detector could benefit from more refined heuristics and syntactic analysis.,,,,,,,,,,,,,,,
"Further research is needed to better detect prevailing moral values and handle sentence complexity.""","The study introduces two versatile, transparent, and domain-independent approaches (zero-shot and heuristic) for detecting latent moral content in language without training.",,,,,,,,,,,,,,
Results are unprecedented for domain-independent methods,evaluated on the Moral Foundation Twitter Corpus.,,,,,,,,,,,,,,
Future work aims to improve zero-shot model performance and refine the frame-based detector for greater precision.,,,,,,,,,,,,,,,
"Recommendations include enhancing sentence analysis to better detect prevailing moral values and incorporating syntax-based heuristics for more complex inferences.""",Improve Zero-shot model performance to better understand complex and contentious moral values.,,,,,,,,,,,,,,
Enhance the frame-based value detector with syntactic heuristics and frame structure analysis for more complex inferences.,,,,,,,,,,,,,,,
"Develop methods to assign greater weight to significant sentence aspects to more accurately detect prevailing moral values.""","Future research should focus on improving Zero-shot model performance to better understand complex moral values, developing methods that emphasize key sentence aspects, and refining frame-based detectors with syntax-based heuristics for more precise and complex inferences.","Unsupervised, domain-independent, explainable, frame-based approach; evaluated on benchmark Twitter datasets; compared to supervised methods; uses knowledge graphs; no external framing annotations; alternative to Frame Axis technique; focuses on Haidt’s Moral Foundation Theory; not randomized, controlled, blinded, or placebo-controlled.",,"The objectives are to detect latent moral content in natural language using two approaches (zero-shot and heuristic) that do not require training, based on Haidt’s Moral Foundation Theory, and to evaluate these methods on the Moral Foundation Twitter Corpus for effectiveness in moral value detection.",,,,,,,,,,,
Connecting the Dots of Knowledge in Agile Software Development,"Ouriques Raquel, Gorschek Tony, Mendez Daniel, Fagerholm Fabian",2021,reference-manager,,,"The paper highlights that effective implementation relies on balancing technical, creative, collaborative, and integrative skills. Key challenges include terminology misunderstandings, content overload, and trust in boundary artefacts. Solutions involve distributed ownership, structured content processes, ICT use, and regular feedback to align artefacts with users’ needs and improve knowledge transfer.",,,,Case study: Investigated the causes and effects of trust in boundary artefacts.,,"How can software companies effectively manage and optimize property-based resources (PBRs) and boundary artefacts to address knowledge needs, trust, ownership, and usability challenges in agile development environments?","The paper investigates how knowledge-based resources (KBRs) support adaptation to change in agile software contexts. Using a case study and grounded theory, it finds that structured processes for creating and connecting property-based resources (PBRs) are essential. Effective knowledge management prevents overload, confusion, and inefficiency, supporting agile transformation.","The research goal was to understand how knowledge-based resources (KBRs) support property-based resources (PBRs) and boundary artefacts in agile software contexts; using a case study and grounded theory, the principal finding is that structured practices and awareness of user needs improve the effectiveness of knowledge artefacts.",
Grounded theory study: Identified knowledge-based resources (KBRs) and explained their support for changes in agile contexts.,,,,,,,,,,,,,,,
"Systematic literature review: Examined knowledge management strategies and processes in agile software development.""",No information available,"The studies found that knowledge-based resources (KBRs) are essential for adapting to change in agile contexts, supporting innovation and performance through technical, creative, integrative, and collaborative skills.",,,,,,,,,,,,,
Lack of structured processes for capturing and storing knowledge leads to information overload,inefficient searches,and frustration among co-workers.,,,,,,,,,,,,,
"Implementing structured procedures for creating and maintaining property-based resources (PBRs) improves usability and applicability; no statistical significance (p-values) reported.""",Two main studies: a case study on trust in boundary artefacts and a grounded theory study on KBRs (Knowledge-Based Resources) supporting agile changes.,,,,,,,,,,,,,,
Primary outcomes: Identified challenges in producing and using PBRs (Property-Based Resources),including content overload,poor documentation,and ineffective ICT use.,,,,,,,,,,,,
Measured effects: Lack of structured processes leads to confusion,inefficient searches,and frustration among co-workers.,,,,,,,,,,,,,
"No explicit statistical values reported.""
Lack of control","Misunderstandings due to terminology can cause misinterpretation and misuse of artefacts.
ownership issues",and disconnect between users and creators affect artefact effectiveness.,,,,,,,,,,,,,
Inaccurate or unavailable boundary artefacts reduce trust and may lead to workarounds and duplicated information.,,,,,,,,,,,,,,,
Identifying co-workers’ knowledge needs is time-consuming.,,,,,,,,,,,,,,,
"Poor and isolated documentation can overload tools and confuse users.""","Structured processes for creating and maintaining knowledge artifacts (PBRs and boundary artefacts) are essential to avoid information overload, confusion, and inefficiency.",,,,,,,,,,,,,,
Identifying and addressing co-workers’ knowledge needs improves the relevance and usability of artifacts.,,,,,,,,,,,,,,,
Distributed ownership and periodic feedback enhance artifact quality and adaptability in agile environments.,,,,,,,,,,,,,,,
"Effective management of knowledge resources (KBRs) supports agile teams in adapting to market and internal changes.""","Lack of structured processes for adding and maintaining content in artefacts, leading to content overload or insufficient knowledge.",,,,,,,,,,,,,,
Poor and isolated documentation due to absence of formal practices,causing confusion and difficulty in finding relevant content.,,,,,,,,,,,,,,
Challenges in connecting knowledge sources to people’s needs,"especially with varying artefact formats and terminology differences.""","Future research should investigate how to effectively identify and address co-workers’ knowledge needs, improve the structure and content of boundary artefacts, manage terminology misunderstandings, and enhance ownership, accuracy, and control in agile environments. Further exploration of ICTs’ role in connecting knowledge sources is also suggested.",,,,,,,,,,,,,
Privacy-Preserving Synthetically Augmented Knowledge Graphs with Semantic Utility,"Bellomarini Luigi, Catalano Costanza, Coletta Andrea, Iezzi Michela, Samarati Pierangela",2024,reference-manager,,,"The implementation uses the Vadalog System for logic reasoning over knowledge graphs. KLONE and KGUARD anonymize graphs using (k,x)-isomorphism, maximizing privacy (δ-anonymity = 1.0) and utility. Experiments, implemented in Python with NetworkX, show improved privacy over k-Iso, especially when considering derived links.",,,,"(k,x)-isomorphism anonymization: A structural anonymization method ensuring each subgraph of x vertices has k indistinguishable, pairwise-disjoint counterparts with diverse sensitive attributes.",,"How can knowledge graphs be anonymized to prevent re-identification attacks—especially those exploiting derived knowledge—while preserving business semantics, data utility, and diversity of sensitive attributes?","This paper introduces a new structural anonymization method for knowledge graphs (KGs) called (k,x)-isomorphism anonymization, aiming to prevent re-identification while preserving data utility. Two algorithms, KLONE and KGUARD, are proposed and evaluated, showing effective privacy protection with controlled utility loss across real-world datasets.","The research goal is to prevent re-identification in knowledge graphs (KGs) while preserving utility; the approach introduces (k,x)-isomorphism anonymization and two algorithms (KLONE, KGUARD); results show effective privacy protection and utility retention in synthetic KGs, validated on real-world financial datasets.",
Semantic utility metric: Uses a Jaccard-based similarity index to maximize the usefulness of synthetic knowledge graphs for specific tasks.,,,,,,,,,,,,,,,
KLONE and KGUARD algorithms: Two anonymization algorithms applying (k,"x)-isomorphism while optimizing semantic utility.""",The research is implemented in Python using NetworkX. Experiments were run on an AMD EPYC-7763v system with 64GB RAM. There is no explicit mention of the availability of the source code for the project.,,,,,,,,,,,,,
"No information available regarding public access to the source code.""","KLONE and KGUARD achieve perfect δ-anonymity (1.0) across all tested graphs, outperforming k-Iso, which ranges from 0.605 (Bitcoin Alpha) to 1.0 (Infect-Dublin).",,,,,,,,,,,,,,
Both KLONE and KGUARD maintain utility U ≈ 0.000–0.010,indicating no loss of information compared to original graphs.,,,,,,,,,,,,,,
"No p-values or explicit statistical significance values are reported.""","Both KLONE and KGUARD achieve perfect utility U (close to 0) on all real-world graphs, indicating no loss of information.",,,,,,,,,,,,,,
KGUARD consistently shows lower Nodes overhead (%) and smaller Wasserstein distances (Degree ↓,Weights ↓) than KLONE.,,,,,,,,,,,,,,
δ-anonymity = 1.0 for KLONE and KGUARD; state-of-the-art k-Iso achieves lower δ-anonymity (as low as 0.605).,,,,,,,,,,,,,,,
KGUARD generally outperforms KLONE in utility,fidelity,"and privacy protection.""",Simple de-identification is not foolproof; KGs remain exposed to background knowledge attacks.,,,,,,,,,,,,
Existing structural anonymization methods do not consider derived knowledge or preserve business semantics.,,,,,,,,,,,,,,,
Differential privacy struggles with highly correlated network data,often degrading data utility.,,,,,,,,,,,,,,
Few anonymization solutions exist specifically for KGs,"especially regarding sequential publishing and node attributes.""","The proposed (k,x)-isomorphism anonymization effectively protects against re-identification attacks, achieving δ-anonymity = 1.0 across all tested datasets.",,,,,,,,,,,,,
KGUARD outperforms KLONE in minimizing redundant structures and node overhead while maintaining high utility.,,,,,,,,,,,,,,,
Both algorithms preserve utility for downstream tasks,with negligible information loss.,,,,,,,,,,,,,,
"Recommendation: Use KGUARD for better efficiency and lower overhead in real-world applications.""","Existing anonymization methods for knowledge graphs (KGs) often neglect derived knowledge, leading to privacy leaks.",,,,,,,,,,,,,,
Differential privacy struggles with highly correlated graph data,as added noise may not sufficiently protect privacy or may harm data utility.,,,,,,,,,,,,,,
"Few anonymization solutions specifically address sequential publishing and node attribute privacy in KGs.""",,,,"The objectives of the study are to: (R.1) protect entities from re-identification attacks that exploit reasoning; (R.2) preserve the knowledge encoded in the knowledge graph for downstream business tasks; and (R.3) ensure diversity for sensitive attributes, even in the presence of derived edges.",,,,,,,,,,,
Prior knowledge-guided multilevel graph neural network for tumor risk prediction and interpretation via multi-omics data integration,"Yan Hongxi, Weng Dawei, Li Dongguo, Gu Yu, Ma Wenji, Liu Qingjie",2024,reference-manager,10.1093/bib/bbae184,,The implementation insights show that pathway aggregation and graph neural network modules improve prediction accuracy and stability over standard neural networks and other dimensionality reduction methods. The model performs well across various time divisions and clinical data. The explanation methods effectively identify gene pathways strongly linked to patient risk.,,,,Ablation experiments were conducted to assess the effectiveness of different model components and dimensionality reduction methods.,,"How can a Multilevel Graph Neural Network that integrates multi-omics data, gene regulatory networks, and pathway information improve prediction performance and interpretability in cancer research compared to existing methods?","The study aims to improve cancer risk prediction using multi-omics data by introducing a pathway aggregation module with a graph neural network. Through ablation studies and comparisons with other methods, the proposed approach achieved superior AUC and stability. Key genes and pathways identified correlate strongly with patient risk, supporting the model’s interpretability.","The research goal is tumor risk prediction and interpretation using multi-omics data; the approach integrates prior knowledge-guided multilevel graph neural networks, and the principal finding is that this method outperforms others in AUC and identifies key genes and pathways strongly correlated with patient risk.",
A multilevel graph neural network (GNN) was used,incorporating regulatory network information across multiple omics datasets.,,,,,,,,,,,,,,
"Grad-CAM was employed to explain prediction results and identify important gene pathways.""",The research is reproducible. The source code is available at https://github.com/Y-Claw/Multilevel-GNN. All data are from public databases and can be accessed via public databases or the GitHub project. No further reproducibility limitations are mentioned in the context.,"The proposed method achieved superior AUC values across all datasets (e.g., LGG: 0.885±0.006) compared to other dimensionality reduction methods and neural network baselines.",,,,,,,,,,,,,
Key genes and pathways identified showed statistically significant associations with survival (e.g.,MAPK signaling pathway log-rank test p-values: 1.606e-11,2.204e-07).,,,,,,,,,,,,,
"The model’s predictions effectively stratified patients into high- and low-risk groups with significant survival differences (Kaplan–Meier log-rank test for LGG: 2.34e-13).""",Key genes were identified using a GNN-Explainer-based method.,,,,,,,,,,,,,,
The mean importance score across all samples was used for each gene.,,,,,,,,,,,,,,,
Top three key genes per omics with adjusted P-values < 0.05 were selected.,,,,,,,,,,,,,,,
Patients were split into high and low importance score groups for survival analysis.,,,,,,,,,,,,,,,
Survival analysis showed significant differences between groups (log-rank test values provided,e.g.,4.573e-05 for GBM,"2.34e-13 for LGG).""",No explicit limitations or shortcomings are mentioned in the provided context.,,,,,,,,,,,
"No self-reported problems or suggestions for further research are stated.
,The proposed method outperforms other dimensionality reduction and aggregation approaches in prediction accuracy and stability across cancer datasets.",,,,,,,,,,,,,,,
Key genes and pathways identified by the model show strong correlations with patient risk and survival,validated by survival analysis.,,,,,,,,,,,,,,
The model’s explanation methods effectively highlight biologically relevant genes and pathways.,,,,,,,,,,,,,,,
"Recommendation: Use the model for multi-omics data integration and interpretation in cancer research.""",Limited exploration of different time divisions and their impact on model performance; further research is needed.,,,,,,,,,,,,,,
The integration of additional clinical information into the model requires more systematic investigation.,,,,,,,,,,,,,,,
"Comparative evaluation with other dimensionality reduction methods suggests further optimization and exploration are necessary.""",,,,"The objectives of the study are to identify key genes and pathways using a GNN-Explainer-based interpretation method, conduct survival analysis on these genes and pathways, and demonstrate the effectiveness of the model’s explanation methods in correlating crucial genes and pathways with patient risk.",,,,,,,,,,,
TFSF: Topological and Feature Space Fusion with Spatio-Temporal Modeling for Crop Yield Prediction,"Xu Shifeng, Zhou Yijing, Yu Xiaoyang, Huang Cuiting, Wu Chao",2023,reference-manager,10.1109/cscwd57460.2023.10152724,,"The TFSF-GNN model outperforms all baseline methods in soybean yield prediction, achieving the lowest MAE, MAPE, and RMSE across 2015-2017. Ablation studies show both topological and feature space fusion are crucial for performance. The model effectively leverages spatio-temporal data and multi-channel graph convolution for improved accuracy.",,,,"Preprocessing: Climate, soil quality, and management data are processed to form feature vectors for each county and year.",,"How can spatio-temporal modeling using a multi-channel graph convolutional neural network (TFSF-GNN) effectively extract and fuse correlated information from climate, soil, and management data to improve crop yield prediction accuracy across different regions?","The paper aims to improve crop yield prediction by proposing the TFSF-GNN model, which fuses temporal and spatial information using multiple graph convolutional networks with attention mechanisms. Experiments on soybean data (1980–2017) show TFSF-GNN outperforms traditional and deep learning models, achieving higher accuracy and better generalization.","The research goal is crop yield prediction; the approach is the TFSF-GNN model that fuses temporal, topological, and feature space information using multi-channel graph convolution and attention; results show TFSF-GNN achieves higher accuracy and better generalization than statistical and deep learning baselines on real soybean yield data.",
Temporal Feature Extraction: Convolutional Neural Networks (CNNs) extract low-level features,followed by Gated Recurrent Units (GRUs) to capture growth trends over multiple years.,,,,,,,,,,,,,,
"Multi-channel Graph Convolution: Graph Neural Networks (GNNs) model both topological (county adjacency) and feature space relationships to improve prediction accuracy.""",,"The proposed TFSF-GNN model achieved the lowest errors (2015-2017 mean MAE: 2.906, MAPE: 5.682%, RMSE: 3.701), outperforming all statistical and deep learning baselines.",,,,,,,,,,,,,
Ablation studies show each component of TFSF-GNN contributes to improved accuracy,with the full model performing best.,,,,,,,,,,,,,,
"No explicit p-values or statistical significance values are reported.""
2015: MAE 2.971
2016: MAE 2.556
2017: MAE 3.469","The proposed TFSF-GNN model achieved the lowest errors among all models:
MAPE 0.0619
MAPE 0.0467
MAPE 0.06789","RMSE 3.924
RMSE 3.243
RMSE 4.299",,,,,,,,,,,,,
Deep learning models outperformed statistical models in all metrics.,,,,,,,,,,,,,,,
Ablation study showed TFSF outperformed its variants,confirming the effectiveness of the multi-space convolution module.,,,,,,,,,,,,,,
Inductive learning (RMSE) for TFSF ranged from 3.2049 to 4.7738 depending on γ (testing/training nodes).,,,,,,,,,,,,,,,
Statistical values: fitting curve R²=0.80,"Correlation=0.90 for TFSF-GNN predictions vs. ground truth.""","Assumes independent and identically distributed (I.I.D.) samples, ignoring potential correlations between units.",,,,,,,,,,,,,
Spatially homogeneous transitions in meteorological and soil conditions are not fully captured.,,,,,,,,,,,,,,,
"The model may not generalize well to regions outside the main soybean planting states.""",The TFSF-GNN model achieves higher prediction accuracy and better generalization than benchmark models for soybean yield prediction.,,,,,,,,,,,,,,
Simultaneous use of temporal and spatial (topological and feature) information,with multi-channel graph convolution and attention,is effective.,,,,,,,,,,,,,
Ablation studies confirm the importance of combining topological and feature space information and the common convolution layer.,,,,,,,,,,,,,,,
"Future work should explore dynamic feature graphs and federated learning for privacy-preserving model training.""",Making the feature graph dynamic during training to allow adaptive relations among nodes in feature space.,,,,,,,,,,,,,,
Applying the model in federated learning scenarios to enable training without sharing private data.,,,,,,,,,,,,,,,
"Improving the fusion of spatial and feature information for more accurate and generalizable yield predictions.""","In future work, the feature graph will be made dynamic during training to better learn adaptive relations among nodes. Additionally, the model will be adapted for federated learning, enabling training without sharing private data across different areas.","The study is an observational, retrospective, multi-site, comparative modeling study using annual crop growth data from 1980–2017 across 13 US states. It compares traditional statistical models and deep learning models, including ablation studies, but is not randomized, blinded, controlled, or placebo-controlled.",,"The objectives of the study are to improve crop yield prediction accuracy by utilizing a proposed TFSF-GNN model that integrates climate, soil, and management data, and to compare its performance against traditional statistical and deep learning models using the USA soybean yield dataset.",,,,,,,,,,,
Multimodal Reasoning with Multimodal Knowledge Graph,"Lee Junlin, Wang Yequan, Li Jing, Zhang Min",2024,reference-manager,10.xxxx/xxxxxxx.xxxxxxx,,Implementation Insights Summary:,,,,,,,,,
"The efficacy of the method depends on the quality of the knowledge retrieval strategy; poor retrieval reduces answer accuracy. Evaluation was limited to four LLMs and two tasks due to computational constraints. MR-MKG outperforms LLaVA in most categories with fewer parameters. Adding multimodal knowledge graphs and alignment steps consistently improves performance.""",,"The research goal is to enhance large language models' multimodal reasoning using multimodal knowledge graphs (MMKGs); the approach, MR-MKG, integrates image, text, and knowledge triplets from MMKGs; results show MR-MKG achieves new state-of-the-art accuracy on multimodal question answering and analogy reasoning tasks.","The paper proposes MR-MKG, a method to enhance large language models’ multimodal reasoning using multimodal knowledge graphs (MMKGs). Using a two-stage framework for tasks like ScienceQA, MR-MKG retrieves and integrates image, text, and knowledge triplets. Experiments show state-of-the-art performance, with ablation studies confirming each component’s effectiveness.",,,How can multimodal knowledge graphs be leveraged to enhance the multimodal reasoning capabilities of large language models in tasks such as question answering and analogy reasoning?,,,,,,,,,"Multimodal-CoT prompting: A two-stage framework that first generates a rationale and then predicts the answer, using both text and image features."
Sub-MMKG retrieval: Relevant subgraphs from a multimodal knowledge graph are retrieved based on question context using embedding and cosine similarity.,,,,,,,,,,,,,,,
"Knowledge Graph Embedding (KGE) with RGAT: Uses Relational Graph Attention Networks to encode graph structures and represent multimodal knowledge.""",,"The proposed MR-MKG approach achieves new state-of-the-art results on ScienceQA (up to 93.63% accuracy) and MARS (up to 41.0% Hits@1), outperforming previous models.",,,,,,,,,,,,,
RGAT (Relational Graph Attention Network) yields the best performance among knowledge graph embedding methods,with 92.78% accuracy on ScienceQA and 40.5% Hits@1 on MARS.,,,,,,,,,,,,,,
Incorporating multimodal knowledge graphs and cross-modal alignment provides significant performance improvements,especially on samples requiring visual reasoning (up to 3.78% and 1.41% gains,"respectively); no p-values are reported.""",MR-MKG outperforms all baseline methods on ScienceQA and MARS datasets in average accuracy and Hits@1.,,,,,,,,,,,,
MR-MKG (FLAN-T5-11B) achieves 92.78% accuracy on ScienceQA,surpassing LLaVA by +1.86%.,,,,,,,,,,,,,,
MR-MKG (Visual\_LLaMA-2 7B) achieves Hits@1 of 0.405 on MARS,a 10.4% increase over baseline.,,,,,,,,,,,,,,
Ablation study: Adding KG,MMKG,and cross-modal alignment improves accuracy by up to 6.70% on ScienceQA and 10.8% Hits@1 on MARS.,,,,,,,,,,,,,
Using MMKG and alignment yields greater improvements on samples requiring visual knowledge (1.41% and 0.54% increases,"respectively).""",The effectiveness of the retrieved sub-multimodal knowledge graph depends on the success of the knowledge retrieval strategy; ineffective retrieval can lead to missing relevant knowledge and reduce LLM accuracy.,,,,,,,,,,,,,
"Evaluation was limited to four LLMs and two multimodal reasoning tasks due to computational constraints; larger models and broader tasks remain unexplored.""","MR-MKG significantly enhances multimodal reasoning in LLMs by leveraging multimodal knowledge graphs, achieving new state-of-the-art results on ScienceQA and MARS tasks.",,,,,,,,,,,,,,
RGAT is the most effective knowledge graph embedding method.,,,,,,,,,,,,,,,
Retrieval strategy should be tailored to the task; text-only retrieval performed best.,,,,,,,,,,,,,,,
"Future work should improve retrieval schemes and scale evaluation to larger models and more tasks.""",The need to refine the knowledge retrieval scheme to provide more precise and relevant knowledge for multimodal reasoning tasks.,,,,,,,,,,,,,,
Scaling up the method to larger language models and evaluating performance on a broader range of multimodal reasoning tasks.,,,,,,,,,,,,,,,
"Tailoring retrieval strategies to the specific nature of each problem rather than relying on a single modality.""","Future research should focus on improving the knowledge retrieval scheme to ensure more accurate and relevant knowledge for multimodal reasoning tasks. Additionally, scaling the method to larger language models and evaluating performance on a wider range of multimodal reasoning tasks is recommended.",No information available,,,,,,,,,,,,,
Applying Personal Knowledge Graphs to Health,"Shirai Sola, Seneviratne Oshani, McGuinness Deborah L.",2021,reference-manager,,,"Implementation Insights highlight that PHKGs (Personal Health Knowledge Graphs) face challenges in update timing, privacy, and maintenance. Trade-offs exist between up-to-date patient knowledge and privacy. New insights: PHKGs require solutions for structuring, collecting, and linking personal health data while ensuring privacy and practical access. No concrete PHKG implementations exist yet.",,,,"Collection and storage of personal health knowledge: Developing infrastructure to gather and process data from various sources, ensuring privacy and consistent storage.",,"How can personal health knowledge graphs (PHKGs) be effectively developed, linked, and maintained to enable personalized, knowledge-driven decision-making in health care while addressing challenges of data collection, integration, and privacy?","The paper explores the concept of Personal Health Knowledge Graphs (PHKGs) for personalized healthcare. It reviews existing work, highlights challenges in collecting, linking, and maintaining personal health data, and concludes that significant research is needed to develop PHKGs for effective, privacy-respecting, knowledge-driven healthcare decision-making tools.","The paper's research goal is to explore the use of personal health knowledge graphs (PHKGs) for personalized, knowledge-driven healthcare; its approach is a review of existing work and identification of key challenges; the principal finding is that significant research is needed to address collection, linkage, and privacy issues for effective PHKG implementation.",
Linking personal health knowledge to external knowledge graphs: Connecting personal data to external sources using entity linking and provenance for disambiguation.,,,,,,,,,,,,,,,
Maintenance of personal health knowledge: Updating and synchronizing knowledge between personal and external graphs,"addressing inconsistencies and privacy concerns.""",,No quantitative results or statistical significance (p-values) are reported in the context.,,,,,,,,,,,,
The primary findings highlight that Personal Health Knowledge Graphs (PHKGs) offer potential for personalized,knowledge-driven healthcare but face challenges in collection,linkage,and maintenance,especially regarding privacy and data consistency.,,,,,,,,,,,
The paper concludes that significant research is needed to develop methodologies for structuring,collecting,linking,"and maintaining personal health knowledge in PHKGs.""","No primary outcomes, results, or measured effects (including statistical values) are reported in the provided context.",,No concrete implementation of PHKGs exists.,,,,,,,,,
Limited consensus on the definition of PKGs.,,,,,,,,,,,,,,,
Challenges in collecting and processing heterogeneous personal health data.,,,,,,,,,,,,,,,
Privacy and access restrictions for personal medical information.,,,,,,,,,,,,,,,
Difficulties in linking and disambiguating entities across KGs.,,,,,,,,,,,,,,,
Maintenance challenges,including update frequency and privacy trade-offs.,,,,,,,,,,,,,,
"Potential inconsistencies when updating linked knowledge.""","Significant research and implementation challenges remain for Personal Health Knowledge Graphs (PHKGs), especially in collection, linkage, and maintenance.",,,,,,,,,,,,,,
Key issues include structuring personal health data,ensuring privacy,and developing effective linking and update mechanisms.,,,,,,,,,,,,,
Addressing these challenges is essential for leveraging PHKGs in personalized,"knowledge-driven healthcare decision-making.""","Developing infrastructure and methodologies for collecting, structuring, and storing personal health knowledge from diverse sources while ensuring privacy.",,,,,,,,,,,,,
Creating standardized workflows for linking personal health knowledge to external knowledge graphs and resolving disambiguation challenges.,,,,,,,,,,,,,,,
Addressing maintenance challenges,including update triggers,conflict resolution,"and balancing up-to-date knowledge with patient privacy.""",Key future research directions include: developing effective structures for capturing personal health knowledge; creating methodologies for collecting and using this knowledge while maintaining privacy; addressing challenges in linking and maintaining personal health knowledge; and resolving trade-offs between up-to-date information and patient privacy.,,,,,,,,,,"The objectives are to review existing work on personal knowledge graphs (PKGs), explore their integration into personalized healthcare as personal health knowledge graphs (PHKGs), and identify key research and implementation challenges that must be addressed to leverage PHKGs for personalized, knowledge-driven decision-making in health care.",
PriPL-Tree: Accurate Range Query for Arbitrary Distribution under Local Differential Privacy,"Wang Leixia, Ye Qingqing, Hu Haibo, Meng Xiaofeng",2024,reference-manager,XX.XX/XXX.XX,,Implementation Insights Summary:,,,,,,,,,
PriPL-Tree uses a piecewise linear function to model data,improving range query accuracy under local differential privacy (LDP). It applies frequency and slope refinements to resolve inconsistencies,and adaptive grids for multi-dimensional queries. Experiments show up to 81.9% MSE reduction,outperforming competitors,"especially on non-leptokurtic data.""",,,How can range queries under local differential privacy (LDP) be answered accurately and efficiently for arbitrary and high-dimensional data distributions using novel data structures and adaptive grids?,,,,,"Twice Partitioning Strategy: Sequential interval partitioning on two distributions (FˆEM and FˆEMS) to fit both jagged and smooth data, improving effectiveness.",,"The paper aims to improve range query accuracy under local differential privacy using the PriPL-Tree method. It uses both synthetic and real-world datasets, evaluating performance via mean squared error (MSE). PriPL-Tree consistently achieves the lowest MSE, especially in high-dimensional settings, showing significant improvements over existing methods.","The paper's main objective is to improve range query accuracy under local differential privacy (LDP); it introduces the PriPL-Tree approach, which uses private piecewise linear fitting and adaptive partitioning, and achieves up to one order of magnitude better accuracy (MSE reduction averaging 56.7%) compared to existing methods."
Search Acceleration Strategy: Multi-granular search to efficiently identify optimal breakpoints,reducing computation time.,,,,,,,,,,,,,,
"Theoretical and Experimental Time Complexity Analysis: Comparison of construction and query time complexities across methods using hierarchical tree structures.""","The research is reproducible. The source code, data, and other artifacts have been made available at https://github.com/LeixiaWang/PriPLT.","PriPL-Tree reduces mean squared error (MSE) by 10.6%–81.9% (average 56.7%) on Gaussian, MixGaussian, Loan, and Salary datasets, and by 14.9% on Cauchy, Zipf, Adult, and Financial datasets.",,,,,,,,,,,,,
PriPL-Tree consistently achieves the lowest MSE across varying data and query dimensions,outperforming competitors.,,,,,,,,,,,,,,
"No explicit p-values or statistical significance measures are provided in the context.""
On Cauchy","PriPL-Tree reduces Mean Squared Error (MSE) by 10.6%–81.9% (average 56.7%) on Gaussian, MixGaussian, Loan, and Salary datasets.
Zipf",Adult,and Financial datasets (leptokurtic),PriPL-Tree achieves a modest average MSE reduction of 14.9%.,,,,,,,,,,,
On the 50-D IPUMS dataset,PriPL-Tree reduces MSE by 46.34%–72.22% (average 60.67%) for varying privacy budgets and by 56.80%–91.72% (average 69.90%) across query dimensions.,,,,,,,,,,,,,,
PriPL-Tree consistently achieves the lowest MSE among all competitors.,,,,,,,,,,,,,,,
For high-dimensional range queries,PriPL-Tree’s superiority increases with lower attribute correlations.,,,,,,,,,,,,,,
MSE decreases with increasing query dimension in the IPUMS dataset,"unlike in highly correlated Gaussian datasets.""","Existing methods assume uniform data distribution and uniform domain decomposition, which is unrealistic for real-world data and leads to non-uniform errors and suboptimal estimates.",,,,,,,,,,,,,
High runtime complexity of AHEAD prevents its use on high-dimensional (50-D) datasets within reasonable time limits.,,,,,,,,,,,,,,,
"Results may not generalize to datasets with different attribute correlations.""","PriPL-Tree significantly reduces mean squared error (MSE) by 10.6%–81.9% (average 56.7%) on most datasets, but only by 14.9% on highly peaked (leptokurtic) distributions.",,,,,,,,,,,,,,
"It consistently outperforms competitors in accuracy and efficiency
For leptokurtic data","especially in higher dimensions and larger domains.
PriPL-Tree’s performance matches the best existing method",PrivNUD.,,,,,,,,,,,,,
Recommendation: Use PriPL-Tree for diverse data distributions,"but expect limited gains on highly leptokurtic datasets.""",Need for automatic and data-aware machine learning models to further enhance estimation in LDP scenarios.,,,,,,,,,,,,,
Improving accuracy for multi-dimensional range queries by dynamically adjusting grids to data density.,,,,,,,,,,,,,,,
"Addressing both non-uniform error and LDP noise error in arbitrary data distributions.""","For future work, the study suggests exploring automatic and data-aware machine learning models to further enhance estimation in Local Differential Privacy (LDP) scenarios.",,,No information available,,,,,,,,,,,
Evaluating Transformer Models for Suicide Risk Detection on Social Media,"Kaczmarek Jeremi I., Pokrywka Jakub, Gorzelanczyk Edward J.",2024,reference-manager,,,"The implementation used straightforward, general-purpose neural models without advanced techniques. Fine-tuned GPT-4o (a decoder-only model) outperformed other methods. Despite model simplicity, the approach achieved second place, highlighting the effectiveness of robust pre-trained models and simple methodologies for suicide risk detection on social media.",,,,Fine-tuning DeBERTa models (base and large) on the provided training dataset.,,"How can general-purpose neural language models be leveraged to effectively detect and classify suicide risk levels in social media texts, while addressing challenges such as label imbalance and ensuring ethical standards in data collection and participant anonymity?","The paper addresses suicide risk detection on social media, aiming to improve public health strategies. It evaluates three machine learning approaches, finding that a fine-tuned decoder-only model performed best, achieving second place in a competition. The study highlights ethical and data challenges, emphasizing collaboration with healthcare institutions.","The research goal was to detect suicide risk on social media; the approach compared encoder-only and decoder-only models (with in-context learning and fine-tuning), finding that a fine-tuned decoder-only model (GPT-4o) achieved the best results, earning second place in the IEEE BigData 2024 Cup.",
Testing GPT-4o in two configurations: in-context learning (using only input prompts) and fine-tuning via API.,,,,,,,,,,,,,,,
"Manual annotation of posts into four suicide risk levels and 17 suicide trigger categories.""","The research uses datasets derived from Reddit posts, with the original dataset including contextual information and detailed annotations. The competition dataset is more limited, lacking context and specific trigger labels. No source code for the project is mentioned or provided in the context.","The fine-tuned decoder-only model (GPT-4o) achieved the best performance, with a weighted F1 (wF1) score of 74.8 for the ensemble, outperforming in-context learning and encoder-only models.",,,,,,,,,,,,,
The approach secured second place in the competition,with a final wF1 score of 75.5.,,,,,,,,,,,,,,
"No p-values or statistical significance measures were reported.""
Results: DeBERTa and GPT-4o models were fine-tuned and evaluated; GPT-4o CoT 100 shot achieved 73.4","Primary outcomes measured: Classification of suicide risk categories (Indicator, Ideation, Behavior, Attempt) from online posts.
GPT-4o CoT 500 shot achieved 73.1 (metric not specified).",,,,,,,,,,,,,,
"Measured effects: No explicit statistical significance values provided.""",Dataset lacks contextual data and only includes four broad risk categories per post.,,,,,,,,,,,,,,
Excludes labels for specific suicide triggers,reducing predictive value.,,,,,,,,,,,,,,
Data collected only from r/SuicideWatch (Jan 2021–Dec 2022),limiting generalizability.,,,,,,,,,,,,,,
COVID-19 pandemic timing may affect applicability.,,,,,,,,,,,,,,,
"Underrepresentation of the """"""""attempt"""""""" category.",,,,,,,,,,,,,,,
Context limited to one week,insufficient for long-term risk patterns.,,,,,,,,,,,,,,
May not represent broader Reddit or non-engaged users.,,,,,,,,,,,,,,,
Methodological biases from online-only data.,,,,,,,,,,,,,,,
"Low patient participation and ethical concerns limit feasibility of more representative datasets.""","The fine-tuned decoder-only model (GPT-4o) achieved the best performance, with a wF1 score of 75.5, outperforming in-context learning and encoder-only models.",,,,,,,,,,,,,,
"Simple methods and model ensembling
Leveraging robust","without advanced techniques
pre-trained models is recommended for similar tasks.""","were highly effective
The dataset lacks labels for specific suicide triggers, reducing predictive value and missing critical risk factor insights.",resulting in second place in the competition.,,,,,,,,,,,,
Current datasets may not represent the broader Reddit community or users outside r/SuicideWatch,limiting generalizability.,,,,,,,,,,,,,,
There is a need to develop datasets based on clinically confirmed suicidal tendencies,"ensuring higher reliability and objectivity.""","Future research should focus on developing datasets based on clinically confirmed suicide risk, expanding contextual data beyond one week, including more diverse online communities, and addressing challenges in recruiting representative populations, especially those not engaging with mental health services or who have died by suicide.",,,,"The objective is to develop a dataset based on online activity from patients whose suicidal tendencies have been confirmed or ruled out by qualified professionals, aiming for a more objective and reliable method to assess suicide risk, while ensuring ethical standards, anonymity, and participant safety.",,,,,,,,,
BUILD-KG: Integrating Heterogeneous Data Into Analytics-Enabling Knowledge Graphs,"Schatz Kara, Hou Pei-Yu, Gulyuk Alexey V., Yingling Yaroslava G., Chirkova Rada",2023,reference-manager,10.1109/bigdata59044.2023.10386570,,"BUILD-KG is a domain-agnostic, human-in-the-loop workflow for integrating heterogeneous scientific and experimental data (e.g., spreadsheets, annotated images, regularized text) into a unified knowledge graph (KG). It combines structured and unstructured data, involves domain experts for accuracy, and enables richer analytics and accelerated scientific discovery.",,,,BUILD-KG workflow: A domain-agnostic process for integrating heterogeneous scientific and experimental data into a unified knowledge graph (KG).,,"How can a domain-agnostic, human-in-the-loop workflow be developed to integrate heterogeneous scientific and experimental data from multiple sources into a single unified knowledge graph that enables richer analytics and accelerates scientific discovery?","The paper introduces BUILD-KG, a domain-agnostic workflow for integrating heterogeneous scientific and experimental data into a unified knowledge graph (KG). The methodology involves human-in-the-loop collaboration with domain scientists. Applied to STEPS-center data, BUILD-KG enables richer analytics and accelerates scientific discovery by unifying diverse data sources.","The paper's main objective is to develop BUILD-KG, a domain-agnostic workflow for integrating diverse scientific data into a unified knowledge graph (KG); the key method involves human-in-the-loop processing and semantic tagging; principal finding: BUILD-KG enables accurate, domain-aware KG construction, accelerating scientific discovery.",
Humans-in-the-loop: Close collaboration with domain scientists during data preprocessing to ensure accurate data representation in the KG.,,,,,,,,,,,,,,,
Data preprocessing steps: Obtain raw data and semantic descriptions,generate sample semantic sentences,validate/correct with scientists,"and format data for KG integration.""",,,,,,,,,,,,"Primary outcomes measured include absorbance (in nm) of solutions containing E. coli and orthophosphate at 10°C, with values of 0.234, 0.240, and 0.226 nm for three trials."
Molecular dynamics simulations measured binding rate,binding strength,speed of binding,and affected protein regions.,,,,,,,,,,,,
"No statistical values reported.""",Integrating data into a knowledge graph (KG) may not be trivial for research teams to accomplish on their own.,,,,,,,,,,,,,,
The process requires input from domain scientists,which can be a resource burden.,,,,,,,,,,,,,,
"The paper suggests further research on involving domain scientists as humans-in-the-loop.""",The study successfully developed a procedure to convert raw scientific data into a structured knowledge graph (KG) using semantic tagging and data formatting.,,,,,,,,,,,,,,
The resulting KG enables further use and exploration by domain scientists.,,,,,,,,,,,,,,,
"Recommendations include validating semantic sentences with domain experts and formatting data to meet specific requirements before conversion.""","Lack of automated, domain-agnostic workflows for integrating heterogeneous data into unified knowledge graphs (KGs).",,,,,,,,,,,,,,
Challenges for domain scientists in adding connections across converted data sources without familiarity with each other's projects.,,,,,,,,,,,,,,,
"Need for effective involvement of domain experts as humans-in-the-loop to ensure accurate and useful KG construction.""",,,,,,,,,,,,,,,
Construction and Application Research of Intelligent Education Knowledge Graph Based on Multi-modal Learning,"Li Haiping, Duan Wenjing",2024,reference-manager,10.1109/ieca62822.2024.00029,,Implementation Insights Summary:,,,,,,,,,
The AI assistant system using a knowledge graph improved teaching outcomes,with a 35% increase in learning efficiency,21% higher exam pass rates,"and over 85% user satisfaction. Multi-modal recommendations further boosted effectiveness by up to 31%. The educational decision support system saved repetitive work and achieved satisfaction scores of 87 (teachers) and 91 (administrators). New insight: Multi-modal fusion is highlighted as a key future research direction due to its superior performance.""",,,"The research goal was to enhance intelligent education using a deep learning-based multi-modal knowledge graph; the approach combined entity recognition, relation extraction, and semantic matching for personalized recommendations; results showed up to 31% improved learning effectiveness, 26% higher score improvement, and 23% reduced learning time.","Construction of a multi-modal educational knowledge graph using deep learning for feature and relationship extraction from text, images, and videos.",,,,,,"How can a multimodal educational knowledge graph, constructed using deep learning-based entity recognition and relationship extraction from heterogeneous data, support intelligent education applications such as personalized learning path recommendations and teaching resource matching, and what are its effects on teaching outcomes and user satisfaction?",,"The paper aims to enhance intelligent education using an AI assistant system based on a knowledge graph. Through experiments with online platforms and decision support systems, the study found improved learning efficiency (up to 35%), higher exam pass rates (21%), and high user satisfaction. Multi-modal approaches showed superior effectiveness."
Experimental evaluation comparing personalized learning path recommendations based on the knowledge graph versus manual educator recommendations.,,,,,,,,,,,,,,,
"Intelligent matching algorithm utilizing the knowledge graph for precise teaching resource recommendations.""",,"The AI assistant system improved learning efficiency by 35%, increased exam pass rates by 21%, and achieved over 85% user satisfaction in a mathematics course.",,,,,,,,,,,,,
Experimental group students had a 26% average score improvement and a 23% reduction in learning time compared to the control group.,,,,,,,,,,,,,,,
"Multi-modal recommendations improved learning effectiveness by up to 31%; the knowledge graph achieved 83% entity recognition and 89% relationship extraction accuracy. No p-values reported.""",Students using the AI assistant system in mathematics saw a 35% improvement in learning efficiency and a 21% increase in exam pass rates.,,,,,,,,,,,,,,
Over 85% of students expressed satisfaction with the AI assistant.,,,,,,,,,,,,,,,
Experimental group had a 26% average exam score improvement vs. 18% in the control group.,,,,,,,,,,,,,,,
Experimental group reduced average learning time to 25 hours vs. 32.5 hours in the control group (23% reduction).,,,,,,,,,,,,,,,
Multi-modal recommendations improved learning effectiveness by up to 31%.,,,,,,,,,,,,,,,
Knowledge graph entity recognition accuracy: 83%; relationship extraction accuracy: 89%.,,,,,,,,,,,,,,,
"Teacher satisfaction score: 87; administrator satisfaction score: 91.""","The """"uncanny valley"""" phenomenon may cause discomfort due to the AI assistant's human-like behavior.",,,,,,,,,,,,,,
Some users found responses confusing or not detailed enough.,,,,,,,,,,,,,,,
Recommended exercises can be too challenging.,,,,,,,,,,,,,,,
"The study suggests further research on multi-modal fusion for improved performance.""","Multi-modal educational knowledge graphs significantly improve personalized learning efficiency and exam scores, reducing learning time by 23%.",,,,,,,,,,,,,,
User satisfaction is high,"but some users experience discomfort due to the AI's human-like behavior (""""""""uncanny valley"""""""" phenomenon).",,,,,,,,,,,,,,
"The approach is validated for practical intelligent education applications and is recommended for future research and development.""","Addressing the """"uncanny valley"""" phenomenon to ensure user comfort and acceptance when humanizing AI assistants in education.",,,,,,,,,,,,,,
Enhancing the accuracy and completeness of multimodal educational knowledge graphs,especially in entity recognition and relationship extraction.,,,,,,,,,,,,,,
"Further optimizing personalized learning path recommendations and intelligent resource matching for improved educational outcomes.""","Future research should focus on improving the handling of the """"uncanny valley"""" phenomenon in AI assistant design, enhancing response clarity and detail, and further exploring multi-modal fusion methods for personalized recommendations. Additional studies could address user discomfort and optimize adaptation to individual learning needs.","The study design includes: experimental research, control group, experimental group, random selection of students, manual assessment, surveys, and case analysis. It is a controlled, comparative study with both quantitative and qualitative evaluation, but there is no mention of randomization, blinding, or multi-site implementation for the experiment.",,"The objectives of the study are to construct a high-quality, multi-modal educational knowledge graph for personalized learning path recommendations, improve learning efficiency and exam performance, support educational decision-making, and ensure positive user experience by addressing issues like the """"uncanny valley"""" in AI-human interaction.",,,,,,,,,,,
Personalized Entity Resolution with Dynamic Heterogeneous Knowledge Graph Representations,"Lin Ying, Wang Han, Chen Jiangning, Wang Tong, Liu Yue, Ji Heng, Liu Yang, Natarajan Premkumar",2021,reference-manager,,,"The implementation shows that jointly learning customer and product embeddings from a merged graph improves performance (+32.9% Dev Acc@1, +24.6% Test Acc@1). Removing personalized features, product embeddings, or ranking degrades results. New insights: richer features (e.g., ratings) and modeling purchase behavior interactions could further enhance accuracy.",,,,"Utilized a product knowledge graph with over 24 million entities, incorporating textual and binary product attributes.",,How can personalized features and a cross-source heterogeneous knowledge graph be leveraged to improve the accuracy of entity resolution for product ranking in the shopping domain using virtual assistants?,"The paper addresses personalized entity resolution in shopping via virtual assistants, proposing a framework that builds a cross-source heterogeneous graph from customer purchase history and product knowledge. Using a neural reranking model, it improves top-ranked product accuracy by 24.6% over QUARTS. Ablation studies confirm the importance of personalized features.","The research goal is to improve personalized entity resolution in shopping by jointly learning customer and product representations using a cross-source heterogeneous graph; the approach combines these embeddings in a neural reranking model, resulting in a 24.6% accuracy gain over the state-of-the-art product search model.",
Employed personalized features,including customer embeddings and purchase history,for model training.,,,,,,,,,,,,,
Conducted ablation studies to assess the impact of removing ranking,personalized features,product embedding,"and joint embedding.""",,,The proposed model achieves relative gains of +32.9% (Dev Acc@1) and +24.6% (Test Acc@1) over the QUARTS baseline.,,,,,,,,,
Ablation studies show removing ranking,personalized features,or product embedding significantly reduces performance; joint embedding of customer and product yields the best results.,,,,,,,,,,,,,
"No p-values or explicit statistical significance values are provided.""",The proposed model achieves +32.9% Dev Acc@1 and +24.6% Test Acc@1 relative gains over QUARTS.,,,,,,,,,,,,,,
Purchased method: +10.5% Dev Acc@1,+8.5% Test Acc@1.,,,,,,,,,,,,,,
ComplEx method: +25.7% Dev Acc@1,+16.1% Test Acc@1.,,,,,,,,,,,,,,
Removing ranking: -17.1% Dev Acc@1,-20.4% Test Acc@1.,,,,,,,,,,,,,,
Removing personalized features: -10.5% Dev Acc@1,-18.0% Test Acc@1.,,,,,,,,,,,,,,
Removing product embedding: +25.2% Dev Acc@1,+19.0% Test Acc@1.,,,,,,,,,,,,,,
Removing joint embedding: +28.1% Dev Acc@1,"+20.4% Test Acc@1.""",Removal of personalized features and product embeddings degrades model performance.,,,,,,,,,,,,,
Customer and product embeddings not jointly learned from merged graph reduce effectiveness.,,,,,,,,,,,,,,,
Errors often occur due to differences in product size,uninformative or similar titles,brand,or attribute confusion.,,,,,,,,,,,,
Customers may not always repurchase previously bought products.,,,,,,,,,,,,,,,
Limited ability to display products in voice user interfaces.,,,,,,,,,,,,,,,
"Future work needed for better integration of personalized features and extension to cross-lingual/cross-media settings.""","The proposed framework significantly improves product purchase prediction, with +32.9% (Dev) and +24.6% (Test) gains over QUARTS.",,,,,,,,,,,,,,
Personalized features and ranking are crucial for optimal performance.,,,,,,,,,,,,,,,
Incorporating more informative features and richer cross-source data is recommended for further improvement.,,,,,,,,,,,,,,,
Future work includes better integration of personalized features and expansion to cross-lingual,"cross-media settings.""","Incorporating more informative features such as average rating, customer reviews, and number of ratings into the framework.",,,,,,,,,,,,,
Building a more comprehensive cross-source customer-product graph by including records from additional sources like music or video history and multimedia features.,,,,,,,,,,,,,,,
"Modeling the interactions among purchase behaviors rather than using a “flat” attention-based method.""","Future research should focus on integrating more informative features (like average rating, customer reviews, and number of ratings), building a more comprehensive cross-source customer-product graph (including data like music or video history), and better modeling interactions among purchase behaviors.",No information available,,"The objective of the study is to improve personalized entity resolution in the shopping domain by leveraging personalized features to accurately rank and return the product most likely to be purchased by a specific customer, given a query and a list of candidate products.",,,,,,,,,,,
ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs,"Ding Zifeng, Li Zongyue, Qi Ruoxia, Wu Jingpei, He Bailan, Ma Yunpu, Meng Zhao, Chen Shuo, Liao Ruotong, Han Zhen, Tresp Volker",2023,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
Experiments use PyTorch on NVIDIA A40 and AMD EPYC 7513. TANGO,TComplEx,and ComplEx are compared for TKG forecasting on ICEWS21,"with embedding sizes selected via grid search. TANGO outperforms others. Data efficiency analysis shows model performance improves with more training data. No new insights beyond these findings.""",,,"The research goal is to evaluate temporal knowledge graph question answering (TKGQA); the approach involves benchmarking models and humans on new question types using ICEWS-based datasets; results show that models like EmbedKGQA outperform baselines, but humans still achieve higher accuracy, highlighting gaps in machine fact reasoning.",Development and use of the Multi-Hop Scorer (MHS) QA model for non-forecasting TKGQA tasks.,,,,,,"What is the primary methodology for generating and categorizing natural language questions based on temporal knowledge graph (TKG) facts in ICEWS21, and how does this support the development and validity of the ForecastTKGQuestions dataset for forecasting TKG question answering?",,"The paper introduces Fact Reasoning Questions (FRQs) to compare human and machine reasoning in temporal knowledge graph question answering (TKGQA). Using models like xERTE and Multi-Hop Scorer, the study generates and annotates FRQs, benchmarks human performance with and without web search, and finds humans achieve up to 0.936 accuracy unaided."
Generation of natural language questions using relation templates based on TKG facts from ICEWS21 and the official CAMEO codebook.,,,,,,,,,,,,,,,
Data efficiency analysis by training models on varying sizes (10%,25%,50%,"75%) of training sets and evaluating performance.""",,,"EmbedKGQA achieved the highest overall MRR (0.278) and Hits@10 (0.443) on EPQs, outperforming BERT and RoBERTa; BERT and RoBERTa scored lower (MRR: 0.253 and 0.161, respectively).",,,,,,,,,
Human performance on FRQs reached 0.954 accuracy,while the best model (ForecastTKGQA) achieved 0.870 accuracy.,,,,,,,,,,,,,,
"Fleiss’ kappa for human annotation agreement was 0.63
ForecastTKGQA achieves the highest results: MRR 0.339 (1-Hop: 0.216
For YUQs and FRQs","indicating substantial agreement; no p-values were reported.""
2-Hop: 0.248)
ForecastTKGQA accuracy: YUQ 0.870","The entity prediction model outperforms all baseline methods.
Hits@1 0.129
FRQ 0.769.",Hits@10 0.517 (2-Hop: 0.386).,,,,,,,,,,,,
Standard deviation for ForecastTKGQA is very low (e.g.,MRR overall: 0.0004).,,,,,,,,,,,,,,
Human performance: YUQ 0.936,FRQ 0.954.,,,,,,,,,,,,,,
LM variants with TKG representations outperform original LMs.,,,,,,,,,,,,,,,
TKG forecasting models (BERT ext,RoBERTa ext) outperform TKGC models (BERT int,RoBERTa int).,,,,,,,,,,,,,
CronKGQA and TempoQR perform poorly,"suggesting TKGC representations may add noise.""","Some FRQs contain erroneous examples because answers are solely determined by xERTE, leading to cases where Hard Negatives may be more suitable than the labeled Answer.",,,,,,,,,,,,,
Manual annotation was required to identify reasonable/unreasonable questions,indicating possible subjectivity and annotation bias.,,,,,,,,,,,,,,
"No other explicit limitations or shortcomings are mentioned.""","The proposed forecasting TKGQA tasks and datasets are valid and meaningful, supported by both theoretical and empirical justifications.",,,,,,,,,,,,,,
ForecastTKGQA outperforms all baseline models,with human performance still higher (ForecastTKGQA: 0.870 YUQ,0.769 FRQ; Human: 0.936,0.954).,,,,,,,,,,,,
Increasing training data size steadily improves model performance,proving data efficiency.,,,,,,,,,,,,,,
"Recommendation: Further improve fact reasoning in QA models using insights from FRQs.""","There is a large gap between model and human performance, indicating significant room for improvement in fact reasoning.",,,,,,,,,,,,,,
Improving QA models’ reasoning skills,especially for Fact Reasoning Questions (FRQs),is a key future direction.,,,,,,,,,,,,,
"Further research is needed to enhance forecasting power and answerability in TKGQA methods.""",,,,,,,,,,,,,,,
"KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of Data Science","Helali Mossad, Monjazeb Niki, Vashisth Shubham, Carrier Philippe, Helal Ahmed, Cavalcante Antonio, Ammar Khaled, Hose Katja, Mansour Essam",2024,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
"KGLiDS uses machine learning and knowledge graphs to abstract data science artifacts and their relationships. It combines static code analysis and documentation analysis for accurate semantic extraction
New Insights:",enabling efficient data profiling,discovery,cleaning,and automation. KGLiDS achieves faster performance and similar or better accuracy than state-of-the-art systems.,,,,,,,,,,,
Enriching static analysis with library documentation allows detection of implicit parameters and return types. The system’s library graph reveals usage patterns across data science libraries. KGLiDS supports both predefined APIs and ad-hoc queries,"enabling flexible exploration and automation. Future work includes integrating large language models for more advanced use cases.""",,"The research goal is to enable efficient knowledge sharing in data science by abstracting and linking data science artifacts; the approach uses KGLiDS, a scalable platform with machine learning-based semantic abstraction and knowledge graph construction; results show KGLiDS outperforms state-of-the-art systems in speed with comparable or better accuracy.","The paper introduces KGLiDS, a scalable platform that uses machine learning to abstract and connect data science artifacts in a knowledge graph. Using advanced data profiling and specialized static code analysis, KGLiDS enables efficient discovery, reuse, and automation. Experiments show KGLiDS is faster and as accurate or better than state-of-the-art systems.",,,,,,,,,,"Documentation Analysis: Enriches static program analysis using library documentation to detect data types, parameter names/values, and implicit parameters, building a JSON representation for each class and method.","How can a scalable platform be designed to automatically abstract, interconnect, and leverage the semantics of data science artifacts and pipelines for improved data discovery, cleaning, transformation, and automation in data science?"
Static Code Analysis: Applies lightweight static code analysis tools to infer information from code not obtainable by general-purpose static analysis.,,,,,,,,,,,,,,,
Machine Learning-based Data Profiling: Uses machine learning to analyze and profile data items (datasets,tables,"columns) for constructing a knowledge graph.""",KGLiDS’ reproducibility is supported by the availability of the script to download datasets at KGLiDS’ repository. No explicit source code for the main project is mentioned. Evaluation uses public benchmarks and datasets. No further details about full source code availability are provided.,"KGLiDS outperforms GraphGen4Code in F1-Score across most of 24 AutoML datasets, with a statistically significant improvement (two-tailed t-Test p-value = 0.012, p < 0.05).",,,,,,,,,,,
KGLiDS achieves comparable or better accuracy than state-of-the-art systems while consuming significantly less time.,,,,,,,,,,,,,,,
The platform enables enhanced data discovery,cleaning,transformation,"and automation by abstracting and connecting data science artifacts using a knowledge graph.""",KGLiDS slightly outperforms HoloClean (Aimnet) in F1-Scores for data cleaning on small datasets; HoloClean faces out-of-memory issues on large datasets.,,,,,,,,,,,
For data transformation,KGLiDS achieves comparable or better accuracy than AutoLearn across 17 datasets; AutoLearn times out or runs out of memory on some datasets.,,,,,,,,,,,,,,
KGLiDS is 7.3x faster than SANTOS in preprocessing time and consistently outperforms SANTOS and Starmie in preprocessing and average query times.,,,,,,,,,,,,,,,
"KGLiDS uses less storage (1.49 GB vs 16.55 GB) and less analysis time (1.9 hr vs 37.59 hr) than GraphGen4Code for pipeline abstraction.
In AutoML tasks",KGpip with KGLiDS improves F1-Scores over KGpip with GraphGen4Code in most of 24 datasets; t-Test value = 0.012 (A < 0.05),"indicating significant improvement.""",,KGLiDS is a scalable platform using machine learning to abstract and interconnect data science artifacts via a knowledge graph.,,,,,,,,,,,
It achieves comparable or better accuracy than state-of-the-art (SOTA) systems while using significantly less time.,,,,,,,,,,,,,,,
KGLiDS enables new use cases in data discovery,exploration,reuse,and automation.,,,,,,,,,,,,
"Future work includes integrating large language models (LLMs) for expanded use cases.""",Incorporating large language models (LLMs) into KGLiDS for additional use cases like exploratory data analysis and feature engineering is ongoing and a future direction.,,,,,,,,,,,,,,
Accurate static code analysis for dynamic languages like Python remains challenging and is an open research gap.,,,,,,,,,,,,,,,
"Existing systems do not holistically interlink dataset semantics with pipeline scripts; KGLiDS aims to address this gap.""","Future research should focus on incorporating large language models (LLMs) into KGLiDS for additional use cases, such as exploratory data analysis, feature selection, and feature engineering. This addresses current limitations and expands the system's capabilities beyond its present scope.",,,,,,,,,,,,,,
Building Contextual Knowledge Graphs for Personalized Learning Recommendations Using Text Mining and Semantic Graph Completion,"Abu-Rasheed Hasan, Dornhöfer Mareike, Weber Christian, Kismihók Gábor, Buchmann Ulrike, Fathi Madjid",2023,reference-manager,10.1109/icalt58122.2023.00016,,"The implementation introduces a semantic approach using a text-mining pipeline (TMP) for relation extraction, enhancing knowledge graph (KG) connectivity and contextual representation of learning objects. Key metrics show increased average degree centrality and communities, and decreased modularity and weakly connected components, indicating improved interlinking and context integration.",,,,Systematic literature review: The study conducted a systematic review of existing literature and developed a conceptual framework for quality assessment in Linked Data.,,How can semantic analysis and knowledge graph construction be used to enhance the contextual representation of learning activities (Las) in digital learning platforms for improved personalized recommendations?,,"The research goal is to enhance contextual representation of learning assets (Las) in personalized learning systems by transferring hierarchical data models to knowledge graphs using a semantic text-mining pipeline; the approach improved semantic connectivity among Las, enabling richer contextual recommendations.",
Use of network metrics: Metrics such as average degree centrality,clustering coefficient,weakly connected components,and betweenness centrality were used for analysis.,,,,,,,,,,,,
"Text mining pipeline (TMP): A text mining pipeline was used for semantic similarity calculation of learning object titles and descriptions.""",,"The knowledge graph (KG) achieved an average betweenness centrality (BC) of 15.1, about 10 times higher than the original hierarchical model.",,,,,,,,,,,,,
Average degree centrality increased from 1.079 to 2.262; clustering coefficient (number of communities) rose from 253 to 541,while average modularity score decreased from 0.779 to 0.636.,,,,,,,,,,,,,,
"No explicit p-values or statistical significance are reported in the context.""","The KG has an average betweenness centrality (BC) value of 15.1, about 10 times higher than the original hierarchical model.",,,,,,,,,,,,,,
Average degree centrality increased from 1.079 to 2.262.,,,,,,,,,,,,,,,
Number of communities increased from 253 to 541.,,,,,,,,,,,,,,,
Clustering coefficient decreased from 0.779 to 0.636.,,,,,,,,,,,,,,,
"Weakly connected components decreased from 63 to 35.""",No explicit limitations or shortcomings are stated in the provided context.,,,,,,,,,,,,,,
"No self-reported problems or suggestions for further research are mentioned.
,The proposed semantic approach and text-mining pipeline improved the contextual representation and connectivity of learning activities (Las) in personalized learning systems.",,,,,,,,,,,,,,,
Transforming hierarchical data models into graph models enhanced both structural and contextual data quality.,,,,,,,,,,,,,,,
Key metrics showed increased connectivity,higher degree centrality,and reduced weakly connected components.,,,,,,,,,,,,,
"The approach is adaptable to various hierarchical structures of Las.""",Previous approaches do not explicitly address the role of Learning Activities' (Las) context in Knowledge Graphs (KGs).,,,,,,,,,,,,,,
There is a need to mine further semantic and contextualizing relations between Las beyond current methods.,,,,,,,,,,,,,,,
"Future research should focus on transforming hierarchical structures into contextualizing KGs for improved personalized learning.""",,,,,,,,,,,,,,,
Leveraging Knowledge Graphs for Matching Heterogeneous Entities and Explanation,"Ghassabi Sahar, Behkamal Behshid, Milani Mostafa",2023,reference-manager,10.1109/bigdata59044.2023.10386157,,"The study refines existing evaluation methods to assess explanation quality in EM, using both direct (user feedback) and indirect (behavioral change) approaches. It leverages knowledge graph paths to improve and explain DITTO and HIERGAT models, addressing challenges like feature interaction and explanation granularity. Superior performance and high explanation quality are reported.",,,,"Combined direct and indirect evaluation methods, including user/expert feedback and behavioral observations, to assess explanation quality.",,"How can explanation quality in heterogeneous entity matching be comprehensively evaluated and improved, particularly through the EXKG approach, considering challenges such as cross-record interaction effects, explanation of non-match cases, and sensitivity variation?","The paper aims to comprehensively evaluate explanation quality in Entity Matching (EM) using knowledge graphs (KG). It combines direct user feedback and indirect behavioral measures across six aspects: trust, understandability, usefulness, satisfaction, soundness, and completeness. Results show high user satisfaction, usefulness, and soundness, affirming the approach’s effectiveness.","The research goal is to enhance explainability in entity matching (EM) by integrating knowledge graph (KG) paths into DITTO and HIERGAT models; the approach combines quantitative and qualitative evaluation of explanation quality, and results show improved model performance and highly rated, user-friendly explanations across multiple dimensions.",
Developed and used a Likert-scale questionnaire to measure user perceptions of Understandability,Satisfaction,and Soundness.,,,,,,,,,,,,,
"Compared user selections before and after explanations to indirectly assess Trust and Usefulness.""
60% of participants found the explanations useful and trustworthy; nearly 90% reported high satisfaction; 88.2% endorsed soundness.",,"The proposed approach using knowledge graph (KG) paths improved the performance of DITTO and HIERGAT models, with explanations rated highly for understandability, satisfaction, and soundness.",,,,,,,,,,,,,
"No p-values or statistical significance measures are reported.""","Primary outcomes were evaluated across six aspects: trust, understandability, usefulness, satisfaction, soundness, and completeness.",,,,,,,,,,,,,,
EXKG-D achieved Precision: 81.66 (+7.04 over DITTO),Recall: 92.45 (-1.88),F1: 86.72 (+3.39),AUC: 92.58 (-0.05).,,,,,,,,,,,,
EXKG-H achieved Precision: 68.52 (+10.83 over HIERGAT),Recall: 69.81 (-15.1),F1: 69.16 (+0.46),AUC: 72.92 (+6.36).,,,,,,,,,,,,
"Explanations generated yielded high scores in user evaluation dimensions.""",Lack of meaningful attributions for non-matching predictions.,,,,,,,,,,,,,,
Challenge in handling cross-record interaction effects due to feature independence assumptions in linear models.,,,,,,,,,,,,,,,
Difficulty explaining non-match cases when feature removal does not affect the match score.,,,,,,,,,,,,,,,
Sensitivity variation: balancing feature granularity is challenging.,,,,,,,,,,,,,,,
"Explanations depend on the existence of paths in the knowledge graph.""",Integrating knowledge graphs (KGs) into explanation generation for Entity Matching (EM) improves both effectiveness and interpretability.,,,,,,,,,,,,,,
Explanations generated achieved high user satisfaction (nearly 90%),soundness (88.2%),and usefulness/trustworthiness (60%).,,,,,,,,,,,,,
The approach covers over 90% of matching and non-matching cases,demonstrating adaptability.,,,,,,,,,,,,,,
Recommendation: Tailor model configurations when integrating paths from KGs,"considering model architecture and path characteristics.""","Addressing explainability, robustness, and fairness in entity matching remains a significant challenge and opportunity for future research.",,,,,,,,,,,,,
Effectively integrating paths from external knowledge graphs requires careful consideration of model architecture and configuration,especially regarding the number and length of paths.,,,,,,,,,,,,,,
"Handling incorrect predictions caused by overlapping keywords or recurring tokens in heterogeneous data needs further investigation and suitable solutions.""","Suggested future research directions include addressing the challenge of balancing feature granularity (token vs. attribute level) in explanations, adapting model configurations for different architectures when integrating knowledge graph paths, and further exploring explainability, robustness, and fairness in entity matching (EM) processes.","The study design is a comprehensive evaluation using both quantitative and qualitative approaches. It combines direct (user/expert feedback via Likert-scale questionnaires) and indirect (behavioral observation, calculations, comparisons) methods. The study is observational, non-randomized, non-controlled, and uses a mixed-methods approach.",,"The objectives of the study are to comprehensively evaluate the quality of explanations generated by their approach, focusing on six aspects: trust, understandability, usefulness, satisfaction, soundness, and completeness, using a combination of direct (user feedback) and indirect (behavioral observation) assessment methods.",,,,,,,,,,,
Smart City Digital Twins: A Modular and Adaptive Architecture for Real-Time Data-Driven Urban Management,"Herath Manoj, Alvi Maira, Minerva Roberto, Dutta Hrishikesh, Crespi Noel, Raza Syed Mohsan",2024,reference-manager,10.23919/cnsm62983.2024.10814627,,Implementation Insights Summary:,,,,,,,,,
"The modular Digital Twin architecture uses real-time IoT data for traffic prediction
New Insights:",simulation,and visualization. Early stopping prevents overfitting. Increasing data sampling frequency improves prediction accuracy but raises bandwidth use,requiring a trade-off. The Event Management Subsystem dynamically adjusts sampling for optimal resource use.,,,,,,,,,,,,
The architecture’s modularity enables flexible integration and scalability for smart city applications,supporting secure,"real-time decision-making and resource optimization.""",,"The paper's main objective is to develop a modular Digital Twin software architecture for smart cities using an Edge-Cloud Continuum; the key method is a flexible, subsystem-based design demonstrated in autonomous traffic management, and the principal finding is improved prediction accuracy with higher data sampling, balanced by resource-aware event management.",,"The paper proposes a modular software architecture for smart city traffic management using Digital Twin (DT) technology. It introduces subsystems for event and resource management, utilizes real-time data processing (with Apache Kafka), and supports flexible orchestration across edge and cloud. Results show dynamic IoT data sampling improves prediction performance and resource efficiency.",,,,,,,"Long Short Term Memory (LSTM) network: A type of recurrent neural network used for time-series traffic prediction, trained with seven months of data, 30 epochs, batch size of 32, and early stopping to prevent overfitting.","How can a modular Digital Twin software architecture, leveraging an Edge-Cloud Continuum, be designed to effectively support autonomous smart city traffic management through efficient data management, processing, and integration of supporting functions for scalability, adaptability, and ease of development?",
Simulation module: Used to create and verify models by running what-if scenarios to analyze traffic behavior under various conditions.,,,,,,,,,,,,,,,
Data preprocessing and normalization: Incoming traffic data is preprocessed and normalized,"then classified into traffic intensity categories before storage and analysis.""",,"Prediction errors (MAE, MSE, RMSE) increase as sampling frequency decreases; 1-hour sampling yields the lowest errors (MAE: 0.154, MSE: 0.049, RMSE: 0.221).",,,,,,,,,,,,
Errors with 2-hour sampling are similar to 1-hour,but errors double at 3-hour intervals.,,,,,,,,,,,,,,
"No statistical significance (p-values) reported.""","Prediction errors (MAE, MSE, RMSE) increase as sampling frequency decreases.",,,,,,,,,,,,,,
1-hour sampling frequency yields lowest errors: MAE 0.154,MSE 0.049,RMSE 0.221.,,,,,,,,,,,,,
2-hour frequency errors: MAE 0.226,MSE 0.084,RMSE 0.290.,,,,,,,,,,,,,
Errors double at 3-hour frequency: MAE 0.471,MSE 0.393,RMSE 0.627.,,,,,,,,,,,,,
"Errors continue increasing at 5 and 6 hours.
1-hour and 2-hour frequencies have similar","low error rates; performance degrades significantly at lower frequencies.""","Existing DT architectures are too generalized or specific to industrial applications, limiting suitability for smart cities.",,,,,,,,,,,,,
Lack of modularity in current architectures hinders reusability,scalability,and adaptability.,,,,,,,,,,,,,
Standardization efforts focus on manufacturing and networking,not urban environments.,,,,,,,,,,,,,,
"Literature gap: absence of standard modular architectures for smart city DTs.""","The modular Digital Twin architecture enables flexible, scalable smart city solutions with real-time data acquisition, processing, and decision-making.",,,,,,,,,,,,,,
Increasing traffic data sampling frequency improves prediction accuracy but increases bandwidth usage,requiring a balance between accuracy and resource use.,,,,,,,,,,,,,,
"The Event Management Subsystem dynamically adjusts sampling frequency to optimize performance and resource consumption.""","Lack of modular, standardized software architectures for smart city Digital Twins (DTs), limiting reusability, scalability, and adaptability.",,,,,,,,,,,,,,
Existing DT architectures are either too generalized or tailored for industrial/manufacturing,not adaptable for urban environments.,,,,,,,,,,,,,,
"Insufficient support for DT functions like resource provisioning and software alignment in current architectures.""","Future research should address the lack of standardized, modular software architectures for smart city Digital Twins (DTs), improve adaptability for urban environments, explore integration with emerging technologies like blockchain and 5G, and further investigate the trade-off between prediction accuracy and resource usage in real-time data collection.",,,"The objectives of the study are to propose a modular Digital Twin software architecture specifically for smart cities, focusing on efficient data management, storage, and retrieval, supporting DT-based solutions, and enabling analysis, prediction, and visualization of urban traffic patterns while balancing performance and resource usage.",,,,,,,,,,,
Developing knowledge graph based system for urban computing,"Liu Yu, Ding Jingtao, Li Yong",2022,reference-manager,10.1145/3557990.3567586,,Implementation Insights Summary:,,,,,,,,,
"The UrbanKG system outperforms the traditional NeuMF-RS solution in the site selection task for both Beijing and Shanghai
New Insights:",with higher N@10,H@10,and P@10 scores. UrbanKG effectively fuses multi-source urban data and provides better,more explainable representations,,enhancing downstream urban applications.,,,,,,,,,
UrbanKG not only improves performance but also offers more interpretable results and supports diverse urban scenarios,"validating its broad applicability.""",,"The research goal is to enhance urban site selection by fusing urban data using the UrbanKG system; the approach constructs a knowledge graph-based system, and results show UrbanKG outperforms traditional solutions like NeuMF-RS in key metrics (N@10, H@10, P@10) for both Beijing and Shanghai.","The paper introduces UrbanKG, a knowledge graph-based system designed for urban computing. Using multi-layer data fusion and knowledge distillation, UrbanKG outperforms traditional methods in tasks like site selection and mobility prediction. Results show improved performance and explainability, supporting its wide applicability in urban scenarios.",,,,,,,,,,"Use Cases Analysis: The study investigates representative applications, including site selection, to validate the effectiveness of the UrbanKG system.",How can an urban knowledge graph (UrbanKG) system effectively fuse multi-source spatial-temporal urban data and distill task-specific knowledge to support diverse urban computing applications?
Comparative Evaluation: Performance of UrbanKG is compared against the traditional NeuMF-RS method using metrics such as NDCG,hit ratio,and precision.,,,,,,,,,,,,,
"Data Fusion and Knowledge Distillation: UrbanKG fuses multi-source urban data and distills task-specific knowledge to enhance application performance.""",,"UrbanKG outperforms NeuMF-RS in the site selection task for both Beijing (N@10: 0.219 vs. 0.178, H@10: 0.713 vs. 0.653, P@10: 0.186 vs. 0.155) and Shanghai (N@10: 0.205 vs. 0.168, H@10: 0.671 vs. 0.615, P@10: 0.177 vs. 0.148).",,,,,,,,,,,,,
The UrbanKG system demonstrates superior performance by effectively fusing multi-source urban data and distilling task-specific knowledge.,,,,,,,,,,,,,,,
"No statistical significance (p-values) is reported in the context.""
Beijing results:",UrbanKG outperforms NeuMF-RS in the site selection task for both Beijing and Shanghai.,,,,,,,,,,,,,,
NeuMF-RS: N@10 = 0.178,H@10 = 0.653,P@10 = 0.155,,,,,,,,,,,,,
"UrbanKG: N@10 = 0.219
Shanghai results:",H@10 = 0.713,P@10 = 0.186,,,,,,,,,,,,,
NeuMF-RS: N@10 = 0.168,H@10 = 0.615,P@10 = 0.148,,,,,,,,,,,,,
UrbanKG: N@10 = 0.205,H@10 = 0.671,"P@10 = 0.177""",No information available,"The UrbanKG system outperforms traditional solutions in urban applications, such as site selection and mobility prediction.",,,,,,,,,,,
UrbanKG provides better data representations,enhancing the performance of existing methods.,,,,,,,,,,,,,,
The system offers more explainable and transparent results.,,,,,,,,,,,,,,,
"UrbanKG demonstrates wide applicability and effectiveness for various urban computing research tasks.""","Data fusion of urban data with different structures (tables, sequences, graphs) remains under-explored.",,,,,,,,,,,,,,
Distilling task-specific knowledge from massive urban data containing both useful information and noise is a critical challenge.,,,,,,,,,,,,,,,
"There is a lack of full-featured and user-friendly platforms for researchers and developers in urban computing.""",,,,"The objectives of the study are to investigate representative applications in urban scenarios and demonstrate the effectiveness of the UrbanKG system by showing its superior performance over traditional solutions and its ability to provide more explainable and effective results, specifically in mobility prediction and site selection tasks.",,,,,,,,,,,
Data-centric Artificial Intelligence: A Survey,"Zha Daochen, Bhat Zaid Pervaiz, Lai Kwei-Herng, Yang Fan, Jiang Zhimeng, Zhong Shaochen, Hu Xia",2025,reference-manager,10.1145/3711118,,"Implementation Insights highlight diverse methods for inference data development, including data slicing, algorithmic recourse, adversarial samples, and prompt engineering. Automation levels vary from minimum collaboration to fully automated, learning-based, or programmatic approaches. New insight: Evaluation increasingly focuses on granular, data-centric tasks beyond traditional accuracy metrics.",,,,Programmatic automation: Uses rule-based or statistical programs for tasks like imputing missing values or finding duplicates.,,"What are the necessary tasks, roles of automation and human participation, and current progress in developing, maintaining, and evaluating data to advance data-centric AI?","This paper surveys data-centric AI, aiming to define key concepts, tasks, and challenges. It reviews efficient data collection, integration, visualization, and quality assessment methods, emphasizing automation and human participation. The study organizes literature by goals, analyzes benchmarks, and highlights future research opportunities in data-centric AI.","The research goal is to enhance data-centric AI by addressing data reduction and maintenance; the approach reviews automation and collaboration methods for tasks like data cleaning, feature/sample size reduction, and quality improvement; the principal finding is that both automation and human participation are essential for effective, efficient, and interpretable data management.",
"Learning-based automation: Employs machine learning models to automate tasks such as predicting missing values or optimizing cleaning strategies.
Collaboration-oriented methods: Involve varying degrees of human participation",from full control to minimal involvement,"in processes like data cleaning and labeling.""","Many of the listed research works provide reproducibility, as indicated by """"✓"""". Specifically, 23 out of 36 collected benchmarks incorporate open-source code. However, some works do not provide open-source code, as indicated by """"✗"""". No specific source code links are provided in the context.","Efficient data collection methods, such as dataset discovery and data integration, significantly reduce human labor by leveraging existing datasets and automating relatedness calculations.",,,,,,,,,,,
Visual summarization and clustering for visualization help users gain insights from complex data,but selecting the best visualization format remains controversial.,,,,,,,,,,,,,,
A benchmark survey identified 36 data-centric AI benchmarks (23 with open-source code),using a rigorous filtering process based on relevance,citations,"and publication reputation. No statistical significance (p-values) reported.""","Primary outcomes include evaluation of data-centric AI tasks across the data lifecycle, focusing on data rather than models.",,,,,,,,,,,
Objective assessments use metrics like accuracy,timeliness,consistency,and completeness.,,,,,,,,,,,,
Subjective assessments use trustworthiness,understandability,and accessibility,often via user studies.,,,,,,,,,,,,
"No specific statistical values or measured effects are provided.""","Cleaning and transforming data is challenging due to diverse dataset characteristics, requiring significant time and effort.",,,,,,,,,,,,,,
Designing the appropriate search space for learning-based data preparation methods is difficult and time-consuming.,,,,,,,,,,,,,,,
End-to-end optimization of data storage and retrieval systems is challenging due to data variety and system complexity.,,,,,,,,,,,,,,,
"Other aspects like data access control and system maintenance add further challenges.""","Data visualization leverages human visual processing to aid understanding, but selecting the best format (radial vs. linear charts) is controversial and often requires human input.",,,,,,,,,,,,,,
Data reduction improves model accuracy,efficiency,and interpretability by focusing on essential information and reducing complexity.,,,,,,,,,,,,,
Automation is significant,"but human participation remains essential in tasks like visualization selection and data cleaning.""","Limited research on generating distribution-shifted data to expose model weaknesses, with most benchmarks focusing on detection rather than creation.",,,,,,,,,,,,,
Underexplored areas include evaluation data development and data maintenance compared to training data development.,,,,,,,,,,,,,,,
"Research on graph data as a modality is still emerging and less studied than tabular or image data.""","Future research should explore cross-task automation beyond training data development, develop unified frameworks for automating diverse data-centric AI tasks, and investigate data-model co-design, where data and models evolve together. There is also a need for more benchmarks in underexplored domains and data modalities, such as graph data.",,,"The objectives are to organize data-centric AI into three main goals: 1) training data development, 2) inference data development, and 3) data maintenance, each with specific sub-goals and tasks to improve data quality, evaluation, and management for reliable and insightful AI systems.",,,,,,,,,,,
Enabling knowledge discovery in natural hazard engineering datasets on DesignSafe,"Mehta Chahak, kumar K.",2023,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
"The implementation uses a hybrid human-assisted and automated approach to extract and summarize metadata from scientific datasets
New Insights:",storing summary statistics and relationships in a knowledge graph. Large Language Models help process unstructured data. This enables complex,data-driven queries and advances scientific discovery by revealing hidden relationships.,,,,,,,,,,,,,
Combining domain expertise with automation improves metadata accuracy. The knowledge graph supports advanced queries beyond traditional search,"uncovering scientific phenomena otherwise buried in raw data. Integrating LLMs shows promise for automating metadata extraction from unstructured files.""",,The research goal is to enable advanced knowledge discovery in natural hazard engineering by creating a domain-specific knowledge graph; the approach combines human-assisted metadata extraction and graph modeling; results show this method supports complex queries and data-driven discoveries in liquefaction datasets.,"The paper aims to enable knowledge discovery in natural hazard engineering datasets by constructing a knowledge graph on DesignSafe data. Using a hybrid human-assisted and automated methodology, the study extracts and summarizes metadata, builds a graph database, and integrates LLMs for unstructured data. The approach improves data relationships and supports data-driven discoveries.",,,,,,,,,,Hybrid human-assisted metadata extraction: Combined human input and automated tools to define and extract metadata from tabular CSV data.,"How can constructing a knowledge graph using scientific domain knowledge and metadata extraction enable data-driven discovery and facilitate the identification of complex relationships in large, heterogeneous natural hazard engineering datasets?"
Graph database construction: Used a graph data model (Resource Description Framework,RDF) to represent and query relationships between data entities.,,,,,,,,,,,,,,
LLM-based metadata extraction: Applied Large Language Models (e.g.,"GPT-4) to extract metadata from unstructured XLSX files.""",All code written for the project is open source and can be found at https://github.com/chahak13/tuitus.,The hybrid approach using a domain-specific data synthesis and metadata extraction model enables more comprehensive identification of relationships in natural hazard datasets.,,,,,,,,,,,,
The method demonstrated effectiveness in the “LEAP Liquefaction” case study,allowing complex queries beyond traditional lexical search.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""",Developed a hybrid approach for domain-specific data synthesis and metadata extraction in natural hazard engineering datasets.,,,,,,,,,,,,,,
Used summary statistics (mean,median,count,25th percentile,75th percentile) for numeric columns and frequency for ordinal columns.,,,,,,,,,,,
LLM-based extraction from unstructured XLSX files showed promising initial results.,,,,,,,,,,,,,,,
"No explicit statistical values or measured effects reported.""",Lexical searchers are limited to indexed terms and lack semantic understanding.,,,,,,,,,,,,,,
Text mining engines like elastic search fail to capture relationships between entities.,,,,,,,,,,,,,,,
Fully automated systems may miss crucial domain-specific information.,,,,,,,,,,,,,,,
Heterogeneous and unstructured datasets pose challenges in data extraction.,,,,,,,,,,,,,,,
"LLMs are probabilistic and may produce inconsistent outputs without structured prompts.""","Developed a hybrid, domain-specific data synthesis and metadata extraction model for natural hazard engineering datasets.",,,,,,,,,,,,,,
Knowledge graphs using graph data models (like RDF) effectively capture complex relationships between data entities.,,,,,,,,,,,,,,,
Large Language Models (LLMs) show promise for extracting metadata from unstructured files,but require structured prompts for consistency.,,,,,,,,,,,,,,
"Recommend integrating human expertise and automated tools for accurate metadata extraction and knowledge discovery.""",Current text mining and analytics engines lack semantic understanding and fail to capture relationships between entities.,,,,,,,,,,,,,,
Fully automated systems may miss crucial domain-specific information,requiring both scientific domain knowledge and cyberinfrastructure skills.,,,,,,,,,,,,,,
"Extracting data from heterogeneous and unstructured datasets remains challenging due to diverse types and file formats.""","Future research should focus on building extensive relationships across complex experimental, field, and simulation datasets, constructing knowledge graphs with millions of edges and node clusters, and fostering collaboration between domain experts and LLM-assisted metadata extraction to enable large-scale data-driven discoveries in heterogeneous natural hazard datasets.",,,,,,,,,,,,,,
A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation,"Chen Kai, Wang Ye, Li Yitong, Li Aiping, Yu Han, Song Xin",2024,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
The pipeline first interpolates missing facts,then extrapolates to predict future events. TPAR,a neural-symbolic model,outperforms symbolic methods like TLogic,especially when chronological order is relaxed. TPAR is robust to data sparsity and noise,,effectively integrates interpolation and extrapolation,"The paper aims to improve temporal knowledge graph (TKG) reasoning by integrating interpolation and extrapolation in a unified pipeline. Using sampled ratios of known facts, the study completes missing data (interpolation) and predicts future events (extrapolation). The proposed TPAR model outperforms baselines, showing superior robustness and effectiveness, especially under data sparsity.",,,"How can a unified Temporal PAth-based Reasoning (TPAR) model effectively perform both interpolation and extrapolation reasoning in temporal knowledge graphs, addressing the limitations of existing methods and improving robustness, interpretability, and performance across varying data sparsity levels?","Experimental Pipeline: Designed a two-step process—first, interpolate missing facts using sampled known data, then extrapolate to predict future events.",,"The paper's main objective is to propose a unified Temporal PAth based Reasoning (TPAR) model using a neural-symbolic approach for both interpolation and extrapolation in temporal knowledge graph reasoning; the key method combines Bellman-Ford Shortest Path Algorithm with recursive encoding, and results show TPAR achieves superior, robust performance.",,"and achieves superior performance across settings."""
Chronological Path Analysis: Compared strict versus relaxed chronological order in temporal paths to assess impact on inference methods.,,,,,,,,,,,,,,,
Dataset Splitting: Divided datasets into training,validation,and test sets,"ensuring temporal order for extrapolation and separate query/fact sets for interpolation.""","No source code for the project is provided in the context. However, the context mentions that all four interpolation datasets can be downloaded from https://github.com/soledad921/ATISE and all four extrapolation datasets from https://github.com/Lee-zix/RE-GCN.",,"TPAR achieves the highest link prediction performance in both interpolation (e.g., ICEWS14: Hits@1 57.03%, MRR 65.07%) and extrapolation (e.g., ICEWS14: Hits@1 36.88%, MRR 46.89%) across all four datasets, outperforming all baselines.",,,,,,,,,
TPAR demonstrates exceptional robustness and superior performance under varying data sparsity and when chronological order constraints are relaxed,unlike symbolic methods such as TLogic.,,,,,,,,,,,,,,
The improvements of TPAR over baselines are statistically significant,"but no explicit p-values are provided in the context.""","Primary outcomes are reported for link prediction tasks (Hits@1, Hits@3, Hits@10, MRR) on four datasets (ICEWS14, YAGO11k, ICEWS05-15, WIKIDATA12k).",,,,,,,,,,,,,
TPAR (Ours) achieves the highest results in both interpolation and extrapolation settings across all datasets.,,,,,,,,,,,,,,,
Example interpolation results (ICEWS14): Hits@1: 57.03%,Hits@3: 69.74%,Hits@10: 80.41%,MRR: 65.07%.,,,,,,,,,,,,
Example extrapolation results (ICEWS14): Hits@1: 36.88%,Hits@3: 52.28%,Hits@10: 65.89%,MRR: 46.89%.,,,,,,,,,,,,
TPAR outperforms baselines such as TLogic and RPC in all reported metrics.,,,,,,,,,,,,,,,
Increasing maximum path length improves TPAR performance (e.g.,YAGO11k Hits@1 rises from 10.03% at length 1 to 17.35% at length 5).,,,,,,,,,,,,,,
Relaxing chronological order in paths increases TPAR performance but decreases TLogic performance.,,,,,,,,,,,,,,,
"Statistical values are reported precisely in tables for all metrics and settings.""",No information available,TPAR consistently outperforms all baselines in both interpolation and extrapolation link prediction tasks across four datasets.,,,,,,,,,,,,,
TPAR demonstrates superior robustness and effectiveness,especially under high data sparsity.,,,,,,,,,,,,,,
Integrating neural and symbolic-based approaches in TPAR yields significant performance gains.,,,,,,,,,,,,,,,
"Recommendation: Use TPAR for robust and accurate temporal knowledge graph reasoning.""","Lack of unified methods: Most existing approaches address either interpolation or extrapolation, not both simultaneously.",,,,,,,,,,,,,,
"Performance drop in pipeline setting: Combining state-of-the-art interpolation and extrapolation methods leads to decreased reasoning performance.
Symbolic reasoning limitations: Symbolic methods struggle with ambiguous and noisy data","highlighting the need for more robust approaches.""","Future research should focus on developing unified methods that can handle both interpolation and extrapolation reasoning, as current approaches are mostly specialized for one setting. Further investigation is needed to address the performance drop in pipeline settings and to improve robustness and interpretability in ambiguous or noisy data.","The study design is an experimental pipeline with three settings (A: 60%, B: 70%, C: 80% fact sampling ratios). It uses time-wise filtered evaluation, splits datasets into training/validation/test by time, and compares state-of-the-art interpolation and extrapolation methods. Grid search is used for hyperparameter tuning.",,,The objectives of the study are to test the hypothesis that completing missing knowledge about the past (interpolation) can enhance the accuracy of predicting future knowledge (extrapolation) by designing a novel experimental pipeline and evaluating various methods on temporal knowledge graph reasoning tasks.,,,,,,,,,
Pseudo-Knowledge Graph: Meta-Path Guided Retrieval and In-Graph Text for RAG-Equipped LLM,"Yang Yuxin, Wu Haoyang, Wang Tao, Yang Jia, Ma Hao, Luo Guojie",2025,reference-manager,,,"The Implementation Insights highlight that combining traditional NLP extraction, LLM-based extraction, and embedding in-graph text chunks in the PKG Builder significantly improves performance. Iterative extraction and verification ensure data completeness and accuracy. Integrating multiple retrieval methods enhances LLM reasoning, scalability, and domain adaptability. No new insights beyond these findings are uncovered.",,,,"Regular Expression Retrieval: Finds precise pattern-based matches, such as exact researcher names or keywords.",,"How can integrating meta-path retrieval with large language models improve the accuracy, coherence, and comprehensiveness of answers in complex multi-hop research queries within heterogeneous knowledge graphs?","The paper introduces a meta-path retrieval method for efficient information extraction from a PKG (Property Knowledge Graph). By pre-constructing meta-paths and using a lightweight, context-aware model, the approach enables rapid, multi-hop exploration of complex relationships. Experiments show improved accuracy, coherence, and comprehensiveness over traditional retrieval methods.","The paper’s main objective is to improve multi-hop retrieval in knowledge graphs; its key method combines pre-constructed meta-paths with a lightweight, context-aware model for efficient meta-path retrieval; principal finding: this approach significantly enhances retrieval efficiency and enables deeper, more insightful relational analysis across domains.",
Vector Retrieval: Identifies contextually related information by measuring semantic similarity,expanding the scope beyond exact matches.,,,,,,,,,,,,,,
Meta-path Retrieval: Uncovers complex,"multi-hop relationships between entities by traversing pre-constructed node sequences in the knowledge graph.""",,"The LLM-PKG approach outperforms other retrieval methods in accuracy, coherence, and comprehensiveness, providing detailed explanations of specific biotechnologies and their agricultural applications.",,,,,,,,,,,,
Meta-path retrieval enables efficient exploration of complex,multi-hop relationships,reducing query latency and uncovering intricate connections.,,,,,,,,,,,,,
"No explicit quantitative results or p-values are provided in the context.""","LLM-PKG outperforms other methods in accuracy, coherence, and comprehensiveness.",,,,,,,,,,,,,,
LLM-PKG provides detailed,relevant answers,clearly identifying specific biotechnologies and their impacts.,,,,,,,,,,,,,
Performance metrics (selected examples from Table 1): LLM-PKG achieves 61.4,48.9,23.1,75.7,85.3,,33.5,"LLMs struggle to retrieve specific information from large data collections, leading to incomplete or imprecise answers.",,,,,,"28.9 (Open Compass/MultiHop-RAG).""",82.3,69.0
Ensuring authenticity and managing private data with LLMs is challenging and costly,especially with frequent model updates.,,,,,,,,,,,,,,
LLMs cannot inherently verify the truthfulness of outputs,requiring third-party fact extraction.,,,,,,,,,,,,,,
Retrieval-Augmented Generation (RAG) is limited when information is scattered,databases are large with low information density,or redundancy is high.,,,,,,,,,,,,,
Traditional RAG systems struggle to identify relationships between information pieces and often rely on insufficient top-1 or top-3 similarity results.,,,,,,,,,,,,,,,
"Vector databases lack mechanisms to ensure diversity and comprehensive fact retrieval.""","The LLM-PKG approach outperforms other retrieval methods in accuracy, coherence, and comprehensiveness.",,,,,,,,,,,,,,
Integrating Regular Expression,Vector,and Meta-path Retrieval creates a more effective and sophisticated retrieval system.,,,,,,,,,,,,,
LLM-PKG provides detailed,up-to-date,and well-structured insights on emerging biotechnologies in agriculture.,,,,,,,,,,,,,
"Recommendation: Combine multiple retrieval methods for optimal answer quality and reasoning.""","Adapting PKG to support multi-turn conversational interactions for more dynamic, context-aware dialogues.",,,,,,,,,,,,,,
Optimizing PKG’s scalability and computational efficiency for real-time applications and large-scale deployments.,,,,,,,,,,,,,,,
Enhancing the accuracy,coherence,"and comprehensiveness of answer generation in complex reasoning tasks.""","Future research should focus on adapting PKG for multi-turn conversations, optimizing scalability and computational efficiency for large-scale deployments, and further improving accuracy, coherence, and comprehensiveness in answer generation. These directions address current limitations and aim to enhance PKG’s capabilities and applicability.",,,,,,,,,,,,
Continuous Personalized Knowledge Tracing: Modeling Long-Term Learning in Online Environments,"Wang Chunpai, Sahebi Shaghayegh",2023,reference-manager,10.1145/3583780.3614822,,Implementation Insights Summary:,,,,,,,,,
CPKT consistently outperforms six baseline models across four real-world datasets,especially on longer student sequences. Personalization and the TA-SSE (transition-aware stochastic shared embeddings) components each improve performance,with their combination yielding the largest gains. TA-SSE also helps prevent overfitting in personalized,"data-scarce scenarios.""",,,"The research goal is to improve long-term student performance prediction by introducing CPKT, a personalized and continuous knowledge tracing approach using transition-aware stochastic shared embeddings (TA-SSE); results show CPKT consistently outperforms six baselines across four real-world datasets.",Comparison of CPKT with six state-of-the-art knowledge tracing (KT) baseline methods for student performance prediction.,,,,,,How does the proposed CPKT model address personalization and continuity challenges in student performance prediction compared to state-of-the-art knowledge tracing methods?,,"The paper proposes CPKT, a model for student performance prediction that addresses personalization and continuous learning. Using four real-world datasets and ablation studies, CPKT outperforms six baseline models. The combination of personalization and TA-SSE components yields the best results, especially for longer learning sequences."
Use of real-world datasets (MORF,ASSIST2015,EdNet,Junyi) for experimental evaluation.,,,,,,,,,,,,
"Statistical significance testing (p-value < 0.05) to assess performance improvements.""","The research is reproducible. The source code for CPKT is available at https://tinyurl.com/mr9c9h5c. Four real-world datasets (MORF, ASSIST2015, EdNet, Junyi) are used, with detailed preprocessing and experimental setups described, including 5-fold user-stratified cross-validation and online training/testing procedures.","CPKT significantly outperforms all baseline models across four real-world datasets, with p-value < 0.05 indicating statistical significance.",,,,,,,,,,,,,
CPKT achieves higher AUC and lower BCE,especially for users with longer learning trajectories; for long trajectories,p-values are 1.51e-05 (AUC) and 1.70e-19 (BCE) vs. DKVMN.,,,,,,,,,,,,,
"Both personalization and SSE components are essential for optimal CPKT performance; neither can be omitted without reducing effectiveness.""",CPKT significantly and consistently outperforms all baselines on all datasets.,,,,,,,,,,,,,,
On MORF (RMSE): CPKT 0.1752 ± 0.0081\* (best),DKT 0.1990 ± 0.0087,DKVMN 0.1995 ± 0.0067.,,,,,,,,,,,,,
On ASSIST2015 (AUC): CPKT 0.7274 ± 0.0032\*,DKT 0.7142 ± 0.0029,IEKT 0.7204 ± 0.0027.,,,,,,,,,,,,,
On EdNet (AUC): CPKT 0.6558 ± 0.0072\*,DKT 0.6349 ± 0.0048,DKVMN 0.6291 ± 0.0070.,,,,,,,,,,,,,
"On Junyi (AUC): CPKT 0.8802 ± 0.0072\*
indicates statistical significance (p-value < 0.05) over best baseline.",DKT 0.8709 ± 0.0072,IEKT 0.8721 ± 0.0026.,,,,,,,,,,,,,
"Removing personalization or SSE components reduces CPKT performance.
For EdNet",CPKT’s AUC increases with longer student trajectories; DKT and DKVMN do not show this trend.,,,,,,,,,,,,,,
In trajectory length analysis (EdNet):,,,,,,,,,,,,,,,
Long group AUC: CPKT 0.6475,DKT 0.6413,DKVMN 0.6315 (p=0.0745 vs DKT,p=1.51e-05 vs DKVMN).,,,,,,,,,,,,
Long group BCE: CPKT 0.6389,DKT 0.6562,DKVMN 0.6942 (p=4.71e-05 vs DKT,p=1.70e-19 vs DKVMN).,,,,,,,,,,,,
AUC: Area Under the Curve,a measure of prediction accuracy (higher is better).,,,,,,,,,,,,,,
RMSE: Root Mean Square Error,a measure of prediction error (lower is better).,,,,,,,,,,,,,,
BCE: Binary Cross Entropy,"a measure of prediction error for binary outcomes (lower is better).""","Maximum sequence lengths for ASSIST2015, EdNet, and Junyi datasets were limited to 1,000, possibly omitting data for users with longer sequences.",,,,,,,,,,,,,
Online learning ablation could not be performed separately due to its integration with personalization.,,,,,,,,,,,,,,,
"No further explicit limitations or suggestions for future research are provided.""","CPKT significantly and consistently outperforms all baseline models across four real-world datasets, especially for students with longer learning trajectories.",,,,,,,,,,,,,,
Both personalization and TA-SSE components individually improve performance,but their combination yields the greatest benefit.,,,,,,,,,,,,,,
CPKT is superior for continuous knowledge tracing and long-term modeling compared to baselines.,,,,,,,,,,,,,,,
"TA-SSE helps prevent overfitting and can be applied to other sequential models.""",,,"The study is an experimental, comparative evaluation using four real-world datasets. It uses a five-fold cross-validation design, compares a new model (CPKT) with six baseline knowledge tracing methods, and includes ablation studies. The study is not randomized, blinded, controlled, or placebo-controlled.",,,"The objectives of the study are to compare the proposed CPKT model with six state-of-the-art knowledge tracing methods for student performance prediction, focusing on addressing personalization and continuity challenges without requiring additional context or features, and to answer three research questions about model performance, component effects, and trajectory length impact.",,,,,,,,,
Bring Privacy To The Table: Interactive Negotiation for Privacy Settings of Shared Sensing Devices,"Zhou Haozhe, Goel Mayank, Agarwal Yuvraj",2024,reference-manager,10.1145/3613904.3642897,,"Implementation Insights highlight that privacy negotiations for shared IoT devices are complex due to user unawareness, social dynamics, and technical barriers. Key new insights include the need for efficient, low-burden systems, support for anonymity, synchronized negotiations, balanced participation, and clear preference management. The ThingPoll prototype addresses these challenges.",,,,"Iterative design process: The study used repeated cycles of designing, building, and evaluating the ThingPoll negotiation system.",,How can a negotiation system like ThingPoll effectively mediate privacy and functionality conflicts among users of smart home IoT devices by eliciting and modeling user preferences during the negotiation process?,"The paper introduces ThingPoll, a negotiation system for shared smart home privacy settings. Using iterative design, a formative study (N=12), and a summative evaluation (N=30), the study found ThingPoll led to high user satisfaction (83.3%), low workload, and effective privacy negotiation, supporting equitable and transparent decision-making.","The research goal was to design and evaluate ThingPoll, an IoT privacy negotiation system; using an iterative design and user studies, the approach enabled negotiation among users, and the principal finding was that negotiation produced the highest satisfaction (83.3%) compared to Homeowner, Majority, and Veto Vote (No Negotiation) approaches.",
Formative and summative studies: Researchers first conducted a formative study (N=12) to observe verbal privacy negotiations,then a summative evaluation (N=30) using simulated negotiation scenarios.,,,,,,,,,,,,,,
Simulated negotiation scenarios: Participants engaged in hypothetical smart home privacy negotiations,"with social contexts (close contacts vs. acquaintances) assigned to groups.""",,"The Negotiation Approach achieved the highest satisfaction rate (83.3% overall; 70% homeowners, 90% guests), outperforming Majority Vote, Veto Vote, and Homeowner’s Preference.",,,,,,,,,,,,
ThingPoll showed high usability and low workload: physical workload (mean = 24.6),frustration (mean = 26.7),temporal demand (mean = 31.2),but moderate mental demand (mean = 44.3).,,,,,,,,,,,,
All users felt informed about data collection,"and most agreed that ThingPoll effectively considered both their own and others’ privacy; no p-values or statistical significance were reported.""","The Negotiation approach achieved the highest overall satisfaction rate at 83.3% (homeowners: 70%, guests: 90%).",,,,,,,,,,,,,
ThingPoll showed high usability and low workload: physical workload (mean = 24.6),frustration (mean = 26.7),temporal demand (mean = 31.2),but higher mental demand (mean = 44.3).,,,,,,,,,,,,
Automatic suggestions reduced user effort (20% strongly agree).,,,,,,,,,,,,,,,
Users valued the ability to share needs and concerns,"contributing to high satisfaction.""",Used simulated scenarios due to difficulty recruiting real-world participants with compatible devices and privacy disclosure.,,,,,,,,,,,,,
Limited to participants familiar with IoT devices; unfamiliar users may have different perspectives.,,,,,,,,,,,,,,,
Small sample size (N=30) limits statistical power; findings are exploratory.,,,,,,,,,,,,,,,
Assumed participants made well-informed privacy choices,which may not reflect reality.,,,,,,,,,,,,,,
Assumed complete and accurate device behavior disclosure,which may not hold true.,,,,,,,,,,,,,,
Simplified ownership and social relationships; real-world situations can be more complex.,,,,,,,,,,,,,,,
Social dynamics and design requirements may need adaptation for varied living arrangements.,,,,,,,,,,,,,,,
Negotiation process may be inefficient or unfair depending on user flexibility and steadfastness.,,,,,,,,,,,,,,,
Some users noted lack of anonymity and synchronization as issues.,,,,,,,,,,,,,,,
"General user education on device privacy features may be needed.""","The negotiation approach achieved the highest satisfaction (83.3%), with both homeowners (70%) and guests (90%) reporting positive experiences.",,,,,,,,,,,,,,
ThingPoll demonstrated high usability and low workload,though some participants found the process mentally demanding.,,,,,,,,,,,,,,
Key design recommendations include supporting balanced participation,minimizing unnecessary explanations,and improving preference tracking.,,,,,,,,,,,,,
"Findings are exploratory due to simulated scenarios and a limited sample size (N=30).""
Real-world","The ethical balance and acceptable distribution of power between homeowners and guests in privacy negotiations remains unresolved.
in-the-wild studies are needed to better understand negotiation dynamics beyond simulated scenarios.",,,,,,,,,,,,,,
"Ensuring equitable opportunities for all users to express preferences during negotiations is still a challenge.""
Study 2: Summative","Future research should explore enabling anonymity for users, asynchronous negotiation mechanisms, and allowing more freedom in expressing preferences. Studies with larger and more diverse participant samples, real-world (in-the-wild) deployments, and consideration of users unfamiliar with IoT devices are also recommended. The balance of power and fairness remains an open question.
non-controlled","Study 1: Observational, formative study with 12 participants, focused on verbal privacy negotiation in smart homes.
multi-user user study with 30 participants",divided into groups of three,using a structured,,system-mediated negotiation approach. Included profile generation and simulated negotiation phases. Not randomized,"The objectives of the study are to observe and analyze how humans negotiate IoT privacy settings, inform the design of the ThingPoll negotiation system, and evaluate the practicality, efficiency, user experience, and adoption willingness of ThingPoll in shared smart home environments.",,,,,,,"not placebo-controlled. Also included an mTurk survey (N=198).""",not blinded
Discerning Individual Preferences for Identifying and Flagging Misinformation on Social Media,"Barman Dipto, Koidl Kevin, Conlan Owen",2024,reference-manager,10.1145/3627043.3659545,,The paper implemented a flag-based warning system with explanatory text to help users judge headline accuracy and reduce misinformation spread. Findings show explanatory narratives increase flag credibility and user trust. New insights suggest personalizing warning systems and using inoculation messages can further improve effectiveness. Limitations include self-report bias and emotional topic influence.,,,,"Within-subject design simulating a social media environment with misinformation flags, some enriched with explanatory context.",,"How do different types of misinformation flagging systems, with and without explanatory context, and individual user characteristics influence social media users’ perception of accuracy, sharing intent, and trust in the flagging system?","The study investigates whether adding explanatory text to misinformation flags on social media improves users’ ability to judge news accuracy. Using a within-subject design with 348 US participants, results show that explanatory narratives increase flag credibility and user understanding, supporting more effective misinformation countermeasures.","The research goal was to evaluate the effectiveness of flag-based misinformation warning systems on social media; using a within-subject design, the study found that adding explanatory text to misinformation flags increases their credibility and helps users better judge news accuracy.",
Online experiment conducted via Qualtrics,recruiting 384 American participants through Prolific with quota matching for gender balance.,,,,,,,,,,,,,,
Use of self-report questionnaires to measure accuracy perception and trust,"including attention checks and exclusion criteria for data quality.""","The research used a within-subject design with 384 American participants, conducted online via Qualtrics. There is no explicit mention of source code availability. The study is described in detail, but reproducibility may be limited without access to the code or materials.","Flagging fake news, especially with explanatory text, significantly reduces perceived accuracy and sharing intent, enhancing users’ ability to distinguish true from false information.",,,,,,,,,,,,
The Kruskal-Wallis and Spearman correlation analyses revealed significant relationships between user characteristics and responses to flagging systems.,,,,,,,,,,,,,,,
"Explanatory text on misinformation flags increases trust and effectiveness; statistical significance (p-values) is implied but not explicitly reported.""",Adding explanatory text to misinformation flags increases their credibility and effectiveness.,,,,,,,,,,,,,,
Enhanced trust in flags with explanations leads to higher adherence to warnings and less sharing of false information.,,,,,,,,,,,,,,,
Spearman correlation and Kruskal-Wallis tests show significant relationships between user characteristics and responses to flagging systems.,,,,,,,,,,,,,,,
Numerical/statistical values are referenced in Figures 4,5,and 6,"but not explicitly provided in the context.""","Data collection relied on self-reports, which may cause social desirability bias.",,,,,,,,,,,
Participants’ interpretation of questions could lead to inconsistent answers.,,,,,,,,,,,,,,,
Focus on COVID-19-related headlines may limit generalizability to less emotional topics.,,,,,,,,,,,,,,,
"Long-term effects and personalization of flagging systems were not investigated.""",Adding explanatory text to misinformation flags on social media increases their credibility and helps users better judge news accuracy.,,,,,,,,,,,,,,
Personalized and nuanced flagging approaches may be more effective than generic solutions.,,,,,,,,,,,,,,,
Future research should explore long-term effects and personalization of misinformation flags.,,,,,,,,,,,,,,,
"Self-reporting and emotionally charged topics are study limitations.""",Investigate the long-term effects of inoculation messages embedded in misinformation flagging systems.,,,,,,,,,,,,,,
Explore the use of Large Language Models (LLMs) to generate inoculation messages.,,,,,,,,,,,,,,,
"Study the impact of personalized misinformation flagging approaches versus “one size fits all” solutions.""","Future research should address limitations of self-reported data, explore effects of misinformation flags on less emotional topics, investigate long-term impacts of inoculation messages, personalize flag designs for different user groups, and examine how user characteristics influence responses to various flagging systems.","Within-subject design; online experiment; simulated social media environment; some headlines marked with misinformation flags (with or without explanatory context); random selection of headlines; quota-matched sample; survey of demographics, cognitive reflection, and personality; Spearman correlation and Kruskal-Wallis test used for analysis.",,The objectives of the study are to: (1) assess the impact of warning flags on users’ accuracy perception and sharing intent; (2) determine if explanations from fact-checking websites enhance trust in the flagging system; and (3) examine how user characteristics affect reactions to the flagging system.,,,,,,,,,,,
DPSUR: Accelerating Differentially Private Stochastic Gradient Descent Using Selective Update and Release,"Fu Jie, Ye Qingqing, Hu Haibo, Chen Zhili, Wang Lulu, Wang Kuncan, Ran Xun",2024,reference-manager,10.14778/3648160.3648164,,"The implementation insights highlight that differential privacy (DP) mechanisms, such as DPSGD and DPSUR, are used to protect sensitive data in deep learning. Utility analysis in high-dimensional spaces is crucial, and enhancements to local differential privacy (LDP) mechanisms improve performance. New insight: Adaptive techniques and noise calibration are key for balancing privacy and utility.",,,,Experimental evaluation using four real datasets and popular machine learning models to assess DPSUR's performance.,,"How can the proposed DPSUR framework, which incorporates selective updates and release mechanisms, achieve rigorous differential privacy guarantees while maximizing model accuracy compared to state-of-the-art solutions?","The paper proposes DPSUR, an algorithm designed to improve differentially private model training. It combines traditional DPSGD with a selective update mechanism, using minimal clipping and Gaussian noise. Experiments show DPSUR outperforms competitors in classification accuracy while maintaining strong privacy guarantees, especially on image datasets.","The research goal is to improve differentially private deep learning; the approach, DPSUR, combines DPSGD with a selective update mechanism using minimal clipping and Gaussian noise; results show DPSUR achieves better utility than DPSGD while maintaining rigorous privacy guarantees.",
Comparison with baseline methods,including DPSGD and its variants (DPSGD-IS,DPSGD-HF,DPSGD-TS,DPAGD).,,,,,,,,,,,
"Implementation of member inference attacks to evaluate the privacy-preserving effect of DPSUR.""
On CIFAR-10",DPSUR even surpasses non-private results at 𝜋 = 4; on MNIST,"DPSUR consistently outperforms all competitors across all datasets and privacy budgets, with at least 1% higher classification accuracy than the second best on three datasets.
the improvement is less significant.",,,,,,,,,,,,,
"The paper provides formal privacy guarantees using Rényi Differential Privacy (RDP) and shows reduced privacy loss due to selective model updates. No explicit p-values are reported.""
On MNIST
On FMNIST
On CIFAR-10
On IMDb","DPSUR consistently outperforms all competitors across all datasets and privacy budgets.
DPSUR accuracy is 98.95% at 𝜋 = 4; non-private is 99.11%.
DPSUR accuracy is 90.18% at 𝜋 = 4; non-private is 90.98%.
DPSUR accuracy is 71.45% at 𝜋 = 4; non-private is 71.12% (DPSUR outperforms non-private).
DPSUR accuracy is 74.14% at 𝜋 = 4; non-private not provided.",,,,,,,,,,,,,,
DPSUR’s classification accuracy is at least 1% higher than the second best on FMNIST,CIFAR-10,and IMDb.,,,,,,,,,,,,,
DPSUR minimizes the loss function and reduces privacy loss consumption.,,,,,,,,,,,,,,,
Moderate noise in SGD can help neural networks escape local minima,"explaining DPSUR’s superior performance.""",,"DPSUR consistently outperforms all competitors in classification accuracy across all datasets and privacy budgets, with at least 1% higher accuracy than the second best (except MNIST).",,,,,,,,,,,,
DPSUR achieves near non-private performance at higher noise levels and can even surpass non-private results due to beneficial noise.,,,,,,,,,,,,,,,
The minimal clipping and selective update strategies maximize utility while controlling privacy loss.,,,,,,,,,,,,,,,
"Recommendation: Optimize the acceptance threshold to balance model utility and convergence.""",Extending DPSUR to larger models and datasets.,,,,,,,,,,,,,,
Theoretical analysis of DPSUR’s convergence speed.,,,,,,,,,,,,,,,
"Exploring optimal threshold selection to maximize utility gain from model updates.""",,,,"The objectives of the study are to select model updates that minimize the loss function while achieving differential privacy, and to reduce privacy loss consumption by deriving the RDP (Rényi Differential Privacy) for the selective Gaussian mechanism. The study aims to improve classification accuracy under privacy constraints.",,,,,,,,,,,
Comprehensive Personal Health Knowledge Graph for Effective Management and Utilization of Personal Health Data,"Hendawi Rasha, Li Juan",2024,reference-manager,10.1109/aimhc59811.2024.00026,,"The PHKG enables integration of diverse personal health data into a unified knowledge graph, supporting detailed personal profiling and improved healthcare decisions. Semantic search and standardized ontologies (HL7, FHIR) enhance usability and interoperability. New insights include enhanced user control, data security, and the potential for personalized health interventions.",,"The PHKG (Personal Health Knowledge Graph) enables comprehensive integration and management of diverse personal health data, addressing challenges like interoperability, privacy, and security.",,No information available,,"How can a Personal Health Knowledge Graph (PHKG) be developed to effectively integrate, manage, and utilize diverse personal health data sources for improved healthcare outcomes and research?","The paper proposes a Personal Health Knowledge Graph (PHKG) to integrate and organize diverse personal health data from sources like EHRs and wearable devices. Using semantic technologies and ontologies, the PHKG enables comprehensive health profiling. Results show improved data integration, with implications for personalized care and healthcare research.","The research goal is to address challenges in personal health data management by proposing a Personal Health Knowledge Graph (PHKG) approach that integrates diverse data sources, with results showing PHKG enables comprehensive health profiling and offers significant potential for improved healthcare outcomes and research.",
PHKG facilitates detailed personal profiling and behavioral understanding,supporting improved healthcare outcomes and research.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""","The proposed PHKG (Personal Health Knowledge Graph) enables integration and management of diverse personal health data (PHD) from sources like EHRs, wearable sensors, mobile apps, and social determinants of health.",,,,,,,,,,,,,,
PHKG offers a unified,comprehensive knowledge graph,addressing challenges in data interoperability,privacy,and security.,,,,,,,,,,,
"No explicit statistical values or quantitative results are provided.""",Challenges persist in ensuring data integration and interoperability.,,,,,,,,,,,,,,
Privacy and data security remain paramount concerns that demand rigorous solutions.,,,,,,,,,,,,,,,
Future research is needed to refine knowledge graph embeddings,improve natural language-driven query mechanisms,"and explore additional use cases and applications.""","The PHKG provides a unified and comprehensive solution for integrating diverse personal health data, addressing interoperability, privacy, and security challenges.",,,,,,,,,,,,
It enables advanced personalized health profiling and deeper insights into patient behavior and health trends.,,,,,,,,,,,,,,,
"The PHKG framework is recommended as a foundation for improved healthcare outcomes and research.""",Need for research on empowering users with easy and transparent access to their own health data through personal knowledge graphs.,,,,,,,,,,,,,,
Addressing privacy and security concerns to ensure only authorized access to personal health data.,,,,,,,,,,,,,,,
Further development of knowledge graph embeddings and natural language-driven query mechanisms,"and exploration of additional healthcare applications.""","Future research should focus on refining knowledge graph embeddings, improving natural language-driven query mechanisms, and exploring additional use cases and applications within healthcare. Addressing privacy and security concerns also remains a critical area for further investigation.",,,,"The objectives of the study are to propose a Personal Health Knowledge Graph (PHKG) for integrating and managing diverse personal health data, address challenges like interoperability, privacy, and security, and demonstrate practical applications to improve healthcare outcomes and research through comprehensive data analysis and utilization.",,,,,,,,,
Personal Research Knowledge Graphs,"Chakraborty Prantika, Dutta Sudakshina, Sanyal Debarshi Kumar",2022,reference-manager,10.1145/3487553.3524654,,Implementation Insights:,,,,,,,,,
The prototype PRKG was built using Neo4j,populated with facts about the researcher using SpERT for entity and relation extraction from scholarly papers. SpERT was trained on the SciERC dataset. New insights: Extracting entities from private sources (emails,"conversations) is less explored; privacy is a key concern.""",,"The paper's main objective is to define and develop personal research knowledge graphs (PRKGs) for researchers by extracting entities and relations from various sources, using methods like deep neural networks, with the principal finding that PRKGs can personalize and enhance scholarly applications and researcher-machine interactions.",,"The paper introduces the concept of a personal research knowledge graph (PRKG) for researchers, focusing on capturing entities and relations related to their research activities. It discusses manual and automated NLP-based methods for populating PRKGs, highlights privacy and sharing concerns, and suggests applications in personalized scholarly tools.",,,,,,,"Activity tracking: Monitoring researcher activities (e.g., downloaded papers, search queries) and extracting entities from them.","How can personal research knowledge graphs (PRKGs) be defined, populated, and shared to effectively support personalized, knowledge-aware applications for researchers while addressing challenges of data collection, privacy, and security?",
Automatic extraction using NLP: Using natural language processing techniques to extract entities and relations from structured or unstructured sources.,,,,,,,,,,,,,,,
"Manual curation: Researcher manually curates entities and relations in the knowledge graph.""",,"The paper introduces the concept of a personal research knowledge graph (PRKG) for researchers, focusing on capturing entities and relations relevant to their research activities.",,,,,,,,,,,,,
PRKGs can be populated manually or automatically using NLP techniques,with trade-offs between accuracy and scalability; privacy and fine-grained access control are important considerations.,,,,,,,,,,,,,,
No quantitative results,statistical significance,"or p-values are reported in the context.""",Primary outcomes:,,,,,,,,,,,,
A prototype Personal Research Knowledge Graph (PRKG) was implemented for a researcher (Sunita) using Neo4j.,,,,,,,,,,,,,,,
Entities and relations were extracted from scholarly papers using SpERT,trained on the SciERC dataset.,,,,,,,,,,,,,,
Extracted facts included research interests,managed labs,tasks,and methods.,,,,,,,,,,,,
Results and measured effects:,,,,,,,,,,,,,,,
Example extracted facts: (‘Sunita’,‘interest’,‘NLP’),(‘Sunita’,‘manages’,,‘NLPLab’),(‘translation’,,,‘word sense disambiguation’).,,‘method’,‘translation’),‘task’,(‘CCLINC’
"No statistical values reported.""",Manual curation of entities and relations is less error-prone but not scalable due to high human intervention.,,,,,,,,,,,,,,
Automatic extraction using NLP techniques can be error-prone.,,,,,,,,,,,,,,,
Extracting certain knowledge entities (like tasks,methods,equipment,ongoing activities) is challenging.,,,,,,,,,,,,
"Limited support for fine-grained access control in existing graph databases.""","Personal research knowledge graphs (PRKGs) provide structured, machine-actionable knowledge about a researcher, supporting personalized applications like search engines and AI assistants.",,,,,,,,,,,,,,
Accurate and timely extraction of relevant entities and relations from diverse sources is challenging.,,,,,,,,,,,,,,,
Security and privacy are critical when implementing and sharing PRKGs.,,,,,,,,,,,,,,,
"Future work includes developing easy-to-use PRKG frameworks and AI chat-bots powered by PRKGs.""",Accurate and timely collection of personally relevant entities and relations from diverse sources is a non-trivial challenge.,,,,,,,,,,,,,,
Security and privacy must be prioritized when implementing,deploying,and sharing PRKGs.,,,,,,,,,,,,,
Extraction of scholarly entities and relations from private sources like emails,conversations,"or social media posts remains less explored.""","Future research should address privacy concerns when tracking researcher activities and extracting information from diverse sources, including multilingual conversations. There is also a need to develop frameworks for easy PRKG creation and to implement AI chat-bots powered by PRKGs. Accurate, timely entity extraction remains a challenge.",,,,,,,,,,,,No information available
Towards Self-organizing Personal Knowledge Assistants in Evolving Corporate Memories,"Jilek Christian, Schröder Markus, Maus Heiko, Schwarz Sven, Dengel Andreas",2023,reference-manager,,,"The paper highlights evaluation challenges in PIM and knowledge work, such as subjectiveness, missing datasets, and privacy issues. To address these, a multi-lane evaluation strategy was used, including user studies, inquiries, and data-driven studies. Implementation insights include Managed Forgetting, condensation, and context-based note-taking for improved user support.",,,,"Short-term studies: Conducted in controlled laboratory settings with 20–50 participants for 30–90 minutes, sometimes using simulated scenarios.",,"How can self-organizing personal knowledge assistants be developed and evaluated to effectively support personal information management and knowledge work, particularly through automation and Managed Forgetting, while addressing challenges such as subjectiveness, privacy, and evaluation methodology?","The paper addresses evaluation challenges in Personal Information Management (PIM) and knowledge work support by developing a multi-lane evaluation strategy. Using short-term lab studies, medium/long-term studies, inquiries, and data-driven approaches, the authors balance methodological rigor, privacy, and dataset limitations. The strategy enables flexible, context-appropriate evaluations.","The research goal was to address evaluation challenges in PIM and knowledge work support by developing a multi-lane evaluation strategy, using both short-term controlled studies and longer-term real-world studies, with results showing this approach effectively balances methodological rigor, participant numbers, and privacy concerns.",
Medium- or long-term studies: Lasting weeks to years,with fewer participants,allowing natural observation of effects over time.,,,,,,,,,,,,,
Inquiries: Participants perform thought experiments or hypothetical scenarios and provide feedback,enabling large-scale,"low-cost data collection.""","The research uses multiple evaluation strategies, including short-term, medium-term, and long-term studies. Data and code are not fully publicly available due to privacy, copyright, and non-disclosure issues. Some demo material is available at https://www.dfki.uni-kl.de/~mschroeder/demo/spread2rml/.","The multi-lane evaluation strategy included short-term studies (20–50 participants, 30–90 minutes), medium/long-term studies (fewer participants, several weeks to years), inquiries (e.g., 140 participants), and data-driven studies.",,,,,,,,,,,
Privacy-preserving measures were effective: in a five-month study,6 of 7 participants agreed with privacy measures.,,,,,,,,,,,,,,
"No statistical significance (p-values) or quantitative results beyond participant counts were reported.""","Short-term studies (20–50 participants, 30–90 min exposure): Significant effects found (p < 0.001), classified as strong effect.",,,,,,,,,,,,,,
Multi-month user study (7 participants,up to 5 months): 249 days,46,552 user events,173 context spaces,,165 files,,,,,,,,393 webpages.,8
All participants agreed cSpaces was easy to understand,use,"and remember; features aligned with user needs and mental models.""","Short-term studies had limited exposure (30–90 minutes), requiring artificial simulation of some aspects.",,,,,,,,,,,,
Medium/long-term studies involved fewer participants due to cost and time.,,,,,,,,,,,,,,,
Publishing full data is restricted by privacy,copyright,and non-disclosure issues.,,,,,,,,,,,,,
"Anonymization/obfuscation methods risk de-anonymization attacks.""","A multi-lane evaluation strategy, combining short-term and long-term studies, effectively addresses challenges in PIM and knowledge work support research.",,,,,,,,,,,,,,
Participants found cSpaces easy to understand,use,and more aligned with their mental models than traditional systems.,,,,,,,,,,,,,
Privacy by design is essential; most participants felt comfortable with implemented privacy measures.,,,,,,,,,,,,,,,
"Recommendation: Continue implementing privacy-protecting features and multi-lane evaluations for robust results.""",Lack of publicly available datasets containing comprehensive user contexts and all relevant content due to privacy and copyright issues.,,,,,,,,,,,,,,
Existing datasets often lack necessary semantic information and neglect important data sources or structures.,,,,,,,,,,,,,,,
"Need for improved evaluation strategies and enriched datasets to better reflect real-world Personal Information Management (PIM) scenarios.""","Future research should focus on creating publicly available datasets that include user contexts and all relevant content, addressing privacy and copyright issues. Further investigation is needed into privacy-preserving data collection, enriching datasets, and evaluating context-aware systems with larger, more diverse participant groups.","Short-term studies: Controlled laboratory conditions, large participant numbers (20–50), short exposure (30–90 minutes), sometimes artificially simulated aspects.",,,,,,,,,,,,,
Medium-/long-term studies: Several weeks to years,fewer participants,natural effects observed,often with researchers/industry partners.,,,,,,,,,,,,
Experiments included control groups and context-switching tasks.,,,,,,,,,,,,,,,
No mention of randomization,blinding,"or placebo controls.""",,,,,,,,,,,,,
"iSummary: Workload-based, Personalized summaries for Knowledge Graphs","Vassiliou Giannis, Alevizakis Fanouris, Papadakis Nikolaos, Kondylakis Haridimos",2024,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
iSummary uses query logs to create personalized knowledge graph summaries,focusing on maximizing coverage—how much of user queries are answered. The algorithm is scalable (linear in query log size),outperforms baselines in both quality and efficiency,"and is implemented in Java. New insight: iSummary uniquely leverages generic query workloads for personalization.""",,,"The research goal is to generate personalized, workload-based semantic summaries for large RDF/S knowledge graphs; the approach, iSummary, uses query logs and user-selected nodes to efficiently create high-quality summaries; results show iSummary outperforms baselines in both coverage and efficiency.","Experimental evaluation using three real-world datasets (DBpedia v3.8, WikiData, Bio2RDF) and their corresponding query workloads.",,,,,,How can personalized semantic summaries of RDF Knowledge Graphs be efficiently generated by leveraging SPARQL query logs to reflect individual user interests while overcoming the limitations of node weighting and computational complexity?,,"The paper introduces iSummary, a scalable method for creating personalized semantic summaries of large Knowledge Graphs. Using query logs instead of user-provided weights or queries, iSummary identifies and links user-relevant resources. The approach outperforms existing methods in both quality and efficiency, with theoretical quality guarantees and linear scalability."
Implementation and testing of the iSummary approach developed in Java on a Windows 10 system.,,,,,,,,,,,,,,,
Use of semantic summarization to extract minimized,"useful information from large RDF knowledge graphs.""","The research is reproducible. The source code and guidelines for downloading datasets and workloads are available online at https://anonymous.4open.science/r/iSummary-47F2/. The implementation uses Java, and detailed experimental setup information is provided.","iSummary is only 0.13 times slower than Random, 14 times faster than GLIMPSE, and 40 times faster than PPR, while outperforming all baselines in coverage.",,,,,,,,,,,,
The approach requires only one or a few user-selected nodes and leverages existing query logs to generate personalized summaries.,,,,,,,,,,,,,,,
"No statistical significance (p-values) is reported in the provided context.""
On WikiData
On Bio2RDF","iSummary outperforms all baselines in coverage, achieving almost double the coverage of GLIMPSE, 17-24% higher than Random, and 32-37% higher than PPR on DBpedia.
iSummary achieves about twice the coverage of other baselines; GLIMPSE fails to process due to memory issues.
iSummary outperforms Random by 25-30%; PPR cannot process the dataset within 24 hours.",,,,,,,,,,,,,,
As summary size (k) increases,coverage increases for all methods.,,,,,,,,,,,,,,
iSummary is only 0.13 times slower than Random,14 times faster than GLIMPSE,"and 40 times faster than PPR in execution time on DBpedia.""",Future work is needed to explore alternative methods for linking κ nodes using the original data graph.,,,,,,,,,,,,
The evolution of personalized summaries over time as user interests change is not addressed.,,,,,,,,,,,,,,,
"Diversity in λ/κ-Personalized summaries is not currently incorporated; summaries may lack variety.""","iSummary outperforms all baselines in coverage and is significantly faster (14x faster than GLIMPSE, 40x faster than PPR, only 0.13x slower than Random).",,,,,,,,,,,,,,
Personalized summaries can be generated using only a few user-selected nodes and generic query logs.,,,,,,,,,,,,,,,
Future work should explore alternative node-linking methods,study summary evolution over time,"and introduce diversity in personalized summaries.""",Explore alternative methods for linking κ nodes in the summary by querying the original data graph only once to minimize overhead.,,,,,,,,,,,,
Study how personalized summaries change over time as user interests drift,considering events,disasters,seasonality,or occasions.,,,,,,,,,,,
"Introduce diversity in λ/κ-Personalized summaries so users are not always presented with the same summary.""","Suggested future research directions include: exploring alternative methods for linking κ nodes using the original data graph, studying how personalized summaries change over time with user input, and introducing diversity in λ/κ-personalized summaries so users receive varied personalized summaries.",,,,,,,,,,,,,,
Graph Neural Networks for Heart Failure Prediction on an EHR-Based Patient Similarity Graph,"Boll Heloisa Oss, Amirahmadi Ali, Soliman Amira, Byttner Stefan, Recamonde-Mendoza Mariana",2024,reference-manager,,,"Implementation Insights highlight several points: The Graph Transformer (GT) outperformed other models, especially in recall for minority, HF-positive cases. Prescription codes (NDC standard) were most predictive. GT’s graph-based approach uncovers relational insights missed by RF. Limitations include reliance on MIMIC-III data, ICD code accuracy, and fixed thresholds.",,,,"Used the MIMIC-III dataset, encoding diagnoses and procedures with ICD-9 and medications with NDC, and processed data using Pandas and PyHealth.",,"How can graph-based deep learning models, particularly Graph Transformers, improve the identification and interpretation of heart failure in clinical data compared to traditional methods, considering the roles of diagnosis, procedure, and prescription codes?","The paper aims to predict heart failure (HF) using patient data from the MIMIC-III dataset, employing graph neural networks (GNNs) and comparing architectures. The Graph Transformer model outperformed others, especially in recall. Prescription codes were most predictive. The study highlights the complexity of healthcare predictions and the value of graph-based interpretability.","The research goal was to predict heart failure using patient similarity graphs; the approach compared three GNN architectures (GraphSAGE, GAT, GT) on MIMIC-III data, finding that the Graph Transformer (GT) with focal loss performed best, especially in identifying high-risk patients, with medications as the most relevant features.",
Selected patients with at least two hospital visits and labeled heart failure (HF) cases based on ICD-9 codes following official guidelines.,,,,,,,,,,,,,,,
Analyzed clinical features (diagnoses,procedures,"prescriptions) using code frequencies and embeddings as node features in a similarity graph.""","The research is reproducible: all code is available at https://github.com/hossboll/patient-gnn. The study used the MIMIC-III dataset, processed with Pandas and PyHealth, and models were implemented using PyTorch Geometric. Experiments were repeated three times to mitigate selection bias.","The Graph Transformer (GT) model achieved the highest F1 score (0.5361), AUROC (0.7925), and AUPRC (0.5168), outperforming baseline algorithms such as Random Forest (RF), KNN, LR, MLP, and GBT.",,,,,,,,,,,
GT’s higher recall is crucial for detecting minority,HF-positive cases,while RF showed similar AUPRC (0.5132),suggesting it could be a resource-efficient alternative with threshold tuning.,,,,,,,,,,,,
"All models benefited from class-imbalance-adapted loss functions; statistical significance (p-values) was not reported in the context.""","The combined use of medication, diagnosis, and procedure data achieved the highest F1 score (0.5361 ± 0.003) and AUPRC (0.5227).",,,,,,,,,,,,,,
Removing medication data led to the largest performance drop.,,,,,,,,,,,,,,,
Graph Transformer (GT) outperformed other models in F1 (0.5361) and AUROC (0.7930).,,,,,,,,,,,,,,,
"Recall was highest with medication data alone.""",Data limited to MIMIC-III; lacks external hospital validation.,,,,,,,,,,,,,,
Single train-validate-test split may cause selection bias; k-fold cross-validation not feasible.,,,,,,,,,,,,,,,
ICD codes for HF labels may lack clinical nuance; alternative cohort identification suggested.,,,,,,,,,,,,,,,
Attention as interpretability mechanism needs further study.,,,,,,,,,,,,,,,
"Fixed threshold for metrics is a limitation; optimizing AUROC suggested.""","The GT with FL model outperformed baseline algorithms in predicting HF, achieving the highest F1 score (0.5531), AUROC (0.7925), and AUPRC (0.5168).",,,,,,,,,,,,,,
Graph-based relationships improved detection of positive HF cases.,,,,,,,,,,,,,,,
Limitations include single-center data,reliance on ICD codes,and fixed metric thresholds.,,,,,,,,,,,,,
Future work should use multi-center data,alternative cohort identification,"and optimize metrics across thresholds.""",Limited generalizability due to reliance on MIMIC-III data; future studies should evaluate data from other hospitals.,,,,,,,,,,,,
Need for improved label accuracy and interpretability,including exploring alternative cohort identification methods and enhancing GNN explainability.,,,,,,,,,,,,,,
Future research should investigate alternative graph representations,inductive learning,"and integration of additional multimodal patient data.""","Future research should evaluate data from other hospitals, explore alternative cohort identification methods, investigate attention as an interpretability mechanism, perform inter-layer analyses, optimize decision thresholds, use dynamic or heterogeneous graphs, enhance GNN interpretability, and incorporate multimodal patient data to improve model robustness and reliability.",,,,,,,,,,,,
Towards a knowledge driven framework for bridging the gap between software and data engineering,"Solanki Monika, Božić Bojan, Dirschl Christian, Brennan Rob",2019,reference-manager,10.1016/j.jss.2018.12.017,,"Implementation Insights highlight that integrating DIO conceptualizations into PoolParty Thesaurus (PPT) improved search efficiency by 50%. The ALIGNED ontology suite supports unified governance, interoperability, and complex requirements integration across diverse, large-scale, data-intensive systems, with ongoing empirical evaluation in real-world use cases.",,,,"Data Lifecycle Management: Includes extraction, storage, authoring, interlinking, enrichment, quality analysis, repair, and publication of data.",,"How can a unified suite of ontologies be developed and applied to effectively integrate and align software and data engineering tasks, processes, and datasets across large-scale, data-intensive systems in diverse domains such as legal information platforms and global history databanks?","The paper presents version 3 of the ALIGNED ontology suite, aiming to semantically integrate software and data engineering tasks. Using requirements analysis and deployment in real-world cases (JURION, Seshat), it demonstrates improved governance and efficiency. The suite enables unified data management, with formal evaluation confirming its practical benefits.","The paper's main objective is to provide a comprehensive overview of the ALIGNED ontology suite for unified governance in software and data engineering, using a requirements-driven approach, with results showing improved efficiency (e.g., 50% faster semantic search) and practical deployment in large-scale, real-world systems.",
Ontology-Based Integration: Utilizes the ALIGNED suite of ontologies for tool integration,unified governance,and provenance tracking across software and data engineering.,,,,,,,,,,,,,
"Requirements Analysis: Conducts thorough analysis of software and data engineering needs to develop ontological models.""","The Seshat project provides an open repository of expert-curated historical data, with provenance information recorded for traceability. However, there is no explicit mention of the availability of source code for the project. Reproducibility is supported for data, but not for software.",The unified governance dashboard using DIO showed a 50% increase in search efficiency over semantically annotated datasets compared to Confluence and JIRA.,,,,,,,,,,,,,
The ALIGNED ontology suite enables integrated governance,supporting complex requirements and interoperability in large-scale,data-intensive systems.,,,,,,,,,,,,,
"No statistical significance (p-values) are reported in the provided context.""","The ALIGNED suite of ontologies was evaluated on four large-scale, data-intensive systems engineering use cases.",,,,,,,,,,,,,,
"Ontologies were verified for logical correctness using DL reasoners for satisfiability
Design suitability","incoherency
elegance","and inconsistencies.
and quality were assessed based on Gruber’s principles: clarity",coherence,extendability,,minimum encoding bias,,,,,,,,,and minimum ontological commitment.
External ontologies (e.g.,PROV-O,SKOS) were extensively reused.,,,,,,,,,,,,,
Ontologies were made publicly available,well-documented,and licensed under Creative Commons Attribution License.,,,,,,,,,,,,,
"Sustainability was ensured via public Github repositories and long-term maintenance by ontology engineers.""
Existing tools (e.g.","No generic, domain-independent design intent capture model is available as a design pattern.
OOPS!) detect design flaws but not logical errors and lack ontology-based error reporting.",,,,,,,,,,,,,,
"Metadata models lack granularity for describing complex datasets in a semantically rich way.
dcat vocabulary cannot further describe dataset distributions.""","The ALIGNED suite of ontologies supports productivity and agility in data and software engineering by providing semantic models for design, processes, and error handling.",,,,,,,,,,,,,,
Integrating requirements and issues as linked data enables unified governance and enhanced query/reporting features.,,,,,,,,,,,,,,,
The suite improves interoperability and addresses complexity in large-scale,data-intensive systems.,,,,,,,,,,,,,,
"Further empirical evaluation of the ontologies is recommended.""
Absence of a generic","Lack of ontologies for integrating and aligning software and data engineering tasks, processes, and datasets.
domain-independent design intent capture model that can be specialized for any design rationale scenario.",,,,,,,,,,,,,,
"Insufficient granularity in metadata models to semantically describe complex datasets in a rich way.""",,,,"The objectives are to document design decisions and requirements in data-intensive systems, support data quality engineering, enable unified views of software and data engineering processes, describe provenance, and develop ontologies closely aligned with application requirements for improved governance and integration.",,,,,,,,,,,
Patterns for Representing Knowledge Graphs to Communicate Situational Knowledge of Service Robots,"Zhang Shengchen, Wang Zixuan, Chen Chaoran, Dai Yi, Ye Lyumanshan, Sun Xiaohua",2021,reference-manager,10.1145/3411764.3445767,,"The implementation insights show that the pattern library supports effective interface design for communicating situational knowledge, but users found some information confusing and hard to find. New insights include the need for hierarchical patterns, classification trees to reduce overload, and exploring multimodal (voice, gesture) interactions for better human-robot knowledge exchange.",,,,Participants arranged hexagonal knowledge cards on a canvas to visually communicate situational knowledge in HRI scenarios.,,"How can patterns for presenting situational robotic knowledge be discovered and formalized to improve the communication of semantic, procedural, and episodic knowledge between humans and robots through graphical interfaces?","The paper investigates how to represent and communicate situational knowledge in Human-Robot Interaction (HRI) through graphical interfaces. Using a qualitative study with scenario comics and card arrangement tasks, the authors identify patterns for presenting semantic, procedural, and episodic knowledge. The findings inform interface design to improve knowledge exchange in HRI.","The research goal is to identify effective patterns for representing situational robotic knowledge graphs; using a qualitative, visual grounded theory approach with university students arranging knowledge cards, the study found distinct low- and high-level patterns for communicating semantic, procedural, and episodic knowledge in human-robot interaction.",
Data analysis used visual grounded theory,including steps like positional coding,abstraction,relational coding,and pattern generation.,,,,,,,,,,,
"Researchers iteratively coded and reviewed diagrams to identify and formalize low-level and high-level patterns.""","The research is highly reproducible. Canvases created by participants, the NVivo source file for coding, and the card source files are all available at https://github.com/tongji-cdi/robot-knowledge-canvases. Detailed coding procedures and codebooks are described, with 99% coding agreement (Cohen’s κ = .79).","Participants gave high ratings for interface clarity (mean=1.58, sd=1.69) and usefulness (mean=1.90, sd=0.88), but lower scores for helpfulness (mean=0.70, sd=1.77) and information clarity (mean=0.80, sd=1.81).",,,,,,,,,,,,,
Eight low-level and four high-level design patterns were identified for presenting situational robotic knowledge.,,,,,,,,,,,,,,,
"Challenges included confusing or repetitive information; recommendations include user-friendly language and avoiding loops in knowledge graphs. No p-values reported.""","Participants gave high ratings for interface clarity (questions 13-16, mean = 1.58, sd = 1.69).",,,,,,,,,,,,,,
The interface was considered useful (question 1,mean = 1.90,sd = .88).,,,,,,,,,,,,,
Helpfulness in using the robot rated positively (question 2,value not fully provided).,,,,,,,,,,,,,,
Lower scores for comprehensibility and perspicuity.,,,,,,,,,,,,,,,
Some found information confusing (question 12,mean = .80,sd = 1.81).,,,,,,,,,,,,,
Some had difficulty finding information (question 11,mean = .80,sd = 1.23).,,,,,,,,,,,,,
Helpfulness in understanding the robot rated lower (question 7,mean = .70,"sd = 1.77).""",Pattern library developed from only twelve participants and nine scenarios; more scenarios and testing may reveal additional patterns.,,,,,,,,,,,,
Datasets were specifically compiled for scenarios,possibly missing real-world complexities.,,,,,,,,,,,,,,
Lower scores for comprehensibility and perspicuity; some participants found information confusing or hard to find.,,,,,,,,,,,,,,,
"Controlled environment may limit generalizability.""",The interface received overall positive ratings for clarity and usefulness in communicating situational knowledge.,,,,,,,,,,,,,,
Some participants found the information confusing or hard to find,especially with large amounts of data.,,,,,,,,,,,,,,
Design recommendations include using Classification Tree patterns to reduce information overload and combining Assorted Entities by common attributes.,,,,,,,,,,,,,,,
Multimodal interaction (voice,gesture,"gaze) is recommended for future interfaces to improve situational knowledge exchange.""",Need for more real-world scenarios and public knowledge graph (KG) datasets to discover additional patterns and enable broader evaluation.,,,,,,,,,,,,
Addressing user confusion caused by loops in KG structures; further research needed on supporting efficient exploration while maintaining hierarchy.,,,,,,,,,,,,,,,
Developing user-friendly ontologies and multimodal interaction (e.g.,voice,"gesture) to improve non-expert understanding and communication.""","Future research should include testing additional scenarios to discover more patterns, building a public real-world knowledge graph dataset, and exploring multimodal interaction (voice, gesture, gaze) with knowledge graph interfaces. Addressing information overload and improving comprehensibility are also recommended directions.","Controlled environment; simulated scenario; formative study; pattern discovery and formalization; non-expert participants; task-based (six tasks); manual compilation of knowledge graph datasets; anonymized data; Wizard-of-Oz testing system; multi-component system (graph database, remote control, Android app); visual communication using cards; iterative coding.",,,,,,,,,,,
Knowledge Graphs Evolution and Preservation,sGhoOL International ISWS REMENROHWEB BUMMER,2019,reference-manager,,,"The implementation identifies community changes in knowledge graphs by comparing snapshots and defining persistence, emergence, merging, and splitting of communities. It uses DBLP data for evaluation, focusing on summarizing community-level changes rather than individual entities. The approach emphasizes automated analysis to derive insights from detected changes.",,,,Use of CrowdTruth metrics to capture and interpret inter-annotator disagreement and quantify ambiguity and bias in knowledge graphs.,,"How can the evolution and preservation of knowledge graphs and their ontologies be observed, characterized, and supported—particularly regarding consistency, integrity, and the prediction of property changes—while facilitating effective updates by non-expert knowledge workers?","The paper aims to track and describe changes within Knowledge Graphs (KGs) to understand evolving concepts, using methodologies like crowdsourcing and blockchain for data collection and validation. Significant findings highlight applications in food data research and medical data privacy. The approach enables transparent, privacy-preserving research.","The paper's main objective is to automatically capture and characterize different types of evolution in Knowledge Graphs using a methodological approach, with principal findings showing the approach can detect atomic, local, and global evolution, and is applicable to domains like scholarly data and food research.",
Crowdsourcing campaign with provenance metadata collection,profiling user contributions,and gathering opinions on controversial statements.,,,,,,,,,,,,,
"Basic statistical analysis of knowledge graph changes
From 2014 to 2015-04","classifying triples as ‘Added’
DBpedia English triples increased from 583 million (4.58 million entities) to 737 million (5.9 million entities).",‘Removed’,"or ‘Edited’ to identify frequently changed properties.""","The research aims for reproducibility by following the FAIR Principles (findability, accessibility, interoperability, reusability) and using the Trusty URI specification to provide persistent identifiers for datasets. However, there is no explicit mention of source code availability for the project.",,"The study identified 161 different triples between DBpedia 2016.10 and 2016.04 for Cristiano Ronaldo, highlighting significant property changes.",,,,,,,,,
"No statistical significance (p-values) is reported in the context.""",DS1 (2014) contains 583 million triples describing 4.58 million things; DS2 (2015-04) contains 737 million triples describing 5.9 million things.,,,,,,,,,,,,,,
In the Christiano Ronaldo case study,161 triples were found to be different between DBpedia 2016.10 and 2016.04 in the mapped infobox properties dataset.,,,,,,,,,,,,,,
The paper proposes metrics: Evolutionary Synchronisation (ES),Change Alignment,"and Evolutionary Dependency (ED) to measure and analyze ontology evolution.""",No explicit limitations or shortcomings of the research study are stated in the provided context.,,,,,,,,,,,,
"No self-reported problems or suggestions for further research are mentioned.
,The study proposes a four-step workflow to collect",analyze,review,and refine biases in crowd-sourced Knowledge Graphs (KGs),improving them by tracking provenance and background information.,,,,,,,,,,,
Involving human input helps identify and address biases in KGs.,,,,,,,,,,,,,,,
Future work includes further experimental evaluation,refining metrics,"and iterative bias control methods.""",Need for understandable summarization and human annotation of detected changes in Knowledge Graphs.,,,,,,,,,,,,
Distinguishing valid evolution from noise,potentially via classifier development and noise elimination.,,,,,,,,,,,,,,
"Providing explanations for detected evolution phenomena to understand underlying causes.""","Future research should investigate food data evolution, health perceptions, technical developments, and economic factors using knowledge graphs (KGs). Further work is needed on predicting KG property growth, linking publishing organizations, timeline analyses, trust in data, and summarizing or explaining detected changes within KGs.","Controlled experiment: artificial cases of evolution are created in knowledge graphs by removing subsets of nodes/links within homogeneous sub-graphs, with varying granularity and repeated sampling.",,,,,,,,,,,,,
Random experiment: random changes (addition,deletion,update) are applied to knowledge graphs,evaluating the approach in unrestricted environments.,,,,,,,,,,,,
No mention of randomization,blinding,control,placebo,multi-site,,retrospective,observational study,"The objectives of the study are to analyze patterns of changes in properties using basic statistical analysis, predict the growth of properties in knowledge graphs by focusing on graph structure, and evaluate the proposed approach through controlled and random experiments to detect and explain property evolution.",,"or systematic review.""",,meta-analysis,parallel design,crossover,stratified
Knowledge Graph Tuning: Real-time Large Language Model Personalization based on Human Feedback,"Sun Jingwei, Du Zhixu, Chen Yiran",2024,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
"KGT personalizes language models by optimizing an external knowledge graph (KG) instead of model parameters
New Insights:",improving efficiency and interpretability. KGT maintains high performance as the query set grows,unlike baselines. Limitation: KGT depends on the model’s ability to follow instructions during personalization.,,,,,,,,,,,,,
KGT’s scalability supports long-term,user-specific knowledge accumulation,"making it suitable for ongoing personalization needs.""",,"The research goal is to personalize large language models efficiently; the approach, KGT, optimizes an external knowledge graph instead of model parameters; results show KGT significantly outperforms baselines in efficacy and paraphrase rates, especially as query set size increases, demonstrating superior scalability and personalization performance.",,"The paper introduces KGT, a method for knowledge editing in language models using only user-provided answers as feedback. Built on CounterFactExtension, KGT optimizes knowledge retrieval and reasoning. Experiments show KGT significantly outperforms baselines in efficacy and paraphrase rates, with efficient resource usage. User feedback on relations is unnecessary.",,,,,,,"Knowledge Graph Tuning (KGT): Personalizes large language models (LLMs) by editing a user’s personalized knowledge graph based on feedback, rather than changing model parameters.","How can we optimize a knowledge graph to enhance large language model reasoning by maximizing the probability of generating correct, personalized answers through effective knowledge retrieval and knowledge-enhanced reasoning?",
Knowledge Retrieval: Uses user feedback or LLM extraction to identify and optimize personalized knowledge triples for improved retrieval and reasoning.,,,,,,,,,,,,,,,
Empirical Evaluation: Conducts experiments on datasets (e.g.,CounterFact) to compare KGT with baseline methods in terms of efficacy,paraphrase rate,"and scalability.""",,,"KGT significantly outperforms baselines in efficacy and paraphrase scores on both CounterFact and CounterFactExtend datasets, with improvements up to 61% (efficacy) and 46% (paraphrase) on Llama3-8B.",,,,,,,,,
KGT maintains high performance as query set size increases,unlike baselines whose performance degrades.,,,,,,,,,,,,,,
"KGT is more efficient
On Llama3-8B","using less memory and time than baselines; no p-values are reported.""
KGT improves efficacy by 39–61% and paraphrase by 32–46% over baselines.",KGT significantly outperforms baselines in efficacy and paraphrase scores on CounterFact and CounterFactExtend datasets.,,,,,,,,,,,,,
KGT achieves efficacy of 94.58%±0.96% and paraphrase of 86.89%±1.37% (Llama3-8B,CounterFact).,,,,,,,,,,,,,,
KGT achieves efficacy of 93.80%±0.36% and paraphrase of 89.22%±1.17% (Llama3-8B,CounterFactExtend).,,,,,,,,,,,,,,
KGT maintains high performance as query set size increases,unlike baselines.,,,,,,,,,,,,,,
KGT achieves the shortest latency (0.15s) and lowest GPU memory cost (15904MB) on Llama3-8B.,,,,,,,,,,,,,,,
KGT reduces GPU memory cost by 57–77% compared to baselines.,,,,,,,,,,,,,,,
"User feedback on relations is not necessary; KGT performs similarly or better using only answer feedback.""",KGT depends on the LLM’s ability to follow instructions when calculating probabilities and collecting query-answer pairs.,,,,,,,,,,,,,,
The method’s effectiveness may be limited if the LLM cannot accurately perform these tasks.,,,,,,,,,,,,,,,
"Further research is suggested to address this dependency.""","KGT significantly outperforms baselines in personalization efficacy and paraphrase rates, with improvements up to 61% and 46% respectively.",,,,,,,,,,,,,,
KGT achieves the lowest latency (0.15s) and GPU memory usage (15,904MB),reducing memory cost by up to 77%.,,,,,,,,,,,,,
KGT scales well with large query sets,maintaining high performance where baselines degrade.,,,,,,,,,,,,,,
"Users only need to provide answers as feedback; relation feedback is unnecessary.""",KGT’s effectiveness depends on the LLM’s ability to follow instructions when calculating key probabilities and collecting relations.,,,,,,,,,,,,,,
Scalability to large query sets and extensive personalized knowledge remains a challenge for some baseline methods,though KGT shows promise.,,,,,,,,,,,,,,
"Further research is needed to enhance efficiency and interpretability in model personalization using knowledge graphs.""","Future research should address the limitation that KGT depends on the LLM’s ability to follow instructions when calculating probabilities and collecting feedback. Further investigation is needed to improve model personalization efficiency and interpretability, and to explore KGT’s scalability for extensive personalized knowledge.",,,"The objectives of the study are to personalize large language models (LLMs) by optimizing an external knowledge graph (KG) instead of model parameters, aiming to maximize the probability of reasoning correct answers using KG-enhanced LLMs, and to improve efficiency, interpretability, and scalability in model personalization.",,,,,,,,,,,
LLM4EduKG: LLM for Automatic Construction of Educational Knowledge Graph,"Sun Jianing, Zhang Zhichao, He Xueli",2024,reference-manager,10.1109/nana63151.2024.00051,,"Implementation Insights focus on maximizing precision in EduTriples extraction, prioritizing correctness over recall. Few-shot and chain of thought (CoT) techniques significantly improve performance. ERNIE and GLM4 achieve the highest precision (>90%). Fine-tuning boosts precision by ~20% and F1 by ~15%, but base models still lag behind.",,,,Structured prompt framework: Guided Large Language Models (LLMs) to extract and evaluate educational triples from text.,,How can structured prompt frameworks be used to automatically extract and evaluate educational triples from unstructured educational text data to construct high-quality educational knowledge graphs (EduKGs)?,"The paper aims to automate the construction of educational knowledge graphs (EduKGs) using Large Language Models (LLMs) with a structured prompt framework. Experiments on four models and various techniques showed high precision, especially for GLM4 and ERNIE. The method is effective, enabling richer knowledge systems and potential for personalized education.","The research goal is to automatically construct educational knowledge graphs using a structured prompt framework with Large Language Models; the approach involves extracting and evaluating educational triples from text, and results show high precision (up to 95.48%) and effectiveness, especially with GLM4 and ERNIE models.",
Model thinking techniques: Compared zero-shot,few-shot,and chain of thought (CoT) methods to assess their impact on model performance.,,,,,,,,,,,,,
Evaluation metrics: Used precision,recall,"and F1-score to measure model effectiveness.""",,"ERNIE and GLM4 achieved the highest precision (>90%) in EduTriples extraction, with ERNIE at 95.24% precision, 86.96% recall, and 90.91% F1-score.",,,,,,,,,,,
Fine-tuning improved precision by ~20% and F1-score by ~15%,but still lagged behind GLM4.,,,,,,,,,,,,,,
"The proposed model thinking technique (chain of thought) achieved 95.48% precision (statistically significant improvement over zero-shot at 71.55%).""","Primary outcomes were measured using precision, recall, and F1-score.",,,,,,,,,,,,,,
ERNIE: Precision 95.24%,Recall 86.96%,F1 90.91%,,,,,,,,,,,,,
GLM4: Precision 95.48%,Recall 86.36%,F1 90.69%,,,,,,,,,,,,,
Qwen-turbo: Precision 83.95%,Recall 84.47%,F1 84.21%,,,,,,,,,,,,,
ChatGLM2-6B: Precision 51.22%,Recall 75.90%,F1 61.17%,,,,,,,,,,,,,
ChatGLM3-6B: Precision 59.26%,Recall 80.00%,F1 68.09%,,,,,,,,,,,,,
ChatGLM2-6B\* (fine-tuned): Precision 71.62%,Recall 81.54%,F1 76.27%,,,,,,,,,,,,,
ChatGLM3-6B\* (fine-tuned): Precision 83.75%,Recall 83.75%,F1 83.75%,,,,,,,,,,,,,
Few-shot technique (GLM4): Precision 84.31%,,,,,,,,,,,,,,,
Zero-shot technique (GLM4): Precision 71.55%,,,,,,,,,,,,,,,
CoT (Chain of Thought) technique (GLM4): Precision 75.61%,,,,,,,,,,,,,,,
"Proposed method (GLM4): Precision 95.48%""",,"The proposed LLM-based method effectively constructs educational knowledge graphs with high precision, especially using GLM4 and ERNIE (precision above 90%).",,,,,,,,,,,,,
Fine-tuning models significantly improves precision (by ~20%) and F1 score (by ~15%),approaching API-deployed models.,,,,,,,,,,,,,,
The approach reduces labor and resource requirements,supporting personalized educational applications.,,,,,,,,,,,,,,
"Future work should focus on large-scale datasets and further personalization via lightweight local model deployment.""",Scaling the application to large-scale datasets while maintaining high accuracy.,,,,,,,,,,,,,,
Personalizing educational practices through deployment of individual models in local lightweight mode.,,,,,,,,,,,,,,,
"Addressing challenges in converting fragmented knowledge into structured knowledge points for automated EduKG construction.""","Future research should focus on extending applications to large-scale datasets while maintaining high accuracy, and further exploring personalized educational practices through local deployment of individual models. Addressing limitations in data quality, model performance on complex tasks, and dataset impact on training are also recommended.",,,"The objectives are to extract structured educational triples from text using the form (entity1, relation, entity2), strictly following predefined relations. The process includes extracting entities, determining their relations, and ensuring all possible triples are captured without adding extra content.",,,,,,,,,,,
A federated graph neural network framework for privacy-preserving personalization,"Wu Chuhan, Wu Fangzhao, Lyu Lingjuan, Qi Tao, Huang Yongfeng, Xie Xing",2022,reference-manager,10.1038/s41467-022-30714-9,,Implementation Insights Summary:,,,,,,,,,
"FedPerGNN uses decentralized user data and privacy-preserving techniques for GNN-based personalization. Key strategies include pseudo interacted item sampling
New Insights:",encrypted item IDs,and local differential privacy (LDP) to protect user data. Increasing pseudo items and noise improves privacy. No raw user-item data is collected.,,,,,,,,,,,,,
FedPerGNN achieves K/M index privacy,and privacy can be tuned by adjusting pseudo item count and noise parameters. Collaboration between servers could risk privacy,"but model update methods still protect private ratings.""",,"The research goal is to achieve privacy-preserving personalization; the approach is FedPerGNN, which protects both ratings and user-item interaction histories using federated graph neural networks; results show FedPerGNN significantly outperforms other privacy-preserving methods and matches the best centralized GNN-based methods in accuracy.",,"The paper aims to improve privacy-preserving personalization using graph neural networks (GNNs). Using public datasets and comparing with baseline methods, the proposed FedPerGNN approach effectively leverages high-order user-item interactions, achieving better or comparable accuracy while protecting user privacy. FedPerGNN significantly reduces prediction error versus other privacy-preserving methods.",,,,,,,"FedPerGNN: A privacy-preserving federated learning framework using local Graph Neural Networks (GNNs) on user devices, coordinated by a central server, with privacy-preserving model updates and graph expansion.","How can decentralized graph neural network methods like FedPerGNN enable effective and privacy-preserving personalization in online applications while addressing challenges related to communication efficiency, scalability, and security against malicious clients?",
Performance Evaluation: Comparison of FedPerGNN with centralized and federated personalization methods using RMSE on six benchmark datasets.,,,,,,,,,,,,,,,
"Communication Cost Analysis: Assessment of model communication cost per client under different privacy and expansion settings.""","The research is reproducible: all datasets used are publicly available, and the source code is provided at https://github.com/wuch15/FedPerGNN45. Experiments and implementation details are described in the Methodology section and Supplementary Information for reproducibility.","FedPerGNN achieves lower RMSE (root mean square error) than baseline methods across MovieLens, Flixster, Douban, and Yahoo datasets, indicating improved prediction accuracy.",,,,,,,,,,,,,
FedPerGNN uniquely combines high-order user-item interaction modeling with privacy protection for both ratings and interaction items,unlike other methods.,,,,,,,,,,,,,,
Performance differences are statistically significant,with error bars representing mean results and 95% confidence intervals (n = 5),"but specific p-values are not provided.""","Primary outcome: FedPerGNN achieves the lowest RMSE on Flixster (0.980 ± 0.006), Douban (0.775 ± 0.001), Yahoo (20.7 ± 0.325), ML-100K (0.910 ± 0.001), ML-1M (0.839 ± 0.003), and ML-10M (0.793 ± 0.0002).",,,,,,,,,,,,
FedPerGNN significantly outperforms FCF and FedMF (p < 0.1).,,,,,,,,,,,,,,,
No significant difference between FedPerGNN and the best method on Yahoo (p > 0.1).,,,,,,,,,,,,,,,
FedPerGNN protects both user ratings and interaction items,unlike other methods.,,,,,,,,,,,,,,
Communication cost increases with more pseudo interacted items (M); optimal privacy/performance trade-off at M = 1000.,,,,,,,,,,,,,,,
"FedPerGNN is compatible with different GNN architectures and serves as a general framework for GNN-based personalization.""",Many existing methods cannot protect user privacy due to centralized user data storage.,,,,,,,,,,,,,,
Privacy-preserving methods cannot exploit high-order graph information and only protect user ratings,not interaction histories.,,,,,,,,,,,,,,
Local user data may be too small to train accurate models.,,,,,,,,,,,,,,,
Too many graph expansion rounds cause performance decrease due to over-smooth problem.,,,,,,,,,,,,,,,
"Download communication cost is much larger than upload and increases with expansion rounds.""",Optimal personalization performance is achieved with three graph expansion rounds and a moderate number of pseudo interacted items (M ≈ 1000).,,,,,,,,,,,,,,
Most communication cost comes from downloading anonymous neighbor user embeddings,which increases with expansion rounds.,,,,,,,,,,,,,,
Using three expansion rounds balances performance and communication cost,making the approach practical.,,,,,,,,,,,,,,
Download bandwidth requirements are higher than upload,"but acceptable for real-world use.""",,"Future research should address defending FedPerGNN against attacks from malicious clients and platforms, and explore effective, secure deployment of FedPerGNN in real-world personalization systems while preserving privacy. There is also a need to relax the strong assumption of a trusted third-party server.",,,,,,,,,,,,
Knowledge graph driven medicine recommendation system using graph neural networks on longitudinal medical records,"Mishra Rajat, Shridevi S.",2024,reference-manager,10.1038/s41598-024-75784-5,,"KGDNet outperforms all baselines on the MIMIC-IV dataset across PRAUC, F1, Jaccard, and DDI rate metrics. Its DDI rate can be controlled using the λ hyperparameter, balancing safety and accuracy. Ablation studies confirm the importance of KGDNet’s modules. KGDNet reliably mimics clinician prescriptions in real-world scenarios.",,,,"Instance-based models: Recommend medicines using only the current admission record, without considering patient medical history.",,No information available,"The paper aims to improve medicine recommendation by leveraging patient medical history using longitudinal-based models and Electronic Health Records (EHRs) like MIMIC-IV. Using knowledge graphs and attention mechanisms, the proposed KGDNet model outperforms baselines in safety and accuracy, supporting safer, more personalized medication recommendations for complex cases.","The research goal is to improve safe and effective medicine recommendation; the approach is the novel KGDNet framework using personalized knowledge graphs, GNNs, RNNs, and Transformer-based attention; the results show KGDNet outperforms baselines on MIMIC-IV EHR data in accuracy and minimizing Drug-Drug Interactions.",
Longitudinal-based models: Use temporal dependencies in a patient's medical history,often leveraging Electronic Health Records (EHRs),for personalized and safer recommendations.,,,,,,,,,,,,,
"Use of Electronic Health Records (EHRs): Incorporate comprehensive historical medical data for improved recommendation accuracy.""",,"KGDNet achieved the highest PRAUC (0.7657 ± 0.0015), F1 Score (0.6765 ± 0.0017), Jaccard (0.5218 ± 0.0018), and controlled DDI Rate (0.0665 ± 0.0010) among all compared models.",,,,,,,,,,,,,
KGDNet effectively balances accuracy and safety by adjusting DDI rates using a threshold hyperparameter λ.,,,,,,,,,,,,,,,
"No explicit p-values or statistical significance values are reported in the context.""
KGDNet achieved:","Primary outcomes were measured using DDI Rate, PRAUC, F1 Score, Jaccard, and Avg. # of meds.",,,,,,,,,,,,,,
DDI Rate: 0.0665 ± 0.0010,,,,,,,,,,,,,,,
PRAUC: 0.7657 ± 0.0015,,,,,,,,,,,,,,,
F1 Score: 0.6765 ± 0.0017,,,,,,,,,,,,,,,
Jaccard: 0.5218 ± 0.0018,,,,,,,,,,,,,,,
Avg. # of meds: 19.2273 ± 0.0912,,,,,,,,,,,,,,,
KGDNet outperformed all baselines in PRAUC,F1 Score,and Jaccard.,,,,,,,,,,,,,
KGDNet effectively controlled DDI rates below the EHR test data base rate (0.0781).,,,,,,,,,,,,,,,
Ablation studies showed removing modules (knowledge graph,DDI graph,fusion,"attention) reduced performance.""",No explicit limitations or shortcomings are stated in the provided context.,,,,,,,,,,,
"No self-reported problems or suggestions for further research are mentioned.
,KGDNet improves medication recommendation accuracy and reduces Drug-Drug Interactions by leveraging knowledge graphs and advanced neural networks.",,,,,,,,,,,,,,,
KGDNet outperforms existing models on the MIMIC-IV EHR dataset across PRAUC (0.7657),Jaccard (0.6765),F1 (0.5218),and maintains a competitive DDI rate (0.0665).,,,,,,,,,,,,
The study recommends adopting knowledge graph-driven frameworks like KGDNet for safer,"more effective medicine recommendations.""","Limited research on integrating semantic, relational, and ontological knowledge for personalized medicine recommendation using Electronic Health Records (EHRs).",,,,,,,,,,,,,
Need for improved methods to minimize Drug-Drug Interactions (DDIs) while maintaining recommendation accuracy.,,,,,,,,,,,,,,,
"Insufficient exploration of hierarchical and temporal modeling of patient admission histories for medication recommendation.""",,,,"The objectives of the study are to develop a deep learning model (KGDNet) that uses Electronic Health Records and external medical information (like Drug-Drug Interaction data and ontologies) to provide personalized medicine recommendations, outperform baselines, control DDI rate, and analyze the impact of model components through ablation studies.",,,,,,,,,,,
Multimodal joint learning for personal knowledge base construction from Twitter-based lifelogs,"Yen An-Zi, Huang Hen-Hsen, Chen Hsin-Hsi",2020,reference-manager,10.1016/j.ipm.2019.102148,,"The system performs similarly for users with many or few tweets, showing robustness even with limited data. Multimodal-MTL-BiLSTM achieves the best results across tasks. Explicit and implicit life events are both well captured, with users expressing locations explicitly and actions implicitly. Head word matching improves evaluation accuracy.",,,,"Joint learning approach: Used for life event detection, explicit life event recognition, and implicit life event recognition, comparing unimodal (text only) and multimodal (text and image) models.",,How can life events be effectively extracted from tweets and represented as knowledge base facts using frame semantics to enable knowledge-based question answering and personal memory recall?,The paper aims to extract life events from tweets using frame semantics and convert natural language into knowledge base facts. The methodology involves multi-stage models for event detection and semantic parsing. Results show strong annotation agreement and effective extraction performance. The research enables applications like memory recall and living assistance.,The research goal is to extract life events from tweets and represent them as frame semantics and knowledge base facts; the approach involves a system for life event extraction and personal knowledge base construction; results show strong annotation agreement and F-scores up to 0.8594 for explicit subjects.,
Frame semantic parsing: Extracts and represents life events in tweets as structured knowledge base facts.,,,,,,,,,,,,,,,
Performance evaluation: Used F-score as the main metric,with data split into training,validation,"and test sets.""",,,"The annotation agreement for tweet types achieved a Cohen's kappa of 0.6341; average F-scores for KB relations, subjects, and times were 0.7174, 0.8424, and 0.7809, respectively.",,,,,,,,,
Explicit information extraction consistently outperformed implicit extraction (e.g.,explicit subjects F-score: 0.8594 vs. implicit: 0.7908).,,,,,,,,,,,,,,
"The system performed comparably across users regardless of tweet volume; no p-values or statistical significance measures were reported.""",Cohen's kappa for tweet type annotation: 0.6341,,,,,,,,,,,,,,
KB relations extraction average F-score: 0.7174 (Explicit: 0.7382,Implicit: 0.6517),,,,,,,,,,,,,,
Subjects extraction average F-score: 0.8424 (Explicit: 0.8594,Implicit: 0.7908),,,,,,,,,,,,,,
Predicates extraction explicit F-score: 0.7485,,,,,,,,,,,,,,,
Objects extraction average F-score: 0.7029 (Explicit: 0.7175,Implicit: 0.5542),,,,,,,,,,,,,,
Times extraction average F-score: 0.7809 (Explicit: 0.794,Implicit: 0.7042),,,,,,,,,,,,,,
Role of subjects average F-score: 0.8424 (Explicit: 0.8594,Implicit: 0.7208),,,,,,,,,,,,,,
Role of objects average F-score: 0.7043 (Explicit: 0.7232,Implicit: 0.5167),,,,,,,,,,,,,,
Role of times average F-score: 0.7507 (Explicit: 0.7667,Implicit: 0.6542),,,,,,,,,,,,,,
Pipeline system performance (F1 scores):,,,,,,,,,,,,,,,
Life event detection: 93.91%,,,,,,,,,,,,,,,
Explicit life event recognition: 44.90%,,,,,,,,,,,,,,,
Implicit life event recognition: 80.48%,,,,,,,,,,,,,,,
Frame semantic parsing: 58.06%,,,,,,,,,,,,,,,
Life event quadruples generation: 41.60%,,,,,,,,,,,,,,,
Removing metadata or BERT features reduces performance in all subtasks.,,,,,,,,,,,,,,,
System achieves comparable results across users,"regardless of tweet volume.""","No explicit limitations, shortcomings, or suggestions for further research are stated in the provided context.",The proposed system effectively extracts life events from tweets and represents them as knowledge base facts using frame semantics.,,,,,,,,,,,,
It performs comparably well for both frequent and infrequent users,even with limited training data.,,,,,,,,,,,,,,
Explicit life events are easier to extract than implicit ones.,,,,,,,,,,,,,,,
"The system supports applications like personal memory recall through structured event extraction.""",Investigating question answering systems over personal and world knowledge bases.,,,,,,,,,,,,,,
Improving extraction of implicit information in knowledge base construction.,,,,,,,,,,,,,,,
"Enhancing end-to-end performance in the pipelined workflow for life event extraction from tweets.""",,"The study design is an observational study using annotated tweets. It involves binary classification, sequence labeling with the BIO scheme, and joint learning approaches (unimodal and multimodal models). The dataset is split into training, validation, and test sets by timestamp. Performance is evaluated using F-score and Cohen's kappa.",,"The objectives of the study are to extract life events from tweets, represent them using frame semantics, and transform natural language into knowledge base facts.",,,,,,,,,,,
Multimodal Reasoning with Multimodal Knowledge Graph,"Lee Junlin, Wang Yequan, Li Jing, Zhang Min",2024,reference-manager,,,"Implementation Insights highlight that MR-MKG effectively enhances multimodal reasoning by leveraging multimodal knowledge graphs. It outperforms parameter-efficient models like LLaMA-Adapter and LaVIN, and shows greater gains as model size increases. New insight: MR-MKG surpasses LLaVA in most categories despite using fewer parameters, indicating high efficiency.",,,,"Multimodal-CoT prompting: A two-stage framework that first generates a rationale and then predicts the answer, using both text and image information.",,How can multimodal knowledge graphs be leveraged to enhance the multimodal reasoning capabilities of large language models in tasks such as multimodal question answering and analogy reasoning?,"The paper proposes MR-MKG, a method to enhance large language models’ multimodal reasoning using multimodal knowledge graphs (MMKGs). Through experiments on tasks like ScienceQA and MARS, MR-MKG achieves state-of-the-art results. Ablation studies confirm each component’s effectiveness. The approach demonstrates improved reasoning by integrating image, text, and knowledge triplets.","The research goal is to enhance multimodal reasoning in large language models using multimodal knowledge graphs (MMKGs); the approach, MR-MKG, integrates image, text, and knowledge triplets from MMKGs; results show MR-MKG achieves new state-of-the-art accuracy and efficiency on ScienceQA and MARS tasks.",
Sub-MMKG retrieval: Relevant subgraphs from a multimodal knowledge graph are retrieved based on text or image features using cosine similarity.,,,,,,,,,,,,,,,
Knowledge graph embedding (KGE): Entities and relations are embedded and transformed to enhance reasoning,"with architectures like RGAT evaluated.""",,"MR-MKG achieved new state-of-the-art accuracy on ScienceQA (up to 93.63%) and MARS (Hits@1: 41%), outperforming previous models.",,,,,,,,,,,,
RGAT knowledge graph embedding showed the best performance (ScienceQA: 92.78%,MARS: 40.5% Hits@1).,,,,,,,,,,,,,,
Using multimodal knowledge graphs and cross-modal alignment led to statistically significant improvements,"with up to 3.78% gain in targeted ScienceQA samples and 6.6% in MARS.""","MR-MKG (FLAN-T5-11B) achieves 92.78% average accuracy on ScienceQA, outperforming previous methods with fewer trainable parameters.",,,,,,,,,,,,,
"MR-MKG (FLAN-UL2-19B) achieves 93.63% average accuracy
On MARS","the highest reported.
MR-MKG (Visual\_LLaMA-2 7B) achieves Hits@1 of 0.405",a 10.4% increase over baseline Visual\_LLaMA-2 7B (0.286).,,,,,,,,,,,,,
MR-MKG surpasses LLaVA in all ScienceQA categories except SOC,with an average accuracy gain of +1.86%.,,,,,,,,,,,,,,
"Ablation studies show adding KG
On MARS","MMKG
adding KG","and cross-modal alignment to Visual\_FLAN-T5-11B improves accuracy by 3.78%
MMKG","1.41%
and alignment to Visual\_LLaMA-2 7B increases Hits@1 by 0.066","and 0.54% respectively on image-requiring samples.
0.095",,and 0.108 respectively.,,,,,,,,,
"RGAT architecture yields highest ScienceQA accuracy (92.78%) and MARS Hits@1 (40.5) among KGE architectures tested.""",The effectiveness of the retrieved sub-multimodal knowledge graph depends on the quality of the knowledge retrieval strategy; poor retrieval can lead to missing relevant knowledge and reduce LLM accuracy.,,,,,,,,,,,,,,
Evaluation was limited to four LLMs and two tasks due to computational constraints; larger models and more tasks remain unexplored.,,,,,,,,,,,,,,,
The MMKG may lack relevant information,limiting its usefulness for some questions.,,,,,,,,,,,,,,
"Ambiguities in the knowledge base can result in retrieving unrelated or confusing information.""","MR-MKG significantly enhances multimodal reasoning in LLMs by leveraging multimodal knowledge graphs, achieving new state-of-the-art results in ScienceQA and MARS tasks.",,,,,,,,,,,,,,
RGAT is the most effective knowledge graph embedding method.,,,,,,,,,,,,,,,
Retrieval strategy effectiveness depends on task modality; text-only retrieval performs best for ScienceQA.,,,,,,,,,,,,,,,
"Future work should improve retrieval schemes and scale evaluations to larger models and more tasks.""",The effectiveness of the retrieval scheme is crucial; improving retrieval strategies for more accurate multimodal knowledge is a key future direction.,,,,,,,,,,,,,,
Evaluation was limited to four LLMs and two tasks; future work should scale to larger models and broader tasks.,,,,,,,,,,,,,,,
Tailoring retrieval strategies to specific problem characteristics,rather than relying on a single modality,"is important.""","Future research should focus on improving the knowledge retrieval scheme to provide more accurate and relevant knowledge for multimodal reasoning tasks. Additionally, scaling the method to larger language models and evaluating its performance on a wider range of multimodal reasoning tasks is recommended.",No information available,,,,,,,,,,,
"Digital Twins’ Advancements and Applications in Healthcare, Towards Precision Medicine","Papachristou Konstantinos, Katsakiori Paraskevi F., Papadimitroulas Panagiotis, Strigari Lidia, Kagadis George C.",2024,reference-manager,10.3390/jpm14111101,,"Implementation Insights highlight that Digital Twin (DT) technologies in healthcare enable real-time monitoring, personalized interventions, and improved decision-making. Key steps include data harmonization, privacy protection, and feature extraction. New insights show DTs support precision medicine by tailoring treatments and identifying risks, but challenges remain in data quality, bias, and accessibility.",,,,"Comprehensive literature search across multiple databases (PubMed, IEEE Xplore, Scopus) using relevant keywords to identify studies on Digital Human Twins in healthcare.",,"How do Digital Human Twin (DHT) technologies contribute to the advancement of precision medicine, and what are the key challenges and considerations in their application for person-centered healthcare?","This review aimed to synthesize recent advancements in Digital Human Twin (DHT) technologies for precision medicine. Using a systematic review of 74 peer-reviewed articles, the study identified key DHT applications, challenges, and implications, concluding that DHTs hold significant promise for advancing personalized healthcare.","The research goal was to review recent advancements in Digital Human Twin (DHT) technologies for precision medicine using a systematic literature review approach, and the principal finding is that DHTs enhance precision medicine by enabling personalized diagnostics, treatment planning, and addressing key challenges like data quality, privacy, and equity.",
Systematic screening and filtering of articles based on inclusion criteria,focusing on peer-reviewed studies from the last ten years.,,,,,,,,,,,,,,
Thematic classification and thorough review of 74 selected articles to extract key findings,insights,"and implications.""",,"The review synthesized findings from 74 peer-reviewed articles, focusing on Digital Human Twin (DHT) technologies, precision medicine applications, and related challenges.",,,,,,,,,,,
Key themes included IoT,AI,cloud computing,virtual reality,risk stratification,,diagnosis,and data issues like quality,,,security,and privacy.,bias,targeted therapies,planning,modeling
"No quantitative results or statistical significance (p-values) were reported in the provided context.""",The primary outcomes focus on the role of Digital Human Twin (DHT) technologies in advancing precision medicine.,,,,,,,,,,,,,,
DHTs enable highly personalized therapies by capturing unique genetic,metabolic,and environmental characteristics.,,,,,,,,,,,,,
DHTs can improve disease modeling,risk stratification,rapid diagnosis,surgical planning,targeted therapies,,and drug discovery.,,,,,,,,,
"No specific statistical values or quantitative results are provided.""",Medical data quality varies due to hospital procedures and staff experience.,,,,,,,,,,,,,,
Data collection is costly and time-consuming.,,,,,,,,,,,,,,,
Ensuring data accuracy,consistency,and interoperability is challenging.,,,,,,,,,,,,,
Datasets often have racial,gender,and demographic biases.,,,,,,,,,,,,,
"Limited accessibility may worsen the digital divide.
Robustness",accountability,transparency,"and fairness are ongoing challenges.""","Digital Human Twin (DHT) technologies play a significant role in advancing precision medicine, enabling applications like risk stratification, rapid diagnosis, and targeted therapies.",,,,,,,,,,,
Key challenges include ensuring data quality,addressing bias,and maintaining security,privacy,equity,,and accessibility.,,,,,,,,,
Recommendations emphasize data harmonization,privacy protection,"and standardized protocols for effective DHT implementation.""","Develop robust data governance frameworks to address privacy, consent, and data ownership issues.",,,,,,,,,,,,
Ensure AI transparency and fairness to mitigate bias and inequitable outcomes,especially for underrepresented populations.,,,,,,,,,,,,,,
"Create secure data-handling protocols to safeguard sensitive health information and support ethical use of Digital Human Twins (DHTs).""","Future research should focus on creating robust data governance frameworks, ensuring AI transparency and fairness, and developing secure data-handling protocols. Addressing data quality, bias, privacy, consent, and data ownership are key gaps that need further investigation for ethical and effective use of Digital Human Twins (DHTs).","The study design is a systematic review. It involved screening titles and abstracts, removing duplicates, applying specific inclusion criteria, and thoroughly reviewing 74 peer-reviewed articles published in the last ten years. The articles were classified into major themes and synthesized to inform the discussion and conclusions.",,"The objectives of the study are to review and synthesize recent research on Digital Human Twin (DHT) technologies and their applications in precision medicine, focusing on person-centered risk stratification, rapid diagnosis, disease modeling, surgical planning, targeted therapies, and drug discovery, while addressing related challenges and implications.",,,,,,,,,,,
Knowledge Graph Generation and Enabling Multidimensional Analytics on Bangladesh Agricultural Data,"Pratap Deb Nath Rudra, Rani Das Tithi, Chandro Das Tonmoy, Shafkat Raihan S. M.",2024,reference-manager,10.1109/access.2024.3416388,,Implementation Insights are:,,,,,,,,,
The study models and integrates Bangladesh agriculture open data using ontologies and multidimensional (MD) semantics.,,,,,,,,,,,,,,,
Data is semantically integrated from diverse sources,published as Linked Data (LD) and FAIR data,and enables OLAP-style analysis.,,,,,,,,,,,,,
"The resulting BDAKG knowledge graph supports federated analytics and user-friendly exploration.""",,"The paper's main objective is to enable sustainable, FAIR, and analytical management of Bangladesh agricultural data by semantically modeling, integrating, and publishing it as a knowledge graph (BDAKG), with the principal finding that this approach supports multidimensional analysis and federated analytics across diverse data sources.","The paper aims to create BDAKG, a semantic knowledge graph for Bangladesh agriculture open data, by integrating multiple sources using advanced data representation and Linked Data principles. Methodology includes semantic annotation, data integration, and linking to external datasets. Results show improved data FAIRness, OLAP compatibility, and support for federated analytics, enabling actionable insights for sustainability.",,,"How can knowledge graph generation and semantic web technologies be applied to sustainably manage, integrate, and enable multidimensional analytics of Bangladesh agricultural data to fulfill FAIR principles and support data-driven decision-making for sustainable development goals?",,,,,,,,,Data warehousing and OLAP (Online Analytical Processing): Used to organize and analyze agriculture data for multidimensional analysis and ad hoc queries.
Knowledge graphs and Semantic Web techniques: Applied for semantic integration,representation,and exploration of agriculture data using Linked Data and RDF.,,,,,,,,,,,,,
Data mining and machine learning algorithms: Utilized to extract patterns and insights,"such as forecasting crop yields.""","The research provides reproducibility through publicly available data dumps at http://bike-csecu.com/datasets/agri/, a SPARQL endpoint at https://bdakg.bike-csecu.com/sparql/, and an interactive OLAP interface with source code at https://github.com/bi-setl/SETL. Competency questions and a user interface are also provided.","BDAKG contains 9 levels, 33 level attributes, 272 level members, 363 external links, and 1738 RDF triples, all semantically linked to external databases.",,,,,,,,,,,,
BDAKG enables OLAP operations and answers queries in under one minute,compared to over two hours manually,demonstrating significant efficiency improvement.,,,,,,,,,,,,,
"No statistical significance (p-values) or quantitative experimental results beyond time comparisons are reported.""","BDAKG has 9 levels, 33 level attributes, 272 level members, 363 external links, and 1,738 RDF triples.",,,,,,,,,,,,,,
Extraction phase of ETL process takes 60.24% of total time due to extra preprocessing and cleansing.,,,,,,,,,,,,,,,
Quality evaluated by completeness (schema,property,linkability),timeliness,granularity,,FAIRness,,,,,,,,and correctness.,OLAP compatibility
"Property completeness issue: missing scientific name for onion.""","Most agriculture data sources are unsustainable, static, and read-only, limiting interactive analysis and integration.",,,,,,,,,,,,,,
Datasets do not follow FAIR principles,making them hard to find,integrate,and reuse.,,,,,,,,,,,,
Semantic heterogeneity exists due to inconsistent data descriptions across sources.,,,,,,,,,,,,,,,
Data is mostly published at an aggregate level,limiting detailed understanding.,,,,,,,,,,,,,,
Data are not globally interlinked,hindering cross-dataset analysis and comparison.,,,,,,,,,,,,,,
"No research has addressed sustainable management and presentation of Bangladesh agricultural data.""","BI tools are underutilized in agriculture, especially in Czech small farms, despite IT investments.",,,,,,,,,,,,,,
Most agricultural data are not FAIR-compliant,limiting discoverability,integration,and reuse.,,,,,,,,,,,,
The BDAKG knowledge graph enables semantic definition,integration,and analytical exploration of Bangladesh agricultural data.,,,,,,,,,,,,,
Recommendation: Adopt Semantic Web and Linked Data principles for sustainable,"interoperable agricultural data management.""","Lack of sustainable management and presentation of Bangladesh agricultural data, especially regarding FAIR principles and semantic integration.",,,,,,,,,,,,,
Limited availability of agriculture datasets as semantic linked data,hindering integration and interactive analysis.,,,,,,,,,,,,,,
"Need to apply data mining techniques for forecasting and pattern extraction across broader sectors beyond agriculture.""","Future research should address sustainable management and presentation of Bangladesh agricultural data, semantic heterogeneity, lack of FAIR principles, limited data integration, and insufficient analytical exploration. Further investigation is needed into semantic definition, efficient integration, and multidimensional analytics using knowledge graphs and federated data analysis.",,,"The objectives of the study are to investigate the application of Business Intelligence (BI) tools and techniques to agriculture data, generate a knowledge graph (BDAKG), and assess its performance, quality, and ability to support multidimensional analytics and exploratory insights, including OLAP operations.",,,,,,,,,,,
Privacy preservation in federated learning: An insightful survey from the GDPR perspective,"Truong Nguyen, Sun Kai, Wang Siyao, Guitton Florian, Guo YiKe",2021,reference-manager,10.1016/j.cose.2021.102402,,Implementation Insights are:,,,,,,,,,
Detailed documentation of data processing operations and their purposes is essential.,,,,,,,,,,,,,,,
Necessity and proportionality of each operation must be assessed.,,,,,,,,,,,,,,,
Security and privacy risks are identified,with mitigation via Secure Aggregation and Homomorphic Encryption.,,,,,,,,,,,,,,
FL systems face transparency and fairness challenges.,,,,,,,,,,,,,,,
"Further research is needed on efficient privacy techniques and bias mitigation.""",,"The research goal is to assess privacy preservation in federated learning (FL) under GDPR, using a systematic analysis of data processing operations and privacy risks; the key method is evaluating FL’s compliance with GDPR principles and privacy-preserving techniques; the principal finding is that FL alone is insufficient for GDPR compliance due to residual privacy risks.","The paper investigates privacy preservation in federated learning (FL) from the GDPR perspective. It systematically describes FL data processing, assesses necessity and risks, and reviews privacy-preserving measures. The study finds that FL alone does not ensure GDPR compliance, highlighting the need for additional safeguards and thorough impact assessments.",,,"How can federated learning systems be designed and operated to ensure compliance with the GDPR, addressing privacy risks, transparency, and the challenges of implementing effective privacy-preserving measures?",,,,,,,,,"Data anonymisation: Removing or hiding sensitive information (e.g., personally identifiable information) to prevent identification of individuals in datasets."
Gradient descent variants: Using batch,stochastic,and mini-batch gradient descent methods to optimize machine learning models.,,,,,,,,,,,,,
Privacy-preserving techniques: Implementing Secure Aggregation,Homomorphic Encryption,"and Differential Privacy to protect data during federated learning.""",,"Federated learning (FL) improves privacy by keeping data on-device, but exchanging model parameters can still leak sensitive information, making FL not inherently GDPR-compliant.",,,,,,,,,,,
Traditional anonymization techniques (like k-anonymity,l-diversity,t-closeness) are insufficient against linkage attacks; differential privacy offers stronger guarantees.,,,,,,,,,,,,,
Privacy-preserving methods (Secure Aggregation,Homomorphic Encryption,Differential Privacy) mitigate some risks,"but vulnerabilities like model poisoning and inference attacks remain; no p-values or quantitative results reported.""",Primary outcomes:,,,,,,,,,,,
Federated learning (FL) enables distributed ML training without aggregating sensitive data,improving model performance and privacy.,,,,,,,,,,,,,,
FL is not inherently GDPR-compliant due to potential privacy attacks via exchanged model parameters.,,,,,,,,,,,,,,,
Privacy-preserving techniques (e.g.,Secure Aggregation,Homomorphic Encryption,Differential Privacy) are implemented but have limitations.,,,,,,,,,,,,
There is a trade-off between system efficiency and privacy guarantee.,,,,,,,,,,,,,,,
Results and measured effects:,,,,,,,,,,,,,,,
"No explicit statistical values or quantitative results are provided in the context.""","Data minimization is challenging in traditional ML, as it's hard to determine the minimal necessary data for training.",,,,,,,,,,,,,,
"Back-door attacks can inject unauthorized purposes
Determining adequate","risking exposure of personal information; no current solution exists for this in FL.
limited",and relevant data before ML execution is problematic.,,,,,,,,,,,,,
GDPR requirements may overly restrict ML operations.,,,,,,,,,,,,,,,
Redesigning ML algorithms for privacy is technologically and financially demanding.,,,,,,,,,,,,,,,
"Trade-off between efficiency and privacy-guarantee may reduce system performance.
k-anonymity-based anonymization techniques cannot defend against linkage attacks if adversaries know sensitive attributes.""","Federated Learning (FL) enhances privacy by keeping data on user devices and only sharing model parameters, but some privacy risks remain.",,,,,,,,,,,,,,
FL helps comply with GDPR data minimization by not collecting original data,yet back-door attacks and model poisoning remain challenges.,,,,,,,,,,,,,,
More research is needed on interpretable,unbiased ML models and balancing privacy,accuracy,and fairness in FL.,,,,,,,,,,,,
Recommendations include thorough documentation,risk assessment,"and implementing advanced privacy-preserving techniques.""","Further development of efficient cryptographic and privacy techniques for decentralized collaborative learning, especially to counter model poisoning and inference attacks.",,,,,,,,,,,,
In-depth research on transparency,interpretability,and algorithm fairness in federated learning (FL) systems,particularly for decentralized,non-IID data.,,,,,,,,,,,
Thorough exploration of trade-offs between privacy utility,accuracy,interpretability,"and fairness in FL frameworks.""","Future research should focus on developing efficient cryptographic and privacy techniques for federated learning (FL), improving transparency, interpretability, and fairness in FL algorithms, and exploring trade-offs between privacy, utility, accuracy, and fairness. Research is also needed on adapting centralized AI/ML approaches to decentralized, non-IID data settings.",,No information available,,,,,,,,"The objectives of the study are: to systematically describe and justify data processing operations and their purposes; assess the necessity, proportionality, security, and privacy risks of each operation; and review technical measures and compliance with GDPR, including data subject rights and secure communication protocols.",
Intelligent Approach for Heterogeneous Data Integration: Information Processes Analysis Engine in Clinical Remote Monitoring Systems,"Khovrichev Mikhail, Elkhovskaya Liubov, Fonin Vladimir, Balakhontceva Marina",2019,reference-manager,10.1016/j.procs.2019.08.188,,"The paper proposes an intelligent approach for integrating heterogeneous data into a unified event log for remote monitoring of arterial hypertension and chronic heart failure. By analyzing event cycles (meta-states), it enables process optimization, critical transition identification, and development of personalized predictive models. New insight: meta-states enhance process analysis.",,,,Automated integration of heterogeneous data and knowledge sources into a single event log for process analysis.,,How can an intelligent approach for automated integration of heterogeneous data and knowledge sources be designed and implemented to optimize remote monitoring and treatment processes for patients with arterial hypertension and chronic heart failure in personalized healthcare?,"The paper aims to develop an intelligent approach for integrating heterogeneous data in clinical remote monitoring systems. Using data mining, process mining, and knowledge formalization, the methodology consolidates event logs to design accurate process models. The main finding is improved detection and elimination of data errors, enhancing process compliance and patient safety.","The research goal was to design an intelligent approach for integrating heterogeneous data in personalized healthcare; the method involved developing a software component for event log analysis in remote monitoring systems, and results showed successful process reconstruction and identification of critical transitions for optimizing patient care.",
Hybrid technology for data integration at logical and semantic levels,converting integrated data into event chains.,,,,,,,,,,,,,,
Use of process mining,data mining,"and knowledge mining techniques to analyze and formalize information processes.""",,The intelligent integration approach enables automated combination of heterogeneous data and knowledge sources for analyzing chronic disease management in remote monitoring.,,,,,,,,,,,
The system reconstructs event timelines (measurement,server receipt,“Red zone” generation) to identify process delays; only “Red zone” events were analyzed for processing speed.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""",The intelligent approach enabled automated integration of heterogeneous data into a single event log for analyzing chronic disease processes.,,,,,,,,,,,,,,
Experimental studies involved remote monitoring and counseling of patients with arterial hypertension (AH) across three scenarios.,,,,,,,,,,,,,,,
The final event log included 219 patients (73 males,146 females),with event chains ranging from 5 to 1228 events.,,,,,,,,,,,,,
"Average observation time was 35.2 weeks; median was 41.4 weeks.
10 types of events and 4 roles were identified.",,,,,,,,,,,,,,,
"Time response analysis focused on """"""""Red zone"""""""" events",evaluating delays between patient measurement,server receipt,"and """"""""Red zone"""""""" generation.",,,,,,,,,,,,
"No explicit statistical values or measured effects are provided in the context.""",Data sources heterogeneity and diversity may lead to integration errors and failures.,,,,,,,,,,,,,,
Models mismatch in different subsystems can cause tracking issues for vital parameters.,,,,,,,,,,,,,,,
Not every measurement time was possible to detect,leading to incomplete analysis.,,,,,,,,,,,,,,
Simulation models generate heterogeneous data,making optimality analysis difficult.,,,,,,,,,,,,,,
The study mainly focuses on arterial hypertension and chronic heart failure,"limiting generalizability.""",The intelligent integration approach enables reconstruction and evaluation of remote monitoring processes for patients with hypertension and heart failure.,,,,,,,,,,,,,
Analysis of time indicators helps identify potential delays in data transmission that could negatively impact patient outcomes.,,,,,,,,,,,,,,,
"Focusing on """"""""Red zone"""""""" events allows assessment of system processing speed.",,,,,,,,,,,,,,,
"Recommendation: Use integrated event logs to optimize monitoring and treatment processes.""",,"Future research should focus on developing effective methods and algorithms for processing unstructured and incomplete medical data, improving semantic data integration and interoperability, testing scalability and fault tolerance in virtual environments, and creating personalized predictive models based on meta-states in remote monitoring processes.",,,,"The objectives of the study are to design and propose an architecture for integrating heterogeneous event logs from remote monitoring of patients (mainly with arterial hypertension and chronic heart failure), enabling reconstruction and analysis of the monitoring and treatment process, and ensuring compliance with regulations through process model optimization.",,,,,,,,,
"Knowledge Graph Completion: A Bird’s Eye View on Knowledge Graph Embeddings, Software Libraries, Applications and Challenges","Garga Satvik, Roy Dwaipayan",2022,reference-manager,,,"Implementation Insights highlight that comparing top models is challenging due to varied training techniques, loss functions, regularization, and hyperparameter tuning. Recent studies show that model performance can change significantly with different training and calibration methods. Proper calibration and search space selection are crucial for reliable Knowledge Graph Embedding (KGE) evaluation.",,,,"Similarity Based Methods: These use the structure of the graph to assign similarity scores between node pairs, predicting links based on how similar or close nodes are within the graph.",,"What are the current approaches, challenges, and applications of knowledge graph-based question answering systems, and how do different methodologies—such as semantic parsing, information retrieval, embedding, and deep learning—contribute to advancing question answering over knowledge graphs?","This paper provides a comprehensive review of knowledge graph completion (KGC), focusing on traditional and modern techniques, including knowledge graph embeddings (KGE), reinforcement learning, and multimodal data. It analyzes software libraries, discusses real-world applications, identifies open research challenges, and offers practical guidance for researchers in KGC.","The paper's main objective is to provide a comprehensive survey of knowledge graph embedding (KGE) methods for link prediction, using a systematic review approach, and its principal finding is a detailed comparative analysis of KGE models, software libraries, and open research challenges in knowledge graph completion.",
Knowledge Graph Embeddings (KGE): This technique represents entities and relationships as low-dimensional vectors,enabling efficient link prediction and analysis.,,,,,,,,,,,,,,
"Reinforcement Learning Techniques: Methods like DeepPath and Minerva are used to infer complex queries in knowledge graph completion tasks.""","The research is highly reproducible, as several open-source libraries for knowledge graph embedding (KGE) are available, such as OpenKE, AmpliGraph, Pytorch BigGraph, Pykg2vec, LibKGE, GraphVite, PyKeen, DGL-KE, and TORCH-KGE. Source code is distributed for these projects.","The study reviews key evaluation metrics for Knowledge Graph Completion (KGC): Mean Reciprocal Rank, Mean Rank, and Hits@K, explaining their calculation and interpretation.",,,,,,,,,,,,,
Recent developments in reinforcement learning for KGC and the integration of real-world knowledge (numeric,text,images,temporal data) are examined.,,,,,,,,,,,,
No explicit quantitative results,primary findings,"or statistical significance (p-values) are provided in the context.""","Primary outcomes include the application of knowledge graphs in health informatics and drug discovery, improving accuracy and speed in clinical data extraction and mapping diseases to symptoms.",,,,,,,,,,,,
Experimental results show that model performance varies with training techniques; RESCAL outperformed state-of-the-art models like TuckER and ConvE.,,,,,,,,,,,,,,,
Evaluation metrics used: Mean Rank (lower is better),Mean Reciprocal Rank (higher is better),and Hits@K (higher is better).,,,,,,,,,,,,,
"No specific statistical values are provided.""",Limited by space and time constraints.,,,,,,,,,,,,,,
Focused only on KGE for link prediction; other KGC tasks like triple and entity classification not explored.,,,,,,,,,,,,,,,
Only static knowledge graphs considered; dynamic,heterogeneous,and bipartite graphs not addressed.,,,,,,,,,,,,,
Difficulty comparing model performance due to varied training techniques and parameter tuning.,,,,,,,,,,,,,,,
No ablation studies conducted.,,,,,,,,,,,,,,,
Shallow embedding approaches lack parameter sharing and cannot handle new nodes after training.,,,,,,,,,,,,,,,
Knowledge protection in cybersecurity KGs needs further research,"especially for rapid updates and incident isolation.""","The study provides a comprehensive overview of traditional and modern knowledge graph embedding (KGE) techniques for link prediction, including reinforcement learning and multimodal approaches.",,,,,,,,,,,,,
Comparative analysis of open-source libraries and methodologies highlights current strengths and gaps.,,,,,,,,,,,,,,,
Key research challenges identified include robustness,scalability,and knowledge transfer.,,,,,,,,,,,,,
The study recommends further exploration of dynamic,heterogeneous,"and bipartite knowledge graphs.""",Methods for isolating digital security incidents using knowledge graphs and rapidly updating them need further exploration.,,,,,,,,,,,,
More research is needed on areas beyond link prediction,such as triple classification,entity classification,and dynamic,heterogeneous,,and bipartite graphs.,,,,,,,,,
Open challenges include robustness,scalability,few-shot learning,knowledge transfer,"and multi-path predictions.""",,"Future research should explore areas beyond link prediction, such as triple and entity classification, dynamic and heterogeneous graphs, calibration, reciprocal relations, different training strategies, robustness, scalability, few-shot learning, knowledge transfer, multi-path predictions, and rapid updating of knowledge graphs for cybersecurity applications.",,,,,,,"The objectives of the study are to review and analyze knowledge graph embeddings (KGE) for link prediction, examine traditional and modern techniques, evaluate reinforcement learning methods, compare software libraries, discuss real-world applications, and identify open research challenges and future directions in knowledge graph completion.",,
Docs2KG: Unified Knowledge Graph Construction from Heterogeneous Documents Assisted by Large Language Models,"Sun Qiang, Luo Yuanyi, Zhang Wenxiao, Li Sirui, Li Jichunyang, Niu Kai, Kong Xiangrui, Liu Wei",2024,reference-manager,,,"Implementation Insights highlight that Docs2KG integrates heterogeneous, unstructured documents (e.g., PDFs, Excel, images) into a unified knowledge graph (KG), preserving source references for Retrieval Augmented Generation (RAG). Its dual-path strategy combines deep learning-based layout analysis and markdown parsing, enabling dynamic, extensible, and multimodal data extraction and semantic representation.",,,,"Extraction of multimodal data (tables, texts, images, figures) from diverse formats using tools like BeautifulSoup for HTML and pandas for Excel.",,"How can a unified knowledge graph framework, assisted by large language models, be constructed from heterogeneous unstructured documents to enable effective integration, semantic representation, and retrieval of multimodal information while maintaining references to the original data sources?","The paper proposes Docs2KG, a system that integrates unstructured, heterogeneous documents (PDF, Excel, etc.) into a unified Knowledge Graph (KG). Using dual-path processing and multimodal extraction, Docs2KG enables semantic and structural queries. The approach improves information retrieval and supports Retrieval Augmented Generation, reducing hallucination in language models.","The research goal is to unify extraction and integration of multimodal unstructured data using the Docs2KG framework, which combines deep learning-based document layout analysis and structured parsing; the principal finding is that Docs2KG enables dynamic, extensible knowledge graph construction for effective semantic and structural information retrieval.",
Integration of modality-specific extraction models into a unified Knowledge Graph (KG) framework (Docs2KG) to represent semantic and structural relationships.,,,,,,,,,,,,,,,
"Semantic and structural proximity-based information retrieval using embedding models and similarity search within the KG.""","The research uses open-source libraries such as markdownify (source code: https://github.com/matthewwithanm/python-markdownify), BeautifulSoup, pandas, and imgkit. There is no explicit mention of source code for the main Docs2KG project. No further reproducibility details are provided.","Docs2KG enables extraction and integration of multimodal data (tables, text, images, figures) from heterogeneous unstructured documents into a unified knowledge graph (KG).",,,,,,,,,,,,,
Combining PDF and Excel files using Docs2KG allows retrieval of population information from 2011 to 2021,which is not possible with either file alone.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""","Primary outcome: Integration of population data from 2011 to 2021 using Docs2KG, combining PDF and Excel sources.",,,,,,,,,,,,,,
Result: Neither file alone provided meaningful insights; integration enabled extraction of relevant population information.,,,,,,,,,,,,,,,
Measured effect: Visualization showed connections between events in 2011 and 2021 referenced in the introduction section.,,,,,,,,,,,,,,,
"No statistical values reported.""",No explicit limitations or shortcomings are stated in the provided context.,,,,,,,,,,,,,,
"No self-reported problems or suggestions for further research are mentioned.
,Docs2KG effectively integrates heterogeneous",unstructured data (PDF,Excel) into a unified Knowledge Graph (KG),enabling meaningful information extraction.,,,,,,,,,,,,
Combining multimodal data sources is essential; insights cannot be derived from single files alone.,,,,,,,,,,,,,,,
The system reduces time,effort,"and risk of outdated or hallucinated knowledge in retrieval tasks.""","Extraction of multimodal data (including tables, texts, images, and figures) from diverse formats remains a major challenge.",,,,,,,,,,,,
Integrating modality-specific information extraction models into a single unified framework is still unresolved.,,,,,,,,,,,,,,,
"Achieving meaningful data representation with references to the original source for improved knowledge retrieval and reduced hallucination.""",,,,The objectives of the study are:,,,,,,,,,,,
To extract multimodal data (tables,texts,images,figures) from diverse formats.,,,,,,,,,,,,
To integrate modality-specific extraction models into a unified framework.,,,,,,,,,,,,,,,
"To represent data meaningfully with references to the source using knowledge graphs via the Docs2KG system.""",,,,,,,,,,,,,,,
Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic Encryption for Collaborative Anti-Money Laundering,"Effendi Fabrianne, Chattopadhyay Anupam",2024,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
The paper developed two privacy-preserving pipelines for AML detection using Fully Homomorphic Encryption (FHE): a GNN pipeline (using GIN with TFHE,optimized by quantization and pruning) and a graph-based XGBoost pipeline (using a Graph Feature Preprocessor). XGBoost achieved over 99% accuracy,F1-score,precision,and recall on balanced data,Creation of two modified AML datasets using undersampling (for balanced data) and random sampling (for imbalanced data).,with graph features improving F1-score by 8% on imbalanced data. XGBoost maintained performance in both clear and FHE-encrypted settings,explore other privacy methods,"This paper introduces a privacy-preserving approach for collaborative anti-money laundering (AML) detection using fully homomorphic encryption (FHE). It develops GNN and XGBoost pipelines, achieving over 99% accuracy with XGBoost. The approach maintains privacy and performance but faces computational overhead, suggesting future work on efficiency and scalability.","How can Fully Homomorphic Encryption (FHE), specifically TFHE, be integrated with graph-based machine learning models to enable privacy-preserving, collaborative anti-money laundering (AML) detection across financial institutions without compromising data confidentiality or predictive performance?",,"The research goal was to develop a privacy-preserving collaborative AML detection method using FHE; the approach involved GNN and graph-based XGBoost pipelines with TFHE, and the principal finding was that XGBoost achieved over 99% accuracy while maintaining performance under FHE, demonstrating robust privacy-preserving capabilities.","and address scalability.""",increasing graph feature complexity reduced inference time. Bayesian optimization efficiently tuned hyperparameters. Future work should improve FHE compatibility,822.54x). Interestingly,but FHE inference was much slower (up to 88
Development of privacy-preserving machine learning pipelines using Fully Homomorphic Encryption (FHE),specifically the TFHE scheme.,,,,,,,,,,,,,,
Implementation of graph-based XGBoost and Graph Neural Network (GNN) pipelines,"incorporating graph feature preprocessing.""",,"The XGBoost model achieved over 99% accuracy, F1-score, precision, and recall on the balanced dataset, with no significant performance difference between unencrypted and FHE-encrypted inference.",,,,,,,,,,,,
On the imbalanced dataset,F1-score improved by 8% with graph-based features,but overall F1-score and recall were lower (F1: 0.3056–0.3867; Recall: 0.1897–0.2500).,,,,,,,,,,,,,
FHE-encrypted inference incurred significant computational overhead (up to 88,822.54x slower),"but increasing graph feature complexity reduced inference time; no p-values were reported.""","On the balanced AML dataset, the Bayesian optimized XGBoost model achieved over 99% accuracy, F1 score, precision, and recall with both unencrypted and FHE-encrypted data.",,,,,,,,,,,,
Adding graph-based features did not significantly improve performance on the balanced dataset.,,,,,,,,,,,,,,,
FHE encryption caused inference time to increase by over 100,"000 times compared to unencrypted inference.""",Use of synthetic and modified datasets due to lack of access to real-world financial crime data.,,,,,,,,,,,,,
Computational constraints from Fully Homomorphic Encryption (FHE) limited dataset size and model complexity.,,,,,,,,,,,,,,,
No GPU support in Concrete ML,resulting in slower CPU-based experiments.,,,,,,,,,,,,,,
Scalability to larger datasets remains untested.,,,,,,,,,,,,,,,
Legal and regulatory barriers hinder comprehensive data sharing across institutions.,,,,,,,,,,,,,,,
"Further research needed to improve FHE model compatibility and explore alternative privacy-preserving methods.""",The proposed privacy-preserving approach using FHE enables collaborative AML detection without compromising predictive performance.,,,,,,,,,,,,,,
XGBoost achieved over 99% accuracy,F1-score,precision,and recall on balanced data,with graph-based features improving F1-score by 8% on imbalanced data.,,,,,,,,,,,
FHE incurs significant computational overhead.,,,,,,,,,,,,,,,
"Future work should improve FHE efficiency and explore alternative privacy-preserving methods.""",Improving Fully Homomorphic Encryption (FHE) compatibility and performance of models.,,,,,,,,,,,,,,
Investigating alternative privacy-preserving methods,such as differential privacy or multi-party computation,to enhance privacy-preserving capabilities.,,,,,,,,,,,,,
"Assessing scalability of solutions to accommodate larger datasets.""","Future research should focus on improving FHE compatibility and model performance, exploring alternative privacy-preserving methods like differential privacy or multi-party computation, and assessing scalability for larger datasets. These directions address current limitations and aim to enhance privacy-preserving machine learning for AML detection.",,,"The objectives of the study are to develop and evaluate privacy-preserving machine learning pipelines for collaborative anti-money laundering (AML) detection using Fully Homomorphic Encryption (FHE), focusing on robust model performance, efficient public key sharing, and handling computational constraints while preserving privacy.",,,,,,,,,,,
A Question-Answering Assistant over Personal Knowledge Graph,"Liu Lingyuan, Du Huifang, Zhang Xiaolian, Guo Mengying, Wang Haofen, Wang Meng",2024,reference-manager,10.1145/3626772.3657665,,"Implementation Insights are as follows: Data from various Android applications is simulated and processed using Large Language Models (LLMs) for Information Extraction. A task decomposition method based on Chain of Thought (CoT) improves extraction. The system integrates Symbolic Semantic Parsing, FAQ Matching, and Neural Semantic Parsing for efficient, flexible QA.",,,,"Simulated user data generation using a large language model (LLM) to mimic text from various Android applications, considering distinct textual styles.",,"How can a unified PKGQA system integrate data from various applications to provide accurate, efficient, and flexible personalized question answering for users?","The paper presents a unified PKGQA system that integrates data from various Android applications to enable accurate, personalized question answering. Using LLM-based information extraction and a modular approach (Symbolic Semantic Parsing, FAQ Semantic Matching, Neural Semantic Parsing), the system achieves efficient, flexible, and accurate responses. Experimental results confirm its effectiveness.","The research goal is to build a unified PKGQA system for integrating multi-application data; the approach combines Symbolic Semantic Parsing, FAQ Semantic Matching, and Neural Semantic Parsing; results show accurate, efficient, and flexible question answering with strong performance on multi-hop and multi-conditional queries.",
Information Extraction (IE) based on LLMs,utilizing a task decomposition method with Chain of Thought (CoT) for fine-grained extraction.,,,,,,,,,,,,,,
Implementation of Symbolic Semantic Parsing,FAQ Semantic Matching,"and Neural Semantic Parsing for question answering.""",,"Removing any PKGQA system component (Symbolic Semantic Parsing, Neural Semantic Parsing, FAQ Semantic Matching) reduces overall accuracy; Symbolic Semantic Parsing and Neural Semantic Parsing are especially important for multi-hop and multi-conditional questions.",,,,,,,,,,,
The system answers simple queries in milliseconds and complex ones in 2-3 seconds.,,,,,,,,,,,,,,,
"No explicit p-values or quantitative accuracy metrics are provided.""","The PKGQA system demonstrates accurate and efficient processing, providing fast and precise responses for both simple (milliseconds) and complex (2-3 seconds) queries.",,,,,,,,,,,,,,
Ablation experiments show that removing Symbolic Semantic Parsing or Neural Semantic Parsing reduces overall accuracy,especially for multi-hop and multi-conditional questions.,,,,,,,,,,,,,,
All three modules (Symbolic Semantic Parsing,FAQ Semantic Matching,"Neural Semantic Parsing) are important for optimal performance.""",No information available,"The PKGQA system effectively integrates data from multiple applications to deliver accurate, personalized user experiences with efficient and flexible processing.",,,,,,,,,,,
Symbolic Semantic Parsing and Neural Semantic Parsing modules are crucial for high accuracy,especially on multi-hop and multi-conditional questions.,,,,,,,,,,,,,,
The system provides acceptable response times: milliseconds for simple queries and 2–3 seconds for complex ones.,,,,,,,,,,,,,,,
"The user-friendly GUI enhances usability by clearly presenting structured knowledge and additional information.""",,,No information available,,,"The objectives of the study are to design a comprehensive and detailed PKG schema tailored for mobile data, integrate data from various applications into a unified PKGQA system, and develop an efficient, accurate, and user-centric question-answering system with flexible processing and a user-friendly interface.",,,,,,,,,
A Hybrid Optimization and Machine Learning Framework for Urban Traffic Management Using Cyber-Physical Digital Twin Architecture,"Ramal P.Janaki, Anbalagan E.",2024,reference-manager,10.1109/upcon62832.2024.10983125,,"The implementation leverages Google Maps APIs for real-time, multi-source traffic data collection, including traffic density, lane counts, and road conditions. Integrating AI and ML, especially ANN, improves prediction accuracy and signal optimization. Challenges include data integration, algorithm scalability, and handling unexpected traffic surges.",,,,"Real-time traffic data collection using Google Maps APIs and SDKs, including Street View, Elevation, and Aerial View APIs, to gather comprehensive traffic and infrastructure data.",,"How can the Cyber-Physical Digital Twin (CPDT) architecture, integrating real-time data and hybrid optimization algorithms, be used to model and optimize urban traffic systems for improved traffic flow and reduced congestion?","The paper aims to improve urban traffic management by optimizing signal timings and traffic flow using Artificial Bee Colony and Particle Swarm Optimization algorithms. Using real-world data and machine learning, the proposed model reduced average vehicle waiting time from 120 seconds (traditional) to 75 seconds, enhancing traffic efficiency.","The research goal was to improve urban traffic management using a hybrid approach combining Artificial Bee Colony, Particle Swarm Optimization, and Transformers; results showed reduced average vehicle waiting time from 120 to 75 seconds compared to traditional methods.",
Data cleaning and preprocessing,including outlier removal,normalization,and interpolation of missing data.,,,,,,,,,,,,
"Integration of optimization algorithms and Transformer networks within the CPDT framework for adaptive urban traffic management.""",,The proposed model reduced average vehicle waiting time from 120 seconds (traditional method) to 75 seconds.,,,,,,,,,,,,,
The approach improved traffic flow management and decreased vehicle delay times compared to conventional systems.,,,,,,,,,,,,,,,
"No p-values or statistical significance data are provided in the context.""",Primary outcomes:,,,,,,,,,,,,,,
Enhanced reliability of traffic predictions.,,,,,,,,,,,,,,,
Improved traffic flow management.,,,,,,,,,,,,,,,
Decreased vehicle delay times.,,,,,,,,,,,,,,,
Results and measured effects:,,,,,,,,,,,,,,,
Average vehicle waiting time reduced from 120s (traditional) to 75s (proposed).,,,,,,,,,,,,,,,
Average travel time reduced from 300s to 225s.,,,,,,,,,,,,,,,
Fuel consumption reduced from 50L to 35L.,,,,,,,,,,,,,,,
Reductions: fuel consumption by 30%,travel time by 25%,waiting time by 37.5%.,,,,,,,,,,,,,
Mean Absolute Percentage Error (MAPE) less than 5% for each period.,,,,,,,,,,,,,,,
Superior prediction accuracy,lower Mean Absolute Error (MAE),"and faster computational time compared to other models.""",,The proposed model significantly reduced average vehicle waiting time from 120 seconds (traditional) to 75 seconds.,,,,,,,,,,,
The approach improved traffic prediction reliability,managed flow,and decreased vehicle delays.,,,,,,,,,,,,,
Visual and numerical analyses confirmed superior performance over conventional methods.,,,,,,,,,,,,,,,
"Recommendation: Adopt optimization and machine learning for urban traffic management.""","Limited scalability of current AI-driven traffic management algorithms, making it difficult to expand their use in large urban areas.",,,,,,,,,,,,,,
Restricted geographic scope in traffic prediction studies,questioning the generalizability of results.,,,,,,,,,,,,,,
"Difficulty managing unexpected traffic surges during emergencies with decentralized adaptive systems.""","Future research should address scalability issues, integration of diverse data sources, and management of unexpected traffic surges. There is also a need to explore the deployment of advanced models like transformers and hybrid AI systems in broader urban contexts and during emergency situations.",,,"The objectives of the study are to determine the optimal timing for traffic signals to minimize total vehicle waiting time at intersections and to enhance route planning to reduce fuel consumption and travel time, using Particle Swarm Optimization and Artificial Bee Colony algorithms.",,,,,,,,,,,
"A Brief Survey on Deep Learning-Based Temporal Knowledge Graph Completion
LSTM-based methods: Incorporate temporal information into relations and use LSTM (Long Short-Term Memory) networks to encode merged information.
CNN-based methods: Apply convolutional neural networks (CNN) to extract features in temporal knowledge graph completion tasks.""","Jia Ningning, Yao Cuiyou
No information available","2024
The highest Hits@1 under the time-wise filtered setting is 61.2% (HyGNet), showing significant room for improvement in deep learning-based TKGC methods.",reference-manager,10.3390/app14198871,,"Implementation Insights highlight that current deep learning-based TKGC (Temporal Knowledge Graph Completion) methods have significant room for improvement, with the highest Hits@1 at 61.2%. CNN-based methods show promise but need fairer comparisons. Large language models underperform, and integrating temporal information remains challenging. Further research and verification are needed.",,,,"Attention-based methods: Use attention mechanisms to model the importance of entities and relations in temporal knowledge graphs, capturing features with time and graph structure information.",,"What are the main categories, challenges, and future research directions of deep learning-based temporal knowledge graph completion (TKGC) methods?","This paper reviews deep learning-based temporal knowledge graph completion (TKGC) methods, categorizing them into eight types. It highlights recent advances, identifies key challenges like few-shot TKGC and interpretability, and suggests future research directions. The study emphasizes the need for unified methods and improved interpretability in TKGC.","The research goal is to review deep learning-based temporal knowledge graph completion (TKGC) methods, the approach is a comprehensive survey categorizing methods into eight core techniques, and the principal finding is that while progress exists, significant room for improvement and several future research directions remain.",
CNN-based ConvTKG achieves a Hits@1 of 67.4% (raw setting),but fairer comparisons require consistent evaluation settings.,,,,,,,,,,,,,,
Large language models (Llama,Vicuna) perform worse than most deep learning methods,with Hits@1 values of 38.6% and 39.2%,"respectively; their application in TKGC is still exploratory.""","The highest Hits@1 value under the time-wise filtered setting is 61.2% (HyGNet), showing room for improvement.",,,,,,,,,,,
ConvTKG (raw setting) achieves Hits@1 of 67.4% and Hits@10 of 86.2%.,,,,,,,,,,,,,,,
HSAE achieves Hits@1 of 81.7% and Hits@10 of 94.1%.,,,,,,,,,,,,,,,
DEGAT achieves Hits@1 of 72.4% and Hits@10 of 87.1%.,,,,,,,,,,,,,,,
THOR achieves Hits@1 of 75.0% and Hits@10 of 88.2%.,,,,,,,,,,,,,,,
TA-TransE has a low Hits@1 value of 9.6%.,,,,,,,,,,,,,,,
Large language models (Llama,Vicuna) have lower results than most deep learning-based methods (Hits@1: Llama-2-7b-CoH 38.6%,Vicuna-7b-CoH 39.2%).,,,,,,,,,,,,,
Evaluation metrics: Mean Rank (MR),Mean Reciprocal Ranking (MRR),"and Hits@N (percentage of test quadruples ranked ≤ N).""","Highest Hits@1 value under time-wise filtered setting is only 61.2%, showing room for improvement.",,,,,,,,,,,,
ConvTKG needs further verification under static and time-wise filtered settings for fair comparison.,,,,,,,,,,,,,,,
TA-TransE increases parameters and does not fully exploit TKGC characteristics,resulting in lower performance.,,,,,,,,,,,,,,
Large language models in TKGC are still exploratory,with lower results and high resource requirements.,,,,,,,,,,,,,,
Few-shot TKGC research is in its infancy.,,,,,,,,,,,,,,,
"Unified methods for various knowledge graph types are lacking.""",Few-shot TKGC is an important but early-stage research area due to the long-tail distribution of relations in temporal knowledge graphs.,,,,,,,,,,,,,,
Developing unified methods for various types of knowledge graphs is a key future direction.,,,,,,,,,,,,,,,
Current deep learning-based TKGC methods have significant room for improvement,with Hits@1 values not exceeding 61.2%.,,,,,,,,,,,,,,
Large language models in TKGC are still exploratory and underperform compared to other methods,"requiring further research and new modeling strategies.""",Few-shot TKGC: Addressing the challenge of relations with few examples in temporal knowledge graphs remains in its early stages and needs further research.,,,,,,,,,,,,,
Unified methods for various KGC types: Developing approaches that can complete different types of knowledge graphs with a single method is a key future direction.,,,,,,,,,,,,,,,
"Interpretability: Improving the interpretability of deep learning-based temporal knowledge graph completion methods is a promising research area.""","Future research should focus on few-shot temporal knowledge graph completion (TKGC), developing unified methods for various types of knowledge graphs, and improving the interpretability of deep learning-based TKGC methods. There is also a need for fairer comparisons and further exploration of large language models in TKGC.","The study is a review (survey) analyzing deep learning-based temporal knowledge graph completion (TKGC) methods. It subdivides methods into eight categories by core technique, summarizes benchmark datasets and evaluation protocols, and presents experimental results under various settings (static filtered, raw, time-wise filtered). No mention of randomization, blinding, or control groups.",,,,,,,,,,,,,
LLMasMMKG: LLM Assisted Synthetic Multi-Modal Knowledge Graph Creation For Smart City Cognitive Digital Twins,"Mandal Sukanya, O'Connor Noel E.",2024,reference-manager,10.1609/aaaiss.v4i1.31795,,"The implementation features a modular, scalable pipeline for constructing multi-modal knowledge graphs (MMKGs) using large language models (LLMs) and synthetic data. Key insights include the potential for advanced reasoning, explainability, schema alignment, and integration of diverse data types, supporting flexible, transparent, and extensible smart city applications.",,,,Synthetic data generation: Creating and using artificial data for both text and sensor modalities to construct the MMKG.,,"How can large language models be leveraged to construct comprehensive, robust, and explainable multimodal knowledge graphs for smart city digital twins, addressing challenges such as data fusion, parameter optimization, scalability, robustness to noise, and integration of real-world and synthetic data?","This paper aims to enhance smart city digital twins (SC CDTs) by using large language models (LLMs) to construct synthetic multi-modal knowledge graphs (MMKGs). The methodology integrates diverse data sources, addresses data scarcity and privacy, and demonstrates improved explainability. Results show promise for more sustainable, citizen-centric urban management.","The paper's main objective is to develop a novel LLM-based approach for constructing synthetic MMKGs to enhance SC CDTs; the key method integrates heterogeneous data using LLMs for entity recognition, relationship extraction, and synthetic data generation; principal finding: this approach addresses data sparsity, privacy, and integration challenges in urban environments.",
LLM-guided knowledge extraction: Employing large language models to extract and integrate knowledge from heterogeneous,multi-modal data sources.,,,,,,,,,,,,,,
Multimodal data fusion: Combining diverse data types (e.g.,text,sensor readings,social media) into a unified,"semantically rich knowledge representation.""",,"The research is reproducible, as the codebase is publicly available at https://github.com/sukanyamandal/LLMasMMKG. The methodology, including synthetic data generation and knowledge extraction techniques, is described in detail, supporting reproducibility.",,,,,,,,,"The paper introduces a novel approach using LLMs to construct synthetic multi-modal knowledge graphs (MMKGs) for smart city cyber-physical digital twins (SC CDTs), addressing data fusion, automated knowledge extraction, and data sparsity."
The methodology enables integration of heterogeneous data sources,enhances data privacy,and demonstrates feasibility through a proof-of-concept implementation with synthetic data.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported; future work will focus on rigorous quantitative evaluation and parameter sensitivity analysis.""","Approximately 10,000 synthetic data points per domain were generated, covering one year.",,,,,,,,,,,,,,
Data included text and sensor modalities from smart healthcare,transportation,grid,and social media.,,,,,,,,,,,,
Sentence-BERT embeddings unified text and sensor data based on semantic similarity.,,,,,,,,,,,,,,,
Final output: RDF file with ontology and populated knowledge graph (KG).,,,,,,,,,,,,,,,
"No statistical values or measured effects reported.""",Need for parameter optimization and sensitivity analysis for LLM-based KE process.,,,,,,,,,,,,,,
Limited robustness to noisy,inconsistent,or erroneous real-world data.,,,,,,,,,,,,,
Challenges in combining real-world and synthetic datasets,including potential biases.,,,,,,,,,,,,,,
Scalability and generalization to diverse city contexts not yet addressed.,,,,,,,,,,,,,,,
Lack of rigorous quantitative evaluation and real-world case studies.,,,,,,,,,,,,,,,
Explainability and trustworthiness of the LLM-based KE process require improvement.,,,,,,,,,,,,,,,
Reasoning and inference capabilities need enhancement.,,,,,,,,,,,,,,,
Synthetic data generation techniques require further refinement to match real-world data nuances.,,,,,,,,,,,,,,,
Comparative analysis of different LLM architectures is pending.,,,,,,,,,,,,,,,
Human-in-the-loop verification for reliability is not yet implemented.,,,,,,,,,,,,,,,
"Ethical implications and potential biases of LLMs and synthetic data remain unresolved.""","The study presents a novel method using LLMs to build comprehensive, interconnected multi-modal knowledge graphs (MMKGs) for smart city (SC) cognitive digital twins (CDTs), addressing data fusion, automated knowledge extraction, and data sparsity.",,,,,,,,,,,,,,
LLMs enable seamless integration of diverse data sources,enhancing privacy by reducing reliance on sensitive real-world data.,,,,,,,,,,,,,,
Recommendations include further quantitative evaluation,expanding data modalities,improving explainability,robustness,scalability,,and addressing ethical implications such as bias.,,,,,,,,,
The approach aims to empower data-driven decisions and support sustainable,efficient,"and citizen-centric urban environments.""","Optimizing parameters and conducting sensitivity analysis for the LLM-based knowledge extraction process, including GPT-4 settings and similarity thresholds.",,,,,,,,,,,,
Enhancing robustness to noisy,inconsistent,and erroneous real-world data in smart city contexts.,,,,,,,,,,,,,
"Developing optimal strategies for combining real-world and synthetic datasets while mitigating biases.""","Future research should focus on parameter optimization and sensitivity analysis, robustness to noisy data, combining real-world and synthetic datasets, scalability, generalization to diverse city contexts, fusion of heterogeneous data sources, comparative analysis of LLMs, human-in-the-loop verification, refining synthetic data generation, explainability, and comprehensive KG quality evaluation.",,,"The study aims to: optimize parameters in the LLM-based knowledge extraction process, analyze sensitivity to these parameters, enhance robustness to noisy data, develop strategies for combining real-world and synthetic datasets, and scale/generalize the approach for diverse city contexts and large, heterogeneous urban data.",,,,,,,,,,,
Multimodal sensor fusion framework for residential building occupancy detection,"Tan Sin Yong, Jacoby Margarite, Saha Homagni, Florita Anthony, Henze Gregor, Sarkar Soumik",2022,reference-manager,10.1016/j.enbuild.2021.111828,,"The implementation insights highlight a modular, scalable framework using low-powered, wireless sensors (possibly solar-powered) and embedded devices like Raspberry Pi. This approach allows independent inference for each data stream, making the system robust to missing data and easier to deploy on resource-limited hardware. New insight: modularity enhances flexibility and resilience.",,,,"Multimodal data acquisition: Sensor hubs collected temperature, humidity, eCO2, TVOC, ambient light, audio, and images in six homes over 4–8 weeks, focusing on common areas for privacy.",,"How can a multimodal sensor fusion framework, integrating environmental, image, and acoustic data, be developed and evaluated to achieve accurate, transferable, and privacy-preserving occupancy detection in residential buildings?","The paper aims to develop a high-performing, transferable multimodal occupancy detection framework for residential buildings using sensor fusion of environmental, image, and audio data. Using ensemble methods and new spatiotemporal models, the study demonstrates accurate, privacy-preserving occupancy detection, validated on multiple datasets, with implications for smart energy-saving building systems.","The research goal is to develop a high-performing, transferable multimodal occupancy detection framework for residential buildings using sensor fusion and Occ-STPN models; the approach combines environmental, image, and acoustic data with feature/decision-level fusion; results show strong prediction accuracy and transferability across multiple datasets.",
Symbolic Dynamic Filtering (SDF): Time series sensor data were discretized using uniform partitioning (UP) and maximum entropy partitioning (MEP) to filter noise and compress data.,,,,,,,,,,,,,,,
"Audio preprocessing: Raw audio was processed using band-pass filtering
Temperature","full-wave rectification
relative humidity","downsampling
and illuminance were consistently ranked as the most informative environmental sensors for occupancy detection by both mutual information and random forest feature importance.","and filter-wise linear scaling before model training.""",,,"The proposed multimodal Occ-STPN framework achieved up to 93.09% accuracy (FMMSE: 0.003375) for occupancy detection using six appliance power predictors, outperforming univariate Occ-STPN and LDA methods.",,,,,,,,,
"No p-values or explicit statistical significance values were reported in the provided context.""",Primary outcomes:,,,,,,,,,,,,,,
The multivariate Occ-STPN model achieved the highest accuracy (93.09%) and lowest FMMSE (0.003375) using six predictors (Dishwasher,Entertainment,Fridge,Freezer,Kettle,,Stereo) on the ECO household 2 dataset.,,,,,,,,,
With one predictor (Stereo),accuracy was 91.46% and FMMSE was 0.0139594.,,,,,,,,,,,,,,
"The model demonstrated improved performance with more predictors.
Results:",,,,,,,,,,,,,,,
Multivariate Occ-STPN outperformed univariate Occ-STPN and LDA in both accuracy and FMMSE across all predictor sets.,,,,,,,,,,,,,,,
The model correctly predicted most occupied states,with minor time lag in detecting unoccupied states.,,,,,,,,,,,,,,
Measured effects (statistical values):,,,,,,,,,,,,,,,
Accuracy ranged from 91.28% to 93.09% for multivariate Occ-STPN depending on predictors used.,,,,,,,,,,,,,,,
"FMMSE ranged from 0.003156 to 0.00397 for multivariate Occ-STPN.""","Data collected only from six homes in Boulder, Colorado, limiting generalizability.",,,,,,,,,,,,,,
Sensor hubs placed only in common areas,not in bedrooms or bathrooms,due to privacy concerns.,,,,,,,,,,,,,
Ambient light sensor precision unspecified.,,,,,,,,,,,,,,,
Modular approach may face challenges with varying input dimensions and missing modalities.,,,,,,,,,,,,,,,
"Heavy computation load for multimodal deep neural networks on embedded systems.""","A multimodal sensor fusion framework for residential occupancy detection was proposed, unifying camera, acoustic, and environmental data.",,,,,,,,,,,,,,
Two Occ-STPN model variants with feature-level and decision-level fusion showed high prediction accuracy,scalability,and transferability.,,,,,,,,,,,,,
A new causal metric (FMMSE) was introduced for fairer evaluation of prediction delays.,,,,,,,,,,,,,,,
Recommendation: Implement low-cost,"low-power sensor systems integrated with HVAC and lighting controls for energy efficiency.""","Implementation and deployment of the proposed occupancy detection system in real buildings, focusing on energy-saving and efficiency improvements.",,,,,,,,,,,,,
Development of low-powered,wireless sensor systems using solar photovoltaic cells and backscattering communication technology.,,,,,,,,,,,,,,
Integration of the occupancy detection system with existing HVAC and lighting control systems for smart,"occupancy-informed operation.""","Future research should focus on implementing and developing a practical occupancy detection system for energy-saving in buildings. This includes using low-powered, solar-powered sensors with wireless communication, embedded computation devices, and integrating the system with HVAC and lighting controls for smart, occupancy-informed building management.",,,,"The objectives of the study are: (1) to extract meaningful features important for occupancy detection, and (2) to conceal any personally identifiable information (PII) from the collected data. These support public data release and commercial system development with privacy protection.",,,,,,,,,
CTINEXUS: Automatic Cyber Threat Intelligence Knowledge Graph Construction Using Large Language Models,"Cheng Yutong, Bajaber Osama, Tsegai Saimon Amanuel, Song Dawn, Gao Peng",2025,reference-manager,,,"CTINEXUS achieves efficient, adaptive knowledge extraction with minimal data and tuning. Using two prompt examples balances effectiveness and efficiency. Sorting examples by ascending similarity (kNN-ascend) improves results. GPT-4 outperforms other models. For entity merging, a 0.6 threshold and text-embedding-3-large yield optimal precision and recall.",,,,"Annotation by three PhD students with expertise in threat intelligence, using independent annotation and arbitration to ensure quality and reduce bias.",,"How does CTINEXUS improve the efficiency and accuracy of cybersecurity knowledge graph construction compared to existing CTI knowledge extraction methods, particularly in entity alignment and relation prediction?","The paper introduces CTINEXUS, a system for constructing cybersecurity knowledge graphs. Using a three-phase methodology—triplet extraction, hierarchical entity alignment, and long-distance relation prediction—CTINEXUS outperforms state-of-the-art methods in triplet extraction. Rigorous annotation and evaluation show substantial agreement and improved performance, supporting more accurate cybersecurity knowledge extraction.","The research goal is to improve cyber threat intelligence (CTI) knowledge extraction; the approach is CTINEXUS, which uses multi-phase triplet extraction and entity alignment; results show CTINEXUS outperforms state-of-the-art baselines in constructing accurate cybersecurity knowledge graphs.",
Evaluation using inter-annotator agreement measured by Cohen’s kappa,with scores of 0.80 (triplet extraction),0.78 (entity alignment),and 0.61 (relation prediction).,,,,,,,,,,,,
Comparative analysis against state-of-the-art baselines (EXTRACTOR and LADDER),"adapting outputs and formats for fair evaluation.""",,"CTINEXUS outperformed EXTRACTOR and LADDER in cybersecurity triplet and entity extraction, with F1-scores of 87.65 (vs. 62.29 for EXTRACTOR) and 90.13 (vs. 71.13 for LADDER).",,,,,,,,,,,,
Annotation quality was high,with an average Cohen’s kappa of 0.73,indicating substantial agreement.,,,,,,,,,,,,,
"No p-values or explicit statistical significance values are reported.""",CTINEXUS outperformed EXTRACTOR in all metrics for cybersecurity triplet extraction.,,,,,,,,,,,,,,
Inter-annotator agreement (Cohen’s kappa): 0.80 (triplet extraction),0.78 (entity alignment),0.61 (relation prediction); average: 0.73.,,,,,,,,,,,,,
CTINEXUS triplet extraction (GPT-4): F1-Score 87.65,Precision 93.69,Recall 82.34.,,,,,,,,,,,,,
End-to-end prompting reduced input/output token consumption by 98.9%/97.3%.,,,,,,,,,,,,,,,
"GPT-4 outperformed GPT-3.5 in entity grouping and relation prediction across all demonstration numbers.""","Requires carefully chosen, high-quality demonstration examples with correct answers and prompt format.",,,,,,,,,,,,,,
Performance degrades with incorrect or misformatted demonstration samples.,,,,,,,,,,,,,,,
Needs a minimum of 100 labeled data samples.,,,,,,,,,,,,,,,
Data imbalance in the demonstration set can cause biased content generation and reduced effectiveness.,,,,,,,,,,,,,,,
LLMs can generate hallucinations (plausible but factually incorrect outputs),especially in smaller models.,,,,,,,,,,,,,,
"Hallucination detection and mitigation are left for future work.""","CTINEXUS outperforms existing methods (EXTRACTOR, LADDER) in cybersecurity triplet extraction, requiring minimal data and parameter tuning.",,,,,,,,,,,,,,
Using two prompt examples balances effectiveness and efficiency; more examples do not always improve results.,,,,,,,,,,,,,,,
Larger backbone models (e.g.,GPT-4) significantly boost performance.,,,,,,,,,,,,,,
"Recommend integrating visual analytics for enhanced interpretability and decision support.""",Need for foundation models that can ingest and refresh large-scale threat intelligence in near real time.,,,,,,,,,,,,,,
Exploration of downstream applications based on the CSKG and partial user input is left for future work.,,,,,,,,,,,,,,,
"Integration of visual analytics into CTINEXUS to enhance interpretability and support timely cybersecurity decision-making.""","Future research should explore downstream applications of CTINEXUS, such as integrating visual analytics to aid analysts in identifying behavior patterns and relationships. Additionally, developing foundation models for real-time, large-scale threat intelligence ingestion and addressing challenges with heterogeneous data sources are recommended directions.","The study is an observational comparative analysis. It examines three prompt example permutation strategies (random, kNN-ascend, kNN-descend) and compares CTINEXUS with two baselines (EXTRACTOR, LADDER). The design includes systematic evaluation, adaptation of formats, and merging of datasets for fair comparison. No randomization or blinding is mentioned.",,"The objectives of the study are to comprehensively evaluate CTINEXUS’s performance in constructing accurate cybersecurity knowledge graphs, compare CTINEXUS with existing CTI knowledge extraction methods, and investigate how different settings affect entity alignment, relation prediction, and cybersecurity triplet extraction.",,,,,,,,,,,
Explore-Construct-Filter: An Automated Framework for Rich and Reliable API Knowledge Graphs,"Sun Yanbang, Huang Qing, Ren Xiaoxue, Xing Zhenchang, Li Xiaohong, Wang Junjie",2020,reference-manager,,,"The paper introduces an automated Knowledge Graph (KG) construction method using LLMs, featuring three modules: exploration (for comprehensive schema discovery), construction (extracting instances from text), and filtering (removing unreliable data). This “explore-construct-filter” strategy improves efficiency, richness, and reliability compared to baselines. New insight: full schema combinations enhance KG comprehensiveness.",,,,"Explore-Construct-Filter framework: A three-stage process involving exploration, construction, and filtering of knowledge graph (KG) schemas and triples.",,How can the Explore-Construct-Filter framework be used to construct a comprehensive and reliable API knowledge graph by discovering diverse entity types and relation types?,"The paper aims to evaluate the effectiveness of an Explore-Construct-Filter framework for API knowledge graph (KG) construction. Using structured prompt designs and the GPT-4o model, the study compares its method against seven baselines. Results show improved richness and reliability of API KGs, with high annotation agreement (Cohen’s Kappa: 0.78, 0.82).","The research goal is automated, reliable, and comprehensive knowledge graph (KG) construction using LLMs; the approach is an Explore-Construct-Filter framework with modules for schema exploration, instance extraction, and probabilistic filtering; results show improved KG richness and reliability compared to baselines.",
Baseline comparison: Evaluation against seven baseline methods,including MKC,APIRI,GraphRAG,and EDC,,to assess effectiveness.,,,,,,,,,
"Annotation and agreement: Manual annotation of instance triples with inter-annotator agreement measured by Cohen’s Kappa (0.78
At this threshold","0.82).""
precision (P)","The research provides detailed prompt designs, parameter settings, and KG schemas in the appendix, supporting reproducibility. Source code for baselines is available: MKC (\[35]), APIRI (\[36]), GraphRAG (\[37]), and EDC (\[38]). No explicit mention of source code for the main method.
recall (R)","The optimal threshold for the KG filtering module is achieved in Case 3, balancing reliability and richness of the knowledge graph (KG).
and F1 scores for Case 3 are 0.67",0.84,,and 0.75,,,,,,,,,respectively.
"Cohen’s Kappa coefficients for annotation agreement are 0.78 and 0.82
At threshold @0.90
At threshold @0.92
At threshold @0.94","indicating almost perfect agreement; statistical significance (p-values) is not reported.""
Case 3 achieves precision 0.67
Case 3 achieves precision 0.66
Case 3 achieves precision 0.64","The optimal threshold in the KG Filtering module is chosen from Case 3 to balance reliability and richness.
recall 0.84
recall 0.82
recall 0.80","F1 score 0.75.
F1 score 0.73.
F1 score 0.71.",,,,,,,,,,,,
KG schema validity in Case 3: 34 total triples,26 correct triples.,,,,,,,,,,,,,,
"Cohen’s Kappa coefficients for annotation agreement: 0.78 and 0.82 (almost perfect agreement).""","Manual annotation may introduce subjective bias, though mitigated by dual annotators and high kappa values (>0.75).",,,,,,,,,,,,,,
Threshold selection in the KG filtering module may not be optimal,potentially filtering out valuable triples or retaining low-confidence ones.,,,,,,,,,,,,,,
Method performance was not tested on other seed texts,possibly limiting generalizability.,,,,,,,,,,,,,,
"Adaptability to other domains may be limited due to differing data structures and semantics.""",The Explore-Construct-Filter framework effectively enhances the richness and reliability of API Knowledge Graphs (KGs).,,,,,,,,,,,,,,
The KG exploration module discovers more entity and relation types,improving KG richness.,,,,,,,,,,,,,,
The KG filtering module increases the reliability of the API KG.,,,,,,,,,,,,,,,
The full combination strategy generates more reliable type triples,"enhancing comprehensiveness.""",Adapting the method to different fields: The method’s adaptability may be limited by varying data structures and semantic characteristics across domains.,,,,,,,,,,,,,
Integration with KG retrieval tools: Future work includes integrating with technologies like GraphRAG for comprehensive knowledge extraction,analysis,and utilization.,,,,,,,,,,,,,
"Enhancing model adaptability: Adjusting the method for specific data structures and semantic requirements in diverse fields.""","Future research should focus on integrating the proposed automated KG construction method with KG retrieval tools like GraphRAG to develop a comprehensive knowledge extraction, analysis, and utilization toolkit. Further exploration of the method’s adaptability to other fields and data structures is also suggested.","The study design is experimental, involving implementation and comparison of multiple baseline methods (MKC, APIRI, GraphRAG, EDC, Ourw/oKE, Ourw/oKF, Ourw/oFC) and the proposed method. It uses independent annotation with conflict resolution and Cohen’s Kappa for agreement. All methods use the GPT-4o model.",,"The objectives of the study are to evaluate the effectiveness of the Explore-Construct-Filter strategy, determine the optimal threshold in the KG Filtering module, assess KG construction performance, test the framework’s effectiveness, and examine its generalizability across different LLMs.",,,,,,,,,,,
Automated Monitoring Method for Enterprise Microservices Network Operation Status Based on Database Knowledge Graph,"Hu Qidi, Long Yujiang, Zhong Ye, Zhang Guangyi, Wei Wei",2024,reference-manager,10.1109/ecnct63103.2024.10704355,,"Implementation Insights highlight that integrating knowledge graphs with NLP and LSTM enables high anomaly detection (15 times/month), accurate load prediction (92.38%), and effective resource optimization (30 recommendations/month). The system provides early warnings (72 hours), achieves 88.74% fault prediction accuracy, and prevented 5 major faults, demonstrating practical effectiveness and scalability.",,,,"Natural Language Processing (NLP) techniques, including Named Entity Recognition (NER), were used to extract entities and relationships from system logs and user activity data.",,"How can knowledge graphs be constructed and applied within a microservices architecture to enhance automated operational management, including fault prediction, configuration optimization, and system reliability?","The paper investigates building knowledge graph systems using a microservices architecture. It employs modular, containerized services with automated deployment, monitoring, and scaling. Key findings show that this approach enables efficient knowledge extraction, integration, and reasoning, resulting in scalable, reliable, and high-quality knowledge bases for AI applications.","The research goal is to enhance microservice system reliability and efficiency through automated operational management; the approach integrates CI/CD, centralized configuration, monitoring, and Kubernetes-based scaling; results show improved service health, rapid scalability, and reduced manual intervention.",
Time-series analysis was performed using LSTM,GPT3,and BERT models to predict resource usage and detect anomalies.,,,,,,,,,,,,,
Knowledge graphs were constructed to integrate,represent,and analyze operational data,"enabling enhanced predictive maintenance.""",,,"The automated monitoring method achieved 92.38% load prediction accuracy, detected anomalies 15 times per month, provided 30 resource optimization recommendations per month, and successfully prevented 5 major faults.",,,,,,,,,
The system significantly improved real-time monitoring and predictive maintenance for enterprise microservices using a knowledge graph integrated with NLP and LSTM.,,,,,,,,,,,,,,,
"No p-values or explicit statistical significance values are reported in the provided context.""",NLP techniques (including NER) identified key entities and dependencies with 85.49% accuracy.,,,,,,,,,,,,,,
LSTM achieved 95.67% accuracy in predicting resource usage but struggled with long-range dependencies.,,,,,,,,,,,,,,,
BERT achieved 97.85% accuracy in predicting resource usage,excelling at complex,long-range patterns.,,,,,,,,,,,,,
Fault prediction accuracy: 88.74%.,,,,,,,,,,,,,,,
Early warning time: 72 hours.,,,,,,,,,,,,,,,
Anomaly detection: 15 times per month.,,,,,,,,,,,,,,,
Load prediction accuracy: 92.38%.,,,,,,,,,,,,,,,
Resource optimization recommendations: 30 times per month.,,,,,,,,,,,,,,,
"Successful fault prevention: 5 incidents.""",,"The study developed an automated monitoring method for enterprise microservices using a database knowledge graph, enhancing real-time monitoring and predictive maintenance.",,,,,,,,,,,,,
Integrating NLP and LSTM enables proactive prediction and management of failures and performance bottlenecks.,,,,,,,,,,,,,,,
The approach introduces scalable,dynamic monitoring adaptable to varied enterprise needs.,,,,,,,,,,,,,,
Recommendation: Employ knowledge graphs and automated scaling for efficient,"resilient microservices operations.""",Limited research on integrating real-time and historical data in knowledge graphs for proactive failure prediction and resource optimization in microservices.,,,,,,,,,,,,,
Challenges in automated scaling and resource management,especially dynamic adaptation to fluctuating loads using knowledge graphs.,,,,,,,,,,,,,,
Need for improved entity resolution and knowledge integration techniques to construct high-quality,"reliable knowledge bases.""","Future research should explore improving the integration of knowledge graphs with advanced NLP and LSTM techniques, address limitations in handling unstructured data, and investigate scalability and adaptability for diverse enterprise environments. Further studies could also refine predictive maintenance and anomaly detection methods for microservices networks.",No information available,,,"The objectives are to automate scaling and resource management using Kubernetes, optimize configuration based on real-time and historical data, enhance predictive maintenance and anomaly detection with NLP and machine learning, and construct a high-quality knowledge base for improved knowledge reasoning and discovery.",,,,,,,,,
Mining literature and pathway data to explore the relations of ketamine with neurotransmitters and gut microbiota using a knowledge-graph,"Liu Ting, Feenstra K Anton, Huang Zhisheng, Heringa Jaap",2023,reference-manager,10.1093/bioinformatics/btad771,,"The BioKetBERT model effectively identifies relations between entities in the same sentence but struggles to classify them as activation or inhibition due to difficulty parsing relational verbs. Improvements could include verb lemmatization, automated image-based pathway extraction, and advanced graph analysis for better knowledge inference and scalability.",,,,Manual fact extraction: Researchers manually curated pathway facts from images in scientific publications.,,"How can a knowledge graph integrating manual curation, automated named entity recognition, and relation extraction be constructed and used to analyze the relationships between ketamine, neurotransmitters, and gut microbiota in the context of ketamine’s antidepressant effects?","The paper presents KetPath, a knowledge graph integrating ketamine pathway data from biomedical literature and databases. Using manual curation, automated named entity recognition, and relation extraction, KetPath enables detailed analysis of ketamine’s effects on neurotransmitters and gut microbiota, revealing mechanisms underlying its sustained antidepressant effects.","The research goal was to improve understanding of ketamine’s antidepressant effects via gut microbiota by constructing the KetPath knowledge graph using manual curation and automated extraction; results show KetPath enables detailed retrieval of biological pathway relations, revealing gut microbes’ role in sustaining ketamine’s effects.",
Automatic named entity recognition: Used CI-er to automatically identify key terms (entities) in biomedical texts.,,,,,,,,,,,,,,,
"Automatic relation extraction: Applied BioKetBERT to automatically extract relationships between identified entities.""","The research is reproducible. The datasets are available at the KetPath repository on GitHub, with a README file explaining integration steps. Users can download datasets, upload them to GraphDB, and use provided SPARQL query protocols. No explicit source code for the project is mentioned.","The KetPath knowledge graph integrates manual and automated data, retrieving 273 relations and 471 entities, and provides more comprehensive biological insights than public databases alone.",,,,,,,,,,,,,
KetPath identifies novel pathway relations,including 16 literature-only connections,showing ketamine’s effects on neurotransmitters and indirect impact on BDNF.,,,,,,,,,,,,,
"No statistical significance (p-values) or quantitative results beyond entity/relation counts are reported.""","Developed KetPath, a knowledge graph integrating ketamine pathway data from publications and databases.",,,,,,,,,,,,,,
Combined manual fact extraction,automatic named entity recognition (CI-er),and relation extraction (BioKetBERT).,,,,,,,,,,,,,
KetPath validated by four biological query cases,retrieving meaningful results on ketamine’s effects on neurotransmitters and gut microbiota.,,,,,,,,,,,,,,
"No explicit statistical values reported.""",BioKetBERT cannot categorize relations as activation or inhibition due to inability to parse relational verbs.,,,,,,,,,,,,,,
Manual curation of pathway facts from images limits scalability; automatic extraction is suggested for improvement.,,,,,,,,,,,,,,,
"Inferring new knowledge from the knowledge graph could be enhanced by advanced graph analysis techniques.""","KetPath effectively integrates and retrieves ketamine pathway knowledge from literature and databases, supporting diverse biological queries.",,,,,,,,,,,,,,
Gut microbiota influences ketamine’s antidepressant effects by regulating neurotransmitters and BDNF; enhancing microbiota diversity may strengthen these effects.,,,,,,,,,,,,,,,
The approach is feasible,biologically useful,"and adaptable to other domains or drugs.""",BioKetBERT struggles to categorize relations as activation or inhibition due to difficulty parsing relational verbs; verb lemmatization could help.,,,,,,,,,,,,
Automatic extraction of pathway facts from images using deep learning is needed to replace manual curation and improve scalability.,,,,,,,,,,,,,,,
New algorithmic graph analysis techniques,such as novel weighting schemes,"are needed to better infer knowledge from knowledge graphs.""","Future research should focus on improving relation categorization by implementing verb lemmatization, developing deep learning methods to automatically extract pathway facts from images, enhancing knowledge inference using advanced graph analysis techniques, and extending the approach to other domains by re-training models on domain-specific annotated databases.",,,,,,,,,,,,"The objectives of the study are to construct a ketamine pathway-related knowledge graph (KetPath) from biomedical publications and public databases, enabling graph analysis of ketamine’s effects on brain neurotransmitters and gut microbiota, and to validate KetPath’s usefulness through representative biological query cases."
Enabling building digital twin: Ontology-based information management framework for multi-source data integration,"Xie X, Moretti N, Merino J, Chang J Y, Pauwels P, Parlikad A K",2022,reference-manager,10.1088/1755-1315/1101/9/092010,,"The paper reviews practical implementations of an ontology-based Information Management Framework (IMF) for building digital twins. It highlights merging BOT and BRICK ontologies for a Foundation Data Model, compares data warehouse and mediator integration approaches, and stresses the importance of data quality evaluation in future work.",,,,Merging BOT and BRICK ontologies in OWL to create a Foundation Data Model (FDM) for representing building topologies and MEP facilities.,,How can a candidate Foundation Data Model (FDM) effectively integrate heterogeneous building data from autonomous sources using merged ontologies and hybrid data integration architectures to support efficient information management for built assets?,"The paper reviews practical implementations of the Information Management Framework (IMF) for building digital twins, focusing on integrating heterogeneous data using ontology-based processes. It proposes merging BOT and BRICK ontologies into a Foundation Data Model (FDM), recommending hybrid data integration architectures. The approach enhances data interoperability and management in building systems.",The research goal is to improve information management for built assets by integrating heterogeneous data; the approach merges BOT and BRICK ontologies into a Foundation Data Model and compares data warehouse and mediator architectures; the principal finding is that both architectures are preferable in different asset management scenarios.,
Comparing and utilizing data integration architectures: data warehouse approach and mediator approach,for integrating heterogeneous data sources.,,,,,,,,,,,,,,
Employing ontology-based processes to extract,transform,"and integrate various data through a common data model and reference data libraries.""",No information available,The study demonstrates that merging BOT and BRICK ontologies creates a feasible Foundation Data Model (FDM) for integrating building data from disparate sources.,,,,,,,,,,,
Data warehouse and mediator integration architectures are preferable in different asset management scenarios,depending on service characteristics.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""",Primary outcome: Demonstrated an ontology-based information management framework for integrating multi-source building data.,,,,,,,,,,,,,,
"Measured effects: The mediator-wrapper approach enables integration of heterogeneous data sources while maintaining their autonomy and allows easy addition of new sources.
Results: No statistical values or quantitative results are provided in the context.""",No building product ontologies or sensor observation ontologies are included in the Foundation Data Model (FDM).,,,,,,,,,,,,,,
The mediator approach may lead to inconsistency and redundancy issues for data stored in separate sources.,,,,,,,,,,,,,,,
The study does not specify a single implementation approach for integration.,,,,,,,,,,,,,,,
Challenges remain in aggregating disparate data due to trust,privacy,ownership,"and curation issues.""",Merging BOT and BRICK ontologies creates a feasible Foundation Data Model (FDM) for building data integration.,,,,,,,,,,,
The data warehouse and mediator approaches each suit different asset management services; hybrid use is recommended.,,,,,,,,,,,,,,,
Effective data integration enables better discovery,fetching,and utilization of heterogeneous building data.,,,,,,,,,,,,,
"Future work should focus on evaluating the quality of integrated data.""",Need to evaluate the quality of integrated data to fully realize the benefits of data integration.,,,,,,,,,,,,,,
Handling inconsistencies and redundancies when merging complementary ontologies like BOT and BRICK.,,,,,,,,,,,,,,,
Developing strategies for integrating heterogeneous,"multi-source data while maintaining autonomy and minimizing redundancy.""","Future research should focus on evaluating the quality of integrated data to fully realize the benefits of data integration. There is also a need to address challenges in merging complementary ontologies, handling inconsistencies, and managing heterogeneous data sources for improved information management in the AEC/FM sector.","Observational study; case study; review of practical implementations; ontology-based data integration processes; demonstration using a real-world building (Alan Reece building); comparison of data integration architectures; no mention of randomization, blinding, control groups, or meta-analysis.",,,"The study aims to review practical implementations of the Information Management Framework (IMF) for building digital twins, elaborate on ontology-based processes for data integration using a common data model and reference data libraries, and compare integration architectures for effective asset management services.",,,,,,,,,
Knowledge graph construction and application in geosciences: A review,Ma Xiaogang,2022,reference-manager,10.1016/j.cageo.2022.105082,,"Implementation Insights highlight the integration of knowledge graphs (KGs) with machine learning in geosciences, emphasizing best practices like API documentation, feedback collection, and data preservation. New insights include the need for user-friendly platforms and the early stage of KG-enabled data analysis applications. No software/code was developed.",,,,"Use case-driven iterative approach: Researchers analyze specific use cases with domain experts to identify entities and relationships, build a prototype knowledge graph (KG), and iteratively enrich it with more use cases.",,"How can a use case-driven, iterative workflow leveraging ontology design patterns and community standards be applied to construct, enrich, and maintain knowledge graphs for improved data management, analysis, and interoperability in geosciences?","The paper reviews the use of knowledge graphs (KGs) in geosciences, aiming to summarize best practices and trends in KG construction and application. Using literature review and workflow analysis, it finds KGs aid data integration, curation, and analysis. The study concludes KGs enhance data-driven geoscience research.","The paper's main objective is to identify best practices for applying knowledge graphs in data curation for geosciences, using a comparative approach with FAIR data principles, and concludes that aligning knowledge graph practices with FAIR principles enhances data ecosystem effectiveness and supports data-driven science.",
Reuse of ontology design patterns: Existing community standards and ontologies are adapted to improve interoperability and usability of KGs.,,,,,,,,,,,,,,,
3C guideline (Correct,Consistent,"Complete): Practitioners verify that entities and relationships in the KG meet these criteria to determine when to stop use case analysis.""","No software/code was developed or used in this paper. Therefore, the research is not directly reproducible via source code.","The paper highlights that applying knowledge graphs (KGs) and FAIR data principles enhances data curation, accessibility, and reuse in geosciences.",,,,,,,,,,,
No quantitative results or statistical significance (p-values) are reported.,,,,,,,,,,,,,,,
The study concludes that best practices,such as providing up-to-date data and complete documentation,support a robust data ecosystem.,,,,,,,,,,,,,
"Explanation: No quantitative data or p-values are available in the context.""
No specific results","The primary outcomes focus on best practices for publishing and using data on the Web, their benefits to the data ecosystem and FAIR data principles, and their relevance to knowledge graphs.
measured effects",or statistical values are provided in the context.,,,,,,,,,,,,,
"No information available on quantitative outcomes or statistical analysis.""",No explicit limitations or shortcomings of the research study are stated in the provided context.,,,,,,,,,,,,,,
No self-reported problems or suggestions for further research are mentioned.,,,,,,,,,,,,,,,
Explanation: The context only lists best practices,benefits,and principles,"but does not discuss limitations.""","Iterative, use case-driven workflows and reuse of ontology design patterns are recommended for effective knowledge graph (KG) construction in geosciences.",,,,,,,,,,,
Adopting best practices from FAIR data principles and Web standards enhances data interoperability,usability,and ecosystem benefits.,,,,,,,,,,,,,
"Collaboration between domain experts and knowledge engineers is essential.
Ethical",licensing,"and privacy guidelines must be established for KG sharing and reuse.""","Entity disambiguation and identification: There is a need for better methods to distinguish and uniquely identify entities, especially from unstructured literature, and to develop quality metrics for knowledge graphs (KGs).",,,,,,,,,,,,
Semantic enrichment and reasoning: Research is needed on improving the detail and logic-based reasoning capabilities of KGs to enhance data curation and analysis.,,,,,,,,,,,,,,,
"KG evolution and versioning: Addressing how KGs can adapt to new scientific discoveries and changing knowledge remains a significant challenge.""","Future research should address KG entity disambiguation, quality measurement, semantic enrichment, reasoning capability, and KG evolution/versioning. There is a need to bridge gaps between geoscience and computer science, develop deeper KG applications, and integrate KGs with machine learning for advanced data analysis.",,,No information available,,,,,,,,,,,
Personal Health Knowledge Graphs for Patients,"Rastogi Nidhi, Zaki Mohammed J.",2020,reference-manager,,,"Implementation Insights highlight that most approaches use brute force or static methods to create Personal Health Knowledge Graphs (PHKG), often limited by device resources and lack of dynamic updating. New insights reveal the need for hybrid, collaborative, and scalable PHKG construction, addressing privacy, data integration, and validation challenges.",,,,Literature review and critique of Knowledge Graph (KG) approaches for extracting personal context from patient data.,,"How can Personal Health Knowledge Graphs (PHKGs) be effectively generated, represented, and integrated with existing knowledge bases to provide personalized health recommendations for patients, while addressing challenges related to data heterogeneity, scalability, validation, and patient privacy?","The paper reviews and critiques approaches for extracting personal context from patient data using small-sized Personal Health Knowledge Graphs (PHKGs). It discusses methods for generating PHKGs, highlights challenges like scalability and integration, and concludes that further research is needed to address validation, representation, and technical limitations for effective personalized health recommendations.","The paper's main objective is to review and critique methods for extracting personal context from patient data using small-sized Personalized Health Knowledge Graphs (PHKGs); it highlights brute-force and dynamic graph generation approaches, concluding that challenges remain in scalability, validation, and effective personal health recommendations.",
Aggregation and integration of heterogeneous data sources (e.g.,environmental sensors,web-based data) into personal health knowledge graphs (PHKGs).,,,,,,,,,,,,,
"Construction of personal knowledge bases using text-based life logs from social media platforms.""",No source code for the project is mentioned in the context. The research discusses methods and challenges for constructing personal health knowledge graphs but does not provide details or links to reproducible code or datasets.,"PHKGs (Personal Health Knowledge Graphs) offer personalized health recommendations but face challenges like static summaries, limited entity updates, and integration with general KGs.",,,,,,,,,,,,,
No standard PHKG representation exists; models vary by use-case,and issues like device memory,data ingestion,and scalability remain unresolved.,,,,,,,,,,,,
No quantitative results,statistical significance,"or p-values are reported.""","No primary outcomes, results, or measured effects (including statistical values) are explicitly stated in the provided context.",,,,,,,,,,,,
The context is a literature review and discussion of challenges,"not a report of experimental or quantitative results.""","Brute force approaches do not address creating relationships, identifying accurate classes, or removing irrelevant entities.",,,,,,,,,,,,,
Lack of standard model for representing PHKG.,,,,,,,,,,,,,,,
Static personalized summaries are hard to update and manage over time.,,,,,,,,,,,,,,,
Device memory and computing resources may limit PHKG size and span.,,,,,,,,,,,,,,,
Sparse or unreliable data from social media and daily journaling.,,,,,,,,,,,,,,,
"No patient-side recommendation feedback in some systems.
Scalability",ingestion,and processing issues with heterogeneous data sources.,,,,,,,,,,,,,
Privacy concerns due to patients’ limited awareness of what to share.,,,,,,,,,,,,,,,
Need for validation mechanisms to ensure PHKG accuracy.,,,,,,,,,,,,,,,
"Further research needed on memory/computation requirements and hybrid approaches.""","PHKGs (Personal Health Knowledge Graphs) offer personalized health recommendations but face challenges in scalability, entity updating, and standard representation.",,,,,,,,,,,,,,
Collaboration between patients and healthcare professionals is recommended for effective PHKG creation.,,,,,,,,,,,,,,,
Device limitations and data integration from heterogeneous sources remain key technical challenges.,,,,,,,,,,,,,,,
Further research is needed to address validation,scalability,"and standardization of PHKGs.""","Lack of standard models for representing PHKGs, leading to inconsistent structures and approaches.",,,,,,,,,,,,
Challenges in dynamically creating,updating,and scaling PHKGs,especially with heterogeneous data sources and limited device resources.,,,,,,,,,,,,
"Need for effective validation mechanisms to ensure PHKGs accurately capture personal patient information and support personalized recommendations.""","Future research should address: dynamic creation and updating of PHKGs, accurate class and relationship identification, removal of irrelevant entities, scalability, validation mechanisms, integration of heterogeneous data sources, memory and computation requirements, hybrid patient-professional collaboration, and improved entity linking with general knowledge bases.",,,,,,,,,,,,,,
HHT: An Approach for Representing Temporally-Evolving Historical Territories,"Charles W., Aussenac-Gilles N., Hernandez N.",2023,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
The algorithm detects and qualifies changes in historical territories using cardinality-based rules for splits,merges,and redistributions. The HHT-SHACL FDD extension identifies flawed or incomplete data,"marking 82 geometry changes as Incomplete in the France region reform test. Composite changes are still properly aggregated despite data flaws.""",,,"The research goal is to represent historical territories without geometric data using the HHT ontology and building blocks; the approach includes an algorithm for detecting and characterizing territorial changes; results show effective multi-level change detection and knowledge graph creation, with available datasets and tools for digital humanities.",HHT (Historical Hierarchical Territories) ontology: Used to represent historical territories with multiple overlaying hierarchies and geometry definitions independent of vector geometry.,,,,,,"How can historical territories and their changes be effectively represented and detected using the HHT ontology and associated algorithms, considering the challenges of multiple overlaying hierarchies, temporal evolution, and efficient change aggregation in knowledge graphs?",,"The paper aims to represent historical territories using the HHT approach, which manages multiple overlaying hierarchies without relying on vector geometry. Using fluents for temporal evolution, the methodology includes algorithms for detecting and aggregating territorial changes. Results show effective change aggregation, though data inconsistencies affect detection accuracy."
Change detection algorithm: Implements SHACLRules and SPARQL to detect,qualify,and aggregate territorial changes in knowledge graphs.,,,,,,,,,,,,,
"HHT-SHACL FDD approach: Used to identify and resolve inconsistencies in data conversion and ensure knowledge graph consistency.""","The research is reproducible. Source code, datasets, and scripts are available at https://github.com/Brainchain09/HHT-SHACL. Additional resources are at https://w3id.org/HHT. The project is licensed under Creative Commons 4.0, with DOIs 10.5281/zenodo.7451702 and 10.5281/zenodo.7451408.","The HHT-SHACL FDD extension detects flawed data by identifying incomplete geometry changes, avoiding erroneous change qualification; in the France Region reform dataset, 82 geometry changes were flagged as Incomplete.",,,,,,,,,,,,,
In the French Third Republic dataset,211 composite changes were detected,with 181 geometrically qualified; 241 feature changes occurred in 1926,matching historical records.,,,,,,,,,,,,
The algorithm accurately aggregates composite changes and highlights data inconsistencies,"but requires all building blocks to be described across the entire timespan; no p-values or statistical significance are reported.""","The algorithm detected 241 feature changes in 1926, matching the greatest territorial reform of the French Third Republic.",,,,,,,,,,,,,
Out of 211 hht:CompositeChange,only 181 are geometrically qualified.,,,,,,,,,,,,,,
In the France: Region reform dataset,82 geometry changes were detected,all denoted as Incomplete by HHT-SHACL FDD.,,,,,,,,,,,,,
Detected feature changes matched those found manually in economists’ studies.,,,,,,,,,,,,,,,
"All region fusions were detected and qualified as merges.""","The algorithm relies on a time-exhaustive description of geometric building blocks, which may not always be available.",,,,,,,,,,,,,,
Missing or imprecise geometry data can cause invalid detection of changes.,,,,,,,,,,,,,,,
The time focus of the knowledge graph must currently be specified in SHACLRules,limiting generality.,,,,,,,,,,,,,,
"Further work is needed to enable more generic use via graph annotation mechanisms.""","The HHT approach effectively represents historical territories and their evolution, supporting multiple hierarchies and intuitive understanding for historians.",,,,,,,,,,,,,,
The HHT-SHACL FDD extension detects data inconsistencies and incomplete knowledge,improving data quality.,,,,,,,,,,,,,,
Overslicing and data inconsistencies are challenges; future work should address time-stamping and source linking to optimize the knowledge graph.,,,,,,,,,,,,,,,
"Recommendations include refining change detection algorithms and exploring hybrid geometry representations.""",Need to minimize overslicing caused by the current fluent-based representation and explore time stamping properties to reduce knowledge graph size.,,,,,,,,,,,,,,
Future work should focus on linking extracted knowledge to its sources within the graph.,,,,,,,,,,,,,,,
Address limitations in the change detection algorithm,"especially handling disappearing/appearing building blocks and inconsistencies in detected changes.""","Future research should explore using time stamping properties instead of creating new objects to reduce knowledge graph size, improve linking knowledge to sources, address inconsistencies in change detection, and consider hybrid geometries. Solutions for handling disappearing/appearing building blocks and refining change representation semantics are also suggested.",,,,,,,,,,,,,
PoC Design: A Methodology for Proof-of-Concept (PoC) Development on Internet of Things Connected Dynamic Environments,"Prasanna K., Ramana Kadiyala, Dhiman Gaurav, Kautish Sandeep, Chakravarthy V. Deeban",2021,reference-manager,10.1155/2021/7185827,,"The Implementation Insights highlight the PoC Design methodology, emphasizing early stakeholder involvement, defining problems by business value, and validating concepts quickly (""""fail fast""""). The study notes challenges in comparing results and suggests future research on cultural perspectives and the impact of strict time frames on projects.",,,,"PoC Design methodology: A step-by-step approach focusing on identifying problems, finding optimal solutions, designing, building, evaluating, and deploying IoT systems for business benefit.",,How does the PoC Design methodology identify and evaluate problems and solutions to maximize business benefit for customers from both technical and business perspectives?,"The paper aims to develop and evaluate the PoC Design methodology for IoT Proof-of-Concept (PoC) projects, focusing on smart street lighting. Using workshops, feasibility studies, and case studies, it finds the methodology secures business benefits, saves resources, achieves 32–45% energy savings, and 100% error detection accuracy.","The research goal is to develop and evaluate a project methodology (PoC Design methodology) for IoT-PoC development, using literature and consultant interviews; the approach involves a case study on outdoor lighting defect detection, and results show 100% error detection accuracy and significant energy savings (32% holidays, 45% normal days).",
Case study: Implementation and evaluation of the proposed methodology in a real-world IoT system measuring temperature and moisture.,,,,,,,,,,,,,,,
Essence framework exercises: Stakeholder and opportunity exercises involving stakeholder engagement and assessment of project challenges,strategies,"and values.""",,"The PoC Design methodology effectively supports IoT-PoC development, securing business benefits and reducing unnecessary resource use.",,,,,,,,,,,
The methodology achieved 32% energy savings during holidays and 45% during normal days; error simulation on 150 errors showed 100% accuracy in error detection.,,,,,,,,,,,,,,,
"No p-values or statistical significance measures are reported.""",The primary outcome was the evaluation of the PoC Design methodology in an IoT case study (detecting street lighting failures).,,,,,,,,,,,,,,
Results highlighted the importance of defining problems and solutions based on business value,calculating IoT initiative potential,involving stakeholders early,and validating concepts with a PoC.,,,,,,,,,,,,
"No statistical values or measured effects are provided.""",The study ignores the cultural perspective of projects.,,,,,,,,,,,,,,
The effect of applying strict time frames to projects was not explored.,,,,,,,,,,,,,,,
"Difficulty in comparing results with other comparable results.""","The PoC Design methodology is effective for developing IoT-Proof of Concepts (IoT-PoC), securing business benefits, and reducing unnecessary resource use.",,,,,,,,,,,,,,
The methodology achieved 100% error detection accuracy in simulations and enabled energy savings of 32% during holidays and 45% on normal days.,,,,,,,,,,,,,,,
"Lessons learned from PoC evaluations should inform future projects and product development.""",The study ignores the cultural perspective of projects; future research should examine this aspect in relation to the PoC Design methodology.,,,,,,,,,,,,,,
The impact of applying strict time frames versus not applying them to projects was not explored and should be investigated.,,,,,,,,,,,,,,,
"There was difficulty comparing the PoC Design methodology results with other comparable results; future work should address this comparison.""","Future research should examine the cultural perspective in PoC Design methodology, explore the impact of strict versus flexible time frames on projects, and compare PoC Design methodology results with other comparable methodologies, as these areas were not addressed in the current study.","The study design is a case study evaluating the PoC (Proof of Concept) Design methodology. It involves workshops, feasibility studies, and stepwise phases (problem definition, solution development, evaluation of potential, design, and deployment). The study is not randomized, controlled, double-blind, or placebo-controlled.",,"The objectives of the study are to develop and evaluate a project methodology for IoT Proof of Concept (PoC) development, focusing on maximizing business benefit by identifying and solving relevant problems, with an emphasis on technological and business advantages, especially for detecting defects in outdoor lighting.",,,,,,,,,,,
A Theoretical Approach for Structuring and Analysing Knowledge Provenance for Visual Analytics,"Christino L., Rezaeipour S., Milios E., Paulovich F.",2023,reference-manager,,,"The implementation of VAKG enables automatic, scalable collection and structuring of user workflows and feedback, allowing researchers to analyze tool usage and user knowledge gain. VAKG does not expand analytical techniques but optimizes their use. Insights include identifying tool limitations and suggesting interface improvements based on user interaction data.",,,,Modeling the VA tool and designing a knowledge graph structure based on a formal VA model and ontology.,,"How can the Visual Analytics Knowledge Graph (VAKG) framework formalize and standardize the acquisition, structuring, and analysis of user behavior and knowledge provenance in visual analytics tools to enable comprehensive understanding and comparison of knowledge generation during data analysis?","The paper introduces the Visual Analytics Knowledge Graph (VAKG) framework, aiming to formalize and analyze user knowledge acquisition in Visual Analytics (VA) tools. Using case studies and surveys, it demonstrates VAKG’s ability to model workflows, track user behavior, and compare knowledge gained, highlighting its structured, theoretically grounded approach.","The paper’s main objective is to formalize the Visual Analytics Knowledge Graph (VAKG) method for modeling and analyzing user knowledge in VA tools; the key approach is an automated, ontology-based knowledge graph pipeline; principal findings show VAKG enables scalable, insightful analysis of user behavior and tool limitations.",
Collecting user behavior and knowledge provenance data to populate the knowledge graph.,,,,,,,,,,,,,,,
Analyzing the resulting knowledge graph to understand user workflows,tool usage,"and insights.""","The reproducibility of the research is partially supported. Source code for the project is available at https://doi.org/10.5281/zenodo.8124221 (Christino L., 2023). No further details about data or experiment reproducibility are provided in the context.","The VAKG framework formalizes the process of modeling, collecting, and analyzing user knowledge gain in Visual Analytics (VA) tools, demonstrated through a case study with 3 researchers using ModKT on 400 documents.",,,,,,,,,,,
Quantitative analysis showed minimal overlap in user interactivity,indicating a vast search space; researchers suggested reducing interactivity and using static visualizations.,,,,,,,,,,,,,,
"No statistical significance (p-values) or detailed quantitative results were reported in the context.""",Three researchers used ModKT with VAKG on 400 documents to find visualization-related articles.,,,,,,,,,,,,,,
VAKG enabled understanding of tool usage,identification of shortcomings,and comparison of user processes.,,,,,,,,,,,,,
Users had nearly no overlap in interactivity,indicating a vast search space.,,,,,,,,,,,,,,
Feedback highlighted layout issues,T-SNE overlap,poor abstract reading experience,and limited word cloud usefulness.,,,,,,,,,,,,
Researchers suggested reducing interactivity and using static visualizations as improvements.,,,,,,,,,,,,,,,
"No specific statistical values were reported.""","VAKG is a conceptual framework, so direct comparison with practical works is limited.",,,,,,,,,,,,,,
User-tracking is seen negatively due to privacy concerns and regulations like GDPR; user consent is a relevant issue.,,,,,,,,,,,,,,,
Integration with other ontologies or models requires further domain-specific research.,,,,,,,,,,,,,,,
VAKG does not solve user-tracking or expand analytical techniques but provides a structure for them.,,,,,,,,,,,,,,,
Further study is needed to assess the impact of user consent and privacy concerns.,,,,,,,,,,,,,,,
Evaluation was limited to example-based analysis due to the novelty of VAKG; no direct benchmarks exist.,,,,,,,,,,,,,,,
"Future work is needed to explore integration with other ontologies and develop new provenance techniques.""","VAKG offers a formalized, theoretically grounded process to model, collect, and analyze user knowledge gain in Visual Analytics (VA) tools.",,,,,,,,,,,,,,
The case study showed VAKG helps identify tool shortcomings and user workflow differences,suggesting improvements like reducing interactivity and using static visualizations.,,,,,,,,,,,,,,
VAKG enables per-user and cross-tool analysis,supporting broader user evaluation comparisons.,,,,,,,,,,,,,,
"Future work includes improving user-tracking and automating provenance techniques for VAKG.""","Existing theoretical models are insufficient for modeling, storing, and linking VA tool usage, user behavior, and knowledge-gathering processes simultaneously.",,,,,,,,,,,,,,
The need for further study on the impact of user-tracking and consent,especially regarding privacy concerns like GDPR.,,,,,,,,,,,,,,
"VAKG’s conceptual framework requires practical validation and extension to bridge VA theory and real-world application.""","Future research should investigate user-tracking, behavior/knowledge provenance, and knowledge graph analysis when applying VAKG in domain-specific cases. There is also a need to explore semi-automatic or automatic provenance techniques and to study the impact of user consent and privacy concerns.",,,"The objectives of the study are to formalize and model the process of user knowledge gain in Visual Analytics (VA) tools using the VAKG method, enabling structured data collection, retention, and analysis of user behaviors, intentions, and knowledge through knowledge graphs for comprehensive provenance and comparative analysis.",,,,,,,,,,,
A Classification for Managing Software Engineering Knowledge,"Kaplan Angelika, Walter Maximilian, Heinrich Robert",2021,reference-manager,10.1145/3463274.3463357,,Implementation Insights Summary:,,,,,,,,,
The implementation focused on classifying primary software engineering (SE) research papers using defined inclusion/exclusion criteria,emphasizing validated empirical or non-empirical methods. Limitations include exclusion of meta-research,possible selection bias from only ICSE 2020 papers,"and interpretation risks. New insight: Classification may need extension for broader research types.""",,,"The research goal is to present and validate a classification for managing software engineering knowledge by analyzing statements in primary SE papers, using a literature review of ICSE 2020 papers, and the principal finding is that the classification works well for constructive contributions but required extension for problem-focused research.",Manual search of ICSE 2020 technical track papers was used as the search strategy.,,,,,,"What kinds of research objects, statement types, and evidence can be identified and classified in primary scientific software engineering papers, and how effective is the proposed classification in capturing these aspects?",,"The paper aims to present a classification for managing knowledge in software engineering (SE) research. Using a manual review of 129 ICSE 2020 papers, it analyzes research objects, statement types, and evidence. The classification aids understanding SE knowledge, but is limited to primary research and validated through literature review."
Inclusion and exclusion criteria were applied to select only primary research papers with validation.,,,,,,,,,,,,,,,
Classification and data extraction were based on analyzing research objects,statement types,"and evidence using a structured data extraction form.""",,"The classification was applied to 129 ICSE 2020 technical track papers, focusing on constructive contributions and their validation, leading to refined definitions for research object subclasses.",,,,,,,,,,,
The study only included full research papers with clear research objects and validated claims; papers without validation or outside primary SE research were excluded.,,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""",The primary outcome was the validation and refinement of a classification system for statements in scientific SE (Software Engineering) papers.,,,,,,,,,,,,,,
The classification worked well for constructive contributions and their validation.,,,,,,,,,,,,,,,
"The classification was extended to include """"""""Problem (1.1)"""""""" as a research object.",,,,,,,,,,,,,,,
"No statistical values or measured effects are reported.""",Classification limited to primary research in SE; meta-research cannot be classified.,,,,,,,,,,,,,,
Not all relevant information is present in research text documents.,,,,,,,,,,,,,,,
Manual search may introduce selection bias; only ICSE 2020 full track papers used.,,,,,,,,,,,,,,,
Potential measurement and exclusion bias due to narrative definitions and inclusion/exclusion criteria.,,,,,,,,,,,,,,,
Ambiguous terminology and unclear semantic relations increase manual effort and false positives.,,,,,,,,,,,,,,,
"Rapidly increasing number of SE papers intensifies these challenges.""",The proposed three-dimensional classification effectively categorizes constructive contributions and their validation in SE papers.,,,,,,,,,,,,,,
"The classification was refined to include research objects like """"""""Problem"""""""" not initially considered.",,,,,,,,,,,,,,,
Limitations include focus on primary research and challenges in extracting all relevant information from research texts.,,,,,,,,,,,,,,,
"A new knowledge management system (KMS) is recommended to make statements first-class entities for better knowledge organization in SE research.""",The classification is limited to primary research in Software Engineering (SE); extending it to include meta-research and other research types is an open question.,,,,,,,,,,,,,,
Not all relevant information is present in research text documents,limiting the current classification.,,,,,,,,,,,,,,
"Future work should extend the literature review to cover a longer time span and broader selection of conferences and journals.""","Future research should extend the literature review to cover a longer time span and include a broader selection of venues, such as additional top-tier conferences and journals. There is also an open question about adapting the classification for meta-research and refining classification dimensions.","The study design is a literature review focused on primary research papers in software engineering (SE). It uses manual search, applies inclusion/exclusion criteria, and analyzes full research papers from ICSE 2020. The review is not randomized, controlled, double-blind, or placebo-controlled. No meta-analysis or systematic review is mentioned.",,"The objectives of the study are to analyze the content of scientific software engineering (SE) papers, specifically focusing on: (1) identifying types of research objects, (2) determining what kinds of statements are made about these objects, and (3) examining how these statements are validated and what evidence is provided.",,,,,,,,,,,
Deep Learning for Sequential Recommendation,"Fang Hui, Zhang Danning, Shu Yiheng, Guo Guibing",2020,reference-manager,10.1145/3426723,,"Implementation Insights are summarized as follows: Side information (like images, text, and dwell time) helps address data sparsity and cold-start issues. Negative sampling size significantly impacts performance, especially increasing from 0 to 32. Embedding methods from NLP are limited; more advanced, sequential-specific embeddings and better modeling of user long-term preferences are needed.",,,,Deep Learning (DL) techniques: Used to model long sequences and improve prediction accuracy in sequential recommendation.,,"What are the key algorithms, influential factors, and evaluation methods in deep learning-based sequential recommendation, and how can future research address current challenges such as comprehensive evaluation, explainability, and effective model design?","This survey systematically reviews deep learning (DL)-based sequential recommendation, aiming to clarify algorithms, influential factors, and evaluation methods. Using comprehensive experimental evaluations on real datasets, it proposes a new classification framework, highlights key challenges, and offers guidance and future directions for designing effective DL-based sequential recommender systems.","The research goal is to systematically survey DL-based sequential recommendation, the approach is proposing a new classification framework and empirically analyzing influential factors, and the principal finding is that these factors significantly impact recommendation accuracy, guiding future research and practice in this area.",
Embedding design: Techniques like item embedding,session embedding,and w-item2vec to represent items and sessions for better learning.,,,,,,,,,,,,,
"Attention mechanisms: Applied alone or combined with other DL methods to enhance model performance by focusing on important parts of the sequence.""","The research is reproducible. The source codes and datasets used in the experiments are shared on Github: https://github.com/sttich/dl-recommendation. Datasets include RSC15, RSC19, and LastFM, with links provided for RSC15 and RSC19 in the context.","Improved models show statistically significant gains (Δ for p ≤ 0.01) over the basic GRU4Rec in Recall@5, MRR@5, NDCG@5, Recall@20, MRR@20, and NDCG@20 on both RSC15 and RSC19 datasets.",,,,,,,,,,,,,
Larger negative sample sizes notably improve model performance,especially from 0 to 32 samples,with diminishing returns as size increases further.,,,,,,,,,,,,,
"The optimal sampling strategy and loss function combination varies by dataset and evaluation metric
On RSC15
BPR-max
Table 6: On RSC15","highlighting the need for validation-based tuning.""
optimal sampling parameter (𝛹) is 0.25 for all loss functions and metrics; on RSC19
TOP1-max
Recall@20 ranges from 0.676±0.0009 (baseline) to 0.865±0.0005 (improved); on RSC19","Increasing negative sample size improves model performance on all evaluation metrics; performance rises sharply from 0 to 32, then increases more slowly (Figures 16, 17).
optimal 𝛹 varies by loss function and metric (Figure 14
and cross-entropy loss functions outperform BPR and TOP1 in all metrics; best loss function depends on dataset (Figures 18
Recall@20 ranges from 0.751±0.0017 to 0.893±0.0071.","15).
19).",,,,,,,,,,,,
Statistical significance: Δ for p-value ≤ 0.01,� for p-value ≤ 0.05,"\* for p-value ≤ 0.1 (Table 6).""",Lack of rigorous and comprehensive evaluations across different models and baselines.,,,,,,,,,,,,
Few effective benchmarks for evaluation,especially in sequential recommendation.,,,,,,,,,,,,,,
Challenges in designing advanced embedding methods specific to sequential recommendation.,,,,,,,,,,,,,,,
Insufficient modeling of user long-term preferences.,,,,,,,,,,,,,,,
Cold-start and data sparsity issues are largely unaddressed.,,,,,,,,,,,,,,,
"Data filtering choices may affect results and generalizability.""",Rigorous and comprehensive benchmarking and evaluation across different DL-based sequential recommendation models are urgently needed.,,,,,,,,,,,,,,
Incorporate side information,advanced loss functions (TOP1-max,BPR-max,cross-entropy),and attention mechanisms to improve performance.,,,,,,,,,,,
Better user representation modules and advanced embedding methods should be developed.,,,,,,,,,,,,,,,
Carefully balance model complexity,computational cost,"and data augmentation strategies.""",Lack of rigorous and comprehensive evaluations and benchmarks across different DL-based sequential recommendation models.,,,,,,,,,,,,
Insufficient focus on user representation and distinguishing different interaction types in current models.,,,,,,,,,,,,,,,
"Need for benchmarking studies to enable fair and effective comparison among various models and techniques.""","Future research should focus on rigorous and comprehensive evaluations across models, better embedding methods, improved modeling of user long-term preferences, and developing sequential recommendation models for specific and cross-domains. There is also a need for effective benchmarks and tailored loss functions for different datasets.",,,,,,,,,,,,,,
Sustainable Development Goal indicator for measuring availability and affordability of medicines for children: a proof-of-concept study,"Joosse Iris R, Mantel-Teeuwisse Aukje K, Suleman Fatima, van den Ham Hendrika A",2023,reference-manager,10.1136/bmjopen-2022-065929,,"The study adapted an existing, approved tool for children, using quality-assured historical datasets with limited age-appropriate medicines. The methodology allows initial comparison but is constrained by outdated data and small sample size. Further prospective data collection is needed for robust analysis and meaningful sensitivity analyses.",,,,"Data collection in six geographical survey areas per country, including urban and other regions, with systematic selection of health facilities across public, private, and other sectors.",,"How can a standardized, child-specific methodology based on SDG indicator 3.b.3 be developed and applied to measure and compare the availability and affordability of age-appropriate medicines for children across different countries?","The study aimed to adapt and validate an existing WHO/HAI methodology to measure the availability and affordability of medicines for children. Using historical, quality-assured datasets from several countries, the adapted tool showed proof of concept but highlighted the need for more comprehensive, child-specific data for future analyses.","The research goal was to adapt the WHO/HAI methodology for measuring access to pediatric medicines; the approach involved developing child-specific core medicine sets and a new affordability parameter (NUNT); results showed successful proof of concept using historical data from three countries, enabling standardized, comparable access assessments.",
Use of an adapted WHO/HAI methodology,including surveying up to 50 medicines (with 14 core medicines) and collecting data on both originator brands and lowest-priced generics.,,,,,,,,,,,,,,
"Introduction of the NUNT (novel parameter) to enable affordability calculations across different child age groups.""","The research uses an adapted, child-specific methodology based on the WHO/HAI methodology. Individual facility data are not publicly available, but aggregated data per medicine and country can be obtained from the HAI website. No source code for the project is mentioned.","The study adapted an existing tool to measure availability and affordability of medicines specifically for children, providing proof of concept using historical datasets.",,,,,,,,,,,,,
Only a modest sample of age-appropriate medicines was included; further analyses with larger,prospectively collected datasets are needed.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""",The primary outcome was proof of concept for a child-specific tool to measure availability and affordability of medicines for children.,,,,,,,,,,,,,,
The adapted methodology used an 80% accessibility threshold at facilities.,,,,,,,,,,,,,,,
Only a modest sample of age-appropriate medicines was surveyed; further analyses on larger datasets are needed.,,,,,,,,,,,,,,,
"No statistical values or measured effects were reported.""","The methodology inherits limitations from the original tool, such as burden of disease weighting and use of the national poverty line for affordability.",,,,,,,,,,,,,,
Only historical data were used,limiting relevance to current situations.,,,,,,,,,,,,,,
Modest sample of age-appropriate medicines surveyed; larger datasets needed.,,,,,,,,,,,,,,,
Uncertainties in NUNT (Number of Units Needed for Treatment) calculations.,,,,,,,,,,,,,,,
Quality and representativeness of underlying GHEs (Global Health Estimates) data are unclear,especially for children.,,,,,,,,,,,,,,
Individual facility data not publicly available; only aggregated data accessible.,,,,,,,,,,,,,,,
Further validation and expert consultation required for core medicine sets and methodology robustness.,,,,,,,,,,,,,,,
"Some included studies had unrepresentative samples or did not consider child-appropriate formulations.""
More comprehensive","The adapted child-specific tool provides proof of concept but was limited by reliance on historical datasets with few age-appropriate medicines.
prospective data are needed for robust analyses and sensitivity testing.",,,,,,,,,,,,,,
The methodology’s strength is its adaptation of an existing,approved tool for children.,,,,,,,,,,,,,,
"Further research is recommended to determine the minimum number of medicines needed for reliable accessibility measurement.""",The methodology’s burden of disease weighting may cause disproportionate representation and requires further analysis of different weighting approaches.,,,,,,,,,,,,,,
Only a modest sample of age-appropriate medicines was surveyed; larger,more comprehensive datasets are needed.,,,,,,,,,,,,,,
The adapted tool was tested only with historical data,"limiting relevance to current situations; prospective data collection is needed.""","Future research should analyze larger, prospectively collected datasets with more age-appropriate medicines, assess the minimum number of medicines needed for reliable accessibility measures, and examine how different burden of disease weighting approaches affect results. Further evaluation of affordability calculations combining national poverty line and LPGW wage is also recommended.","The study design is a proof-of-concept, observational survey using an adapted, standardized WHO/HAI methodology. It uses historical, quality-assured datasets, systematic sampling of medicine outlets across sectors and regions, and data validation. The study is non-randomized, non-controlled, and retrospective. No patient or public involvement.",,,"The objectives of the study were to provide first evidence of a child-specific tool for measuring the availability and affordability of medicines for children, by adapting an existing, formally approved methodology and conducting a proof-of-concept analysis using historical datasets.",,,,,,,,,
Knowledge graph construction for computer networking course group in secondary vocational school based on multi-source heterogeneous data,"Li Gang, Wang Hong, Liu Hong",2022,reference-manager,10.1109/itme56794.2022.00031,,Implementation Insights Summary:,,,,,,,,,
"The paper implemented a knowledge graph in teaching by extracting 239 entities and 521 relationships using algorithms like LDA
New Insights:",TextRank,and TF-IDF. In experiments with two classes,the experimental group using the knowledge graph showed higher academic performance and engagement than the control group.,,,,,,,,,,,,
"Manual refinement of entities and attributes improved accuracy. Using SmartKG for storage and visualization facilitated practical application. The approach promoted teaching quality and student interest.""",,"The research goal is to improve teaching quality by constructing a computer network knowledge graph using data crawling, keyword extraction (LDA, TextRank, TF-IDF), and SmartKG; results show that applying the knowledge graph in teaching enhances students' academic performance and engagement.","The paper aims to construct and apply a computer network knowledge graph for secondary vocational education. Using data crawling, keyword extraction (LDA, TextRank, TF-IDF), and rule-based relationship extraction, 239 entities and 521 relationships were visualized in SmartKG. Results show improved student performance and engagement with knowledge graph-based teaching.",,,"How can the construction and application of a knowledge graph, using keyword extraction and relationship extraction algorithms, enhance the teaching and learning outcomes in computer network courses?",,,,,,,,,"Keyword extraction using LDA\_model, TextRank, and TF-IDF algorithms to identify key terms from crawled data."
Rule-based relationship extraction combined with manual assistance to connect entities and improve accuracy.,,,,,,,,,,,,,,,
"Experimental design with control and experimental groups to evaluate the impact of knowledge graph application in teaching.""
In teaching practice",the experimental group using the knowledge graph scored higher (average 78.52) than the control group (average 69.92) across all question types.,239 entities and 521 relationships were extracted and visualized using SmartKG; attributes were manually completed for each entity.,,,,,,,,,,,,,
"The application of the knowledge graph improved students' academic performance and teaching quality; no p-values or statistical significance were reported.""",239 entities and 521 relationships were extracted and visualized using SmartKG.,,,,,,,,,,,,,,
Three keyword extraction algorithms (LDA,TextRank,TF-IDF) were compared; top keywords and their weights are presented in Table 2.,,,,,,,,,,,,,
"Experimental group using the knowledge graph scored higher (average 78.52) than the control group (average 69.92).
For question types 1",2,and 3,the experimental group outperformed the control group (scores: 22.64 vs. 21.44; 23.20 vs. 20.48; 32.68 vs. 28.00).,,,,,,,,,,,,
"The application of the knowledge graph improved students' academic performance and teaching quality.""","Attributes of each entity were completed manually, which may introduce human error or bias.",,,,,,,,,,,,,,
Relationship extraction used a rule-based approach combined with manual assistance due to the small dataset size,limiting scalability and generalizability.,,,,,,,,,,,,,,
No mention of large-scale or automated validation,"which may affect result robustness.""",The constructed knowledge graph improved students' academic performance and teaching quality compared to conventional methods.,,,,,,,,,,,,,
239 entities and 521 relationships were extracted and visualized using SmartKG.,,,,,,,,,,,,,,,
Manual and algorithmic keyword extraction ensured comprehensive entity and attribute coverage.,,,,,,,,,,,,,,,
"Recommendation: Apply knowledge graphs in vocational education to enhance both theoretical understanding and practical skills.""",Manual completion of entity attributes indicates a need for automated attribute extraction methods.,,,,,,,,,,,,,,
Limited evaluation: The study only compares two classes,suggesting broader validation is needed.,,,,,,,,,,,,,,
"Future work should focus on improving practical operation ability and integrating more diverse data sources for knowledge graph construction.""",,"The study design involved selecting two classes and assigning them as a control group and an experimental group. The experimental group used the constructed knowledge graph in teaching, while the control group used conventional methods. The study compared multi-dimensional data between the two groups.",,"The objectives of the study are to apply a constructed knowledge graph in teaching practice, compare its effects with conventional teaching methods, and analyze its impact on students' interest, enjoyment, participation, and academic achievements in computer network courses.",,,,,,,,,,,
Log Anomaly Detection by Adversarial Autoencoders With Graph Feature Fusion,"Xie Yuxia, Yang Kai",2024,reference-manager,10.1109/tr.2023.3305376,,Implementation Insights Summary:,,,,,,,,,
"GAE-Log integrates event graphs (modeling log sequences) and knowledge graphs (system info) for anomaly detection. Experiments show GAE-Log outperforms baselines
Graph-based methods: Construct graphs (e.g.
Machine learning-based methods: Apply algorithms such as support vector machine (SVM)","especially with both weighted event graphs and knowledge graphs. Removing the knowledge graph causes the largest performance drop
control flow graphs
linear regression (LR)","highlighting its importance. New insight: Combining both graphs is crucial for best results.""
time-weighted graphs
decision tree (DT)","communication finite state machines) from log data to model system behavior and identify anomalies by comparing with normal patterns.
and K-Means clustering to classify and detect anomalous log patterns.""","The research goal is effective log anomaly detection; the approach integrates event graphs and knowledge graphs in the GAE-Log framework; results show GAE-Log outperforms baselines on multiple datasets, achieving an F1-Score of 0.955 for WordSort applications, validating its superior performance.",,"The paper proposes the GAE-Log framework to detect system anomalies by modeling sequential logs with event graphs and knowledge graphs. Using comparison and ablation experiments on multiple datasets, GAE-Log outperforms baseline methods, achieving an F1-Score of 0.955 on WordSort, highlighting its effectiveness and the importance of knowledge graph integration.
GAE-Log achieves superior anomaly detection, with F1-Scores of 0.948 (WordCount), 0.952 (PageRank), and 0.955 (WordSort), outperforming all baselines.",,,,,,,"Statistics-based methods: Use statistical models like hidden Markov models (HMM), event count vectors with linear regression (LR), Kullback–Leibler (K-L) divergence, and statistical histograms to detect anomalies in system logs.",How can graph-based methods be utilized to extract features from system logs and construct models that effectively detect anomalies by capturing execution rules and relationships within the system?,
Integrating both event graphs and knowledge graphs in GAE-Log yields the best results; removing the knowledge graph causes the most significant performance drop.,,,,,,,,,,,,,,,
"Statistical significance (p-values) is not reported in the provided context.""","GAE-Log achieves the best F1-Scores across datasets: 0.948 (WordCount), 0.952 (PageRank), 0.955 (WordSort), 0.999 (OpenStack), and 0.975 (HDFS).",,,,,,,,,,,,,,
Baselines: LogRobust (0.901,WordCount),DT (0.947,PageRank),DeepLog (0.932,,OpenStack),WordSort).,,,,,,OC-SVM (0.222,HDFS),LogCluster (0.71
Ablation study: Integrating both event graphs and knowledge graphs yields the highest performance; removing knowledge graphs causes the most significant performance drop.,,,,,,,,,,,,,,,
"Weighted event graphs further improve results across applications.""","Log data are typically unlabeled, making supervised learning difficult.",,,,,,,,,,,,,,
Manually labeling log data is time-consuming,costly,and often impractical.,,,,,,,,,,,,,
Detecting sophisticated anomalies requires domain knowledge from subject matter experts.,,,,,,,,,,,,,,,
Dependencies among components can propagate failures,"increasing detection difficulty.""","GAE-Log consistently outperforms baseline algorithms in log anomaly detection across multiple datasets, achieving F1-Scores up to 0.999.",,,,,,,,,,,,,
Integrating both event graphs and knowledge graphs is crucial for superior performance; removing the knowledge graph leads to significant performance drops.,,,,,,,,,,,,,,,
"GAE-Log demonstrates strong practical potential for real-world anomaly detection applications.""",No explicit research gaps or future directions are identified in the provided context.,,,,,,,,,,,,,,
"The context focuses on experimental results
,,,,",performance comparisons,and ablation studies.,,,,,,,,,,,,,
From data to insights: the application and challenges of knowledge graphs in intelligent audit,"Zhong Hao, Yang Dong, Shi Shengdong, Wei Lai, Wang Yanyan",2024,reference-manager,10.1186/s13677-024-00674-0,,Implementation Insights Summary:,,,,,,,,,
The paper highlights that knowledge graphs significantly improve audit efficiency,accuracy,and depth in intelligent auditing. Key implementation insights include the need for more automated construction tools,better knowledge representation and reasoning,integration of heterogeneous data (including cloud data),,"and enhanced privacy and security measures.""",,,,On-site investigations and face-to-face interviews: Auditors directly observe and communicate with project managers and staff.,,"How can knowledge graphs and their associated technologies be effectively developed, applied, and optimized to enhance intelligent auditing processes, address domain-specific challenges, and ensure data privacy and security in cloud environments?","This paper investigates the integration of knowledge graphs in intelligent auditing, especially for power engineering projects. Using analysis of key technologies like data extraction and knowledge fusion, the study finds knowledge graphs greatly enhance audit efficiency, accuracy, and risk identification. The paper concludes with future research directions on automation, data integration, and privacy.","The research goal is to investigate knowledge graph technologies for intelligent auditing; the approach involves analyzing key methods like data extraction, fusion, and reasoning; the principal finding is that knowledge graphs significantly enhance audit efficiency, accuracy, and depth, with future research needed on automation, data integration, and privacy.",
Sample inspections: Random or targeted checks are used instead of auditing all records.,,,,,,,,,,,,,,,
"Manual record analysis: Auditors manually inspect and analyze ledger records and financial statements.""",,"Knowledge graphs significantly improve audit efficiency, accuracy, and depth in power engineering projects by automating risk assessments and enabling real-time monitoring.",,,,,,,,,,,,,
Integrating knowledge graphs with cloud-edge collaboration reduces computing latency and enhances the response speed of intelligent auditing systems.,,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are provided in the context.""","Primary outcomes focus on evaluating knowledge graphs using six indicators: Accuracy, Consistency, Completeness, Timeliness, Availability, and Trustworthiness.",,,,,,,,,,,,,,
"Definitions and references for each indicator are provided.
No specific results",measured effects,"or statistical values are reported in the provided context.""",Challenges related to data quality and completeness,,,,,,,,,,,,
Issues with effective representation and integration of diverse knowledge,,,,,,,,,,,,,,,
Scalability and performance limitations,,,,,,,,,,,,,,,
Complexities in knowledge acquisition and maintenance,,,,,,,,,,,,,,,
Difficulties in interpretability and explainability,,,,,,,,,,,,,,,
Privacy and security concerns,,,,,,,,,,,,,,,
Need for more efficient algorithms and automation,,,,,,,,,,,,,,,
Integration of heterogeneous data sources,,,,,,,,,,,,,,,
"Suggestions for future research on privacy protection and cloud optimization""","Knowledge graphs greatly enhance audit efficiency, accuracy, and depth in intelligent auditing.",,,,,,,,,,,,,,
Future research should focus on automating knowledge graph construction,improving knowledge representation and reasoning,integrating heterogeneous data,and ensuring data privacy and security.,,,,,,,,,,,,
"Cloud-edge collaboration with knowledge graphs reduces computing latency and boosts auditing system performance.""",Develop more efficient algorithms and tools to automate knowledge graph construction and optimize performance in cloud environments.,,,,,,,,,,,,,,
Explore more effective knowledge representation and reasoning mechanisms to improve application effectiveness and accuracy.,,,,,,,,,,,,,,,
Research better integration and utilization of heterogeneous data from multiple sources,including cloud-stored data,"to enrich knowledge graphs.""","Future research should develop more efficient algorithms and tools for automated knowledge graph construction, explore better knowledge representation and reasoning mechanisms, improve integration of heterogeneous data (including cloud data), and address data privacy and security in intelligent auditing applications.",No information available,,,,,,,,,,,"The objectives of the study are to investigate and analyze key technologies related to knowledge graphs, explore their applications and challenges in intelligent auditing, and propose future research directions to enhance audit efficiency, accuracy, and depth using knowledge graphs."
Lifelong Personal Context Recognition,"Bontempelli Andrea, Britez Marcelo Rodas, Li Xiaoyue, Zhao Haonan, Erculiani Luca, Teso Stefano, Passerini Andrea, Giunchiglia Fausto",2022,reference-manager,,,"Implementation Insights highlight three main challenges: (1) Representing context to capture any real-world situation and preserve identity across contexts, even with conflicting properties; (2) Embedding machine learning in time and enabling two-way human-AI interaction; (3) Maintaining lifelong alignment through Computer Vision and Natural Language Processing. New insights include the need for structure-aware predictors and robust, incremental learning to handle unpredictable changes.",,,,General Knowledge Representation (KR) mechanism: Defines personal context to let the machine view the world from the user's perspective.,,"How can AI systems achieve lifelong, flexible alignment with humans through general knowledge representation, adaptive machine learning, and continual, bi-directional interaction, enabling effective personal context recognition and mutual understanding in real-world, evolving environments?","The paper aims to develop AI systems capable of lifelong, holistic symbiosis with humans via smart wearables. Using knowledge representation, machine learning, and a machine-human alignment loop, experiments (notably with 158 students over four weeks) show the need for context-aware, adaptable, and ethically grounded AI for real-world human interaction.","The research goal is to develop lifelong human-AI symbiosis via smart wearables using a general Knowledge Representation (KR) mechanism, adaptive Machine Learning (ML), and a machine-human alignment loop; the approach combines context modeling, real-world experiments, and bi-directional interaction, with results based on the SmartUnitn2 dataset.",
General Machine Learning (ML) mechanisms: Enables context recognition and adaptation to changes in the world and user.,,,,,,,,,,,,,,,
Lifelong machine-human alignment loop: Maintains alignment through ongoing,"bidirectional interaction.""",,"The research developed a general knowledge representation (KR) mechanism, machine learning (ML) methods for context recognition, and a lifelong machine-human alignment loop, tested on the SmartUnitn2 dataset with 158 students over four weeks.",,,,,,,,,,,,
Key findings highlight challenges in representing context generality,identity across contexts,and the need for computer vision (CV) and natural language processing (NLP) for effective AI-human alignment.,,,,,,,,,,,,,
"No explicit quantitative results or statistical significance (p-values) are provided in the context.""","No explicit primary outcomes, results, or measured effects (including statistical values) are provided in the context.",,,,,,,,,,,,,,
The context describes research directions,challenges,and the use of datasets (e.g.,SmartUnitn2 with 158 students over four weeks),"but does not report specific outcome data or statistical results.""",,Difficulty representing any possible real-world situation and preserving identity across contexts.,,,,,,,,,
Need for full integration of machine learning over time and bi-directional human-AI interaction.,,,,,,,,,,,,,,,
"Requirement for Computer Vision and Natural Language Processing to address the Semantic Gap.
Lack of real-world",ethics-aware datasets and systematic experimental methodology.,,,,,,,,,,,,,,
User-supplied context annotations are often unreliable or inconsistent.,,,,,,,,,,,,,,,
Concept drift: changes in the world and user understanding over time.,,,,,,,,,,,,,,,
Limitation of using labels instead of full text for context representation.,,,,,,,,,,,,,,,
"Complexity in label meaning across different contexts and over time.""","Context representation must be general and preserve identity across contexts, enabling modeling of changing, possibly conflicting, properties.",,,,,,,,,,,,,,
Machine Learning should be fully embedded in time and support bi-directional human-AI interaction for lifelong alignment.,,,,,,,,,,,,,,,
"AI requires Computer Vision and Natural Language Processing capabilities to align with human understanding and interaction.
Real-world","ethics-aware experiments and interdisciplinary methodologies are essential for progress.""","Achieving generality and identity preservation in context representation, allowing entities to have different, possibly conflicting properties across contexts.",,,,,,,,,,,,,
Embedding machine learning fully in time and enabling bi-directional,cognitively cheap,and computationally affordable human-AI interaction for lifelong alignment.,,,,,,,,,,,,,
Developing systematic methodologies and ethics-aware datasets for real-world,"interdisciplinary experiments in human-machine symbiosis.""",Future research should address: (1) developing general methods for representing context and identity across situations; (2) fully embedding machine learning in time and enabling bidirectional human-AI interaction; (3) providing AI with Computer Vision and Natural Language Processing; (4) designing systematic real-world experiments and ethics-aware datasets.,,,,"The objectives of the study are to develop: (1) a general knowledge representation (KR) mechanism for defining personal context, (2) general machine learning (ML) mechanisms for context recognition and adaptation, and (3) a lifelong machine-human alignment loop through continual, bidirectional interaction.",,,,,,,,,
Consistency Rationing in the Cloud: Pay only when it matters,"Kraska Tim, Hentschel Martin, Alonso Gustavo, Kossmann Donald",2009,reference-manager,10.14778/1687627.1687645,,"The paper introduces statistical policies as a first step toward probabilistic consistency guarantees. Adaptive policies like the Demarcation and Dynamic policies outperform Fixed threshold policies, especially under varying update distributions. Implementation uses efficient statistics gathering, logical logging, and supports runtime protocol switching. Consistency Rationing can be adapted to other systems.",,,,Experiments were run for 300 seconds and repeated 10 times to count oversells and measure costs.,,"How can Consistency Rationing optimize the runtime cost of cloud-based database systems by dynamically adjusting consistency levels based on penalty costs, while maintaining acceptable performance?","The paper introduces Consistency Rationing, a method to optimize cloud database costs by dividing data into three consistency categories (A, B, C) and dynamically switching protocols. Using probabilistic guarantees and temporal statistics, the approach lowers costs and maintains performance. Experiments on Amazon S3 validate its effectiveness.","The research goal is to minimize cloud database costs while maintaining acceptable performance by introducing Consistency Rationing, an approach that adaptively assigns data to different consistency levels using statistical policies, with results showing significant cost reduction and improved performance.",
Three adaptive policies were compared: Fixed threshold policy,Demarcation policy,and Dynamic policy,focusing on numerical data.,,,,,,,,,,,,
"Statistical data collection involved incrementing counters per update and using normal distribution approximations for performance.""",,The Fixed threshold policy with T = 12 is the cheapest for uniformly distributed updates but costly for skewed updates; the Dynamic policy outperforms both Fixed threshold and Demarcation policies in cost.,,,,,,,,,,,,,
The Dynamic policy is 24% slower than C data but competes well in performance while optimizing cost.,,,,,,,,,,,,,,,
"No explicit p-values or statistical significance values are provided.""
For uniform updates","The Dynamic policy achieves the lowest overall cost per transaction among all policies tested.
Fixed threshold policy (T=12) is cheapest; for skewed updates",it is costly.,,,,,,,,,,,,,
Dynamic policy outperforms others in both cost and response time.,,,,,,,,,,,,,,,
Dynamic policy is 24% slower than C data but optimizes cost and prevents oversells.,,,,,,,,,,,,,,,
A data has the slowest response time; C data is fastest.,,,,,,,,,,,,,,,
"With increasing penalty cost
At penalty cost $0.1","Dynamic policy's overall cost approaches A data's cost.
C data's overall cost is $0.74 ($23 for 80-20 distribution) per 1000 transactions.""",Only two consistency levels (session consistency and serializability) are considered.,,,,,,,,,,,,,
Focus is mainly on B category data; A and C categories are not deeply explored.,,,,,,,,,,,,,,,
Advanced statistical methods for threshold setting are not addressed.,,,,,,,,,,,,,,,
Future work needed on better statistical methods,automatic optimizations,budget restrictions,"and relaxing other ACID properties.""",Consistency Rationing optimizes cloud database costs by allowing controlled inconsistencies when penalty costs are lower than consistency costs.,,,,,,,,,,,
Dividing data into three categories (A: strong,B: mixed,C: session consistency) and using adaptive policies significantly reduces operational costs and improves performance.,,,,,,,,,,,,,
Statistical and dynamic policies outperform fixed threshold approaches.,,,,,,,,,,,,,,,
Future work should enhance statistical methods,consider energy and budget constraints,"and explore relaxing other ACID properties.""",Develop better and faster statistical methods for probabilistic consistency guarantees.,,,,,,,,,,,,
Automate optimizations considering additional parameters,such as energy consumption.,,,,,,,,,,,,,,
Incorporate budget restrictions and explore relaxing other ACID principles,like durability,"in the cost function.""","Future research should explore better and faster statistical methods, automatic optimizations for other parameters (like energy consumption), adding budget restrictions to the cost function, and relaxing other ACID principles (such as durability). These are identified as next steps to improve probabilistic consistency guarantees.","The study is an experimental design using the TPC-W benchmark. It features repeated experiments (10 times), each running for 300 seconds, with manipulated data consistency categories (A, C, mixed). The design is non-controlled, observational, and explores effects of different consistency policies on overselling.",,,,,,,,,,,"The objectives of the study are to achieve the best overall cost while maintaining acceptable performance, introduce statistical policies for probabilistic consistency guarantees, and explore improvements such as faster statistical methods, automatic optimizations, budget restrictions, and relaxing ACID principles like durability."
Working memory capacity for continuous events: The root of temporal compression in episodic memory?,"Leroy Nathan, Majerus Steve, D'Argembeau Arnaud",2024,reference-manager,10.1016/j.cognition.2024.105789,,"The study preregistered its design and analysis, ensured transparency by sharing all data and scripts, and used both classical and robust statistical analyses—finding similar results. Analyses with all data (N=107, N observations=2675) confirmed the main conclusions. New insight: robust estimation methods did not alter hypothesis outcomes.",,,,Sample size was determined a priori using power analysis based on Monte-Carlo simulations to ensure at least 90% statistical power.,,No information available,,"The research goal was to investigate how events are temporally compressed in episodic memory; using a mental replay task with videos, the approach measured replay duration, and the principal finding was that participants recalled events as shorter than their actual duration, supporting temporal compression in memory.",
"Stimuli consisted of 25 videos of continuous actions
Study design","edited to create five sets with durations of 3
analysis plan","6
and hypotheses were preregistered; all data and materials were made openly available.""","9
The study's design, analysis plan, and hypotheses were preregistered on OSF (https://osf.io/4hxzs). All data, analysis scripts, and research materials are available at https://osf.io/zkwf2, supporting high reproducibility. No explicit mention of source code for the project is provided.","12
Analyses were based on 2212 observations from 90 participants, using robust linear mixed-effects models.",,and 15 seconds.,,,,,,,,,
Remembered duration (RD) and temporal ratio were modeled as outcomes; statistical significance was assessed with confidence intervals and p-values (alpha = 0.05,two-tailed).,,,,,,,,,,,,,,
"No specific quantitative results
Results: Analyses used linear mixed-effects models with robust estimation (DAStau estimator).",primary findings,"or p-values are explicitly stated in the context.""",Primary outcomes: Remembering duration (RD) and temporal ratio (RD/actual video duration).,,,,,,,,,,,,
Measured effects: Statistical significance assessed via confidence intervals and p-values (alpha = 0.05,two-tailed).,,,,,,,,,,,,,,
"Sample: 2212 observations from 90 participants.""",No information available,"Memory for continuous events is segmented into discrete units, leading to temporal discontinuities when event duration exceeds working memory limits.",,,,,,,,,,,,,
Results were robust across different data cleaning and analysis methods.,,,,,,,,,,,,,,,
"Recommendations include considering working memory constraints when studying episodic memory for extended events.""",,Future studies should independently measure individual differences in working memory (WM) capacity and examine how these relate to the decrease in temporal ratios as event duration increases. This could clarify whether variations in WM forgetting rate explain individual differences in temporal compression.,Preregistered study design and analysis plan,,,,,,,,,,,,
Data exclusions and transformations reported,,,,,,,,,,,,,,,
Linear mixed-effects models (growth curve analyses) used,,,,,,,,,,,,,,,
Robust estimation (DAStau estimator) due to unmet assumptions,,,,,,,,,,,,,,,
Random effects structure determined by likelihood ratio test,,,,,,,,,,,,,,,
"Observational study with 90 participants and 2212 observations""",,,,,,,,,,,,,,,
Personal Knowledge Graphs,"Balog Krisztian, Kenter Tom",2019,reference-manager,10.1145/3341981.3344241,,"Implementation Insights highlight several open challenges for Personal Knowledge Graphs (PKGs): ensuring secure integration with external services, managing storage across devices and cloud, handling offline scenarios, and maintaining privacy. Continuous, two-way synchronization and user involvement in resolving conflicting information are also emphasized as key issues.",,,,"Evaluation using large-scale, open datasets like Wikidata, DBpedia, and Freebase; synthetic data may be used as an alternative due to privacy and data availability challenges.",,"How can personal knowledge graphs be effectively defined, represented, implemented, and integrated with external sources, considering their unique properties, challenges, and the need for structured, user-centric knowledge distinct from general knowledge graphs?","The paper defines personal knowledge graphs (PKGs) as structured, user-centered knowledge graphs and explores their unique challenges compared to general knowledge graphs. It identifies key research questions, discusses evaluation and implementation issues, and concludes that PKGs require dedicated research due to their distinct properties and integration needs.","The research goal is to define personal knowledge graphs (PKGs), distinguish them from general knowledge graphs, and propose a research agenda; the approach involves identifying key properties, challenges, and research questions; the principal finding is that PKGs require new solutions for evaluation, implementation, and representation due to their unique personal context.",
Abstract conceptualization and definition of personal knowledge graphs (PKGs) and related research questions.,,,,,,,,,,,,,,,
"Comparative analysis of PKGs and general knowledge graphs to identify unique challenges and opportunities.""",,"The paper defines personal knowledge graphs (PKGs), distinguishes them from general knowledge graphs, and identifies main challenges and research questions for PKGs.",,,,,,,,,,,,,
Key challenges include lack of large open PKG datasets,complex implementation issues (privacy,access control),and continuous integration with external sources.,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""","No primary outcomes, results, or measured effects (including statistical values) are explicitly stated in the provided context.","Lack of large, open datasets for PKGs, making evaluation difficult.",,,,,,,,,,,,,
High costs and privacy concerns in generating real user interaction data.,,,,,,,,,,,,,,,
Implementation challenges: integration with external services,access control,storage location,multi-device support,offline access,,and ensuring security and privacy.,,,,,,,,,
Need for user involvement in data integration and conflict resolution.,,,,,,,,,,,,,,,
"Open research questions remain.""",The study defines personal knowledge graphs (PKGs) and distinguishes them from general knowledge graphs (KGs).,,,,,,,,,,,,,,
Key challenges include lack of large open datasets,implementation complexities (privacy,storage,access control),and integration of external knowledge.,,,,,,,,,,,
The paper proposes a research agenda with specific research questions to guide future work.,,,,,,,,,,,,,,,
"Recommendation: Further research and coordinated efforts are needed to address open problems and advance PKGs as a distinct subfield.""","Lack of large, open datasets for PKGs, making evaluation difficult due to privacy and cost concerns.",,,,,,,,,,,,,,
Open challenges in implementing PKGs,including access control,storage location,multi-device support,offline functionality,,and ensuring security and privacy.,,,,,,,,,
Need for continuous integration of external knowledge sources with PKGs,"potentially involving user intervention to resolve conflicts.""","Future research should address the lack of large, open datasets for PKGs, challenges in PKG implementation (such as storage, access control, and privacy), entity linking methods, and continuous integration of external knowledge sources, possibly involving user participation to resolve conflicts and manage updates.",No information available,,,"The objectives of the study are to define the concept of a personal knowledge graph (PKG), relate tasks and challenges of PKGs to existing work on knowledge graphs (KGs), and propose a research agenda for PKGs by formulating specific research questions.",,,,,,,,,
Dynamic Graph Neural Networks for Sequential Recommendation,"Zhang Mengqi, Wu Shu, Yu Xueli, Liu Qiang, Wang Liang",2015,reference-manager,,,"The implementation uses DGSR with DGL, embedding size 50, sequence length 50, Adam optimizer (lr=0.01), batch size 50, λ=1e-4, sub-graph order m=4, and 2-3 DAN layers. Larger sub-graphs and higher embedding sizes improve performance, but gains stabilize. DGSR outperforms baselines by leveraging dynamic graph structures.",,,,Dynamic Graph Neural Networks: The proposed DGSR framework uses dynamic graph neural networks to model user-item interactions over time for sequential recommendation.,,"How does the proposed DGSR model, incorporating dynamic graph recommendation networks, perform compared to state-of-the-art sequential recommendation methods, and what are the effects of its key components and hyper-parameters on recommendation accuracy?","The paper investigates DGSR, a dynamic graph-based sequential recommendation model. Using three Amazon datasets, it compares DGSR to state-of-the-art methods. DGSR outperforms baselines, especially in sparse datasets, by explicitly modeling cross-sequence information. The study also analyzes hyper-parameter effects, confirming DGSR’s robustness and superior performance.","The research goal is to improve sequential recommendation by modeling both long-term and short-term user-item interactions using a dynamic graph recommendation network (DGSR); the approach combines dynamic graph neural layers with recurrent or attention modules, and results show DGSR outperforms state-of-the-art methods on three real-world datasets.",
Empirical Evaluation: Extensive experiments are conducted on three real-world datasets to assess the effectiveness of DGSR.,,,,,,,,,,,,,,,
Comparative Analysis: DGSR is compared with several baseline models,including BPR-MF,FPMC,GRU4Rec+,Caser,,SR-GNN,,,,,,"DGSR outperforms all baselines on Amazon-CDs, Amazon-Games, and Amazon-Beauty datasets, achieving up to 10.56% NDCG@10 and 7.75% Hit@10 improvement over the best compared methods.","and HyperRec.""",TiSASRec,HGN
Increasing DGRN layers and sub-graph sampling size generally improves performance,but too many layers or large n can cause performance decline due to over smoothing or noise.,,,,,,,,,,,,,,
DGSR-1 consistently outperforms DGSR-0 and most baselines,"highlighting the effectiveness of the message propagation mechanism in modeling dynamic user preferences; no explicit p-values reported.""",DGSR achieved the highest NDCG@10 and Hit@10 across all datasets:,,,,,,,,,,,,,
Beauty: NDCG@10 = 35.90 (10.56% gain),Hit@10 = 52.40 (7.75% gain),,,,,,,,,,,,,,
Games: NDCG@10 = 55.70 (3.92% gain),Hit@10 = 75.57 (2.15% gain),,,,,,,,,,,,,,
CDs: NDCG@10 = 51.22 (3.81% gain),Hit@10 = 72.43 (1.41% gain),,,,,,,,,,,,,,
DGSR-DA variant (DAT + ATT) achieved the best ablation results.,,,,,,,,,,,,,,,
Increasing DGSR layers improved performance,with optimal layers varying by dataset.,,,,,,,,,,,,,,
Hyper-parameter tuning (layer number,sub-graph order,sequence length,"embedding size) significantly affected results.""","BPR-MF achieves poor performance on three datasets due to only capturing users’ general interests, making it hard to model user behavior sequences.",,,,,,,,,,,
GRU4Rec+ underperforms FPMC in Beauty and Games,possibly due to FPMC's focus on dynamic item transitions,which works better on sparse datasets.,,,,,,,,,,,,,
"No explicit mention of further research directions or additional self-reported limitations.""",Combining long-term and short-term information significantly improves recommendation performance over using either alone.,,,,,,,,,,,,,,
Variants with two modules (long-term and short-term) consistently outperform single-module variants.,,,,,,,,,,,,,,,
Attention and hierarchical gating mechanisms help capture item-item relations,leading to better results.,,,,,,,,,,,,,,
"Considering both central node and neighbor relationships enhances information propagation in dynamic graphs.""",Explicit modeling of dynamic collaborative signals among user sequences in sequential recommendation needs further exploration.,,,,,,,,,,,,,,
Investigating the effects of different hyper-parameter settings (such as DGRN layer number,sub-graph sampling size,sequence length,and embedding size) on model performance.,,,,,,,,,,,,
"Extending the DGSR framework to fuse various single-sequence models by modifying the message propagation mechanism.""",,,,The objectives of the study are:,,,,,,,,,,,
To evaluate how DGSR performs compared to state-of-the-art sequential recommendation methods.,,,,,,,,,,,,,,,
To assess the effectiveness of the dynamic graph recommendation networks component in DGSR.,,,,,,,,,,,,,,,
"To analyze the effects of different hyper-parameter settings on DGSR.""",,,,,,,,,,,,,,,
Lifelong learning on evolving graphs under the constraints of imbalanced classes and new classes,"Galke Lukas, Vagliano Iacopo, Franke Benedikt, Zielke Tobias, Hoffmann Marcel, Scherp Ansgar",2023,reference-manager,10.1016/j.neunet.2023.04.022,,"Parameter reuse (warm restart) maintains high accuracy even with limited history size. Adding unlabeled data after training does not improve inductive model performance. The gDOC extension, which considers class imbalance, outperforms simple DOC adaptations. Omitting old data may aid out-of-distribution detection. The tdiffk measure is robust across temporal granularities.",,,,"Comparison of transductive vs. inductive learning: Models are trained on labeled data, then evaluated with/without adding unlabeled data to assess accuracy changes.",,"Does adding unlabeled data to a graph after training on labeled vertices improve the accuracy of inductively pre-trained models compared to transductively trained models in static graph datasets, and how does this relate to incremental learning scenarios in lifelong graph learning?","The paper investigates methods for unseen class detection and open-set classification, comparing gDOC (with weighted cross-entropy) and DOC. Using datasets DBLP-easy and DBLP-hard, results show gDOC consistently outperforms DOC in MCC and Open F1 Macro scores. Larger history sizes and higher thresholds improve performance. Pre-training enhances model accuracy and stability.","The paper's main objective is to improve unseen class detection in evolving graphs; it introduces gDOC, a class-weighted extension of DOC, and finds that gDOC consistently outperforms DOC in both MCC and Open F1 Macro scores, especially on harder datasets with more classes.",
Incremental vs. once-trained models: Static (once-trained) and incremental (retrained over time) models are compared to analyze generalization and adaptation.,,,,,,,,,,,,,,,
"tdiffk measure: A method to determine history size for training by analyzing time differences in k-hop neighborhoods
On DBLP-hard","ensuring comparability across datasets.""
gDOC achieves F1 scores up to 0.16 (history size ≥6)",while DOC remains below 0.12.,"The gDOC method consistently outperforms DOC in unseen class detection, with higher MCC and Open F1 Macro scores on both DBLP-easy and DBLP-hard datasets.",,,,,,,,,,,,
"Risk reduction (α) does not improve performance; higher thresholds (τ = 0.75) are preferable. No p-values reported.""
On DBLP-easy
On DBLP-hard","gDOC consistently outperforms DOC in both MCC (Matthews Correlation Coefficient) and Open F1 Macro scores for unseen class detection on DBLP-easy and DBLP-hard.
gDOC achieves higher MCC (up to .10) and Open F1 Macro (up to .35) compared to DOC.
gDOC achieves higher MCC (up to .09) and Open F1 Macro (up to .15)",while DOC scores are much lower (MCC as low as .01,F1 as low as .01).,,,,,,,,,,,,
High threshold (τ = 0.75) is preferable for performance.,,,,,,,,,,,,,,,
Risk reduction (lowering detection threshold based on class-specific standard deviation) does not improve performance.,,,,,,,,,,,,,,,
Pre-trained models on static datasets (Cora,Citeseer,Pubmed) have higher accuracy and less variance than non-pre-trained models.,,,,,,,,,,,,,
GAT learns fastest without pre-training; GCN lags on Cora-B,"GAT lags on Pubmed.""","Only a portion of labeled data is used for training in subsequent tasks, which may limit generalizability.",,,,,,,,,,,,,
Labeled data is sampled uniformly at random without class stratification,possibly causing class imbalance.,,,,,,,,,,,,,,
The study does not explore different approaches to removing old training data; future work is suggested.,,,,,,,,,,,,,,,
Out-of-distribution (OOD) detection is limited to crisp decision methods,not OOD scores,due to lack of validation data for threshold tuning.,,,,,,,,,,,,,
"Further research is suggested to adapt more OOD approaches and retrieval/similarity frameworks to graphs.""",Weighting the binary cross-entropy loss in gDOC is essential for unseen class detection in imbalanced graph data.,,,,,,,,,,,,,,
The risk reduction technique from DOC does not improve performance on imbalanced graphs.,,,,,,,,,,,,,,,
Incremental training with limited history sizes achieves nearly the same performance as full history.,,,,,,,,,,,,,,,
"gDOC with weighted loss is recommended for handling class imbalance.""","Explore and adapt more out-of-distribution (OOD) approaches to graphs, such as using the IsoMax loss function.",,,,,,,,,,,,,,
Adapt ideas from the L2AC framework to graphs,integrating explicit retrieval and similarity components.,,,,,,,,,,,,,,
"Analyze why omitting old training data helps detect OOD examples and investigate alternative data removal strategies.""
Knowledge Patterns","Suggested future research directions include: exploring and adapting more out-of-distribution (OOD) approaches to graphs (e.g., using the IsoMax loss function), integrating explicit retrieval and similarity components from the L2AC framework, analyzing why omitting old training data aids OOD detection, and investigating alternative strategies for selecting which vertices to retain.
Clark Peter, Thompson John, Porter Bruce","The study design includes: use of three evolving graph datasets (DBLP-easy, DBLP-hard, PharmaBio) with attributed vertices, vertex labels, time information, evolving vertices/edges/classes over time; annual snapshots; experiments comparing incrementally-trained vs. once-trained models; repeated experiments (10 times) with different random seeds; two train-test split settings (few-many, many-few); and analysis of transductive vs. inductive learning.
2020",reference-manager,"The objectives are to optimize model hyperparameters for improved accuracy, evaluate unseen class detection using Macro-F1 and Matthews correlation coefficient (MCC), and analyze the effect of adding unlabeled data on model performance in transductive versus inductive learning settings.",,"Implementation Insights highlight that knowledge patterns are explicit, reusable abstractions aiding modularization and clarity in ontological engineering. Unlike design or semantic patterns, knowledge patterns are automated within a specific representation scheme. Limitations include manual pattern selection and mapping, with analogical reasoning offering potential future improvements.",,,,Use of knowledge patterns: Explicitly identifying and reusing recurring theory schemata to assemble the knowledge-base from component theories.,,"How can knowledge patterns be used to modularize, reuse, and apply general theories within knowledge-based systems, and what are the benefits and limitations of this approach compared to related methods such as analogical reasoning, design patterns, and semantic patterns?","The paper explores """"knowledge patterns"""" as reusable abstractions for constructing knowledge bases, illustrated through the KB-PHaSE system for astronaut training. Using compositional methods, it modularizes domain knowledge for reuse. The approach enables flexible, implementation-neutral knowledge representation, supporting scalable, formal ontologies and knowledge-based systems.","The research goal is to improve modularization and reuse in formal ontologies by introducing knowledge patterns; the approach involves isolating general theories as reusable units within a representational scheme; the principal finding is that this enables better abstraction, reuse, and construction of large-scale knowledge-based systems.",
Interactive simulation and fault introduction: Training astronauts through a simulator and simulated faults to practice procedures and problem recovery.,,,,,,,,,,,,,,,
Exploratory learning via form-based interface: Allowing users to browse concepts and ask questions,"with the system inferring properties and valid actions.""",,"The knowledge patterns approach enables modularization and reuse of general theories in building knowledge-based systems, as demonstrated in the KB-PHaSE prototype for astronaut training.",,,,,,,,,,,,
KB-PHaSE supports interactive simulation,fault recovery training,and exploratory learning using a reusable knowledge-base.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""","No explicit primary outcomes, results, or measured effects (including statistical values) are stated in the provided context.",Inheritance mechanisms work poorly when an abstract theory applies to a specific theory in multiple ways or when only part of the abstract theory is relevant.,,,,,,,,,,,,,
"Inheritance copies axioms in an """"""""all or nothing"""""""" fashion",making selective transfer difficult.,,,,,,,,,,,,,,
Parameterizing theories complicates axioms and may require additional parameters.,,,,,,,,,,,,,,,
The prototype system (KB-PHaSE) was a small demonstrator,"not intended for operational use.""",Knowledge patterns enable better modularization and reuse of general theories in knowledge-based systems.,,,,,,,,,,,,,
This approach overcomes the limitations of inheritance,especially when abstract theories apply in multiple ways or only partially.,,,,,,,,,,,,,,
It supports building reusable theory libraries,essential for large-scale ontologies.,,,,,,,,,,,,,,
"Recommendation: Use knowledge patterns to assemble knowledge-bases from component theories for flexibility and reuse.""",Limited exploration of automatic application methods for design and semantic patterns; current approaches focus on human guidance rather than computational automation.,,,,,,,,,,,,,,
Need for further formalization and computational support for knowledge patterns to enhance reuse and efficiency in ontology construction.,,,,,,,,,,,,,,,
"Insufficient integration between knowledge patterns and existing formal specification methods in software engineering.""",,No information available,,,,,,,,,,,,,
Defining a Knowledge Graph Development Process Through a Systematic Review,"Tamašauskaitė Gytė, Groth Paul",2023,reference-manager,10.1145/3522586,,"The Implementation Insights highlight the need for more industry case studies, expert evaluations, and comparisons with existing methodologies. Most literature focuses on initial knowledge graph development, not continuous updates. The proposed process offers a structured, evidence-based framework, but its validity is limited by article selection and research design.",,,,"Systematic review: The study conducted a systematic review of literature on knowledge graph development processes, following PRISMA guidelines for transparency.",,"What are the main steps in the knowledge graph development process, how are they interrelated, and to what extent can the proposed unified process be applied and generalized across different contexts and populations?","The paper systematically reviews knowledge graph development processes, synthesizing them into an evidence-based, structured framework. Using PRISMA guidelines, it identifies six main steps: data identification, ontology construction, knowledge extraction and processing, graph construction, and maintenance. The study concludes with recommendations for future research and practical evaluation.","The research goal was to synthesize and structure knowledge graph development processes through a systematic review; the approach involved conceptual analysis of literature workflows, resulting in an evidence-based framework that unifies key steps and guides researchers and practitioners in constructing and managing knowledge graphs.",
Synthesis and analysis: The research synthesized and analyzed various knowledge graph development approaches from multiple articles to create an evidence-based framework.,,,,,,,,,,,,,,,
"Workflow/process mapping: The study mapped and compared different workflows
Quantitative results: 620 process steps were initially extracted (519 unique)","processes
reduced to 414 unique steps after synonym adjustment (182 Level I","and methodologies described in the reviewed articles.""
196 Level II","The research is based on a systematic literature review, following PRISMA guidelines for transparency. No source code for the project is mentioned or provided. The reproducibility relies on the described methodology and article selection process, not on code or data artifacts.
60 Level III).","The paper synthesizes a structured, evidence-based framework for knowledge graph development, identifying six main steps: identify data, construct ontology, extract knowledge, process knowledge, construct the knowledge graph, and maintain the knowledge graph.",,,,,,,,,,,
"No statistical significance (p-values) reported; conclusions highlight the need for further empirical validation and broader evaluation of the proposed process.""","Primary outcome: Identification and synthesis of main steps in knowledge graph development—(i) identify data, (ii) construct ontology, (iii) extract knowledge, (iv) process knowledge, (v) construct the knowledge graph, (vi) maintain the knowledge graph.",,,,,,,,,,,,,,
No statistical values or measured effects reported.,,,,,,,,,,,,,,,
"Results are based on systematic review and conceptual analysis.""","Most reviewed articles lacked a solid framework, relying on project workflows.",,,,,,,,,,,,,,
Internal validity affected by author interpretation and selection bias.,,,,,,,,,,,,,,,
Not all relevant articles may have been retrieved; no snowballing due to time constraints.,,,,,,,,,,,,,,,
Only scientific articles analyzed; evaluation based on two case studies.,,,,,,,,,,,,,,,
Limited generalizability to broader populations.,,,,,,,,,,,,,,,
Further evaluation with experts and organizations suggested.,,,,,,,,,,,,,,,
"Practical implementation of the proposed process not yet tested.""","The study proposes an evidence-based, structured framework for knowledge graph development, synthesizing steps from existing literature.",,,,,,,,,,,,,,
The main steps are: identify data,construct ontology,extract knowledge,process knowledge,construct the knowledge graph,,and maintain it.,,,,,,,,,
"Recommendations include further research on industry cases and broader evaluation methods to assess generalizability.""",Researching additional industry cases to understand how organizations develop knowledge graphs in practice.,,,,,,,,,,,,,,
Evaluating the proposed process with experts and organizations to assess its value and practical use.,,,,,,,,,,,,,,,
"Examining how existing software development and ontology methodologies can be applied or compared to knowledge graph development.""",Suggested future research directions include: studying additional industry cases; evaluating the proposed process with experts and organizations; comparing the process to existing methodologies; developing a knowledge graph using the proposed process; and researching tools and techniques for each development step. Limitations include lack of empirical validation and generalizability.,"Study design characteristics: Systematic review, conceptual analysis, PRISMA guidelines followed, evidence-based framework synthesis, data collected from eight academic sources, inclusion/exclusion criteria applied, evaluation based on two case studies, no empirical generalization, single-author screening with protocol checks.",,"The objectives of the study are to understand the main steps in the knowledge graph development process and how they are interrelated, by conducting a systematic review and conceptual analysis, and to provide guidance for researchers and practitioners constructing and managing knowledge graphs.",,,,,,,,,,,
Temporal Graph Networks for Deep Learning on Dynamic Graphs,"Rossi Emanuele, Chamberlain Ben, Frasca Fabrizio, Eynard Davide, Monti Federico, Bronstein Michael",2020,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
Graph-based embedding modules (TGN-attn,TGN-sum) greatly outperform non-graph (TGN-id); TGN-attn is best but slightly slower.,,,,,,,,,,,,,,
Memory is crucial: models with memory are ~4% more precise but 3× slower.,,,,,,,,,,,,,,,
One graph attention layer suffices in TGN due to memory,improving speed up to 30×.,,,,,,,,,,,,,,
TGN achieves state-of-the-art results on all tasks and datasets,especially excelling on Twitter.,,,,,,,,,,,,,,
"New insight: Combining memory with recent neighbor sampling reduces the number of neighbors needed for optimal performance.""",,"The research goal is to develop TGN, a framework for learning on continuous-time dynamic graphs; the approach uses memory modules and graph-based embedding modules (notably attention-based) to generate up-to-date node embeddings; results show TGN achieves state-of-the-art performance and speed on multiple tasks and datasets.","The paper introduces TGN, a framework for learning on continuous-time dynamic graphs. Using ablation studies, it shows that memory and graph-based embedding modules are crucial for performance. TGN achieves state-of-the-art results on several tasks and datasets, while being faster than previous methods, with broad application potential.",,,"How can a generic framework be designed to effectively learn on continuous-time dynamic graphs, addressing challenges such as memory staleness and efficient message aggregation, to achieve state-of-the-art performance on tasks involving evolving real-world systems?",,,,,,,,,"Ablation studies: Systematically remove or alter components (like memory, embedding modules, and message aggregators) to assess their impact on performance."
Comparative evaluation: Compare TGN variants and baseline models using metrics such as average precision and ROC AUC across multiple datasets.,,,,,,,,,,,,,,,
"Neighbor sampling strategies: Test different neighbor sampling methods (last vs. uniform) and vary the number of sampled neighbors per layer.""","The research is reproducible: all experiments and timings are described in detail, and the code will be made available for all experiments to be reproduced. No explicit source code link is provided in the context.","TGN-attn achieves state-of-the-art average precision in future edge prediction: Wikipedia (98.46% transductive, 97.81% inductive), Reddit (98.70%, 97.55%), Twitter (94.52%, 91.37%).",,,,,,,,,,,,,
Graph-based embedding modules (TGN-attn,TGN-sum) significantly outperform graph-less models; memory modules are crucial for performance.,,,,,,,,,,,,,,
"TGN is faster than previous methods and requires only one attention layer for high performance due to its memory mechanism.""",Primary outcomes:,,,,,,,,,,,,,,
TGN-attn achieved state-of-the-art results in future edge prediction and dynamic node classification on Wikipedia,Reddit,and Twitter datasets.,,,,,,,,,,,,,
In future edge prediction,TGN-attn outperformed the second-best method (DyRep) by over 4% (transductive) and 10% (inductive) on Twitter.,,,,,,,,,,,,,,
TGN-attn achieved 98.46% (Wikipedia),98.70% (Reddit),and 94.52% (Twitter) average precision (transductive).,,,,,,,,,,,,,
For dynamic node classification,TGN-attn achieved 87.81% (Wikipedia) and 67.06% (Reddit) ROC AUC.,,,,,,,,,,,,,,
"TGN-attn is up to 30× faster per epoch than TGAT
Measured effects:",with similar convergence speed.,,,,,,,,,,,,,,
Graph-based projections (TGN-attn,TGN-sum) outperform graph-less models.,,,,,,,,,,,,,,
"Using only one graph attention layer with memory is sufficient for high performance and speed.""",Node features are not present in any dataset; all nodes are assigned the same zero feature vector.,,,,,,,,,,,,,,
Only edge creation (interaction) events are included; datasets lack a wider variety of event types.,,,,,,,,,,,,,,,
"Evaluation on datasets with more event types is left for future work.""","TGN achieves state-of-the-art results on multiple tasks and datasets, outperforming all baselines, especially on the Twitter dataset by over 4% (transductive) and 10% (inductive).",,,,,,,,,,,,,,
Memory and graph-based embedding modules are critical for high performance,enabling up-to-date node representations and capturing long-term information.,,,,,,,,,,,,,,
Using only one graph attention layer with memory maintains high accuracy while making the model up to 30× faster than TGAT.,,,,,,,,,,,,,,,
TGN is recommended for applications in social sciences,recommender systems,"and biological networks; future work should explore advanced model settings and domain-specific adaptations.""",Exploring more advanced settings of the TGN model.,,,,,,,,,,,,
Understanding the most appropriate domain-specific choices for applying TGN.,,,,,,,,,,,,,,,
Investigating applications of TGN in social sciences,recommender systems,"and biological interaction networks.""","The study suggests future research should explore more advanced settings of the TGN model and investigate the most appropriate domain-specific choices for its application, particularly in social sciences, recommender systems, and biological interaction networks.","The study design is an ablation study using three datasets (Wikipedia, Reddit, Twitter) for future edge prediction and dynamic node classification. It is non-randomized, non-blinded, controlled (with strong baselines), multi-site (multiple datasets), uses chronological data splits (70%-15%-15%), and averages results over 10 runs.",,,,,,,,,,,
"Knowledge Graph Construction with a
Façade",,,,,,,,,,,,,,,
": A Unified Method to Access Heterogeneous Data Sources on the Web""","Asprino Luigi, Daga Enrico, Gangemi Aldo, Mulholland Paul",2023,reference-manager,10.1145/3555312,,Implementation Insights Summary:,,,,,,,,,
The paper introduces Facade-X,a unified method for accessing diverse data sources and constructing knowledge graphs using SPARQL Anything. The approach supports many file formats,uses triple-filtering to improve memory efficiency,"and receives positive user feedback. Future work targets streaming large datasets and expanding supported formats.""",,,"The paper's main objective is to streamline Knowledge Graph Construction from heterogeneous web data using a unified façade-based method (Facade-X) implemented in SPARQL Anything; the approach demonstrates theoretical robustness, practical value, and positive user feedback, with performance comparable to state-of-the-art tools.","Use of an intermediate façade (Facade-X) to access heterogeneous data sources as if they were RDF, enabling re-engineering and re-modelling of source data.",,,,,,"How can a unified, façade-based approach—implemented as SPARQL Anything—streamline knowledge graph construction from heterogeneous data sources, addressing usability, scalability, and cognitive complexity compared to alternative solutions?",,"The paper proposes a unified method for accessing heterogeneous web data to streamline Knowledge Graph Construction using Facade-X and SPARQL Anything. Through theoretical and experimental evaluations, it demonstrates comparable performance to existing tools, lossless data conversion, and positive user feedback. Future work includes usability studies and support for large datasets."
Implementation and evaluation of a triple-filtering approach,which only transforms relevant data parts matching SPARQL query patterns.,,,,,,,,,,,,,,
"Empirical evaluation through an online survey targeting Semantic Web practitioners and SPARQL users to assess usability and gather feedback.""","The research is reproducible: the implementation, SPARQL Anything, is available as an open-source project. Queries used in the study are provided as additional material. No direct link to the source code is given in the context, but the tool is described as open-source.",70.3% of users rated ease of learning as very important or essential; 66.6% valued readable mappings; 51.8% prioritized minimizing required languages/syntaxes.,,,,,,,,,,,,,
SPARQL Anything demonstrated comparable performance to state-of-the-art tools,but performance issues may arise with very large datasets.,,,,,,,,,,,,,,
Cognitive complexity analysis showed SPARQL Anything required fewer tokens per query than SPARQL Generate,RML,"and ShExML; no p-values reported.""",The triple-filtering approach in SPARQL Anything reduced execution time by:,,,,,,,,,,,,
43% for the Artists query (joining two CSVs),,,,,,,,,,,,,,,
47% for the Artworks query (selecting artworks and artist names),,,,,,,,,,,,,,,
14% for the Artworks-s query (extracting metadata and subjects),,,,,,,,,,,,,,,
Performance is comparable to state-of-the-art tools.,,,,,,,,,,,,,,,
"Cognitive complexity was measured by counting the number of tokens needed per query across frameworks.""","Evaluation focused mainly on theoretical aspects, not domain-specific models.",,,,,,,,,,,,,,
Performance issues may arise with very large datasets due to memory limits or processing time.,,,,,,,,,,,,,,,
Usability study is still under development; cognitive difficulties and mapping strategies need further investigation.,,,,,,,,,,,,,,,
Current support for connectors to relational and no-SQL databases is lacking.,,,,,,,,,,,,,,,
"Limited support for novel structural design patterns and specific data formats.""","The proposed Facade-X approach enables robust, lossless transformation of heterogeneous data sources into RDF, supporting a wide range of formats.",,,,,,,,,,,,,,
SPARQL Anything demonstrates comparable performance to state-of-the-art tools,with lower cognitive load and verbosity than alternatives.,,,,,,,,,,,,,,
Scalability for very large datasets remains a challenge; future work will focus on streaming and memory optimization.,,,,,,,,,,,,,,,
"Positive user feedback highlights usability; further usability studies are recommended.""",Need for better support in generating expressive OWL ontologies from structurally homogeneous RDF data.,,,,,,,,,,,,,,
Necessity for thorough usability studies to identify cognitive difficulties and strategies in mapping design with SPARQL Anything versus alternatives.,,,,,,,,,,,,,,,
"Future work includes implementing connectors to relational and no-SQL databases like MongoDB.""","Future research directions include: extending the meta-model to cover new structural design patterns (e.g., AMR, Linguistic frames, COCO, YOLO, JAMS), improving support for expressive OWL ontology generation, enhancing scalability for large datasets via query-rewriting and streaming, and conducting thorough usability studies to identify cognitive challenges.",No information available,,"The objectives of the study are to propose a unified method for accessing heterogeneous web data sources to streamline Knowledge Graph Construction (KGC), implement this method in the SPARQL Anything system, evaluate its generality, usability, and performance, and identify future improvements for scalability and usability.",,,,,,,,,,,
Question answering over temporal knowledge graphs based on hierarchical semantic extraction,"Wang Jian, Zhang Wenjuan, He Qi, Zhao Danfeng",2024,reference-manager,10.1109/swc62898.2024.00207,,"The paper introduces HSTQA, which uses a hierarchical semantic extraction method and multi-granularity fusion to better capture both global and local temporal features in questions. Structure-enhanced and relation fusion units further improve temporal reasoning. Ablation studies show each module significantly boosts performance, especially for complex temporal queries.",,,,Hierarchical Semantic Extraction Method: Extracts both global and local semantic information from questions to better capture implicit temporal details.,,How can hierarchical semantic extraction and multi-granularity fusion improve complex temporal reasoning and question answering over temporal knowledge graphs?,"The paper proposes HSTQA, a hierarchical semantic extraction-based model for complex temporal reasoning in temporal knowledge graph question answering (TKGQA). Using modules for semantic extraction, multi-granularity fusion, and relation integration, HSTQA outperforms baselines on the MultiTQ dataset, especially for multi-granularity and single-constraint temporal questions.","The research goal is to enhance complex temporal question answering over temporal knowledge graphs; the approach uses a hierarchical semantic extraction method and multi-granularity fusion in the HSTQA model; results show HSTQA significantly outperforms previous models, especially in handling multi-granularity and implicit temporal reasoning tasks.",
Multi-Granularity Fusion Module: Aligns and harmonizes different temporal granularities between questions and the temporal knowledge graph.,,,,,,,,,,,,,,,
"Structure-Enhanced and Relation Fusion Units: Extract implicit temporal features and integrate them into relational patterns for deeper temporal understanding.""",,"HSTQA achieved the highest Hits@1 (0.401) and Hits@10 (0.744) on the MultiTQ dataset, outperforming all baselines, especially on complex temporal and multi-granularity questions.",,,,,,,,,,,,,
Ablation studies show removing hierarchical semantic extraction drops Hits@1 by 2.4% and Hits@10 by 5.0%,confirming each module’s significant contribution.,,,,,,,,,,,,,,
The improvements are statistically significant,"with HSTQA excelling particularly on """"""""equal"""""""" and """"""""before/after"""""""" temporal question types due to its multi-granularity fusion and structure-enhanced modules.""",HSTQA achieves Hits@1 of 0.401 and Hits@10 of 0.744 overall on the MultiTQ dataset.,,,,,,,,,,,,,
HSTQA outperforms MultiQA by 10.8% in Hits@1 and 10.9% in Hits@10.,,,,,,,,,,,,,,,
For single-constraint questions,HSTQA achieves Hits@1 of 0.502.,,,,,,,,,,,,,,
Removing the hierarchical semantic extraction reduces Hits@1 by 2.4% and Hits@10 by 5.0%.,,,,,,,,,,,,,,,
Removing the structure-enhanced unit drops Hits@1 from 0.401 to 0.348.,,,,,,,,,,,,,,,
"HSTQA shows superior performance in both """"""""equal"""""""" and """"""""before/after"""""""" multi-granularity questions.",,,,,,,,,,,,,,,
"CronKGQA improves over EmbedKGQA by 7.3% (Hits@1) and 14.9% (Hits@10).""","Existing methods often rely on pre-defined rules or extensive subgraph processing, limiting adaptability to complex situations.",,,,,,,,,,,,,,
Accurately modeling and applying temporal constraints,especially in complex scenarios,remains challenging.,,,,,,,,,,,,,
Most models assume a single temporal granularity,overlooking diverse real-world granularities (e.g.,daily,monthly).,,,,,,,,,,,,
MultiQA neglects structural nuances in complex questions,where temporal details influence entities differently.,,,,,,,,,,,,,,
Fuzzy matching for entity identification can overlook semantic nuances,"causing errors in reasoning.""","HSTQA significantly outperforms baseline models in complex temporal reasoning for TKGQA, especially on multi-granularity and single-constraint questions.",,,,,,,,,,,,,
The hierarchical semantic extraction and multi-granularity fusion modules are key to improved performance.,,,,,,,,,,,,,,,
Removing any core module leads to notable performance drops.,,,,,,,,,,,,,,,
"Recommendation: Use HSTQA for superior temporal question answering.""","Most existing approaches overlook structural information in time-constrained questions, limiting complex temporal reasoning.",,,,,,,,,,,,,,
There is a need to better integrate multi-granularity temporal features to handle questions with varying temporal granularities.,,,,,,,,,,,,,,,
"Reliance solely on relational representations can cause loss of important implicit temporal features.""",,,,"The objectives of the study are to enhance complex temporal reasoning for the TKGQA task by proposing HSTQA, a question-answering model that uses hierarchical semantic extraction and a multi-granularity fusion module to improve the accuracy and capability of answering complex temporal questions.",,,,,,,,,,,
"A Review of Privacy-preserving Federated Learning for the Internet-of-Things
Perturbative methods: Add noise to query results or round dataset values to obscure sensitive information.
k-anonymity","Briggs Christopher, Fan Zhong, Andras Peter
l-diversity","2020
t-closeness: Techniques to anonymize data and prevent re-identification through linkage attacks.""",reference-manager,"Privacy preserving methods are categorized as suppressive (removal, aggregation, sampling) or perturbative (noise addition, rounding).",,"Implementation Insights highlight that local differential privacy adds noise to individual data, protecting user privacy but requiring large user numbers for accurate statistics. Apple and Google use this in practice. Federated learning benefits from privacy mechanisms like differential privacy, homomorphic encryption, and secure multi-party computation, but faces challenges in model optimization and continual learning.",,,,"Suppressive methods: Remove or generalize data attributes, restrict queries, or return sampled data to protect privacy.",,"How can privacy-preserving federated learning be effectively implemented to balance data privacy, utility, computational efficiency, and scalability in distributed machine learning, particularly in edge and fog computing environments?","The paper reviews methods for protecting individual privacy in data releases, especially in machine learning. It examines suppressive and perturbative privacy mechanisms, anonymization, homomorphic encryption, and secure multi-party computation. The study highlights the privacy-utility tradeoff and challenges like linkage attacks, concluding that maintaining privacy remains difficult despite existing methods and regulations.","The research goal is to review privacy-preserving methods for data releases in machine learning, the key approach is categorizing methods as suppressive or perturbative (including anonymization and k-anonymity), and the principal finding is that balancing privacy and data utility remains challenging due to risks like linkage attacks.",
Anonymization and k-anonymity can reduce re-identification risk but are vulnerable to linkage attacks,as demonstrated in several real-world cases.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in this section.""",The new method significantly outperforms FedAvg by reducing the number of communication rounds and improving final model accuracy on highly skewed non-IID data.,,,,,,,,,,,,,,
Application of per-coordinate averaging (based on Adam) achieves faster convergence in fewer communication rounds.,,,,,,,,,,,,,,,
Client selection methods achieve faster convergence by selecting clients with faster communication or greater resources.,,,,,,,,,,,,,,,
Variance reduction methods improve convergence speed on non-IID data compared to FedAvg.,,,,,,,,,,,,,,,
Privacy-preserving mechanisms (differential privacy,homomorphic encryption,secure multi-party computation) protect model updates while maintaining model utility,"with some methods achieving user-level differential privacy with only small loss in utility.""",Accumulated privacy loss over training iterations can breach privacy budgets.,,,,,,,,,,,
Collusion between users may lead to privacy breaches.,,,,,,,,,,,,,,,
Queries on correlated data can leak more information than intended.,,,,,,,,,,,,,,,
Selecting an appropriate ϵ (privacy parameter) is non-trivial and context-dependent.,,,,,,,,,,,,,,,
Large ϵ-budgets increase privacy risks.,,,,,,,,,,,,,,,
Local differential privacy introduces high noise,reducing data utility.,,,,,,,,,,,,,,
Balancing privacy and data utility is challenging.,,,,,,,,,,,,,,,
"The review does not address privacy breaches from hacking or theft.""","Increasing client-side computation between communication rounds greatly reduces communication rounds needed for convergence, especially with specific mini-batch sizes and epochs.",,,,,,,,,,,,,,
Compression of parameter updates is important to reduce communication costs without sacrificing model accuracy.,,,,,,,,,,,,,,,
Further research is needed on privacy-preserving methods and adapting federated learning for low-power and resource-constrained devices.,,,,,,,,,,,,,,,
"Leveraging fog computing can reduce latency and improve scalability in federated learning.""",Improving continual learning methods for federated learning to handle evolving data distributions efficiently.,,,,,,,,,,,,,,
Developing better privacy-preserving techniques that balance model performance and user privacy,especially reducing computational costs of methods like homomorphic encryption.,,,,,,,,,,,,,,
"Leveraging fog computing to reduce latency and distribute computational burdens in federated learning.""","Future research should focus on improving how models learn over time in federated learning, especially as data distributions change. Methods like meta-learning, online learning, and continual learning are suggested, with attention to challenges unique to the distributed nature of federated learning.",,,,,,,,,,,,,,
Native Cloud Object Storage in Db2 Warehouse: Implementing a Fast and Cost-Efficient Cloud Storage Architecture,"Kalmuk David, Garcia-Arellano Christian, Barber Ronald, Sidle Richard, Rakopoulos Kostas, Roumani Hamdi, Minor William, Cheung Alexander, Hooper Robert C., Emmerton Matthew, Hoggard Zach, Walkty Scott, Perez Patrick, Santars Aleksandrs, Chen Michael, Olan Matthew, Zilio Daniel C., Sayyid Imran, Li Humphrey, Rampurkar Ketan, Ramachandran Krishna K., Shen Yiren",2024,reference-manager,10.1145/3626246.3653393,,"Implementation Insights focus on integrating LSM trees for optimized data organization, asynchronous write tracking, and efficient bulk inserts. KeyFile was developed for better Db2 integration, replacing RocksDB-Cloud. Experiments show similar insert performance for columnar and PAX clustering, 4x compression, and 2 GB/s write rates. Future work targets broader optimization and adaptive clustering.",,,,Use of various clustering keys for data pages to optimize data organization and query performance via LSM trees.,,"How can the integration of an LSM tree-based storage layer with object storage in Db2 Warehouse optimize data page storage, minimize latency and amplification factors, and improve performance compared to traditional storage subsystems?","The paper aims to integrate an LSM tree storage layer into Db2 to optimize data organization and query performance, using KeyFile as an abstraction over RocksDB. Methodology includes clustering keys, asynchronous write tracking, and bulk load techniques. Experiments show improved performance and storage efficiency. The study concludes with positive implications for cloud data warehousing.",The research goal is to modernize Db2 Warehouse storage for cloud object storage using an LSM tree-based approach; the method integrates LSM trees and optimizations to minimize latency and amplification; results show improved performance and cost savings while retaining existing database capabilities.,
Mechanism to track outstanding asynchronous writes within an LSM tree,leveraging Db2’s asynchronous I/O processing.,,,,,,,,,,,,,,
Experimental evaluation comparing performance,storage block size choices,"and data page clustering methods (columnar vs PAX).""","Some components of the research are reproducible. The source code for RocksDB is available at https://github.com/facebook/rocksdb, and RocksDB-Cloud is available at https://github.com/rockset/rocksdb-cloud. No explicit source code for the main project or KeyFile is provided in the context.","Bulk insert optimization improved elapsed time by 90%, reduced WAL syncs by 98%, and bytes written to WAL by 93%.",,,,,,,,,,,
Trickle-feed optimization increased insert rate by 50%,reduced WAL syncs by 73%,and bytes written to WAL by 68%.,,,,,,,,,,,,,
Columnar clustering achieved 15.8% higher overall QPH than PAX,"with simple queries showing an 84.7% improvement. No p-values reported.""","Columnar clustering QPH is 7x and 5x higher than PAX clustering for 690 GB and 138 GB caches, respectively.",,,,,,,,,,,,,
Bulk insert elapsed time improves by 90% with optimization; WAL syncs and bytes written reduced by 98% and 93%.,,,,,,,,,,,,,,,
Trickle-feed optimization increases insert rate by 50%; WAL syncs and bytes written reduced by 73% and 68%.,,,,,,,,,,,,,,,
"Columnar clustering uses 42% less cache and achieves 16% higher overall QPH
For bulk inserts","with Simple queries QPH 85% higher.
scalability is near perfect up to 10 TB; complex queries off by ~1%","intermediate queries off by ~38%.""",Intermediate queries at 10 TB were off perfect scalability by about 38% due to becoming more disk-bound at higher scales.,,,,,,,,,,,,
The solution currently does not generalize optimizations to other database objects like general indexes and row-organized tables.,,,,,,,,,,,,,,,
"Further improvements are needed for scalability and adaptive clustering based on access patterns.""","The new LSM tree-based storage architecture for Db2 Warehouse improves scalability, availability, and cost efficiency.",,,,,,,,,,,,,,
Integration with RocksDB and tailored optimizations led to reduced storage costs and better query and insert performance.,,,,,,,,,,,,,,,
The solution outperforms previous Db2 versions and leading industry competitors.,,,,,,,,,,,,,,,
"Future work will focus on generalizing optimizations and improving scalability and adaptive clustering.""","Generalizing optimizations for other database objects, such as indexes and row-organized tables.",,,,,,,,,,,,,,
Improving scalability to enhance efficient resource use within the tiered storage layer.,,,,,,,,,,,,,,,
"Enhancing clustering to adapt dynamically to changing data access patterns over time.""","Future research should focus on generalizing optimizations for other database objects like indexes and row-organized tables, improving scalability for efficient resource use in tiered storage, and enhancing clustering to adapt to changing data access patterns over time.",,,"The objectives of the study are to evaluate the performance difference with traditional storage, determine the optimal object storage block size, compare columnar vs PAX clustering for data pages, and assess the effectiveness of each discussed optimization throughout the paper.",,,,,,,,,,,
Graphusion: A RAG Framework for Scientific Knowledge Graph Construction with a Global Perspective,"Yang Rui, Yang Boming, Zhao Xinjie, Gao Fan, Feng Aosong, Ouyang Sixun, Blum Moritz, She Tianwei, Jiang Yuang, Lecue Freddy, Lu Jinghui, Li Irene",2025,reference-manager,10.1145/3701716.3717821,,"Implementation Insights are as follows: The Graphusion pipeline, which constructs knowledge graphs (KGs), outperforms baselines like GPT-4o and GraphRAG across multiple tasks, especially in tasks requiring global understanding. It generates more relevant and convincing answers, introduces expanded entities, and generalizes well to new domains like Japanese medical data.",,,,Entity extraction: Both GraphRAG and Graphusion extract entities from text to build a knowledge graph.,,"What is the primary relationship between learning {entity\_1} and understanding {entity\_2} in the domain of {domain}, and is knowledge of {entity\_1} a necessary prerequisite for comprehending {entity\_2}?","The paper investigates improving knowledge graph construction in natural language processing using the Graphusion method. Through comparative experiments and expert evaluations, Graphusion outperforms baselines like GPT-4o and GraphRAG in accuracy, semantic alignment, and expert-rated criteria, especially in complex educational tasks. The study concludes Graphusion enhances factuality and persuasiveness.","The paper's main objective is to improve knowledge graph construction in NLP by introducing the Graphusion framework, which fuses local and global knowledge graphs; the key method is knowledge fusion, and the principal finding is that Graphusion outperforms baselines in accuracy, similarity, hit rate, and expert evaluation across multiple tasks.","Basic NLP Foundations, Text Readability Metrics, Corpora and Datasets, Machine Learning, Word distributions, Attention models, RoBERTa, ELMo, Transformer, Reading comprehension, GraphRAG, Knowledge Fusion, Seed Extraction, KG Aggregation, Local KG Construction, Generation, Retrieval-Augmented Generation, Chain of Thought Prompting, Semantic parsing, Relation extraction."
Path searching: Identifies a sequence of intermediary entities to connect known and target concepts.,,,,,,,,,,,,,,,
"Sub-graph completion: Expands the knowledge graph by finding hidden associations between entities in a sub-graph.""",,"The Graphusion pipeline outperformed baselines on TutorQA Tasks 1-5, achieving scores of 92.00 (T1), 80.29 (T2), 77.85 (T3), 50.00 (T4), and 15.65 (T5), compared to GPT-4o zs, GPT-4o RAG, and GraphRAG.",,,,,,,,,,,,,
Expert evaluation on Task 6 showed Graphusion had the highest scores in Relevancy (4.85),Coverage (4.91),Convincity (4.72),and Factuality (4.77). Inter-annotator agreement (Kappa) was 0.67,indicating substantial agreement.,,,,,,,,,,,
The study concludes that Graphusion’s core fusion step and well-defined seed concept generation are critical for superior performance,"especially in complex QA tasks; statistical significance is not explicitly reported.""","On Tasks 1-5, """"Ours"""" outperformed others: T1: 92.00, T2: 80.29, T3: 77.85, T4: 50.00, T5: 15.65.",,,,,,,,,,,,,
"Expert evaluation (Task 6): """"""""Ours"""""""" scored highest in Relevancy (4.85)",Coverage (4.91),Convincity (4.72),Factuality (4.77).,,,,,,,,,,,,
"Graphusion produced more comprehensive and relevant entity lists than baselines.
In ablation",zero-shot prompting outperformed CoT (Chain-of-Thought) prompting for entity graph recovery.,,,,,,,,,,,,,,
GPT-4o generated more,but often less relevant,"entities per response (Task 2: 11.04 vs. 2.84; Task 3: 11.54 vs. 2.87).""",Comprehensive evaluation on Japanese medical data was not conducted due to significant human effort required; left for future work.,,,,,,,,,,,,
Incorporating lengthy in-domain documents (LectureBankCD) as external data diminished performance due to noise and processing challenges.,,,,,,,,,,,,,,,
Using neighboring entities for improvement relies on training data,"which is incompatible with zero-shot settings.""","The Graphusion pipeline outperforms GPT-4o and GraphRAG across all tasks, with higher accuracy (e.g., T1: 92.00 vs. 69.20) and expert-rated metrics (Relevancy: 4.85, Factuality: 4.77).",,,,,,,,,,,,,
Graphusion generates more relevant and precise entities,avoiding overly broad or irrelevant terms.,,,,,,,,,,,,,,
The core fusion step and well-defined seed entity generation are critical for improved performance,especially in complex QA.,,,,,,,,,,,,,,
Recommendation: Future work should include comprehensive evaluation on larger datasets,"as current expert evaluation is limited by human effort.""",Need for improved methods to aggregate and fuse knowledge from multiple sources for Knowledge Graph Construction (KGC).,,,,,,,,,,,,,
Challenges in evaluating entity quality,especially avoiding out-of-domain nodes in generated knowledge graphs.,,,,,,,,,,,,,,
Necessity for large-scale,high-quality,"domain-specific corpora to enhance scientific KGC performance.""",,No information available,,,,,,,,,,,
Privacy-Preserving Graph Machine Learning from Data to Computation: A Survey,"Fu Dongqi, Bao Wenxuan, Maciejewski Ross, Tong Hanghang, He Jingrui",2023,reference-manager,,,"The paper focuses on passive methods for node identity disclosure and link re-identification, using probabilistic models like noisy-or. It highlights challenges in disentangling task-relevant from irrelevant information for better privacy-utility trade-off. General privacy mechanisms discussed include graph summarization and switching-based graph generation to anonymize graphs while preserving utility.",,,,"Graph Summarization: Partitions the original graph into clusters, then represents each cluster as a node in the anonymized graph, varying edge connections to enhance privacy.",,"How can passive privacy attacks, specifically node identity disclosure and sensitive link re-identification, be achieved on published graph data, and what background knowledge and mechanisms are involved in both attacking and defending against these threats?","This paper reviews privacy-preserving techniques in graph machine learning, focusing on both data and computational aspects. It examines methods like graph summarization and switching-based graph generation to anonymize graphs, discusses privacy risks, and highlights the need for unified, secure systems and future research directions in privacy-preserving graph learning.","The research goal is to review privacy-preserving techniques in graph machine learning; the approach systematically surveys methods for generating and transmitting privacy-preserving graph data; the principal finding highlights current challenges and envisions a unified, secure graph machine learning system.",
Switching-based Graph Generation: Uses iterative Monte Carlo switching of edges to anonymize the graph while preserving key structural features.,,,,,,,,,,,,,,,
"Differential Privacy Schema: Applies privacy-preserving mechanisms not tailored to specific attackers but for general scenarios.""",,"The paper reviews privacy-preserving techniques in graph machine learning, focusing on both data generation (protecting sensitive information in shared graph data) and computation (enabling secure multi-party computation without sharing raw data).",,,,,,,,,,,,,
It systematically analyzes attacker models (active and passive) and corresponding defense mechanisms,including randomized and differential privacy (DP)-based methods.,,,,,,,,,,,,,,
The paper highlights current limitations,discusses challenges like non-IIDness in federated learning,"and proposes future research directions; no quantitative results or p-values are reported.""","Graph Summarization: Produces anonymized graphs by partitioning the original graph into clusters, each serving as a node in the new graph. The connection strategy varies, resulting in different anonymized graphs.",,,,,,,,,,,,
Switching-based Graph Generation: Uses iterative Monte Carlo switching of edges to anonymize the graph while preserving key features (e.g.,eigenvalues,eigenvectors,harmonic mean of geodesic path,graph transitivity).,,,,,,,,,,,
Both methods aim to protect against node identity disclosure and link re-identification while maintaining utility.,,,,,,,,,,,,,,,
"No explicit statistical values or quantitative results are provided.""","Model heterogeneity: Assumes shared model architecture across clients, which may not be optimal due to different graph sizes and over-smoothing in GNNs.",,,,,,,,,,,,,,
Privacy-preserving graph generation: Can reduce model utility since techniques are not tailored for specific machine learning tasks.,,,,,,,,,,,,,,,
Privacy-preserving computation: Transmitting model parameters or gradients may still leak private data; hidden representations can also leak information.,,,,,,,,,,,,,,,
Cross-client transmission: Subgraph-level FL often requires direct information exchange,raising privacy concerns.,,,,,,,,,,,,,,
Challenges with complex structures: Difficulties in privacy-preserving graph data generation for time-evolving and heterogeneous graphs.,,,,,,,,,,,,,,,
"Need to combine privacy-preserving data generation and computation introduces new challenges.""","The study reviews privacy-preserving techniques for graph machine learning, focusing on both data protection and computation.",,,,,,,,,,,,,,
It highlights methods to defend against forceful attackers and discusses federated learning challenges,especially non-IID data.,,,,,,,,,,,,,,
Current limitations are identified,"and promising future research directions are proposed.""","Combining privacy-preserving data generation and computation poses challenges, especially in maintaining model utility while protecting privacy.",,,,,,,,,,,,,
Disentangling task-relevant and task-irrelevant information in model parameters is needed for better privacy-utility trade-offs.,,,,,,,,,,,,,,,
"Avoiding cross-client transmission in subgraph-level federated learning without degrading model performance remains an open problem.""","Suggested future research directions include: (1) optimizing the distribution of the privacy budget between data generation and computation to improve the privacy-utility trade-off, and (2) disentangling task-relevant from task-irrelevant information to better allocate privacy protection and enhance model performance.",No information available,,"The objectives of the study are to protect published graph data from privacy attacks, specifically: (1) preventing node identity disclosure (revealing the identity or properties of nodes), and (2) preventing link re-identification (inferring sensitive relationships between nodes), especially against passive attackers using background knowledge.",,,,,,,,,,,
PKG API: A Tool for Personal Knowledge Graph Management,"Bernard Nolwenn, Kostric Ivica, Łajewska Weronika, Balog Krisztian, Galusčáková Petra, Setty Vinay, Skjæveland Martin G.",2024,reference-manager,10.1145/3589335.3651247,,"The implementation introduces a user-friendly Personal Knowledge Graph (PKG) API and client, enabling users to manage personal data using natural language. It uses large language models for intent classification, SPO-triple extraction, and preference detection, addressing complexity issues seen in prior systems like Solid. This approach enhances accessibility and practical usability.",,,,"Developed a robust internal data representation for Personal Knowledge Graphs (PKGs), paired with an API and a user-friendly PKG Client.",,How can a user-friendly system be designed to translate natural language statements into structured operations for managing a Personal Knowledge Graph (PKG) using an API and web interface?,"The paper aims to create a simple, user-friendly system for managing Personal Knowledge Graphs (PKGs). Using a web-based PKG Client and an API, users can interact with their PKG through natural language. The main finding is that this approach makes PKGs more accessible, supporting intuitive user interaction and broader applications.","The research goal is to make personal knowledge graphs (PKGs) more user-friendly by enabling natural language interaction; the approach uses large language models to translate user statements into PKG operations, and the principal finding is that their open-source demo demonstrates the practical viability of this intuitive, user-centric method.",
Enabled direct user interaction with PKGs through natural language statements,translating them into structured queries.,,,,,,,,,,,,,,
"Demonstrated the approach with an open-source demo focusing on understanding and representing user preferences.""","The research is reproducible. The complete solution, including source code and a video demonstration, is available at https://github.com/iai-group/pkg-api.","The proposed PKG API enables user-friendly management of personal knowledge graphs (PKGs) through natural language, using a two-stage approach: intent classification/SPO-triple extraction with large language models (LLMs), and entity linking for normalization.",,,,,,,,,,,,,
The system supports operations like ADD,GET,DELETE,and UNKNOWN,with preferences encoded as +1 (like) or -1 (dislike).,,,,,,,,,,,
No quantitative results,statistical significance,"or p-values are reported in the provided context.""","The primary outcome is a user-friendly PKG (Personal Knowledge Graph) management solution, including a web-based PKG Client and PKG API.",,,,,,,,,,,,
The system enables users to interact with their PKG using natural language statements.,,,,,,,,,,,,,,,
The approach demonstrates viability for representing and understanding user preferences in PKGs.,,,,,,,,,,,,,,,
"No statistical values or quantitative measured effects are reported.""",Practical PKG implementations with user-friendly interfaces are scarce.,,,,,,,,,,,,,,
Existing tools are often too complex for non-expert users.,,,,,,,,,,,,,,,
Solid Pods introduce complexity and require a learning curve,making them challenging for ordinary users.,,,,,,,,,,,,,,
"Compatibility issues between Pod providers and Solid apps lead to inconsistent user experiences.""","Personal knowledge graphs (PKGs) can effectively organize and provide personal information, addressing the growing need for user-centric management tools.",,,,,,,,,,,,,,
The study introduces a robust internal data representation,an API,and a user-friendly PKG Client,enabling natural language interaction.,,,,,,,,,,,,
The open-source demo demonstrates the feasibility of intuitive,"user-centric PKGs and encourages further research into broader applications.""",Lack of practical implementations of personal knowledge graphs (PKGs) that directly interface with users; most efforts remain conceptual.,,,,,,,,,,,,,
Existing tools are too complex for non-expert users,highlighting the need for more intuitive,user-friendly interfaces.,,,,,,,,,,,,,
"Future research should explore intuitive user-centric interaction methods and broader applications for PKGs.""",,,,"The objectives of the study are to provide a simple and user-friendly way to manage a PKG using a web interface (PKG Client) and PKG API, enabling natural language interactions for storing, retrieving, and managing statements and preferences in a structured, accessible manner.",,,,,,,,,,,
Knowledge Management System with NLP-Assisted Annotations: A Brief Survey and Outlook,Lin Baihan,2022,reference-manager,,,"The implementation uses relational databases for storage and NLP for annotation, enabling semantic similarity analysis, topic modeling, and text summarization. Insights are automatically generated and annotated, supporting evidence-based decision-making, collaboration, and investigation. New insight: The system creates a hierarchical knowledge graph for efficient resource discovery.",,,,"Topic modeling: A statistical technique to uncover abstract topics in document collections, often using models like Neural Variational Document Model (NVDM), Gaussian softmax construction (GSM), and Embedded Topic Model (ETM).",,"How can a unified knowledge management system using relational databases, topic modeling, text summarization, and symbolic reasoning improve the organization, retrieval, and generation of insights from academic papers and related research data?","The paper surveys existing knowledge management systems (KMS), highlighting challenges in organizing multi-faceted research insights. It proposes a unified framework using relational databases to log hierarchical, connected concepts, enabling improved summarization, topic modeling, and evidence-based decision-making. The approach aims to enhance research, writing, and organizational processes.","The paper's main objective is to address limitations in current knowledge management systems by proposing a unified framework using relational databases and NLP-assisted annotations; the key method is integrating hierarchical logging and AI-driven topic modeling; the principal finding is improved note-taking, brainstorming, and multi-directional relationship management.","Tags: Knowledge management systems, text summarization, topic modeling, symbolic reasoning, semantic similarity, relational databases, NLP-assisted annotation, insight annotation, user interfaces, inventories, annotations, embeddings, connected graph."
Text summarization: Automatic generation of concise summaries from large text databases using extractive and abstraction approaches.,,,,,,,,,,,,,,,
"Case studies: Use of real-world scenarios to clarify problem settings and evaluate knowledge management system applications.""",,"The paper proposes a unified bidirectional knowledge management system (BKMS) using relational databases to log hierarchical and interconnected information, improving research, writing, and evidence-based decision-making.",,,,,,,,,,,,,
It highlights the effectiveness of topic modeling and automatic text summarization for generating concise,interpretable summaries and topic tags from large document collections.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the context provided.""","No explicit primary outcomes, results, or measured effects (including statistical values) are reported in the provided context.",Existing knowledge management systems have limitations in categorizing and organizing multi-faceted insights or relationships.,,,,,,,,,,,,,
Traditional databases are often disjoint from logging systems,limiting concise,collated overviews.,,,,,,,,,,,,,
"Additional future research challenges exist for integrating relational databases and AI/symbolic techniques in knowledge management systems.""","Automatic text summarization and topic modeling enhance knowledge management by generating concise, interpretable summaries and uncovering hidden topics.",,,,,,,,,,,,,,
Interconnected knowledge management systems,especially with IoT,foster open,collaborative ecosystems and increase innovation capacity.,,,,,,,,,,,,
Reference architectures and conversational recommendation systems improve efficiency and decision-making in smart warehouses and other domains.,,,,,,,,,,,,,,,
Future research should address security,collaboration,integration,"and thorough design processes for effective AI-driven knowledge management systems.""","Integration of relational databases with AI and symbolic techniques in knowledge management systems remains a challenge, especially for handling multi-faceted, bidirectional relationships.",,,,,,,,,,,
Improving automatic annotation and curation of topic tags using text data mining and NLP techniques is needed for better insight generation.,,,,,,,,,,,,,,,
Enhancing system interoperability,including import/export with other knowledge management systems,and supporting collaborative use by multiple stakeholders,"requires further research.""","Future research should address secure encryption and storage, collaborative features, integration with other systems, and user customization. Further investigation is needed into market analysis, domain analysis, business process modeling, architecture design, and the application of neuro-symbolic AI and topic modeling in knowledge management systems.",,"Case studies are mentioned as a study design characteristic. No other explicit study design characteristics such as randomized, double-blind, controlled, placebo-controlled, non-controlled, multi-site, retrospective, stratified, crossover design, parallel design, observational study, meta-analysis, or systematic review are stated.",,,,,,,,"The objectives of the study are to evaluate secure encryption and storage, enable collaborative use, gain insights or investigations from the system, allow import/export with other knowledge management systems, and address practical design considerations such as storage, user interface, organizational benefits, latency, customization, and security.",
A systematic literature review of knowledge graph construction and application in education,"Abu-Salih Bilal, Alotaibi Salihah",2024,reference-manager,10.1016/j.heliyon.2024.e25383,,"The paper provides the first comprehensive overview of Knowledge Graph (KG) construction and application in education, analyzing methodologies, strengths, and weaknesses. It highlights research gaps, such as data quality, scalability, biases, and evaluation challenges. New insights include the growing trend in educational KG research and the need for improved frameworks and evaluation metrics.",,,,Systematic Literature Review (SLR): A structured review of research articles focused on KG (Knowledge Graph) construction in education.,,"What are the recent methodologies for constructing knowledge graphs in education, what are their strengths and limitations, and how do these approaches impact key educational domains such as adaptive learning, curriculum design, concept mapping, semantic search, and question answering?","This paper presents the first comprehensive systematic literature review (SLR) of knowledge graph (KG) construction and application in education. Using the PRISMA model, 120 studies were analyzed across five domains. The study highlights KG methodologies, strengths, weaknesses, and research gaps, offering recommendations for future work.","The research goal is to systematically review recent knowledge graph (KG) construction methods in education, using a PRISMA-based approach, and the principal finding is a comprehensive analysis of current methodologies, their strengths and weaknesses, and identification of research gaps for future exploration.",
PRISMA Framework: A standardized process (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) used to guide article selection and review.,,,,,,,,,,,,,,,
Database Search: Comprehensive searches in databases like Elsevier,ACM Digital Library,MDPI,IEEE Xplore,"and Google Scholar using targeted keywords.""",,,,,,,,,,,"This systematic literature review (SLR) is the first comprehensive overview of knowledge graph (KG) construction and application in education, analyzing 120 studies."
The review provides a balanced assessment of KG construction methodologies,highlighting strengths,weaknesses,and limitations such as scalability,generalizability,,and evaluation challenges.,,,,,,,,,
"No explicit quantitative results or statistical significance (p-values) are reported in the context provided.""",The primary outcomes focus on a comprehensive overview of Knowledge Graph (KG) construction and application in education.,,,,,,,,,,,,,,
Results include an in-depth analysis of KG construction methodologies,their strengths,weaknesses,and limitations.,,,,,,,,,,,,
Measured effects involve evaluation criteria such as precision,recall,F1-score,qualitative analysis,clarity,,accessibility,,,,,,,,,and comprehensiveness.
"No explicit statistical values are provided.""","Lack of transparency in technique documentation, hindering replication and progress.",,,,,,,,,,,,,,
Insufficient attention to context-specific knowledge extraction,limiting adaptability to diverse educational scenarios.,,,,,,,,,,,,,,
Absence of standardized formats or ontologies for educational KGs,making sharing and integration difficult.,,,,,,,,,,,,,,
"Wide variability in KG structure due to the diversity of educational domains.""",This study provides the first comprehensive overview of knowledge graph (KG) construction and application in education.,,,,,,,,,,,,,,
It offers an in-depth,balanced analysis of state-of-the-art KG construction methods,highlighting their strengths and weaknesses.,,,,,,,,,,,,,
The study identifies current limitations and gaps,"recommending further research to address these inadequacies.""","Limited discussion and documentation of knowledge extraction techniques, making replication and adaptation difficult.",,,,,,,,,,,,,
Lack of standardization in formats or ontologies for educational Knowledge Graphs (KGs),hindering sharing and integration.,,,,,,,,,,,,,,
"Need for context-aware and transparent knowledge extraction methods tailored to diverse educational scenarios.""","Future research should explore personalized learning paths using artificial intelligence, integration of large language models into knowledge graphs (KGs), cross-domain KGs for interdisciplinary learning, real-time updates, and recommendation systems for lifelong learning. Addressing data quality, scalability, bias, and accurate modeling of student preferences remains essential.","Study design: Systematic literature review (SLR), comprehensive, follows the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) framework, covers articles from 2019–2023, includes studies from multiple databases, limited to English-language articles, and provides balanced assessment of methodologies.",,No information available,,,,,,,,,,,
Driving Digital Engineering Integration and Interoperability Through Semantic Integration of Models with Ontologies,"Dunbar Daniel, Hagedorn Thomas, Blackburn Mark, Dzielski John, Hespelt Steven, Kruse Benjamin, Verma Dinesh, Yu Zhongyuan",2023,reference-manager,,,"The DEFII framework uses MISD and Specified Model Interface to enable tool interoperability and tool-agnostic data access. It standardizes data mapping, supports flexible analysis, and allows middleware for tool-specific needs. Limitations include manual connections and recursive patterns; future research should address complex analysis patterns and semantic mapping.",,,,"Mapping SysML system models to ontology-aligned, tool-agnostic representations stored in a triple store (GraphDB).",,"How does the DEFII framework use ontology-aligned data and standardized interfaces to address integration and interoperability challenges in Digital Engineering, particularly in enabling tool interoperability and automated reasoning across heterogeneous engineering tools and data sources?","The paper aims to demonstrate how the Model Interface Specification Diagram (MISD) enables tool-agnostic, ontology-aligned data exchange for cyber vulnerability analysis. Using SysML and MATLAB, the study maps system models to ontologies, calculates CVSS scores, and shows improved tool interoperability. The DEFII framework supports flexible, reusable data integration.","The research goal is to address integration and interoperability in Digital Engineering by using ontology-aligned data; the approach employs the DEFII framework with the Model Interface Specification Diagram (MISD); results show tool-agnostic, flexible data access and successful CVSS analysis via MATLAB, confirming improved interoperability.",
Using REST API endpoints and SPARQL queries for flexible,tool-agnostic access and analysis of data.,,,,,,,,,,,,,,
"Deploying MATLAB analysis programs to compute and update CVSS scores via the Specified Model Interface.""","The research demonstrates reproducibility by using a REST API endpoint and the Specified Model Interface, allowing access via multiple tools (MATLAB, web dashboard, Python, Java). However, no explicit source code for the project is provided in the context.","The DEFII framework met all three success criteria: mapping data to ontology-aligned stores, flexible tool-agnostic data access, and enabling semantic data transformation.",,,,,,,,,,,,,
The system-wide CVSS (Common Vulnerability Scoring System) score generated was 8.2.,,,,,,,,,,,,,,,
"Automated reasoning inferred 36
19","674 statements from 19
720 explicit and 36","720 explicit ones (expansion ratio: 2.86); no p-values or statistical significance reported.""
674 inferred statements were produced","Primary outcome: The system-wide CVSS (Common Vulnerability Scoring System) score generated was 8.2.
with an expansion ratio of 2.86.",,,,,,,,,,,,
All three framework success criteria were met: data mapping,flexible tool-agnostic access,"and semantic transformation.""","Many elements in the MISD were manually connected, potentially simplifying the interface instantiation.",,,,,,,,,,,,
The analysis used a roll-up pattern,which may not generalize; future research is needed to address recursive analysis in interface specification.,,,,,,,,,,,,,,
"Functional analysis of the three success criteria is still required to fully validate the DEFII framework.""","The DEFII framework enables mapping engineering data to an ontology-aligned data store and supports flexible, tool-agnostic access.",,,,,,,,,,,,,,
Tool interoperability is promoted,allowing various tools (e.g.,MATLAB,Python) to perform analyses using the same interface.,,,,,,,,,,,,
"Future research should address recursive analysis patterns and further simplify interface instantiation.""","Determining how to account for recursive, roll-up analysis patterns (e.g., weight and cost) in the interface specification.",,,,,,,,,,,,,,
Addressing increased complexity of tool interoperability as analyses become more complicated.,,,,,,,,,,,,,,,
Extending the DEFII framework to support additional interface instantiations and data formats beyond REST APIs,"such as CSV files.""","Future research should determine how to account for recursive analysis patterns (like roll-up patterns) in the interface specification. There is also a need to explore integrating multiple MISDs for broader Assessment Flow Diagrams and to investigate more complex applications, such as using SHACL for verification and validation.",,,,"The objectives are to perform a functional analysis of three success criteria to determine if the DEFII framework fulfills its purpose: 1) enable mapping from engineering models to ontology-aligned data, 2) allow flexible, tool-agnostic data access, and 3) support reasoning and inference over the data.",,,,,,,,,
"Toward the Tradeoffs between Privacy, Fairness and Utility in Federated Learning","Sun Kangkang, Zhang Xiaojin, Lin Xi, Li Gaolei, Wang Jing, Li Jianhua",2023,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
"The paper proposes a federated learning (FL) method that balances privacy and fairness using differential privacy. The approach involves fairness training followed by privacy-protection training. Experiments show that increasing privacy can harm fairness but may improve accuracy. The method outperforms FedAvg in privacy and fairness on the Adult dataset.
New Insights:",,,,,,,,,,,,,,,
The study reveals a trade-off: enhancing privacy can reduce fairness,"highlighting the challenge of optimizing both simultaneously in FL. The proposed private fair algorithms address this by integrating both constraints into the optimization process.""",,"The research goal is to design a federated learning method balancing privacy and fairness; the approach combines fairness training and privacy-protection using differential privacy; the principal finding is that increasing privacy can harm fairness but improve accuracy, highlighting a tradeoff between privacy, fairness, and utility.","The paper proposes a privacy-protection fairness Federated Learning (FL) method that balances privacy, fairness, and utility using differential privacy. The method involves fairness and privacy-protection training. Experiments on the Adult dataset show increased privacy can harm fairness, but the proposed algorithms improve accuracy while ensuring both privacy and fairness.",,,,,,,,,,"Use of three fairness metrics: Demographic Parity (DemP), Equalized Odds (EO), and Disparate Impact (DI) to evaluate fairness.","What is the intrinsic relationship between privacy and fairness in federated learning, and how can a privacy-protection method be designed to ensure both model privacy and fairness while improving learning performance?"
Implementation of a privacy-protection fairness Federated Learning (FL) method,combining fairness training and privacy-protection training using differential privacy.,,,,,,,,,,,,,,
"Experimental comparison with benchmark FedAvg algorithms on the Adult dataset to assess performance.""",,The proposed privacy-protection fairness Federated Learning (FL) method increases model accuracy but reduces fairness when privacy is added.,,,,,,,,,,,,,
Experiments on the Adult dataset show a trade-off between privacy,fairness (Demographic Parity,Equalized Odds),and accuracy.,,,,,,,,,,,,
"No explicit p-values or detailed quantitative results are provided in the context.""",Primary outcomes:,,,,,,,,,,,,,,
Proposed a privacy-protection fairness Federated Learning (FL) method combining fairness and privacy constraints.,,,,,,,,,,,,,,,
Evaluated on the Adult dataset using race as the sensitive attribute.,,,,,,,,,,,,,,,
"Results and measured effects:
Without privacy
With privacy (σ = 1)","EO Error and DemP Error converge to zero
EO Error and DemP Error increase","indicating high fairness.
showing reduced fairness.",,,,,,,,,,,,,
Adding privacy increases model accuracy but decreases fairness.,,,,,,,,,,,,,,,
"The proposed algorithm effectively guarantees model privacy in fair FL compared to FedAvg benchmarks.
Statistical values:",,,,,,,,,,,,,,,
No explicit numerical values for EO Error or DemP Error are provided.,,,,,,,,,,,,,,,
Client accuracy examples (no privacy): Black: 32.20%–69.42%,White: 12.26%–88.39%.,,,,,,,,,,,,,,
Client accuracy examples (with privacy): Black: 66.63%–73.75%,"White: 85.70%–88.39%.""","Adding privacy (e.g., differential privacy) decreases fairness, especially harming underrepresented subgroups.",,,,,,,,,,,,,
Stronger privacy protections or small populations cause significant differences in model outputs.,,,,,,,,,,,,,,,
Trade-offs exist between privacy,fairness,and accuracy; improving one may harm the others.,,,,,,,,,,,,,
Cryptographic privacy methods are computationally expensive.,,,,,,,,,,,,,,,
"Limited research on privacy protection for fair federated learning.""",The proposed method effectively protects client model privacy in Federated Learning (FL) while sharing model parameters.,,,,,,,,,,,,,,
Increasing privacy (adding noise) improves model accuracy but reduces fairness between sensitive groups.,,,,,,,,,,,,,,,
The method outperforms benchmark FedAvg algorithms in privacy protection.,,,,,,,,,,,,,,,
"Recommendation: Balance privacy and fairness constraints to optimize both accuracy and fairness.""",Training a separate fairness model for each client in Federated Learning (FL) remains an open problem.,,,,,,,,,,,,,,
There is very little research on privacy protection specifically for fair federated learning.,,,,,,,,,,,,,,,
The intrinsic connection and trade-offs between fairness,privacy,"and utility in FL need further exploration.""","There is very little research on privacy protection for fair federated learning. Future research should explore methods that jointly address privacy and fairness, investigate the intrinsic relationship and trade-offs between them, and develop approaches that ensure both privacy and fairness without sacrificing model accuracy.",,,,,,,,,,,,"The objectives of the study are to propose a privacy-protection fairness Federated Learning (FL) method that protects client model privacy while sharing parameters, balances privacy and fairness using private fair algorithms, and demonstrates effectiveness compared to benchmark algorithms using Adult datasets."
Ubiquitous knowledge empowers the Smart Factory: The impacts of a Service-oriented Digital Twin on enterprises' performance,"Longo F., Nicoletti L., Padovano A.",2019,reference-manager,10.1016/j.arcontrol.2019.01.001,,"The implementation required strong collaboration and structured knowledge. A Service-oriented Digital Twin prototype was deployed in two enterprises, leading to significant reductions in setup and cycle times (e.g., 28.62% decrease in setup time, 44.5% in the first step). Fisher’s LSD test confirmed statistical significance. Human-centric knowledge delivery was key.",,,,"Deployment of an application prototype in two industrial manufacturing case studies (one large, one small enterprise) to evaluate the hypothesis.",,"How can a human-centric manufacturing paradigm, enabled by a Service-oriented Digital Twin delivering ubiquitous knowledge to manufacturing employees, be integrated within Industry 4.0 to improve production and business performance in smart factories?","The paper aims to shift Industry 4.0 from a technology-focused to a human-centric paradigm by introducing the Industrial Internet pyramid and a Service-oriented Digital Twin. Using two test-beds, the study shows that delivering ubiquitous manufacturing knowledge to employees improves time, costs, and process quality, validating this approach.","The paper's main objective is to propose a human-centric Smart Factory model using a Service-oriented Digital Twin; the key method is developing and testing a prototype leveraging augmented reality and vocal interaction, and the principal finding is significant improvements in time, costs, and process quality for enterprises.",Industry 4.0; Human-centric Industrial Internet; Smart Factory; Ubiquitous Knowledge; Service-oriented Digital Twin
Installation and integration of hardware and software (central server with Apache HTTP Server,Apache SOLR,MySQL database,Android devices) for knowledge access and monitoring.,,,,,,,,,,,,
"Use of Fisher’s LSD (Least Significant Difference) statistical analysis to assess production waste rates before and after application use.""",,"Statistically significant reductions in batch cycle times, setup times, and production waste rates were observed after using the application prototype (all adjusted p-values = 0.000; LSD differences exceeded thresholds).",,,,,,,,,,,,,
Maintenance Work Plan generation time decreased from 83.34 to 6.25 minutes (Fin Tube Machine) and from 49.36 to 4.49 minutes (Milling Machine).,,,,,,,,,,,,,,,
Estimated annual cost savings from automated MWP generation are €5,"081.77 per machine.""","Significant reduction in setup times: e.g., first step setup time decreased by 44.5% (from 18.5 min to 10.3 min).",,,,,,,,,,,,,
Cycle times reduced: e.g.,total cycle time decreased by 13.3% (from 240.4 min to 223.1 min).,,,,,,,,,,,,,,
Production waste mean rate reduced (statistically significant).,,,,,,,,,,,,,,,
All improvements confirmed statistically (LSD test,p-value = 0.000).,,,,,,,,,,,,,,
Estimated annual cost savings: 5,"081.77 €/year.""",Lack of clear implementation guidelines and structured information in literature.,,,,,,,,,,,,,
Difficulty in enabling and convincing enterprises and operators to adopt new systems.,,,,,,,,,,,,,,,
Small and medium enterprises are hesitant due to limited budgets and need for quick ROI.,,,,,,,,,,,,,,,
No evidence of real business model transformation in SMEs.,,,,,,,,,,,,,,,
Technological changes can be easily imitated,limiting long-term competitive advantage.,,,,,,,,,,,,,,
Lack of fully coupled human-machine interaction and cyber-human convergence.,,,,,,,,,,,,,,,
Industrial workers need new competencies to work with advanced systems.,,,,,,,,,,,,,,,
"Implementation required strong collaborative effort and was not instantaneous.""","The Service-oriented Digital Twin prototype significantly reduced setup times, cycle times, and production waste in both large and small enterprises.",,,,,,,,,,,,,,
Improvements were statistically significant,confirming enhanced performance in time,cost,and quality.,,,,,,,,,,,,
Benefits were achieved through better knowledge access for operators.,,,,,,,,,,,,,,,
Recommendation: Promote human-centric,"information-rich Industry 4.0 implementations.""",Lack of clear implementation guidelines and structured information for transforming to a human-centric Smart Factory.,,,,,,,,,,,,,
Insufficient research on how Digital Twin knowledge can be accessed and used by manufacturing employees.,,,,,,,,,,,,,,,
"Limited evidence of real business model transformation in small and medium enterprises adopting Industry 4.0.""","Future research should extend the intelligent vocal assistant's capabilities, investigate the statistical link between knowledge management and enterprise performance, and predict the time needed for significant return on investment. Further exploration of knowledge-driven business opportunities and organization-wide transformational change is also recommended.",,,"The study aims to extend Industry 4.0 from a technology-driven to a human-centric paradigm by integrating manufacturing employees with the Cyber-Physical Production System (CPPS) through a Service-oriented Digital Twin, delivering ubiquitous knowledge to improve production and business performance.",,,,,,,,,,,
Sequential Recommendation with Graph Neural Networks,"Chang Jianxin, Gao Chen, Zheng Yu, Hui Yiqun, Niu Yanan, Song Yang, Jin Depeng, Li Yong",2021,reference-manager,10.1145/3404835.3462968,,Implementation Insights Summary:,,,,,,,,,
The paper shows that fusing weak signals via graph convolution strengthens core and target interests,improving performance. Dynamic graph pooling with assignment regularization and weighted readout filters noise and compresses user interests. The framework is flexible for different prediction layers. New insight: even advanced sequential models face short-term memory limits,"justifying the graph-based approach.""",,"The research goal is to improve sequential recommendation by extracting and fusing user interest signals using graph convolution and dynamic graph pooling; the approach constructs an interest graph, applies attentive graph convolution, and dynamic pooling, and results show that these designs significantly enhance recommendation performance by strengthening and filtering user interests.",,"The paper investigates improving sequential recommendation by modeling user interests as weak signals and enhancing them through interest fusion and extraction using graph convolution and pooling strategies. Experiments on two datasets show these methods improve recommendation performance, especially for long behavior sequences. The approach better captures dynamic user preferences than previous methods.",,,,,,,"Interest Graph Construction: Loose item sequences are reconstructed into item-item interest graphs using metric learning, explicitly integrating and distinguishing different user preferences.","What is the effect of different components—interest fusion, interest extraction, and interest evolution—within the proposed sequential recommendation framework on improving recommendation performance?",
Message Passing on Interest Graph: Message passing (graph convolution) is used to merge weak signals into strong signals,enhancing core and target interests.,,,,,,,,,,,,,,
"Interest Extraction via Graph Pooling: Graph pooling with assignment regularization and weighted readout filters noise and compresses user interests for improved recommendation performance.""","The research is reproducible. The source code is implemented using the Microsoft Recommenders framework (https://github.com/microsoft/recommenders) based on TensorFlow (https://www.tensorflow.org). Hyper-parameter settings and dataset details are provided, supporting reproducibility. No specific project code repository is mentioned beyond the framework link.","The proposed SURGE model significantly outperforms all baselines, improving AUC by 0.03 (p-value < 0.001) on Taobao and 0.04 (p-value < 0.001) on Kuaishou datasets.",,,,,,,,,,,,,
Interest fusion and extraction components both yield notable performance gains,with full fusion achieving AUC up to 0.8906 and extraction up to 0.8906.,,,,,,,,,,,,,,
Modeling on compressed sequences benefits all sequential models,especially with AUGRU,"confirming the effectiveness and efficiency of the pooling strategy.""",The proposed method (SURGE) achieves the best performance on both Taobao and Kuaishou datasets.,,,,,,,,,,,,
SURGE improves AUC by ~0.03 (p < 0.001) on Taobao and ~0.04 (p < 0.001) on Kuaishou.,,,,,,,,,,,,,,,
Interest fusion (graph convolution) increases AUC from 0.8307 to 0.8906 and MRR from 0.0317 to 0.4228 (Taobao).,,,,,,,,,,,,,,,
Interest extraction (with pooling,regularization,and readout) increases AUC from 0.8513 to 0.8906 and MRR from 0.3605 to 0.4228 (Taobao).,,,,,,,,,,,,,
SURGE outperforms all baselines in AUC,GAUC,MRR,"and NDCG@2 on both datasets (see Table 2 for exact values).""",Difficulty in fully utilizing older behavior-sequences to estimate current interests.,,,,,,,,,,,
Models struggle with short sequences due to data sparsity.,,,,,,,,,,,,,,,
Performance declines with very long sequences because of increased noise.,,,,,,,,,,,,,,,
Challenging division and integration of long-term and short-term interests.,,,,,,,,,,,,,,,
Existing methods focus more on recent behaviors,"often ignoring long-term behaviors.""",The SURGE model outperforms existing sequential recommendation methods on two real-world datasets.,,,,,,,,,,,,,
Interest fusion and extraction components significantly improve performance by strengthening core interests and filtering noise.,,,,,,,,,,,,,,,
Modeling on compressed sequences benefits all sequential models,especially for long user histories.,,,,,,,,,,,,,,
"Assignment regularization and graph readout further enhance recommendation accuracy.""",The need for more effective modeling of long-term user history to handle noise in long interaction sequences.,,,,,,,,,,,,,,
The importance of improving interest extraction methods to better filter irrelevant information and focus on critical user interests.,,,,,,,,,,,,,,,
"Exploring different interest evolution layer designs for better adaptability and performance in sequential recommendation.""
Ablation study",,Study design characteristics:,,,,,,,,,,,,,
Hyper-parameter study,,,,,,,,,,,,,,,
Comparative/controlled (compares different model components and baselines),,,,,,,,,,,,,,,
Uses two real-world datasets,,,,,,,,,,,,,,,
Evaluates sequential recommendation models,,,,,,,,,,,,,,,
No mention of randomization,blinding,placebo,retrospective,crossover,,parallel,"or systematic review.""",,,"The objectives of the study are to evaluate the effectiveness of the proposed method compared to state-of-the-art sequential recommenders (RQ1), assess its ability to handle sequences of various lengths (RQ2), and analyze the effect of different components in the method (RQ3).",,,meta-analysis,observational,multi-site
The resurrection of digital triplet: A cognitive pillar of human-machine integration at the dawn of industry 5.0,"Alimam Hassan, Mazzuto Giovanni, Tozzi Nicola, Emanuele Ciarapica Filippo, Bevilacqua Maurizio",2023,reference-manager,10.1016/j.jksuci.2023.101846,,"Implementation Insights highlight that current industrial AI systems are still preliminary. Integrating AI and machine learning is crucial for advancing digital twins to digital triplets, enabling deeper understanding, interoperability, and human–machine symbiosis. New insights include the importance of human-centric approaches and cognitive augmentation for Industry 5.0.",,,,Bibliometric analysis: Used to retrieve and analyze peer-reviewed articles from databases like Scopus and Web of Science.,,"How does the evolution from digital twins to digital triplet architecture, integrating intelligent activities and human–machine symbiosis, contribute to the advancement of Industry 5.0 and address the challenges and opportunities in intelligent manufacturing and human cyber-physical systems?","The paper aims to clarify the evolution from digital twins to digital triplets within Industry 5.0, focusing on human–machine symbiosis. Using a systematic literature review and bibliometric analysis of 186 papers (2018–2023), it defines key concepts, identifies research gaps, and proposes a hierarchical framework for digital triplets.","The research goal is to review and classify literature on Digital Twin, Cognitive Digital Twin, and Digital Triplet, using bibliometric analysis; the approach involved systematic screening and classification of 186 relevant papers; the principal finding highlights hierarchical levels and enabling technologies for intelligent digital twins in Industry 5.0.","Keywords or tags for this research include: cyber-physical system, cyber-physical, digital twin, e-learning, engineering process, industry 4, industry 5, artificial intelligence, neural network, intelligent activity, knowledge, kaizen, learning factory, production system, deep learning, computer vision, and learning systems."
PRISMA-based scoping review: Applied for systematic selection,screening,and classification of relevant papers.,,,,,,,,,,,,,
VOS viewer software: Utilized for network data mapping,visualizations,"and exploring interconnections among key research elements.""",,"186 papers were rigorously selected and classified for review, focusing on “Digital Twin,” “Digital Triplet,” and “Industry 5.0” from 2018–2023.",,,,,,,,,,,
Bibliometric analysis identified 109 top keywords in 9 clusters for “Industry 5.0” and “Digital Twin,” and 73 keywords in 7 clusters for “Digital Triplet.”,,,,,,,,,,,,,,
"No statistical significance or p-values were reported in the context provided.""",186 papers were identified as relevant after screening.,,,,,,,,,,,,,,
A co-occurrence map of 189 keywords was created,prioritizing the top 109 most frequently used keywords,grouped into nine clusters.,,,,,,,,,,,,,
Overlay visualization identified 34 key items (e.g.,digital triplet,deep learning,neural networks) as new hotspots after 2020.,,,,,,,,,,,,
"No statistical values reported.""","Current industrial AI systems are in preliminary stages, limiting predictive abilities and understanding of digital twins.",,,,,,,,,,,,,,
Challenges exist in accessing substantial datasets for sustained knowledge discovery.,,,,,,,,,,,,,,,
Limited focus on human-centered intrinsic information and integration in cyberspace.,,,,,,,,,,,,,,,
"Need for interdisciplinary collaboration and further research on human–machine integration and spatial cognition.""","The review highlights the emerging importance of the """"digital triplet"""" concept, especially its role in integrating digital twins, humans, and intelligent systems.",,,,,,,,,,,,,,
Key research hotspots include digital triplet,deep learning,artificial intelligence,and human–machine symbiosis.,,,,,,,,,,,,
The study recommends further research to sustain the symbiosis between physical,digital,"and cyberspace through frameworks like industry 5.0.""","Integration of machine learning and artificial intelligence to enhance predictive abilities and understanding of digital twins, especially moving from traditional simulations to perceptive digital triplet levels.",,,,,,,,,,,,
Advancing human-centered integration in digital twins,focusing on real-time,bidirectional interaction and cognitive-based machine learning for knowledge systemizing.,,,,,,,,,,,,,
Developing BCI-enabled digital triplets,"including semantic reasoning of brain signals and real-time synchronization between humans and digital avatars.""","Future research should focus on advancing AI and digital twins beyond preliminary stages, integrating human cognitive mechanisms, enhancing real-time human–machine interaction, developing cognitive-based machine learning, improving spatial cognition, and exploring BCI-enabled platforms for semantic communication and real-time synchronization between humans and digital avatars.","The study design is a scoping review using a PRISMA-based flowchart. It involved systematic literature search, duplicate removal, title and abstract screening, independent classification by multiple authors, bibliometric analysis, and synthesis of 186 relevant peer-reviewed articles from databases like Scopus and Web of Science.",,,"The objectives of the study are to enhance predictive abilities and understanding of digital twins using artificial intelligence, address challenges in accessing substantial datasets, focus on human-centered information, achieve human–machine integration with intelligent digital twins, and improve the cognitive and spatial capabilities of digital triplets.",,,,,,,,,
A PolyStore Architecture Using Knowledge Graphs to Support Queries on Heterogeneous Data Stores,"Souza Renan Francisco Santos, Azevedo Leonardo Guerreiro, Tesolin Julio Cesar Cardoso, Oliveira Anna C., Soares Elton F. de S., Thiago Raphael M., Moreno Marcio Ferreira",2024,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
The paper used the scientific research method,applying Representation Theory to design an architecture (HKPoly) for querying heterogeneous,unconnected business process data. HKPoly simplifies user queries,reducing complexity by half compared to PostgreSQL FDW,with only up to 30% extra processing time. Provenance is managed natively using knowledge graphs,,unlike most big data tools. Future work includes supporting more query types,"The research goal is to enable querying heterogeneous, unconnected business process data; the approach uses Representation Theory to design a polystore architecture with knowledge graphs (HKPoly); results show improved query complexity and provenance management compared to existing systems.","Scientific research method: Defined the problem, reviewed literature, formulated a research question, designed architecture using Representation Theory, implemented and evaluated the solution through experiments and user query complexity analysis.",,,How can heterogeneous and not explicitly connected data generated by business processes be queried using a federated architecture that leverages knowledge graphs and provenance information to support data linkage and reduce query complexity for users and databases?,"The paper aims to address querying heterogeneous, unconnected business process data. Using the scientific research method and Representation Theory, the authors designed a polystore architecture leveraging knowledge graphs. Experiments showed improved query complexity and processing time compared to PostgreSQL FDW. The approach enhances provenance management and interoperability.",No information available,"and ML-enhanced metadata.""",multimedia data
Representation Theory: Used to design and detail the architecture,its components,and user interactions.,,,,,,,,,,,,,
"Experimental evaluation: Ran experiments with varying data volumes to measure processing time and compared query complexity between HKPoly and SQL.""",,"HKPoly reduces user query complexity: HyQL queries had 7 components versus 14 in equivalent SQL queries, indicating lower cognitive load.",,,,,,,,,,,,,
Experimental results showed HKPoly’s feasibility and scalability,but no specific quantitative performance or p-values are reported.,,,,,,,,,,,,,,
HKPoly enables integrated querying of heterogeneous data stores using a knowledge graph-centric approach,"improving query semantics and provenance handling.""",,No explicit limitations or shortcomings are stated in the provided context.,,,,,,,,,,,,
"No self-reported problems or suggestions for further research are mentioned.
,The proposed architecture enables querying heterogeneous",unconnected business process data with reduced user query complexity—over two times less complex than SQL.,,,,,,,,,,,,,,
Query processing time overhead is under 30% compared to PostgreSQL FDW,showing practical applicability.,,,,,,,,,,,,,,
Further improvements are needed to optimize query processing time.,,,,,,,,,,,,,,,
"The architecture is flexible for implementation with various technologies.""","Lack of solutions for querying heterogeneous data generated by unconnected business processes, especially regarding data linkage across diverse data stores.",,,,,,,,,,,,,,
Need for broader approaches that encompass heterogeneous data models,including NoSQL databases,and address data linkage,which current OBDA frameworks do not.,,,,,,,,,,,,
Absence of implementation and experimental evaluation in existing frameworks,"highlighting the need for practical validation and performance analysis.""",,,,,,,,,,,,,,
Preliminary Systemic Model of (Human) Digital Twin,"Naudet Yannick, Stahl Christoph, Gallais Marie",2023,reference-manager,10.1145/3594806.3596596,,"Implementation Insights from the paper highlight the need for a systemic, unified model for Human Digital Twin (HDT), grounded in General Systems Theory. Key considerations include regulation, transparency, flexibility, behavior modeling, and environmental context. New insight: HDT models must integrate both human and technical system perspectives for coherence.",,,,"Literature review: The study conducted a literature review focused exclusively on the """"Human Digital Twin"""" keyword to summarize recent research advances.",,"What is a generic and unifying systemic model and definition of the Human Digital Twin (HDT) concept, distinguishing it from technical Digital Twins and grounded in systems theory, to guide future research across application domains?","The paper aims to establish a generic, systemic model and definition of the Human Digital Twin (HDT), grounded in systems theory. Using a literature review, it distinguishes HDT from technical Digital Twins, proposes a conceptual model, and highlights key aspects like regulation, transparency, adaptability, and human behavior.","The research goal is to establish a generic, systemic model of the Human Digital Twin (HDT) using systems theory; the approach involves a literature review and conceptual modeling; the principal finding is a unifying, systemics-grounded definition and model of HDT as a specific class of Digital Twin.","Digital Twin, Human Model, Industry4.0"
Conceptual modeling: The authors proposed a generic and unifying conceptual model of Human Digital Twin,grounded in General Systems Theory.,,,,,,,,,,,,,,
"Comparative analysis: The study compared technical system Digital Twins and Human Digital Twins to highlight fundamental differences.""",,"The paper proposes a generic, systemic model of Human Digital Twin (HDT), grounded in General Systems Theory, to unify definitions and concepts across disciplines.",,,,,,,,,,,,,
Key findings highlight HDT’s unique challenges and functions compared to technical Digital Twins,especially in healthcare and Industry 4.0.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""","No primary outcomes, results, or measured effects (including statistical values) are explicitly stated in the provided context.","Regulation and ethics: Challenges with data identifiability, traceability, privacy, ownership, and security.",,,,,,,,,,,,,
Transparency and trust: Need for explainability and intuitive,personalized interfaces.,,,,,,,,,,,,,,
Dynamism and flexibility: Requirement for continuous learning and adaptation to environmental or regulatory changes.,,,,,,,,,,,,,,,
Behavior and cognitive mechanisms: Difficulty in modeling human factors,"including emotions.""","The study proposes a generic, systemic model for Human Digital Twin (HDT), grounded in General Systems Theory.",,,,,,,,,,,,,
Key implications include the need for regulation,ethics,transparency,trust,dynamism,,flexibility,,,,,,,,,and consideration of human behavior and cognitive mechanisms.
Recommendations: Ensure unique identification,GDPR compliance,explainability,intuitive interfaces,adaptability,,"and human-centric design.""",,,,,,,,,"Lack of a unifying, systemic model for Human Digital Twin (HDT); existing models often reinvent or omit key system components."
Absence of a common universe of discourse and standardized definitions across disciplines for HDT.,,,,,,,,,,,,,,,
Need for systematic reviews and further research to formalize and define HDT,"especially regarding its conceptual differences from technical Digital Twins.""","Future research should include a systematic review to complete and formalize the Human Digital Twin (HDT) concept. Further investigation is needed on regulation, ethics, transparency, trust, dynamism, flexibility, and understanding human behavior and cognitive mechanisms in HDT development.",,,,"The objectives of the study are to ensure regulation and ethics (including data privacy and security), promote transparency and trust (with explainability and user-friendly interfaces), enable dynamism and flexibility (continuous learning and adaptation), and account for human behavior and cognitive mechanisms in Human Digital Twins (HDTs).",,,,,,,,,
"A survey on semantic data management as intersection of ontology-based data access, semantic modeling and data lakes","Hoseini Sayed, Theissen-Lipp Johannes, Quix Christoph",2024,reference-manager,10.1016/j.websem.2024.100819,,"Implementation Insights highlight the use of indicators for personalized data lake exploration, semantic layers for heterogeneous data, and pipelines involving lexical enrichment, semantic annotation, and relationship creation. New insights include the potential of AI (LLMs) for advanced data integration and the need for scalable, community-driven solutions for broader applicability.",,,,Schema analysis: Standard procedure for analyzing data lake schemas during data ingestion.,,"What are the current developments, challenges, and gaps in integrating semantic modeling, ontology-based data access (OBDA), and data lakes for effective management and access to heterogeneous data?","This survey reviews recent developments in semantic data management (SDM) for semantic data lakes, focusing on methods for managing, modeling, and integrating heterogeneous data using semantic technologies. The study categorizes and compares approaches, highlights challenges like initial overhead and evaluation accuracy, and concludes that closer integration of Big Data and Semantic Web technologies is needed for scalable solutions.","The research goal is to survey recent semantic data management methods for data lakes, the approach classifies and compares techniques for semantic modeling and ontology-based data access, and the principal finding highlights ongoing challenges and the need for better integration of Big Data and Semantic Web technologies.",
Semantic lifting: Transformation that annotates data sets with semantic labels,preparing them for semantic access.,,,,,,,,,,,,,,
"Semantic modeling with tools like Karma: Uses techniques such as conditional random fields (CRF) and Steiner tree algorithms to suggest and learn semantic labels and relationships.""",,"Creating semantic models for heterogeneous data sets involves significant initial overhead and requires substantial human input, despite some automation proposals.",,,,,,,,,,,,,
The accuracy of automated semantic labeling and modeling methods remains uncertain beyond specific test sets; benchmarks like SemTab and VC-SLAM are emerging to address this.,,,,,,,,,,,,,,,
There is a current gap between data lake platforms,OBDA,and semantic technologies,"but integration of Big Data and Semantic Web technologies is expected to improve solutions. No p-values or quantitative results are provided.""","No explicit primary outcomes, results, or measured effects (including statistical values) are stated in the provided context.",,"Most methods focus on tabular data, lacking uniform support for all NoSQL data models.",,,,,,,,,
Human verification and refinement of semantic models remain necessary; full automation is unlikely.,,,,,,,,,,,,,,,
Solutions for Big Data have limited query/mapping expressiveness and are tied to specific platform versions.,,,,,,,,,,,,,,,
Enhanced user interfaces for non-technical users are needed.,,,,,,,,,,,,,,,
"Further research is needed to customize and optimize AI (LLMs) for specific data integration tasks.""","There is a significant gap between current data lake platforms, OBDA (Ontology-Based Data Access), and semantic technologies for modeling heterogeneous data contexts.",,,,,,,,,,,,,,
Creating semantic models for diverse data sets requires substantial initial effort and human input.,,,,,,,,,,,,,,,
Automated semantic labeling and modeling are advancing,but their accuracy beyond test sets remains uncertain.,,,,,,,,,,,,,,
"Enhanced integration of Big Data and Semantic Web technologies is recommended for future semantic data lake solutions.""",High initial overhead and usability challenges in creating and mapping semantic models for heterogeneous data; more focus needed on reducing required human input.,,,,,,,,,,,,,,
Lack of comprehensive evaluation and standard benchmarks for automated semantic labeling and modeling methods; accuracy beyond test sets remains unclear.,,,,,,,,,,,,,,,
Limited technical interoperability,"especially for NoSQL data models; current methods mainly support tabular data and need to ensure compatibility with W3C Semantic Web standards.""",Future research should address: reducing initial overhead and improving usability in semantic modeling; evaluating the accuracy of automated semantic labeling; achieving technical interoperability across diverse NoSQL data models; enhancing technical abstraction for non-experts; improving applicability and maturity of OBDA solutions for Big Data; and leveraging AI/LLMs for data integration tasks.,No information available,,,"The objectives are to establish an evaluation framework for semantic data management (SDM) by identifying criteria to compare and assess existing proposals, and to illustrate the State-of-the-Art in SDM, focusing on semantic data lakes and semantic modeling for data integration.",,,,,,,,,
InfoMAE: Pair-Efficient Cross-Modal Alignment for Multimodal Time-Series Sensing Signals,"Kimura Tomoyoshi, Li Xinlin, Hanna Osama, Chen Yatong, Chen Yizhuo, Kara Denizhan, Wang Tianshi, Li Jinyang, Ouyang Xiaomin, Liu Shengzhong, Srivastava Mani, Diggavi Suhas, Abdelzaher Tarek",2025,reference-manager,10.1145/3696410.3714853,,"InfoMAE jointly optimizes encoders and decoders with AdamW and Cosine scheduler, using discriminators for density-ratio estimation. It achieves real-time inference (<1s) on low-end devices. Ablation studies show shared, private, and temporal components are crucial. InfoMAE remains robust under sparse data and augmentations, outperforming baselines but increases training overhead.",,,,Joint multimodal pretraining: Pretraining on large-scale synchronized multimodal data to align representations across modalities.,,How can information theory-based factorization enable efficient and effective distributional alignment of multimodal representations for self-supervised learning with limited multimodal data in IoT sensing applications?,"InfoMAE is a multi-stage self-supervised learning framework for multimodal IoT sensing. It pretrains modality-specific encoders, then aligns cross-modal representations using information theory-based objectives. Experiments show InfoMAE outperforms baselines, is robust under sparse data, and enables efficient, high-quality multimodal learning with minimal performance degradation.","The research goal is to develop InfoMAE, a pairing-efficient multi-stage self-supervised learning approach for multimodal IoT sensing; the method uses information theory-based optimization for cross-modal alignment with limited pairs, and results show superior efficiency and effectiveness over existing frameworks in real-world applications.","Keywords: Multimodal sensing, Self-supervised learning, Internet of Things"
InfoMAE framework: Uses an autoencoder architecture with density ratio estimation and discriminators for distributional cross-modal alignment.,,,,,,,,,,,,,,,
"Backbone encoder (SWIN Transformer): Extracts time-frequency representations from spectrogram patches using local attention within shifted windows.""",The research is reproducible. The source code for the project is available at https://github.com/tomoyoshki/InfoMAE.,"InfoMAE outperforms baselines (GMC, FOCAL) in sparse multimodal pairing, achieving highest accuracy (Acc) and F1 scores (e.g., Acc 0.8794, F1 0.8786 at 1% pairing).",,,,,,,,,,,,,
Ablation studies show removing shared or private components significantly degrades performance,confirming the importance of factorized representation.,,,,,,,,,,,,,,
"InfoMAE achieves real-time inference (<1 second) on Raspberry Pi 4; no p-values or statistical significance values are reported.""","InfoMAE outperforms baselines (CMC, GMC, FOCAL, MAE, etc.) in cross-modal alignment, especially under sparse multimodal pair conditions (e.g., at 1% pair ratio: Acc 0.8794, F1 0.8786).",,,,,,,,,,,,,,
InfoMAE maintains high accuracy and F1 across increasing multimodal pair ratios (e.g.,at 50%: Acc 0.9377,F1 0.9367).,,,,,,,,,,,,,
Ablation studies show removing shared or private components significantly degrades performance,highlighting their importance.,,,,,,,,,,,,,,
InfoMAE demonstrates robustness to augmentation choices and strong generalization in joint multimodal pretraining.,,,,,,,,,,,,,,,
"InfoMAE incurs higher computational overhead compared to contrastive SSL baselines due to its autoencoder architecture and density ratio estimation.""",InfoMAE incurs additional computational overhead during pretraining due to its autoencoder architecture and density ratio estimation.,,,,,,,,,,,,,,
Distribution-based alignment cannot fully eliminate sampling biases,which may affect learned representations.,,,,,,,,,,,,,,
Further research is needed to improve robustness and generalization under extreme data sparsity.,,,,,,,,,,,,,,,
Preprocessing and calibrating raw signals require modality-specific domain knowledge and are labor-intensive.,,,,,,,,,,,,,,,
IoT sensor data can be sparse,noisy,and incomplete due to deployment heterogeneity,"leading to poor-quality or missing multimodal pairs.""","InfoMAE achieves superior efficiency and effectiveness in multimodal IoT sensing, outperforming existing SSL frameworks, especially under limited multimodal pairs.",,,,,,,,,,,
It maintains robust performance even with extremely sparse multimodal data.,,,,,,,,,,,,,,,
Key components (shared,private,temporal) are critical for cross-modal alignment.,,,,,,,,,,,,,
"Future work should address computational overhead and sampling bias.""",InfoMAE incurs additional computational overhead during pretraining; future work should explore more efficient attention mechanisms and alternative density ratio estimation techniques.,,,,,,,,,,,,,,
Distribution-based alignment cannot fully eliminate sampling biases; further research is needed to develop more robust alignment methods for extreme data sparsity.,,,,,,,,,,,,,,,
"Investigating concurrent unimodal pretraining could improve efficiency and performance.""","Future research should explore concurrent unimodal pretraining, optimized attention mechanisms like FlashAttention, and alternative density ratio estimation techniques without training discriminators to improve efficiency. Further work is also needed to develop more robust alignment methods that mitigate sampling errors and improve generalization under extreme data sparsity.","Study design characteristics include: self-supervised learning (SSL), supervised learning, modality pair-efficient self-supervised learning, joint multimodal pretraining, ablation studies, use of unimodal and paired multimodal data, evaluation on real-world IoT applications, and experimental comparisons to standard multimodal SSL frameworks. No mention of randomization, blinding, or control groups.",,"The specific aims of the study are to align factorized shared and private representations of multimodal time-series data using information theory-inspired objectives. InfoMAE’s key learning objectives include distributional alignment, instance-level alignment, temporal locality, and reconstruction to capture meaningful and modality-specific representations for downstream tasks.",,,,,,,,,,,
Temporal Knowledge Graph Completion Using Box Embeddings,"Messner Johannes, Abboud Ralph, Ceylan Ismail Ilkan",2022,reference-manager,10.1609/aaai.v36i7.20746,,"BoxTE is a temporal knowledge graph embedding model that is fully expressive and captures many temporal inference patterns, except composition. It achieves state-of-the-art results on benchmarks, is robust under parameter constraints, and naturally extends to higher-arity knowledge bases. New benchmarks for higher-arity temporal data are needed.",,,,"BoxTE: A method that models temporal knowledge graphs by defining time-induced relation head and tail boxes, enabling the capture of cross-time inference patterns using box containment and intersection.",,"How does the BoxTE temporal knowledge graph embedding model achieve full expressiveness and capture a rich class of temporal inference patterns, and how does its inductive capacity and robustness contribute to state-of-the-art performance in temporal knowledge graph completion tasks?","The paper introduces BoxTE, a temporal knowledge graph embedding model designed for temporal knowledge graph completion (TKGC). Using empirical evaluation, BoxTE achieves state-of-the-art results, demonstrating full expressiveness, strong inductive capacity, and robustness. The study highlights the need for benchmarks on higher-arity temporal knowledge bases.","The paper's research goal is temporal knowledge graph completion; it introduces BoxTE, a fully expressive embedding model, and demonstrates through empirical results that BoxTE achieves state-of-the-art performance and robustness on benchmark datasets, even under bounded-parameter constraints.",
Standard and bounded-parameter experiments: Evaluation of BoxTE and competing models on temporal knowledge graph completion benchmarks (ICEWS14,ICEWS5-15,GDELT) in both standard and parameter-bounded settings.,,,,,,,,,,,,,
Comparative analysis: Performance comparison with existing models (e.g.,TTransE,DE-SimplE,"TeMP) using metrics such as Mean Reciprocal Rank (MRR) and Hits@k.""",,,"BoxTE achieves state-of-the-art performance on ICEWS14 and GDELT in the bounded-parameter setup, with only a 0.03 drop in MRR on ICEWS14, 0.10 on GDELT, and 0.09 on ICEWS5-15 compared to the standard setup.",,,,,,,,,
The optimal value of parameter k varies by dataset: k ≥ 2 is best for ICEWS14,k = 3 for ICEWS5-15,and k = 1 for GDELT,reflecting dataset complexity and dimensionality trade-offs.,,,,,,,,,,,,
"BoxTE is fully expressive
On ICEWS14","captures a wide range of temporal inference patterns
BoxTE's MRR drops by 0.03 in the bounded-parameter setting; on GDELT","and provides a robust baseline for temporal knowledge base completion under computational constraints. No p-values or statistical significance are reported.""
it drops by 0.10; on ICEWS5-15","BoxTE achieves state-of-the-art performance on GDELT and ICEWS14 in both standard and bounded-parameter settings.
by 0.09.",,,,,,,,,,,,
BoxTE is robust without temporal smoothness regularization,unlike TNTComplEx and ChronoR.,,,,,,,,,,,,,,
Optimal k varies: k ≥ 2 is best for ICEWS14,k = 3 for ICEWS5-15,and k = 1 for GDELT.,,,,,,,,,,,,,
"BoxTE outperforms TeMP and is competitive with TNTComplEx and ChronoR.""",BoxTE cannot capture the composition inference pattern across time.,,,,,,,,,,,,,,
In the parameter-bounded setting,reducing dimensionality causes significant loss in representation capacity,especially on dense datasets like GDELT.,,,,,,,,,,,,,
The optimal value of k (a model parameter) varies substantially among datasets and is not obvious.,,,,,,,,,,,,,,,
"Further details about parameter counts are only provided in the appendix.""","BoxTE achieves state-of-the-art performance on multiple temporal knowledge graph completion (TKGC) benchmarks, even under parameter constraints.",,,,,,,,,,,,,,
BoxTE is robust and maintains strong performance without temporal smoothness regularization,unlike competing models.,,,,,,,,,,,,,,
Capturing relational temporal dynamics is more important than higher dimensionality in complex datasets.,,,,,,,,,,,,,,,
"Future work should introduce benchmarks for higher-arity temporal knowledge base completion.""","Lack of established benchmarks for higher-arity temporal knowledge graph completion, limiting evaluation of models like BoxTE.",,,,,,,,,,,,,,
Need for new benchmarks involving higher-arity facts to study model performance in this setting.,,,,,,,,,,,,,,,
Further research required to develop expressive,"inductively rich temporal knowledge graph embedding (TKGE) models.""","One suggested future research direction is introducing new benchmarks for temporal knowledge base completion involving higher-arity facts, to study the performance of BoxTE and other models in this setting. This addresses the current lack of established benchmarks for higher-arity temporal knowledge graph completion.",,,,,,,,,,,,,
Unifying context with labeled property graph: A pipeline-based system for comprehensive text representation in NLP,"Hur Ali, Janjua Naeem, Ahmed Mohiuddin",2024,reference-manager,10.1016/j.eswa.2023.122269,,Implementation Insights Summary:,,,,,,,,,
"The system uses graph-based analysis with Neo4j and integrates advanced NLP modules (e.g.
Syntactic","EVITA
lexical","Heideltime
and morphological analysis: Techniques are used to capture word-level information","DBpedia Spotlight) via Docker. Strengths include high precision in numeric and temporal expression detection. Weaknesses are limited nested entity detection and low temporal links coverage. Future work targets these gaps.""
determine meanings",and establish grammatical relationships for comprehensive text representation.,,"The research goal is to advance NLP text representation using an LPG-based unified context modeling approach; the method integrates NLP components with graph-based representations, and results show promising performance but highlight limitations in nested entity detection and temporal link coverage.","Document preprocessing: Text data is cleaned and normalized to remove irrelevant information, correct errors, and assign metadata, preparing it for further analysis.",,,,,,"How can autonomous, domain-agnostic techniques be developed and improved to generate comprehensive, context-rich text representations for advanced NLP tasks, addressing challenges in context integration, event participant detection, and temporal relation coverage?",,"The paper investigates advanced text representation methods in NLP, focusing on graph-based and LPG-based unified context modeling. Using a pipeline approach, the study evaluates its system against the MEANTIME corpus, finding strengths in semantic and temporal detection but lower recall for nested entities. The research promises improved NLP applications."
Entity linking and disambiguation: Tools like DBpedia-spotlight or Entity-Fishing assign unique identifiers to entity mentions,"resolving references and creating entity instances.""",,"The proposed system achieved high F1 scores for numeric value extraction (0.940) and temporal expression detection (0.943), demonstrating strong performance in these areas.",,,,,,,,,,,,
Lower F1 scores were observed for entity instances (0.550) and temporal links (0.338),indicating areas needing improvement.,,,,,,,,,,,,,,
"No statistical significance (p-values) was reported in the context.""","Entity mentions: Precision 0.629, Recall 0.773, F1 score 0.693",,,,,,,,,,,,,,
Event mentions: Precision 0.782,Recall 0.663,F1 score 0.718,,,,,,,,,,,,,
Entity instances: Precision 0.434,Recall 0.750,F1 score 0.550,,,,,,,,,,,,,
Numeric values: Precision 0.893,Recall 0.993,F1 score 0.940,,,,,,,,,,,,,
Temporal expression detection: Precision 0.965,Recall 0.921,F1 score 0.943,,,,,,,,,,,,,
Temporal expression normalization: Precision 0.982,Recall 0.737,F1 score 0.842,,,,,,,,,,,,,
Event participants: Precision 0.619,Recall 0.649,F1 score 0.634,,,,,,,,,,,,,
Temporal links: Precision 0.494,Recall 0.257,"F1 score 0.338""","Does not detect nested entities, resulting in lower recall for entity mention detection and entity instances.",,,,,,,,,,,,
Relies only on verb-based Semantic Role Labeling (SRL); lacks nominal SRL,missing participants in nominal events.,,,,,,,,,,,,,,
Limited coverage of temporal links (tlinks),including MEASURE,simultaneous,IBEFORE,and links between main/subordinate event mentions and timexes.,,,,,,,,,,,
Challenges in achieving effective domain-agnostic and task-agnostic text representations due to complexity in accommodating diverse requirements.,,,,,,,,,,,,,,,
Existing methods focus mainly on sentence-level analysis,"requiring additional post-processing for broader context.""",The proposed system offers a promising approach to text representation in NLP using LPG-based unified context modeling.,,,,,,,,,,,,,
Key limitations include incomplete detection of nested entities,limited event participant identification,and insufficient temporal link coverage.,,,,,,,,,,,,,
Future work should focus on improving entity detection,event participant identification,"and temporal relation coverage for more robust NLP applications.""",Expand event participant detection by incorporating nominal Semantic Role Labeling (SRL) to capture participants in nominal events.,,,,,,,,,,,,
Improve temporal links (tlinks) coverage,especially for MEASURE relations,simultaneous tlinks,IBEFORE relationships,and links between main and subordinate event mentions.,,,,,,,,,,,
Address limitations in entity mention detection,entity linking/disambiguation,event participants detection,"and temporal links coverage.""","Future research should address: expanding Semantic Role Labeling to include nominal SRL for better event participant detection; improving temporal links coverage, especially for MEASURE, simultaneous, IBEFORE, and various event-timex relationships; detecting nested entities; refining LPG-based representations; and evaluating on diverse datasets and tasks.",,,,,,,,,,"The objectives of the study are to evaluate the proposed system’s performance in accurately identifying and labeling different semantic elements, and to compare its output with a manually annotated gold standard dataset to assess alignment with expected annotations.",
Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models,"Yuan Chenhan, Xie Qianqian, Huang Jimin, Ananiadou Sophia",2024,reference-manager,10.1145/3589334.3645376,,Implementation Insights Summary:,,,,,,,,,
The paper introduces ExplainTemp,a 26k-entry dataset for explainable temporal reasoning,constructed using a novel Temporal Knowledge Graph-Instructed Generation (TKGIG) approach. Fine-tuned TimeLlama models,especially ChatTimeLlama-7b,"show significant improvements in prediction and explanation quality. Larger models do not always outperform smaller ones.""",,,"Can large language models effectively perform explainable temporal reasoning for future event prediction, and how do their capabilities compare to traditional methods, particularly regarding the impact of instruction tuning and the clarity of their reasoning processes?",,,,,Template-generated explanation evaluation prompts were used to assess prediction correctness based on reasoning steps.,,"The paper aims to robustly evaluate large language models (LLMs) for explainable temporal reasoning and event forecasting. Using expert human annotation and standardized guidelines, models were assessed on correctness, completeness, and fluency. TimeLlama models, fine-tuned for temporal tasks, showed improved logical inference. High inter-rater agreement confirmed dataset quality.",The research goal is to develop explainable temporal reasoning with large language models; the approach involves fine-tuning Llama models (TimeLlama) for event forecasting using temporal datasets; results show TimeLlama outperforms baselines in accuracy and explanation quality for complex temporal event prediction tasks.
Revision prompts required revising explanations to ensure logical progression and alignment with reasoning.,,,,,,,,,,,,,,,
Baseline experiments involved finetuning and evaluating models like LSTM,BERT,Flan T5,BART,and Llama2 using automatic (precision,,recall,"BertScore) and human evaluation.""",,,"Human annotation showed high inter-rater agreement: Cohen’s kappa for correctness (0.74), completeness (0.66), and fluency (0.98) overall.",,"The research is reproducible, as the source code is available at https://github.com/chenhan97/TimeLlama. The project provides detailed dataset construction, evaluation guidelines, and metrics, supporting reproducibility of results.",ROUGE,BLEU,F1
LSTM and BERT baselines achieved overall F1 scores of 10.7 and 21.4,respectively,on the ExplainTemp dataset.,,,,,,,,,,,,,
The testing dataset is high-quality and standardized,"with low-scoring samples excluded; statistical significance (p-values) not reported.""","Two annotators rated 1,200 explanations for correctness, completeness, and fluency.",,,,,,,,,,,,,
Cohen’s kappa scores: correctness (0.74),completeness (0.66),fluency (0.98) indicate high inter-rater agreement.,,,,,,,,,,,,,
Low-scoring samples were excluded,resulting in a high-quality testing dataset.,,,,,,,,,,,,,,
TimeLlama models (7b/13b) outperformed baselines in BLEU,ROUGE,and BertScore metrics.,,,,,,,,,,,,,
TimeLlama-7b achieved BLEU avg 59.9,ROUGE1 46.3,BertScore F1 90.2.,,,,,,,,,,,,,
ChatTimeLlama-7b achieved BLEU avg 61.9,ROUGE1 48.2,BertScore F1 88.8.,,,,,,,,,,,,,
Finetuning led to improvements of 44.0 (positive),32.5 (negative),56.3 (neutral),and 49.2 (overall) across categories.,,,,,,,,,,,,
"Increasing model size did not always improve performance.""",The dataset may exhibit gaps or contain unintended biases.,,,,,,,,,,,,,,
Large language models risk generating harmful,biased,or logically incoherent content (hallucination).,,,,,,,,,,,,,
Current methods focus on simple temporal tasks; complex event forecasting and explainable reasoning remain underexplored.,,,,,,,,,,,,,,,
Evaluation prioritizes accuracy over fluency,"which may affect assessment balance.""","The study presents a high-quality, standardized testing dataset for evaluating explainable temporal reasoning in large language models (LLMs).",,,,,,,,,,,,,
Instruction tuning on high-quality datasets significantly improves LLMs’ explainable temporal reasoning abilities.,,,,,,,,,,,,,,,
ChatTimeLlama-7b outperforms baselines in explanation quality and prediction accuracy.,,,,,,,,,,,,,,,
"Recommendation: Use instruction tuning and high-quality datasets to enhance LLMs’ temporal reasoning and explanation capabilities.""","Limited exploration of LLMs' abilities in complex event forecasting, which requires multi-step temporal reasoning and future prediction.",,,,,,,,,,,,,,
Underexplored area of explainable temporal reasoning,specifically models justifying their predictions to improve transparency.,,,,,,,,,,,,,,
"Need to assess the impact of instruction tuning and new datasets on LLMs' temporal prediction and explanation capabilities.""","Future research should address the unexplored potential of LLMs in complex event forecasting requiring multi-step temporal reasoning, and the underexplored area of explainable temporal reasoning. Expanding benchmark datasets to include more diverse temporal reasoning tasks is also recommended to further assess and improve LLM capabilities.",,,The objectives of the study are to: 1) assess if LLMs can predict future events by reasoning over complex event relations and compare them to traditional methods; 2) examine the effect of instruction tuning with a new temporal knowledge graph dataset on LLMs’ temporal prediction; 3) evaluate LLMs’ ability to explain their predictions.,,,,,,,,,,,
LinkGuard: Link Locally Privacy-Preserving Graph Neural Networks with Integrated Denoising and Private Learning,"Qi Yuxin, Lin Xi, Liu Ziyao, Li Gaolei, Wang Jingyu, Li Jianhua",2024,reference-manager,10.1145/3589335.3651533,,"The implementation integrates denoising directly into GNN learning under link LDP, dynamically adjusting edge weights using node similarity (cosine similarity) and noisy embeddings. This unified approach consistently improves accuracy across various models and privacy budgets, outperforming previous methods and enhancing the privacy-utility trade-off.",,,,"Topology denoising during GNN training: A dynamic, training-adaptive denoising method is applied on the server side to improve privacy and utility.",,How can an integrated denoising and private learning framework improve the balance between privacy and utility in graph neural networks under link-level local differential privacy guarantees?,"The paper addresses under-denoising in Link LDP GNNs by integrating permuted topology denoising with GNN private learning. Using Cora and Citeseer datasets, the proposed dynamic denoising framework consistently improves accuracy across various GNNs and privacy levels, achieving a better balance between privacy and utility than existing methods.","The research goal is to improve the utility-privacy trade-off in Link LDP GNNs by dynamically integrating topology denoising with private learning; the approach adaptively combines denoising and GNN training, and results show consistent accuracy improvements across datasets and backbones, demonstrating better privacy and utility balance.","Privacy-Preserving, Differential Privacy, Graph Neural Network"
Link Local Differential Privacy (Link LDP): The framework preserves local privacy of graph links using LDP mechanisms during GNN training.,,,,,,,,,,,,,,,
Node classification on citation networks: Experiments use node classification tasks on Cora and Citeseer datasets,"with backbone GNNs (GCN and GAT) to evaluate utility-privacy trade-offs.""",,"The proposed framework consistently outperforms original LDPGen, DPRR, L-DPGCN, and Blink across all privacy budgets (𝜋 = 4, 3, 2, 1) on Cora and Citeseer datasets, with improvements up to 7.64%.",,,,,,,,,,,,
The framework achieves greater utility improvements under stronger privacy (lower 𝜋),e.g.,7.53% and 7.64% on Citeseer at 𝜋 = 3 and 1.,,,,,,,,,,,,,
"Integrating dynamic denoising with private GNN learning yields a better utility-privacy trade-off; all improvements are statistically significant (standard deviations ≤ 0.35).""","The primary outcome measured is model utility, quantified by accuracy.",,,,,,,,,,,,,,
The proposed framework outperforms original LDPGen,DPRR,L-DPGCN,and Blink across all privacy budgets (𝜋 = 4,3,,2,,,,,,,,,1) on Cora and Citeseer datasets.
Reported accuracy improvements range from +0.05 to +7.64 percentage points.,,,,,,,,,,,,,,,
"Gains are higher under stronger privacy (lower 𝜋).""",,"The proposed framework dynamically integrates topology denoising with private GNN learning, improving the balance between privacy and utility.",,,,,,,,,,,,,
It consistently outperforms state-of-the-art Link LDP GNNs across various privacy budgets and backbones,with higher accuracy gains under stronger privacy.,,,,,,,,,,,,,,
"Recommendation: Adopt dynamic denoising during GNN training for better privacy-utility trade-off.""",The under-denoising issue on the server side due to separating noisy topology denoising and GNN private learning into different stages.,,,,,,,,,,,,,,
The field of DP GNN is still nascent,with many unresolved challenges.,,,,,,,,,,,,,,
"Need for improved integration of permuted topology denoising with GNN private learning for better privacy-utility trade-off.""",,,,"The objectives of the study are to evaluate the proposed method in terms of privacy strength and model utility, specifically: (1) to compare the framework's performance with Link LDP guaranteed GNNs, and (2) to assess how the framework balances privacy and utility across different GNNs.",,,,,,,,,,,
Temporal heterogeneity in cognitive architectures,"Sandoval-Arrayga Carlos Johnnatan, Palacios-Ramirez Gustavo, Ramos-Corchado Felix Francisco",2024,reference-manager,10.1016/j.cogsys.2024.101265,,The implementation demonstrates that temporal heterogeneity enables faster parameter optimization and greater resilience to noise compared to temporal homogeneity. The peek-end rule heuristic further stabilizes the system and leverages long-term memory. Ongoing work aims to dynamically regulate input quantities and improve agent interoception.,,,,"Comparison of two cognitive architectures: one with temporal heterogeneity and one with temporal homogeneity, using a constant topology.",,"Does modeling temporal heterogeneity in cognitive architectures, inspired by human brain properties, enhance the autonomy and human-like behavior of virtual agents compared to architectures with temporal homogeneity?","The paper investigates whether incorporating temporal heterogeneity—different processing speeds in cognitive modules—into cognitive architectures leads to more human-like behavior. Using an evolutionary algorithm, two models (heterogeneous vs. homogeneous) were compared on adaptation steps and energy use. Results suggest temporal heterogeneity supports more efficient, human-like learning and behavior.",The research goal is to explore if modeling temporal heterogeneity in cognitive architectures creates more human-like behavior; the approach uses a bio-inspired model tested in a virtual world; results show that temporal heterogeneity can aid in achieving more stable and human-like agent behavior.,"Keywords: Cognitive architectures, Temporal, Heterogeneity, Reference frameworks."
Use of an evolutionary algorithm to optimize architecture parameters and favor better-performing agents.,,,,,,,,,,,,,,,
Mixed-method approach: quantitative measurement of adaptation steps and energy consumption,"and qualitative analysis of agent knowledge.""","The research provides detailed descriptions of the experiment, metrics, and implementation in Sections 5 and 9. Data will be available on request. There is no explicit mention of source code availability for the project.","Temporal heterogeneity enabled agents to reach higher fitness (coin collection) more quickly than temporal homogeneity, except in one model (Fig. 6(c)).",,,,,,,,,,,,
All models achieved at least 40 coins after 30,000 generations; temporal heterogeneity showed faster evolution,especially with long-term memory.,,,,,,,,,,,,,
"The peek-end rule model with temporal heterogeneity demonstrated resilience to noise and capitalized on long-term memory
Results:","but no explicit p-values or statistical significance were reported.""",Primary outcome: Fitness measured as the number of coins collected by agents.,,,,,,,,,,,,,
All models achieved at least 40 coins after 30,000 evolution steps (without long-term memory).,,,,,,,,,,,,,,
"Models with temporal heterogeneity generally evolved faster to higher fitness than those with temporal homogeneity.
45","000 runs were conducted for each of the 6 models.""",The study of the influence of temporal heterogeneity in different cognitive architecture topologies is beyond the scope of this work.,,,,,,,,,,,,,
The model is designed to mimic a mature,healthy human brain; developmental changes and disorders are not addressed.,,,,,,,,,,,,,,
Biological reasons for brain myelination are not explored.,,,,,,,,,,,,,,,
Ideal values for temporal heterogeneity are not determined.,,,,,,,,,,,,,,,
All buffers require the same number of inputs before generating an output; dynamic regulation is not implemented yet.,,,,,,,,,,,,,,,
The agent’s interoception (e.g.,"hunger sensing) is not yet human-like; improvements are ongoing.""","Temporal heterogeneity in cognitive architectures can enable faster convergence, reduced energy consumption, and more human-like behavior.",,,,,,,,,,,,,
The peek-end rule heuristic contributes to system stability and leverages long-term memory.,,,,,,,,,,,,,,,
A limitation is that all buffers require the same number of inputs; future work aims to address this with dynamic regulation.,,,,,,,,,,,,,,,
Enhancing agent interoception (e.g.,"hunger sensing) is a recommended direction for future research.""",The study does not address the influence of temporal heterogeneity in different cognitive architecture topologies.,,,,,,,,,,,,,
Determining the ideal values for temporal heterogeneity is not the objective; future work could explore optimal parameterization.,,,,,,,,,,,,,,,
"Current buffers require the same number of inputs before output; ongoing work aims to develop biologically inspired dynamic regulation.""","Future research should investigate the influence of temporal heterogeneity in different cognitive architecture topologies, determine ideal values for temporal heterogeneity, and further explore the existence and role of buffers in the human brain, as robust biological evidence is still lacking.",Study design characteristics: mixed-method study; includes both qualitative and quantitative analysis; uses a virtual world with agents placed one at a time; compares two versions of the same cognitive architecture (temporal heterogeneity vs. homogeneity); employs an evolutionary algorithm for parameter optimization; experiment is controlled for architecture topology.,,"The objectives of the study are to qualitatively compare the most abstract percepts agents use to make decisions and to quantitatively measure how effective certain percepts are in adapting to the environment, specifically by exploring whether modeling temporal differences can help create more human-like behavior in virtual agents.",,,,,,,,,,,
Collaborative Knowledge Graph Fusion by Exploiting the Open Corpus,"Wang Yue, Wan Yao, Bai Lu, Cui Lixin, Xu Zhuo, Li Ming, Yu Philip S., Hancock Edwin R.",2020,reference-manager,,,"The implementation uses a collaborative framework with explorer and supervisor processes. The explorer extracts triples using benchmark-based supervision, while the supervisor evaluates and enriches the knowledge graph. Experiments show improved performance and knowledge graph quality, especially with enriched triples and larger explorer hidden dimensions.",,,,"Benchmark-based Supervision JEE (BJEE): A joint model that extracts entities, event triggers, and arguments together, supervised by benchmark entity pairs sampled from a knowledge graph.",,"How can a unified framework that combines joint event extraction with knowledge graph fusion effectively construct and enrich a high-quality, domain-oriented knowledge graph from open text corpora while addressing challenges in relation alignment, knowledge graph quality, and knowledge sharing between subtasks?","The paper aims to improve knowledge graph fusion from open text by proposing a collaborative framework with explorer and supervisor processes. Using benchmark-based supervision and contrastive learning, the method extracts and evaluates knowledge triples. Experiments show superior performance over baselines in both joint event extraction and knowledge graph embedding tasks, enhancing knowledge graph quality.","The paper's main objective is knowledge graph fusion from open corpora using a collaborative framework that combines joint event extraction and a benchmark-based supervision mechanism, with results showing superior performance over state-of-the-art baselines in both extraction and knowledge graph enrichment tasks.",
Supervisor Process: Iteratively enriches training knowledge triples using extracted results from the explorer process.,,,,,,,,,,,,,,,
"Collaborative Knowledge Graph Fusion: Alternates between extracting triples from text and enriching a prior knowledge graph using relation alignment and benchmark-based supervision.""",The research provides a prototype system implemented with PyTorch. Source code links are included: https://github.com/hkharryking/labeled NYT CoNLL. Additional resources used are Stanford CoreNLP and Wikidata. No further details on full code or reproducibility procedures are given.,"The proposed system outperforms state-of-the-art baselines in both joint event extraction and knowledge graph embedding tasks, achieving F1 scores up to 98.2 and MRR up to 0.0294.",,,,,,,,,,,,,
Enriched triples from the Collaborative Knowledge Graph Fusion framework significantly improve model performance,enhancing the quality of the seed knowledge graph.,,,,,,,,,,,,,,
"Statistical significance (p-values) is not reported in the provided context.""","BJEEwn18 and BJEEfb15k models achieved highest F1 scores across datasets: up to 98.2 (WebNLG), 99.0 (NYT), 95.7 (CoNLL), and 95.5 (ACE2005).",,,,,,,,,,,,,,
Supervisor model achieved Hit@10,Hit@20,Hit@30 = 100.0 and MRR = 0.0294 on FB15K (KGF task).,,,,,,,,,,,,,
Iterative explorer-supervisor process improved overall JEE performance with more rounds.,,,,,,,,,,,,,,,
BJEE models significantly outperformed pure BERT and other baselines in Precision,Recall,"and F1.""","Difficulties in aligning RDF triples, especially relation alignment between open text sources and prior knowledge graphs.",,,,,,,,,,,,
Challenges in maintaining knowledge graph quality when merging unaligned triples,potentially leading to unreliable results.,,,,,,,,,,,,,,
Difficulties in sharing knowledge between sub-tasks,causing error propagation and degraded performance.,,,,,,,,,,,,,,
"Suggestions for further research are offered in the conclusion.""",The Collaborative Knowledge Graph Fusion framework significantly improves Joint Event Extraction (JEE) performance across multiple datasets.,,,,,,,,,,,,,,
The iterative supervisor-explorer mechanism enhances both entity and event extraction,outperforming baseline models like BERT.,,,,,,,,,,,,,,
Performance increases with more iterative rounds and optimized model parameters (e.g.,hidden dimensions,CNN kernels).,,,,,,,,,,,,,
"Recommendation: Further improve explorer processes for additional gains; the framework supports fully automatic knowledge graph fusion in future work.""","Difficulty in aligning RDF triples, especially relation alignment, between open text sources and prior knowledge graphs.",,,,,,,,,,,,,,
Challenges in maintaining knowledge graph quality when merging unaligned triples,potentially leading to unreliable or low-quality knowledge graphs.,,,,,,,,,,,,,,
Difficulty in sharing knowledge between sub-tasks,"causing error propagation and degraded sub-task performance.""","Future research should focus on: (1) improving alignment of RDF triples, especially relation alignment; (2) maintaining knowledge graph quality when merging unaligned triples; and (3) developing reliable methods for sharing knowledge between sub-tasks to reduce error propagation and enhance overall performance.",,,,"The objectives of the study are to achieve knowledge graph fusion with an open corpus by: (1) extracting knowledge triples from unstructured texts using joint event extraction (JEE), and (2) evaluating and enriching a prior knowledge graph with these extracted triples through knowledge graph fusion (KGF).",,,,,,,,,
Procreation of training data using cognitive science in temporal data processing for burnt paddy fields mapping,"Singhal Mragank, Payal Ashish, Kumar Anil",2021,reference-manager,10.1016/j.rsase.2021.100516,,"The implementation used cognitive science to generate training data for temporal datasets, enabling mapping of burnt paddy fields using the Modified Possibilistic c-Means (MPCM) algorithm. The approach achieved 98% overall accuracy and an F-Score of 0.96, with minimal differences between training and testing membership values.",,,,"Cognitive science-based methodology was used to generate and expand training data from temporal datasets, utilizing field-collected geo-tagged samples.",,"How can cognitive science be applied to generate training data for future or past temporal data sets to improve the mapping accuracy of MPCM classifiers in identifying frequently occurring classes, such as paddy stubble burnt fields, using remote sensing imagery?","The paper investigates using cognitive science to generate training data for temporal datasets in mapping burnt paddy fields. Using seed data from field visits, the study applies cognitive science principles and the Modified Possibilistic c-Means (MPCM) algorithm, achieving 98% overall accuracy and an F-Score of 0.96.",The research goal was to use cognitive science to generate temporal training datasets for MPCM classifiers; the approach involved using seed training data and Class-Based Sensor-Independent Indices (CBSI); results showed 98% overall accuracy and an F-Score of 0.96 in mapping burnt paddy fields.,"Keywords: Cognitive science, Soft classification, Modified possibilistic c-means"
Modified Possibilistic c-Means (MPCM) algorithm was applied for mapping burnt paddy fields.,,,,,,,,,,,,,,,
Class-Based Sensor-Independent Indices (CBSI),specifically CBSI-NDVI,"were used to reduce spectral dimensionality.""",,Training and testing produced membership values with very small differences; mean membership differences (Δ) ranged from −0.04 to 0.04.,,,,,,,,,,,
The proposed approach achieved 98% overall accuracy and an F-Score of 0.96 for burnt paddy fields; Kappa value was 0.88.,,,,,,,,,,,,,,,
"The largest burnt area detected was approximately 80.27 sq. km on 6th Nov 2019; statistical significance (p-values) not reported.""",Training and testing gave membership values with very small differences (mean membership difference Δ ranged from −0.04 to 0.04).,,,,,,,,,,,,,,
The overall accuracy for burnt paddy fields was 98% with an F-Score of 0.96.,,,,,,,,,,,,,,,
For dry biomass: Precision 1.00,Recall 0.91,F-Score 0.95,Kappa 0.90,Overall Accuracy 95%.,,,,,,,,,,,
For burnt fields: Precision 0.92,Recall 1.00,F-Score 0.96,Kappa 0.88,Overall Accuracy 98%.,,,,,,,,,,,
"Largest burnt area detected was approximately 80.27 sq. km on 6th Nov 2019.""","Classification techniques used only one data dimension, while temporal multispectral data have both spectral and temporal dimensions, limiting input to classification algorithms.",,,,,,,,,,,,,,
"It is not possible to state which classifier is best for all tasks due to varying image characteristics and study purposes.""","The proposed approach achieved high accuracy in identifying burnt paddy fields, with an overall accuracy of 98% and an F-Score of 0.96.",,,,,,,,,,,,,,
Training and testing membership values showed very small differences,indicating model consistency.,,,,,,,,,,,,,,
"The CBSI-NDVI method effectively reduced spectral dimensionality while maintaining temporal information.""",Current classification techniques cannot process both spectral and temporal dimensions of temporal multispectral data simultaneously.,,,,,,,,,,,,,,
There is a need to generate training data for future or past temporal datasets using cognitive science and seed training data.,,,,,,,,,,,,,,,
"Users still face challenges in identifying appropriate spectral bands (red and NIR) in large multispectral datasets.""",,No information available,,"The main objective of this study is to use cognitive science as knowledge of experience to generate training datasets for future or past temporal datasets for MPCM classifiers, specifically for mapping burnt paddy fields at frequent intervals using remote sensing data.",,,,,,,,,,,
Learner Modeling and Recommendation of Learning Resources using Personal Knowledge Graphs,"Ain Qurat Ul, Chatti Mohamed Amine, Meteng Kamdem Paul Arthur, Alatrash Rawaa, Joarder Shoeb, Siepmann Clara",2024,reference-manager,10.1145/3636555.3636881,,"The implementation features an interactive UI allowing users to switch between video and article recommendations, displaying keyphrases and similarity scores for transparency. User studies showed higher satisfaction, perceived usefulness, and controllability with the PKG-based approach. Most users preferred recommendations considering concepts from all slides, enhancing diversity and novelty.",,,,"Online user study: Recruited 31 Masters/Ph.D. students to interact with the system, view recommendations, and complete questionnaires.",,"How can Personal Knowledge Graphs (PKGs) be effectively leveraged to model students’ knowledge and recommend learning resources in a MOOC environment, and how do PKG-based approaches compare to content-based methods in terms of system accuracy and perceived benefits for learners?","The paper investigates how Personal Knowledge Graphs (PKGs), graph convolutional networks, and sentence encoders can model students’ knowledge and recommend learning resources in MOOCs. Using offline and user studies with four recommendation variants, results show keyphrase extraction improves user experience, while content-based document variants achieve the highest recommendation precision. PKG-based approaches increase user satisfaction and perceived usefulness.","The research goal was to improve learning resource recommendations in MOOCs by modeling students’ knowledge using Personal Knowledge Graphs (PKGs), graph convolutional networks (GCNs), and sentence encoders (SBERT); the approach enhanced recommendation diversity and usefulness, though content-based document variants achieved the highest accuracy (Precision@5 = 68%).",
Offline evaluation: Used statistical measures (Precision@k,Mean Average Precision,Mean Reciprocal Rank) to assess recommendation quality.,,,,,,,,,,,,,
ResQue evaluation framework: Assessed perceived system qualities,usefulness,satisfaction,"and behavioral intentions via a 5-point Likert scale.""",,,"The content-based document variant achieved the highest Precision@5 (0.68), while the PKG-based document variant had the highest MRR (0.81) and MAP (0.78); however, these results were not statistically significant.",,,,,,,,,
Keyphrase extraction improved average similarity scores (up to 0.92) and enhanced perceived system qualities,including accuracy,novelty,and diversity.,,,,,,,,,,,,
Participants reported higher satisfaction,perceived usefulness,and novelty with PKG-based variants,"especially those using keyphrase extraction.""","Four recommendation variants were evaluated: PKG-based keyphrase, PKG-based document, content-based keyphrase, and content-based document.",,,,,,,,,,,
Content-based document variant achieved the highest Precision@5: 0.68 (68%).,,,,,,,,,,,,,,,
PKG-based document: Precision@5 = 0.64; PKG-based keyphrase: Precision@5 = 0.59; Content-based keyphrase: Precision@5 = 0.65.,,,,,,,,,,,,,,,
Keyphrase variants yielded higher similarity scores (PKG-based keyphrase: 0.92; content-based keyphrase: 0.88).,,,,,,,,,,,,,,,
Document variants outperformed in precision metrics.,,,,,,,,,,,,,,,
Keyphrase extraction improved user experience and perceived relevance.,,,,,,,,,,,,,,,
Statistical measures used: Precision@k,Mean Reciprocal Rank (MRR),Mean Average Precision (MAP).,,,,,,,,,,,,,
User evaluation confirmed benefits in perceived accuracy,novelty,diversity,usefulness,satisfaction,,"and intentions to use/watch.""",,,,,,,,,"Findings are based on one particular ERS, restricting generalizability."
Imprecise extraction of DNU concepts may have affected PKG-based recommendation accuracy.,,,,,,,,,,,,,,,
Outcomes from user studies and offline evaluations might contradict each other,raising concerns about offline evaluation validity.,,,,,,,,,,,,,,
Further research suggested on explanation,transparency,confidence,"and trust.""","Combining PKGs, GCNs, and SBERT effectively models students’ knowledge and improves recommendation quality.",,,,,,,,,,,
Content-based document variant achieved the highest Precision@5 (68%),while PKG-based variants scored 59% and 64%.,,,,,,,,,,,,,,
Users were more satisfied with PKG-based recommendations.,,,,,,,,,,,,,,,
Most users preferred recommendations considering all previously misunderstood concepts,"not just current slide content.""","Imprecise extraction of DNU (Did Not Understand) concepts led to irrelevant or missing key concepts, reducing recommendation accuracy.",,,,,,,,,,,,,
The YouTube API constraint limited the number of DNU concepts transmitted,causing less relevant recommendations.,,,,,,,,,,,,,,
"Future work should improve DNU concept selection by considering their similarity to the current slide for increased relevance.""","Future research should investigate benefits on user-centric aspects such as explanation, transparency, confidence, and trust. Further studies are needed to validate findings beyond the single ERS used, addressing generalizability and exploring how personalized ERSs can empower learners and enhance satisfaction and engagement.","The study was an online, non-randomized, non-controlled, single-site user study. It involved 31 participants (Masters and Ph.D. students) interacting with four recommendation variants via Microsoft Teams, using questionnaires and the ResQue evaluation framework. The study was not blinded or placebo-controlled.",,,,,,,,,,,,,
FedGNN: Federated Graph Neural Network for Privacy-Preserving Recommendation,"Wu Chuhan, Wu Fangzhao, Cao Yang, Huang Yongfeng, Xie Xing",2021,reference-manager,10.475/123\_4,,"The implementation uses local differential privacy (LDP) on user gradients and adds random pseudo-item gradients to protect privacy. A privacy-preserving user-item graph expansion method exchanges encrypted neighbor embeddings, enabling high-order information use without leaking private data. Proper tuning of hyperparameters balances privacy and model performance.",,,,"Federated Learning: User data remains on local devices, and only local model updates (gradients) are shared with a central server for global model training, preserving privacy.",,How can a federated graph neural network framework be designed to provide privacy-preserving personalized recommendations by leveraging decentralized user data while effectively modeling high-order user-item interactions?,"The paper investigates privacy-preserving recommendation methods, proposing FedGNN, which uses federated learning and graph neural networks. Experiments on six benchmark datasets show FedGNN achieves competitive or superior recommendation accuracy compared to centralized and privacy-preserving baselines, while effectively protecting user privacy through encrypted data and pseudo item sampling.","The research goal is privacy-preserving personalized recommendation; the approach is FedGNN, a federated graph neural network; the principal finding is that FedGNN achieves competitive or superior recommendation performance compared to centralized and privacy-preserving baselines while protecting user privacy.","Personalized recommendation, Graph neural network, Privacy-preserving, Federated learning"
Local Differential Privacy with Pseudo Interacted Item Sampling: Gradients are protected using local differential privacy,and pseudo interacted items are sampled to obscure real user interactions.,,,,,,,,,,,,,,
"Privacy-Preserving User-Item Graph Expansion: Encrypted item IDs and user embeddings are used to expand local user-item graphs without exposing private information.""",,"FedGNN achieves the best performance among privacy-preserving methods, with RMSE values of 0.989 (Flixster), 0.790 (Douban), 21.1 (Yahoo), 0.920 (ML-100K), 0.848 (ML-1M), and 0.803 (ML-10M).",,,,,,,,,,,,,
Incorporating high-order user-item graph information improves recommendation accuracy compared to first-order methods.,,,,,,,,,,,,,,,
FedGNN protects both ratings and user-item interaction histories,"while other privacy-preserving methods only protect ratings; no explicit p-values reported.""","FedGNN achieves the best performance among privacy-preserving methods, with RMSE values: Flixster 0.989, Douban 0.790, Yahoo 21.1, ML-100K 0.920, ML-1M 0.848, ML-10M 0.803.",,,,,,,,,,,,,
FedGNN is compatible with different GNN architectures and benefits from incorporating high-order user-item graph information.,,,,,,,,,,,,,,,
FedGNN protects both ratings and user-item interaction histories,unlike FCF and FedMF,which only protect ratings.,,,,,,,,,,,,,
Increasing the number of pseudo interacted items improves privacy protection but increases communication cost; A=1,000 balances privacy and performance.,,,,,,,,,,,,,,
Lower privacy budget (ε) means better privacy protection; RMSE increases as privacy budget decreases.,,,,,,,,,,,,,,,
"FedGNN achieves comparable or better recommendation accuracy than centralized methods while protecting user privacy.""",Model training at the beginning is not beneficial for learning precise user and item representations.,,,,,,,,,,,,,,
Model performance declines with increased noise strength; too much noise or too small a privacy budget harms accuracy.,,,,,,,,,,,,,,,
Large numbers of pseudo interacted items increase communication cost.,,,,,,,,,,,,,,,
Sparse rating matrices require more pseudo items,"further increasing communication cost.""","FedGNN achieves competitive recommendation performance while protecting user privacy, comparable to centralized GNN-based methods.",,,,,,,,,,,,,
Privacy is preserved through local gradient sharing,encrypted item IDs,and pseudo interacted item sampling.,,,,,,,,,,,,,
Recommended hyperparameters: gradient clip threshold 0.1 and noise strength 0.2 for optimal privacy-accuracy tradeoff.,,,,,,,,,,,,,,,
Setting the number of pseudo interacted items (𝐴) to 1,000 balances privacy,performance,"and communication cost.""","Limited ability of existing privacy-preserving methods (FCF, FedMF) to incorporate high-order user-item graph information, which FedGNN addresses.",,,,,,,,,,,
Need for further exploration of different GNN architectures and their compatibility with privacy-preserving frameworks.,,,,,,,,,,,,,,,
"Effectiveness of fixed versus fully trainable neighbor user embeddings requires deeper investigation.""",,,,,,,,,,,,,,,
Guidelines for designing and building an automated multimodal textual annotation system,"Kim Joshua Y, Yacef Kalina",2023,reference-manager,10.1145/3610661.3616182,,"Implementation Insights highlight the need for expert annotators for sensitive content, a two-pass annotation process (automated then manual), and the importance of balancing technical and layman annotations. Early supervised learning tests save time, but features useful for algorithms may not benefit human users. Annotation is labor-intensive, requiring 4–5 hours per hour of data.",,,,Use of multiple existing multimodal research datasets to test and inform extraction algorithms before collecting new data.,,"What are the key guidelines for designing an automated multimodal textual annotation system to address user needs, reduce bias, and improve the efficiency and accuracy of multimodal feature extraction and presentation?","The paper aims to design and build an automated multimodal annotation system for interpreting various communication cues. Using multiple existing datasets and extraction algorithms, the study compares systems, highlights challenges like bias, and recommends a two-pass (automated and manual) approach. Early supervised learning tests help validate features and save resources.","The research goal is to design automated multimodal annotation systems tailored to user needs; the approach involves proposing guidelines and comparing existing systems, and the principal finding is that user-focused customization and usability are crucial, with MONAH achieving higher usability but lacking some technical features.",
Two-pass annotation approach: first automated,then manual review,to reduce both system and individual biases.,,,,,,,,,,,,,
"Application of computer vision and speech processing techniques for automatic multimodal feature extraction.""","The research uses multiple existing multimodal datasets (e.g., IEMOCAP, EQClinic, MOSI, MOSEI) and extraction algorithms (e.g., OpenFace). There is no explicit mention of the source code for the project being available. Reproducibility relies on public datasets and described algorithms.",The MONAH system provides the widest variety of multimodal annotations but lacks within-talkturn silence annotations and parallel representation of overlapping talk.,,,,,,,,,,,,,
MONAH achieved higher usability scores on the System Usability Scale compared to human-annotated Jefferson transcripts.,,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the context.""",Applying extraction algorithms on multiple existing multimodal datasets helps identify pitfalls and ensures system resilience.,,,,,,,,,,,,,,
Using the IEMOCAP dataset revealed difficulty distinguishing head nodding from shaking due to camera angle affecting nose landmark tracking.,,,,,,,,,,,,,,,
MONAH system offers the widest annotation scope but lacks within-talkturn silence and overlapping talk representation.,,,,,,,,,,,,,,,
"MONAH system achieved higher System Usability Scale scores than human-annotated Jefferson transcripts.""",Proposals are based only on experience with multi-modal narrative systems.,,,,,,,,,,,,,,
Limited knowledge of how other systems were built due to lack of literature.,,,,,,,,,,,,,,,
Guidelines assume advanced technological capabilities,which are still evolving.,,,,,,,,,,,,,,
Technical challenges in multimodal feature extraction (e.g.,bit rate,file encoding,synchronization).,,,,,,,,,,,,
Manual annotation can introduce individual biases.,,,,,,,,,,,,,,,
"Limited control over data capture in some datasets.""",The proposed guidelines offer a valuable starting point for designing effective and user-friendly automated multimodal annotation systems.,,,,,,,,,,,,,,
Limitations include reliance on experience with multimodal narrative systems,limited knowledge of other systems' development,and assumptions about current technological capabilities.,,,,,,,,,,,,,
Recommendations emphasize considering user needs,ethical implications,"and system resilience.""",Limited understanding of how other automated multimodal annotation systems are built due to lack of available literature.,,,,,,,,,,,,
Guidelines assume advanced technological capabilities (e.g.,computer vision,3-D modeling),which are still evolving and may affect effectiveness.,,,,,,,,,,,,
"Need for balancing technical versus layman annotations to ensure both precision and user accessibility.""","Future research should address gaps such as limited knowledge of how other systems are built, evolving technological capabilities in computer vision and 3-D modeling, and the need for user studies to assess the effectiveness of annotation presentation. Integration of gesture, gaze, and laughter detection tools also warrants further exploration.",No information available,,"The objectives of the study are to design and build an automated multimodal annotation system that can interpret various modalities according to user needs, reduce individual annotation biases, improve accessibility for sensory-impaired users, and address challenges in cue selection, customization, and annotation accuracy.",,,,,,,,,,,
An ecosystem for personal knowledge graphs: A survey and research roadmap,"Skjæveland Martin G., Balog Krisztian, Bernard Nolwenn, Łajewska Weronika, Linjordet Trond",2024,reference-manager,10.1016/j.aiopen.2024.01.003,,"The paper surveys personal knowledge graphs (PKGs), highlighting gaps and challenges in practical deployment. Key insights include: lack of consensus on PKG definition, need for holistic ecosystem approaches, and difficulty integrating existing components. The authors propose a new PKG definition emphasizing individual data ownership and personalized services.",,,,Comprehensive survey: The study systematically reviews and synthesizes existing research on personal knowledge graphs (PKGs).,,"How can the challenges of constructing, populating, and managing Personal Knowledge Graphs (PKGs) from diverse and unstructured data sources be addressed to enable effective integration, standardization, and adoption within a PKG ecosystem?","The paper surveys current research on personal knowledge graphs (PKGs), aiming to clarify definitions, identify challenges, and propose a unifying architecture. It highlights gaps in holistic approaches, emphasizes data ownership and personalized services, and notes difficulties in integrating PKG components. Establishing standards and adoption remain key challenges.","The research goal is to survey and synthesize work on personal knowledge graphs (PKGs); the approach is a comprehensive literature review focusing on population, representation, and utilization; the principal finding is a lack of consensus on PKG definitions and the need for holistic, integrated solutions.",
Comparative analysis: It explicitly compares different definitions and approaches to PKGs,proposing a new definition and unifying architecture.,,,,,,,,,,,,,,
Categorization framework: Research is organized and evaluated using a PKG ecosystem framework with aspects like population,representation/management,"and utilization.""",,There is no clear consensus on the definition of a personal knowledge graph (PKG); the authors propose a new definition emphasizing individual data ownership and personalized services.,,,,,,,,,,,
Key challenges include lack of holistic approaches,integration difficulties,and the need for PKG standards and adoption by service providers.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""",The paper identifies the establishment and adoption of PKG (Personal Knowledge Graph) standards as the most important open challenges.,,,,,,,,,,,,,,
Synchronizing updates between PKG and private data sources is highlighted as a non-trivial challenge.,,,,,,,,,,,,,,,
"No statistical values or measured effects are reported.""",Lack of discussion on the interface between the owner and the PRKG.,,,,,,,,,,,,,,
Administrative service for direct user interaction with PKG is largely disregarded.,,,,,,,,,,,,,,,
Limited details on how PKGs are used for personalization in downstream applications.,,,,,,,,,,,,,,,
"Underdeveloped integration and interaction among PKG ecosystem components.
Potential for biases",such as confirmation bias and data bias,in personalized services.,,,,,,,,,,,,,
Insufficient focus on representation,management,and synchronization of knowledge in PKGs.,,,,,,,,,,,,,
Challenges in synchronizing updates between PKGs and unstructured private data sources.,,,,,,,,,,,,,,,
"Need for PKG standards and their adoption by service providers remains an open challenge.""",There is no clear consensus on the definition of a personal knowledge graph (PKG); a new definition is proposed focusing on individual data ownership and personalized services.,,,,,,,,,,,,,,
Research on PKGs lacks a holistic approach,with some aspects receiving more attention than others.,,,,,,,,,,,,,,
Integrating existing building blocks into practical PKG solutions is challenging,especially regarding synchronization with private,unstructured data.,,,,,,,,,,,,,
Recommendations include considering the broader ecosystem,improving synchronization methods,"and clarifying user responsibilities in sensitive domains.""","Lack of a comprehensive, holistic approach that considers the entire PKG ecosystem rather than isolated aspects.",,,,,,,,,,,,
Difficulty in integrating existing building blocks into practical,unified PKG solutions.,,,,,,,,,,,,,,
"Need for establishing PKG standards and ensuring their adoption by service providers.""","The study suggests future research should focus on establishing PKG standards, integrating PKG components holistically, addressing administration challenges (especially with sensitive data), synchronizing updates between PKGs and original data sources, and exploring incentive structures and regulatory controls for PKG ecosystem adoption.",,,"The objectives of the study are to identify and address challenges in extracting and synchronizing facts from free text clinical notes into a PKG, establish PKG standards, promote their adoption by service providers, and emphasize the need for a unifying architecture for PKG ecosystems covering population, representation and management, and utilization.",,,,,,,,,,,
Embedding Temporal Knowledge Graphs in a Product Space of Heterogeneous Geometric Subspaces,"Pan Jiaxin, Nayyeri Mojtaba, Li Yinan, Staab Steffen",2023,reference-manager,,,"Implementation Insights highlight that some factors considered during training were missed during testing, causing bias in scores. The re-implementation revealed much lower LCGE results than originally reported (e.g., ICEWS14 MRR: 61.6 vs. 92.5). HGE improves backbone models efficiently, especially on dense datasets, with fewer parameters.",,,,Re-implementation of baseline models: The study re-implemented codes for comparison and attached them in the supplementary material.,,"How can the proposed HGE model effectively represent and predict complex temporal patterns, such as temporal (anti-)symmetry, temporal inverse, evolution, and temporary relations, in temporal knowledge graphs compared to existing baseline models?","The paper aims to improve temporal knowledge graph embeddings by introducing HGE, a temporal-geometric attention mechanism. Using attention-based product space and vector sharing, HGE efficiently fuses relational and temporal information. Experiments show HGE consistently enhances backbone models, especially on dense datasets, demonstrating improved link prediction performance.","The research goal is to improve temporal knowledge graph (TKG) link prediction; the approach introduces HGE, a temporal-geometric attention mechanism, which, when added to various backbone models, consistently enhances performance—especially on dense datasets—demonstrating HGE’s effectiveness and efficiency in combining temporal and relational information.",
Ablation study: Experiments were conducted on the backbone TNT-ComplEx to assess the effectiveness of each component.,,,,,,,,,,,,,,,
Link prediction evaluation: The model was evaluated using the link prediction task,"generating candidate quadruples and comparing results.""","The research is partially reproducible. The authors re-implemented the codes and attached their implementation in the supplementary material’s code/LCGE new folder. However, for some baselines (DyERNIE and HSAE), code is incomplete or not published, limiting full reproducibility.","The HGE module consistently improves backbone models’ performance on all datasets, with MRR increases up to 8.8% (TNTComplEx+HGE on GDELT).",,,,,,,,,,,,
HGE achieves comparable results to TLT-KGE with about half the parameters,showing higher efficiency.,,,,,,,,,,,,,,
"No explicit p-values or statistical significance values are reported.""","Primary outcomes focus on link prediction performance (MRR, Hits@1/3/10) across datasets (ICEWS14, ICEWS05-15, GDELT, Wikidata12k).",,,,,,,,,,,,,,
Adding HGE to backbones (TeRo,TComplEx,TNTComplEx,TLT-KGE) consistently improves results,especially on dense datasets (e.g.,,GDELT: TeRo+HGE Hits@10 improves by 31.1%).,,,,,,,,,
On ICEWS05-15 (sparse),HGE sometimes decreases performance (TeRo MRR: -1.5%).,,,,,,,,,,,,,,
TNTComplEx+HGE achieves results comparable to TLT-KGE with half the parameters.,,,,,,,,,,,,,,,
"Statistical values: Improvements range from 0.1% to 31.1% depending on model/dataset/metric.""",References considered during training but missed during testing cause bias in final scores and rankings.,,,,,,,,,,,,,,
Inconsistent inference issues exist in LCGE’s original code (see Appendix J).,,,,,,,,,,,,,,,
DyERNIE baseline not included due to incomplete code for hyperbolic spaces.,,,,,,,,,,,,,,,
"HSAE baseline not included because the author did not publish the codes.""","HGE consistently improves backbone model performance on all datasets, especially on dense datasets like GDELT.",,,,,,,,,,,,,,
The temporal-geometric attention mechanism in HGE effectively combines relational and temporal information.,,,,,,,,,,,,,,,
HGE achieves comparable results to more complex models with fewer parameters,demonstrating efficiency.,,,,,,,,,,,,,,
Recommendation: Use HGE to enhance temporal knowledge graph models,"particularly for dense datasets.""","The need for more consistent and reliable evaluation metrics and implementations, as inconsistent inference issues were found in LCGE’s original code.",,,,,,,,,,,,,
Limited availability of complete and reproducible code for some baseline models (e.g.,DyERNIE,HSAE),hindering fair comparison.,,,,,,,,,,,,
"Further exploration of HGE’s representational approach beyond parameter increase for improved efficiency and effectiveness.""",,,,,,,,,,,,,,,
Skefl: Single-Key Homomorphic Encryption for Secure Federated Learning,Zhao Dongfang,2023,reference-manager,,,Summary of Implementation Insights:,,,,,,,,,
Skefl achieves over 80% accuracy within 10 rounds,matching FedAvg and FedHE. Skefl Dist() adds negligible overhead,while Skefl Aggr() adds 24–38% more computation time than FedAvg. ATSS.Split() is the slowest ATSS step,"especially on complex datasets. No new insights beyond these findings are stated.""",,,"The research goal is to enhance privacy and accuracy in Federated Machine Learning using the Skefl protocol; the approach employs Asymmetric Threshold Secret Sharing (ATSS), and results show Skefl achieves over 80% accuracy within 10 rounds, matching baselines, while maintaining privacy and incurring moderate computational overhead.",Comparison of aggregation algorithms: The study compares the proposed Skefl protocol with baseline algorithms FedAvg and FedHE for accuracy in federated machine learning.,,,,,,"How can homomorphic encryption be efficiently and securely applied in federated learning to prevent model leakage from collusion, while maintaining high accuracy and practical performance?",,"The paper introduces Skefl, a protocol for secure and efficient federated learning using homomorphic encryption and secret sharing. Through rigorous experiments on CloudLab, Skefl achieves over 80% accuracy within 10 aggregation rounds, balancing strong privacy guarantees with competitive performance and manageable computational overhead compared to baseline methods."
Experimental testbed: Experiments are conducted on a 10-node CloudLab cluster,with repeated trials and average results reported.,,,,,,,,,,,,,,
Asymmetric Threshold Secret Sharing (ATSS): The study evaluates execution times of ATSS primitives (ATSS.Split(),ATSS.Combine(),"ATSS.Aggregate()) across different datasets.""",,"Skefl, FedAvg, and FedHE all rapidly converge to over 80% accuracy within 10 aggregation rounds across MNIST, FMNIST, CIFAR-10, and SVHN datasets.",,,,,,,,,,,
Skefl Aggr() incurs 24%–38% more computational overhead than FedAvg,while Skefl Dist() has negligible overhead.,,,,,,,,,,,,,,
"No p-values or statistical significance measures are reported.""","Skefl, FedAvg, and FedHE all achieved over 80% accuracy within 10 aggregation rounds on MNIST, FMNIST, CIFAR-10, and SVHN datasets.",,,,,,,,,,,,,,
ATSS.Split() had the longest execution time among ATSS primitives,especially for complex datasets (CIFAR-10,SVHN: tens of seconds).,,,,,,,,,,,,,
Skefl Dist() incurred negligible overhead,matching FedAvg efficiency.,,,,,,,,,,,,,,
"Skefl Aggr() added 24%-38% overhead compared to FedAvg.""",No explicit limitations or shortcomings are stated in the provided context.,,,,,,,,,,,,,,
"No self-reported problems or suggestions for further research are mentioned.
,Skefl achieves over 80% accuracy within 10 aggregation rounds",matching FedAvg and FedHE,while preserving privacy.,,,,,,,,,,,,,
Skefl Dist() adds negligible computational overhead,similar to FedAvg; Skefl Aggr() incurs 24–38% more overhead.,,,,,,,,,,,,,,
ATSS primitives are efficient on simple datasets but slower on complex ones.,,,,,,,,,,,,,,,
Skefl balances accuracy,privacy,"and computational cost effectively.""",Limited exploration of defense strategies against advanced model poisoning attacks in federated learning.,,,,,,,,,,,,
Need for more efficient homomorphic encryption schemes that balance security and performance.,,,,,,,,,,,,,,,
Insufficient evaluation of protocols like Skefl in diverse,"real-world federated learning environments.""",,,,,"The objectives of the study are to present Skefl, a new protocol for efficient and secure homomorphic encryption in federated learning, establish its provable security, and implement it in a practical federated learning framework, demonstrating promising results across various benchmarks.",,,,,,,,,
MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,"Yue Xiang, Ni Yuansheng, Zhang Kai, Zheng Tianyu, Liu Ruoqi, Zhang Ge, Stevens Samuel, Jiang Dongfu, Ren Weiming, Sun Yuxuan, Wei Cong, Yu Botao, Yuan Ruibin, Sun Renliang, Yin Ming, Zheng Boyuan, Yang Zhenzhu, Liu Yibo, Huang Wenhao, Sun Huan, Su Yu",2024,reference-manager,,,"Implementation Insights highlight the importance of standardized data annotation and quality assurance, effective empirical evaluation pipelines, and user-friendly annotation interfaces. OCR and captioning enhancements did not significantly improve text-only model performance, emphasizing the need for models that integrate both textual and visual information for complex multimodal tasks.",,,,"Data Collection: The benchmark was collected in three stages, starting with reviewing common university materials.",,"What processes and protocols are most effective for collecting, annotating, and validating a large-scale, high-quality multimodal question dataset across diverse academic subjects?","The paper introduces a large-scale, multi-discipline, multimodal benchmark for expert-level AGI. The main objective is to evaluate models' understanding and reasoning across diverse subjects using annotated questions, including those with multiple images. The methodology involves rigorous data collection, annotation, and validation. Results highlight subject distribution and model evaluation. The benchmark advances multimodal AGI assessment.","The research goal is to create a massive multi-discipline multimodal benchmark (MMMU) for expert AGI; the approach involves collecting, annotating, and validating diverse questions with images across subjects; the principal finding is a comprehensive dataset enabling robust evaluation of multimodal understanding and reasoning models.",
Image Type Distribution Analysis: A horizontal bar chart was used to visually represent the number of samples in each image category.,,,,,,,,,,,,,,,
"Model Performance Evaluation: The performance of selected models was reported on 30 different image types using quantitative accuracy metrics.""","The research describes a rigorous, multi-stage data collection and validation process, including detailed annotation protocols and quality control. There is mention of dataset release, but no explicit information or link to the source code for the project is provided.","The primary findings include comprehensive evaluation results of multiple models (e.g., GPT-4V, Qwen-VL-7B, Fuyu-8B) across 30 image types, with GPT-4V achieving the highest scores (e.g., 55.7 overall, up to 100.0 on advertisements).",,,,,,,,,,,,,
Quantitative results show detailed model performance per image type,such as GPT-4V scoring 61.8 on tables and 75.9 on paintings.,,,,,,,,,,,,,,
"No statistical significance (p-values) or explicit conclusions are provided in the context.""","The primary outcome measured was the incidence density (/1000 person years) of hand, foot, and mouth disease in the placebo group.",,,,,,,,,,,,,,
Result: Incidence density in the placebo group was 19.3 per 1000 person years.,,,,,,,,,,,,,,,
"Statistical calculation: (94 new cases / 4873.0 person years) × 1000 = 19.3.""",Lack of specialized knowledge leads to domain-specific perceptual errors (29%).,,,,,,,,,,,,,,
Flawed reasoning causes significant errors (26%).,,,,,,,,,,,,,,,
Textual Understanding Error (6%),Rejection to Answer (3%),Annotation Error (2%),Answer Extraction Error (1%).,,,,,,,,,,,,
Over-prioritization of textual information over visual inputs.,,,,,,,,,,,,,,,
Some subjects excluded due to lack of multimodal problems.,,,,,,,,,,,,,,,
"Data contamination risk addressed but remains a concern.""","The study provides detailed examples and protocols to guide annotators in handling various question types, including those with multiple images.",,,,,,,,,,,,,,
Accurate file naming and referencing are emphasized for multi-image questions.,,,,,,,,,,,,,,,
The appendix serves as a key reference for annotation consistency and quality.,,,,,,,,,,,,,,,
"No explicit recommendations beyond annotation guidance are provided.""","There is a significant performance gap between open-source and closed-source models, especially in complex reasoning tasks.",,,,,,,,,,,,,,
OCR and captioning enhancements do not significantly improve text-only multimodal models,highlighting the need for better integration of textual and visual information.,,,,,,,,,,,,,,
Models perform worse in disciplines requiring intricate perception and complex reasoning,"indicating a need for improvement in these areas.""","Future research should address gaps in visual perception, knowledge representation, reasoning abilities, and multimodal joint understanding. Key areas include improving grounding tasks, enhancing complex reasoning (especially with lengthy chains or calculations), and mitigating biases from manual curation and subject focus.",,,,,,,,,,,,,
"A Comprehensive Survey on Automatic Text Summarization with Exploration of LLM-Based Methods
Abstractive Methods: Generate new summary sentences not present in the original text
Hybrid Methods: Combine extractive and abstractive approaches","Zhang Yang, Jin Hanlei, Meng Dan, Wang Jun, Tan Jinghua
using structured models (tree/graph-based) or generative models (seq2seq
first extracting key sentences and then refining or rewriting them with an abstractive model.""","2025
RNNs
The research uses an automated, reproducible pipeline for collecting and categorizing ATS papers and datasets, including web crawling and LLM-based filtering. Core and supplementary datasets are mostly open source and accessible via provided URLs. No explicit source code for the project is mentioned.","reference-manager
Transformers).
The paper presents an automated algorithm combining keyword-based searches and Large Language Model (LLM) filtering, improving the accuracy and efficiency of collecting ATS (Automatic Text Summarization) research papers.",,,"Implementation Insights highlight the development of automated frameworks for fact-checking summaries, implicit-based text exploration systems for healthcare, and models integrating clinical terms for improved summarization. The paper also introduces an automated retrieval algorithm for collecting ATS research, adaptable to other domains, and emphasizes LLM-based summarization advancements.",,,,Extractive Methods: Select important sentences or phrases directly from the original text using unsupervised (statistical ranking) or supervised (machine learning/deep learning classification) techniques.,,"What are the latest advancements, challenges, and methodologies in Automated Text Summarization (ATS), including dataset collection, categorization, evaluation metrics, and the impact of Large Language Model (LLM)-based approaches across various domains such as news, scientific papers, novels, and blogs?","The paper reviews Automatic Text Summarization (ATS), focusing on extractive, abstractive, and hybrid methods. It examines statistical, clustering, and graph-based techniques, highlighting their strengths and weaknesses. The main objective is to improve information retrieval efficiency. The study concludes that method choice depends on resource needs and application context.","The research goal is to provide a comprehensive review of Automated Text Summarization (ATS), the key method is an automated paper retrieval algorithm combining keyword search and LLM-based prompting, and the principal finding is that recent LLM-based methods have significantly advanced ATS flexibility and performance.",
"Datasets are categorized into """"""""core"""""""" (over 100 citations) and """"""""supplementary"""""""" groups",with manual screening ensuring classification accuracy.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""
Results: Core datasets are central and widely used; supplementary datasets offer diversity for specific challenges.","Primary outcomes: The paper identifies and categorizes ATS (Automatic Text Summarization) datasets into """"core"""" (over 100 citations) and """"supplementary"""" (fewer than 100 citations) groups, summarized in Tables 2 and 3.",,,,,,,,,,,,,,
"Measured effects: No statistical values reported.""",LLMs may not generalize well to domain-specific tasks; need for better domain adaptation.,,,,,,,,,,,,,,
LLMs are black-box models,limiting explainability and interpretability.,,,,,,,,,,,,,,
Difficulty handling extremely long documents; risk of truncation or missing information.,,,,,,,,,,,,,,,
Current methods do not address multimodal summarization.,,,,,,,,,,,,,,,
Potential for bias and fairness issues in summaries.,,,,,,,,,,,,,,,
Limited flexibility and scalability in some LLM-based methods.,,,,,,,,,,,,,,,
Efficiency concerns and increased computational cost.,,,,,,,,,,,,,,,
“Hallucination” (generation of inaccurate content) remains a challenge.,,,,,,,,,,,,,,,
Need for improved evaluation methods for consistency,factual accuracy,"and interpretability.""","Extractive, abstractive, and hybrid summarization methods each have distinct strengths and limitations.",,,,,,,,,,,,
LLM-based methods offer flexible,accurate,and scalable summarization but raise concerns about bias and fairness.,,,,,,,,,,,,,
Future research should focus on mitigating bias and enabling interactive,personalized summarization.,,,,,,,,,,,,,,
"Hybrid approaches can provide more comprehensive summaries by combining extractive and abstractive techniques.""","Optimizing the efficiency of large language models (LLMs) for summarization, including model distillation, pruning, and quantization to reduce computational cost.",,,,,,,,,,,,,,
Developing more objective,comprehensive,and accurate evaluation metrics for ATS.,,,,,,,,,,,,,
"Expanding research on LLM-based summarization techniques
Paper Title (use style: paper title)","including in-context learning
Nouvanty Vanya, Suryanto T., Faroqi A.","prompt engineering
2023","and few-shot learning.""
reference-manager","Future research should address robust domain adaptation, improve explainability, handle longer documents, enable multimodal summarization, mitigate bias and fairness issues, optimize scaling and efficiency, and develop interactive or personalized summarization methods. These directions stem from current limitations and emerging questions in LLM-based summarization.",,"The implementation in \[56] uses motion sensors and simulation software to assess and improve workplace ergonomics. By analyzing movement data, small changes—like adjusting cart height or tool type—were shown to enhance ergonomic safety. This demonstrates the value of integrating real movement data with digital simulations for ergonomic optimization.",,,,"Literature and patent review: Searches were conducted in Google Scholar and Web of Science, collecting and screening papers and patents based on relevance and implementation of Human Digital Twin (HDT).",,"What are the current developments, applications, benefits, and legal challenges of Human Digital Twins (HDTs), and how can future research address their limitations to maximize their impact across various sectors, especially healthcare?","This paper surveys the Human Digital Twin (HDT) concept, aiming to provide a comprehensive overview of HDT definitions, differences from Digital Twins (DT), and applications. Using literature, patents, and industrial projects, it finds HDTs have strong potential in healthcare and other fields, improving decision-making and efficiency. Data privacy remains a key concern.","The objectives of the study are to enhance the efficiency of information retrieval and analysis using Automated Text Summarization (ATS), with specific aims to develop and evaluate ATS methods for summarizing news articles, novels, and scientific papers, and to organize and categorize relevant ATS datasets.
The research goal is to comprehensively review Human Digital Twin (HDT) efforts; the approach involves systematic literature and patent analysis; the principal finding is that HDTs are rapidly growing, especially in healthcare, with strong potential to improve decision-making, process optimization, and safety across multiple fields.",
Dual reviewer assessment: Two reviewers independently evaluated papers for eligibility,implementation,and quality,assigning scores.,,,,,,,,,,,,
"Data-driven selection: Only high-quality papers (final score ≥ 3) were included in the survey.""",,"Human Digital Twins (HDTs) are a rapidly growing research area, especially in healthcare, with 9 deployments identified, showing broad applications from medical diagnostics to sports performance tracking.",,,,,,,,,,,,,
HDTs demonstrate potential to improve decision-making,optimize processes,and enhance safety and efficiency across healthcare,manufacturing,transportation,,and sports.,,,,,,,,,
"No statistical significance (p-values) or quantitative results are reported in the provided context.""",Primary outcomes include improved accuracy and success rate of brain aneurysm surgeries using interactive visualization and simulation tools.,,,,,,,,,,,,,,
The Simulia Living Heart project aims to develop highly accurate personalized digital human heart models for education,training,device design,testing,diagnosis,,and regulatory science.,,,,,,,,,
"No specific statistical values or measured effects are provided.""","Legal concerns regarding data privacy, ownership, and protection.",,,,,,,,,,,,,,
Unclear data ownership and consent issues for HDT creation and use.,,,,,,,,,,,,,,,
Potential for bias and discrimination in decision-making using HDTs.,,,,,,,,,,,,,,,
Need for improved cybersecurity due to increasing cyber threats.,,,,,,,,,,,,,,,
Lack of transparency in data usage and sharing.,,,,,,,,,,,,,,,
Weak security in connected devices (IoH).,,,,,,,,,,,,,,,
Ethical and legal management frameworks are lacking.,,,,,,,,,,,,,,,
Limited research in expanding HDT applications beyond healthcare and manufacturing.,,,,,,,,,,,,,,,
Need for improved HDT accuracy and realism.,,,,,,,,,,,,,,,
"Technology maturity is still required for broader adoption.""","HDTs (Human Digital Twins) are a rapidly growing research area with strong potential to transform healthcare, manufacturing, and transportation by simulating human bodies and behaviors.",,,,,,,,,,,,,,
Healthcare is the most promising and active field for HDT deployment,with 9 initiatives identified in this survey.,,,,,,,,,,,,,,
Key legal concerns include data privacy,ownership,and potential bias or discrimination in decision-making.,,,,,,,,,,,,,
Future research should focus on improving HDT accuracy,realism,"and modeling techniques.""","Expansion of HDT applications to new fields and sectors beyond healthcare and manufacturing, identifying areas with high commercial and societal impact.",,,,,,,,,,,,
Improvement of HDT accuracy and realism to better mimic real human systems for more useful and relevant applications.,,,,,,,,,,,,,,,
Addressing cybersecurity,data privacy,and ethical/legal concerns to ensure responsible,secure,"and compliant HDT adoption.""",,"Future research should focus on improving HDT (Human Digital Twin) accuracy and realism, expanding HDT applications to new sectors, addressing legal issues like data privacy and ownership, and overcoming current limitations to enable wider adoption and impactful human-digital interactions.",,,,,,,"The objectives of the study are to provide a comprehensive overview of current Human Digital Twin (HDT) efforts, explain basic Digital Twin (DT) concepts, introduce HDT and its differences from DT, and review scientific literature, patents, and industrial initiatives related to HDT.",,
A Survey on Accuracy-oriented Neural Recommendation: From Collaborative Filtering to Information-rich Recommendation,"Wu Le, He Xiangnan, Wang Xiang, Zhang Kun, Wang Meng",2021,reference-manager,,,"Implementation Insights highlight that the influence of historical items depends on the target item, suggesting flexible model design. Graph-based models perform well without complex activation functions. Reproducibility and fair evaluation remain challenges, emphasizing transparent experimental settings and unified benchmarks. Adjusting model components allows adaptation to various recommendation scenarios.",,,,"Path Based Methods: Use meta-paths and paths to capture high-order connections between users and items, converting these paths into embedding vectors for recommendation.",,"What are the key approaches, challenges, and future directions in recommendation modeling for recommender systems with a focus on accuracy, including collaborative filtering, content-enriched, and context-aware models?","This survey reviews recommender system modeling with a focus on accuracy. It categorizes and summarizes key collaborative filtering and neural models, discusses evaluation frameworks, and highlights the need for transparent, reproducible research. The paper concludes by emphasizing benchmarking, graph-based models, and future research directions for improved recommendation systems.","The research goal is to survey recommender system modeling for accuracy, using a comprehensive paper collection from top venues and keywords; the approach organizes methods by modeling type and scenario; the principal finding is a taxonomy that aids understanding and design of accurate recommendation models.",
Attention Mechanism: Selects the most relevant information from auxiliary data to enhance representation learning in recommendation models.,,,,,,,,,,,,,,,
"Graph Learning (GNN-based methods): Utilizes graph structures to obtain structural information and high-order correlations for improved recommendations.""","Reproducibility in recommendation research is challenging due to varying datasets, experimental settings, and model sensitivity. While some datasets (e.g., MIND, Yelp) and evaluation frameworks exist, concerns remain about reproducibility and published claims. Releasing code, datasets, and settings is encouraged, but no specific source code is provided.","The survey systematically reviews neural recommender models, categorizing them into collaborative filtering, content enriched, and temporal/sequential models, and summarizes influential research and main contributions.",,,,,,,,,,,,,
It highlights the need for transparent experimental settings,reproducibility,and standardized benchmarking datasets for fair and accurate evaluation.,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the provided context.""","No explicit primary outcomes, results, or measured effects (including statistical values) are reported in the provided context.",,,,,,,,,,,,,,
The context discusses challenges in benchmarking,evaluation,"and reproducibility in recommender systems but does not present specific experimental results or statistical findings.""",Potential biases in algorithmic decision processes.,,,,,,,,,,,,
Long-tailed items are less likely to be recommended.,,,,,,,,,,,,,,,
Multi-stakeholder concerns (explainability,fairness,balance) are not fully addressed.,,,,,,,,,,,,,
Reproducibility issues due to model tuning and diverse experimental setups.,,,,,,,,,,,,,,,
Difficulty in fair performance comparison across models and datasets.,,,,,,,,,,,,,,,
"Lack of unified benchmarking datasets.""",Transparent experimental settings and reproducibility are crucial for fair comparison in recommender systems.,,,,,,,,,,,,,,
There is an urgent need for large,up-to-date benchmarking datasets and unified evaluation frameworks.,,,,,,,,,,,,,,
Future research should focus on explainability,fairness,and multi-objective goals in recommendation models.,,,,,,,,,,,,,
"Encouraging theoretical studies and reproducibility analysis is recommended.""","The need for a large, up-to-date benchmarking recommendation dataset to track state-of-the-art models and enable fair comparisons.",,,,,,,,,,,,,,
Advancing graph-based models and self-supervised learning for diverse recommendation scenarios.,,,,,,,,,,,,,,,
Improving reproducibility through transparent experimental settings,code and dataset release,"and unified evaluation frameworks.""","Future research should focus on: creating large, up-to-date benchmarking datasets; developing unified evaluation frameworks; improving reproducibility and transparency; designing graph-based and self-supervised models; addressing fairness, explainability, and multi-stakeholder balance; and conducting theoretical and reproducibility analyses in recommender systems.","The context describes survey and review studies, not primary research studies. The survey is comprehensive, covering various recommendation modeling methods, data types, and evaluation frameworks. It is not randomized, controlled, blinded, or experimental; rather, it is a systematic literature review and taxonomy of existing methods.",,,,,,,,,,,"The objectives are to advance recommender systems by improving benchmarking, ensuring fair and reproducible comparisons, exploring graph-based and self-supervised learning models, addressing multi-objective goals like explainability and fairness, and integrating auxiliary data to enhance recommendation performance."
GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large Language Models,"Liao Ruotong, Jia Xu, Li Yangzhe, Ma Yunpu, Tresp Volker",2024,reference-manager,,,"GenTKG integrates temporal logical rule-based retrieval (TLR) and few-shot instruction tuning (FIT), both boosting performance. Index prompts outperform lexical ones, easing data leakage concerns. GenTKG remains strong even with minimal training data and few-shot samples, showing robust generalizability and efficient alignment for temporal relational forecasting with large language models.",,,,Cross-domain generalizability experiments: 16 cross-checking experiments were conducted to evaluate model performance across different datasets.,,"How can the GenTKG framework align large language models with generative temporal knowledge graph forecasting, and how does its performance, generalizability, and component effectiveness compare to existing embedding-based, rule-based, and LLM-based methods?","The paper introduces GenTKG, a framework aligning large language models (LLMs) with generative forecasting on temporal knowledge graphs (tKGs). Using few-shot, parameter-efficient instruction tuning, GenTKG achieves superior temporal link prediction (Hits@1/3/10) across four benchmark datasets, outperforming conventional methods. GenTKG generalizes well and reduces training needs.","The research goal is to improve temporal knowledge graph forecasting; the approach uses a retrieval-augmented large language model (GenTKG) with temporal logical rule-based retrieval and parameter-efficient instruction tuning; results show GenTKG achieves state-of-the-art performance, surpassing embedding-based, rule-based, and LLM-based methods on key benchmarks.","Temporal knowledge graphs, generative forecasting, recommendation systems, anomaly detection, knowledge graph reasoning, large language models, embedding-based models, rule-based methods, zero-shot learning, few-shot inductive learning, entity alignment, in-context learning, open-source datasets, data protection, generalizability."
Few-shot tuning: Experiments used varying numbers of training samples (K = 16,512,1024) to assess the impact on model performance.,,,,,,,,,,,,,
Baseline comparisons: GenTKG was compared against embedding-based,rule-based,"and LLM-based in-context learning methods.""","GenTKG’s reproducibility is supported by open-sourced datasets (ICEWS, GDELT, YAGO) with clear licensing. However, there is no explicit mention or link to the source code for the project itself in the provided context. Experimental settings and implementation details are described for reproducibility.","GenTKG achieves state-of-the-art performance, surpassing embedding-based, rule-based, and LLM-based in-context learning methods on four datasets in Hits@1 and Hits@3, with comparable Hits@10 results.",,,,,,,,,,,
GenTKG outperforms the best embedding-based model (xERTE) and rule-based model (TLogic),with the highest gain on GDELT (over 58% higher Hits@1).,,,,,,,,,,,,,,
"Few-shot experiments show GenTKG consistently improves as training samples increase and outperforms naive ICL with as few as 16 shots; no p-values or statistical significance reported.""","GenTKG achieves state-of-the-art performance, surpassing all conventional methods on four datasets in Hits@1 and Hits@3, with comparable Hits@10.",,,,,,,,,,,,,,
GenTKG outperforms xERTE (embedding-based) on ICEWS14,ICEWS18,GDELT,and Timetraveler on YAGO; highest gain: over 58% higher Hits@1 on GDELT.,,,,,,,,,,,,
Compared to TLogic (rule-based),GenTKG performs better on Hits@1 and Hits@3,with similar Hits@10. Slight Hits@10 drops on ICEWS14/ICEWS18 are attributed to TLogic’s dataset-specific design.,,,,,,,,,,,,,
Statistical values: Relative difference for GDELT Hits@1 is 58.59% (ICEWS18 vs. original),"and for YAGO Hits@1 is -10.77% (ICEWS14 vs. original).""","GenTKG is limited by the input context window of LLMs (e.g., LLaMA2: 4096 tokens, ~50 history facts), which restricts performance.",,,,,,,,,,,,,
Sequential order of events affects performance; non-ascending orders deteriorate results.,,,,,,,,,,,,,,,
"Potential improvements include better retrieval strategies and LLMs with longer context windows.
Inductive",zero-shot,"or few-shot tasks are left for future research.""","GenTKG achieves state-of-the-art performance on four datasets, outperforming embedding-based, rule-based, and LLM-based methods in key metrics (Hits@1, Hits@3).",,,,,,,,,,,,
GenTKG maintains strong generalizability,performing well even with limited training data.,,,,,,,,,,,,,,
Few-shot tuning with as few as 16 samples yields significant improvements.,,,,,,,,,,,,,,,
"Recommendation: Use GenTKG for efficient and accurate temporal relational forecasting.""","GenTKG is limited by the input context window of LLMs, restricting the number of historical facts and thus performance.",,,,,,,,,,,,,,
There is potential to improve GenTKG by integrating better retrieval strategies and using LLMs with longer context windows.,,,,,,,,,,,,,,,
Future work includes exploring GenTKG's generalization in inductive,zero-shot,"or few-shot temporal knowledge graph tasks.""","GenTKG is limited by the input context window of LLMs, restricting history length and performance. Future research should explore better retrieval strategies, prompt LLMs with longer context windows, and investigate applications in inductive, zero-shot, or few-shot tKG tasks.","The study design includes cross-domain generalizability experiments, cross-checking settings, single dataset evaluation, and comparison with baseline methods. It uses parameter-efficient instruction tuning (LoRA), open-source LLMs, and evaluates both in-domain and cross-domain performance without retraining. The design is comparative, multi-dataset, and experimental.",,,,,,,,,,,
Integrating Historical Person Registers as Linked Open Data in the WarSampo Knowledge Graph,"Koho Mikko, Leskinen Petri, Hyvönen Eero",2020,reference-manager,10.1007/978-3-030-59833-4\_8,,"The implementation uses probabilistic record linkage (RL) with weighted metadata comparisons (e.g., names, dates, military unit) to link person records from heterogeneous registers. Aggregated data enriches person profiles in an actor ontology, enabling unified biographies and research. Military rank and unit are key for disambiguation.",,,,"A repeatable data transformation pipeline converts source spreadsheets into RDF, mapping columns to RDF properties.",,"How can heterogeneous historical person registers about Finnish Second World War soldiers be integrated into a unified knowledge graph using probabilistic record linkage, and what metadata fields are most effective for disambiguating person identities in the military history context?","The paper aims to integrate heterogeneous military historical person registers using probabilistic record linkage, demonstrated with data on 100,000 Finnish WWII soldiers in WarSampo. Using logistic regression to weigh metadata fields, the method creates a unified knowledge graph, enhancing biographical research and public exploration. The solution is scalable and openly available.","The paper's research goal is to integrate heterogeneous historical person registers using a probabilistic record linkage approach, resulting in a reconciled knowledge graph; key findings show that metadata fields like military rank and unit are crucial for disambiguation, enabling richer biographical research and open data publication.",
Automatic probabilistic entity linking links records to domain ontologies,improving interoperability.,,,,,,,,,,,,,,
"Probabilistic record linkage assigns weights to metadata field comparisons to disambiguate person records.""
https://github.com/SemanticComputing/warsa-linkers
https://github.com/SemanticComputing/Casualty-linking
https://github.com/SemanticComputing/WarPrisoners""
In RL1","The research is reproducible. Source code for the project is available at:
Record linkage precision was manually evaluated as 1.00 for both RL1 (150/620 links) and RL2 (200/1397 links); recall is considered adequate but not precisely measured.
620 DRs (11%) were linked to existing person instances; in RL2",1255 PRs (30%) were linked.,,,,,,,,,,,,,
Most important comparison weights for matching: date of birth (2.4),family name (2.3),municipality of birth (2.0),military rank (1.8),"given names (1.4).""",,"In RL2 scenario, the most important comparison weights for disambiguating person records are: date of birth (2.4), family name (2.3), municipality of birth (2.0), military rank (1.8), given names (1.4), occupation (0.9), military unit (0.8).",,,,,,,,,
RL1 scenario: 620 death records (DRs) linked to 5611 pre-existing person instances (11%).,,,,,,,,,,,,,,,
RL2 scenario: 1255 prisoner records (PRs) linked to 99,667 pre-existing person instances (30%); 2945 new person instances created.,,,,,,,,,,,,,,
"Aggregated information from multiple sources provides more complete soldier biographies than individual sources.""",Manual recall evaluation is difficult due to the large number of possible pairs and limited information in some records.,,,,,,,,,,,,,,
The data registers (DRs) contain many errors,making it hard to determine true negatives.,,,,,,,,,,,,,,
"Ambiguous or false matches near the threshold complicate confident recall assessment.""","Date of birth is the most important metadata field for disambiguating person records, followed by family name and municipality of birth.",,,,,,,,,,,,,,
Aggregating information from multiple sources enables more complete soldier biographies.,,,,,,,,,,,,,,,
The record linkage method achieved a precision of 1.00 in both evaluated scenarios.,,,,,,,,,,,,,,,
"Integrating person registers into a knowledge graph supports advanced historical research.""",Need for a user perspective for aggregated person instances to enable prosopographical analysis over all persons.,,,,,,,,,,,,,,
Adoption of a blocking strategy for integrating considerably larger person registers to improve scalability.,,,,,,,,,,,,,,,
"Applicability and adaptation of the approach to other studies integrating historical person registers.""","Future research should develop a perspective for aggregated person instances to enable prosopographical analysis over all persons. For larger person registers, adopting a blocking strategy based on metadata values is recommended to reduce comparisons. The approach can also be applied to integrate additional historical person registers.",,,,,,,,,,,,,,
EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs,"Pareja Aldo, Domeniconi Giacomo, Chen Jie, Ma Tengfei, Suzumura Toyotaro, Kanezashi Hiroki, Kaler Tim, Schardl Tao B., Leiserson Charles E.",2019,reference-manager,,,"Implementation Insights are as follows: EvolveGCN uses recurrent neural networks (GRU or LSTM) to evolve GCN weights over time, adapting to dynamic graphs. Summarization condenses node embeddings to match required dimensions. The -O version extends LSTM to matrices. Choice between -H and -O depends on node feature informativeness.",,,,"Use of recurrent neural networks (GRU and LSTM) to evolve GCN parameters over time, enabling temporal modeling in dynamic graphs.",,How can evolving the parameters of a graph convolutional network (GCN) using recurrent neural networks (RNNs) improve the modeling and prediction of dynamic graphs across various tasks and data sets?,"The paper introduces EvolveGCN, aiming to improve dynamic graph learning. Using recurrent neural networks (GRU/LSTM) to evolve GCN parameters over time, experiments on multiple datasets and tasks show EvolveGCN outperforms baselines in edge and node classification, and link prediction. The method is effective for dynamic graphs, especially with informative node features.","The paper's main objective is to capture dynamic graph changes for tasks like edge and node classification using EvolveGCN, which combines graph convolutional networks with recurrent neural networks; experiments show EvolveGCN outperforms baseline methods, confirming its effectiveness.",
Summarization technique to reduce node embeddings into representative vectors for recurrent input,selecting top-k weighted rows.,,,,,,,,,,,,,,
Experiments conducted on synthetic and real-world dynamic graph data sets,"with hyperparameter tuning via validation sets.""",,"EvolveGCN (both versions) outperforms GCN and GCN-GRU in edge classification (BC-OTC, BC-Alpha, Reddit) based on micro-averaged F1 scores, demonstrating its effectiveness.",,,,,,,,,,,,
For node classification (Elliptic),EvolveGCN-O surpasses static GCN but not GCN-GRU; dynamic models perform better,especially for the minority (illicit) class.,,,,,,,,,,,,,
"No explicit p-values or statistical significance values are reported in the context.""","For edge classification (BC-OTC, BC-Alpha, Reddit), EvolveGCN-H and EvolveGCN-O achieved higher F1 scores (micro average) than GCN and GCN-GRU.",,,,,,,,,,,,,,
"For node classification (Elliptic)
For link prediction","EvolveGCN-O outperformed static GCN in minority F1 but was less effective than GCN-GRU; all models' performance dropped after a dark market shutdown.
EvolveGCN achieved the best MAP and MRR on SBM",UCI,and AS; on BC-OTC and BC-Alpha,"it outperformed GCN baselines but was inferior to DynGEM and dyngraph2vec.""",,Precision and recall results are omitted due to space limitations.,,,,,,,,,
All methods perform poorly after the dark market shutdown (step 43),especially non-dynamic models; even dynamic models cannot reliably handle such emerging events.,,,,,,,,,,,,,,
Previous methods require knowledge of all nodes over time,limiting performance on new or disappearing nodes.,,,,,,,,,,,,,,
Node embedding approaches struggle with nodes that frequently appear or disappear,making it hard for RNNs to learn irregular behaviors.,,,,,,,,,,,,,,
"The choice between -H and -O versions depends on the informativeness of node features versus graph structure.""","EvolveGCN outperforms GCN and GCN-GRU in edge classification across all tested datasets, confirming its effectiveness.",,,,,,,,,,,,,,
For node classification on highly imbalanced data,dynamic models (including EvolveGCN-O) are more effective than static GCN,but GCN-GRU performs best.,,,,,,,,,,,,,
All models' performance drops during emerging events,with non-dynamic models affected most.,,,,,,,,,,,,,,
Recommendation: Use dynamic models like EvolveGCN for evolving graph tasks,"and select the version based on node feature informativeness.""","Dynamic models struggle to adapt to sudden, emerging events (e.g., dark market shutdown), leading to degraded performance.",,,,,,,,,,,,,
There is a need for improved methods to handle highly imbalanced classes,especially for minority class detection in node classification.,,,,,,,,,,,,,,
"Further research is needed to enhance model reliability during unexpected real-world events.""",,,,"The objectives of the study are to implement and evaluate EvolveGCN (including the -O version) for dynamic graph tasks such as link prediction, edge classification, and node classification, using various data sets, and to compare its performance against baseline methods using metrics like F1, MAP, and MRR.",,,,,,,,,,,
A Context Model for Personal Data Streams,"Giunchiglia Fausto, Li Xiaoyue, Busso Matteo, Rodas-Britez Marcelo",2022,reference-manager,,,"Implementation Insights focus on representing and managing heterogeneous personal data streams as sequences of """"personal situational contexts""""—each encoding an individual's subjective perspective. Unlike prior work, this approach supports real-time, person-centric services, addressing challenges like data heterogeneity and varying abstraction levels. New insight: Emphasis on runtime exploitation.",,,,Collection of data streams from smartphone sensors and user questionnaires to capture personal situational context.,,"How can personal data streams be represented at the knowledge level as sequences of situational contexts using Knowledge Graphs to enable user-understandable, person-centric services and improved human-machine interaction?","The paper aims to model a person's situational context by representing data streams from sensors and user input as sequences of knowledge graphs (KGs). Using the Smart University dataset, the methodology involves abstract conceptualization, schema definition (ETG), and context graph sequences. The approach enables user-understandable, real-time, person-centric services.",The research goal is to model personal situational context from data streams; the approach uses a knowledge-level representation of sensor and user-provided data collected over time; the principal finding is that this model enables real-time exploitation of heterogeneous data for person-centric services.,"Keywords: Personal Situational Context, Data Streams."
Use of Experience Sampling Method (ESM),where participants report thoughts and behaviors at regular intervals.,,,,,,,,,,,,,,
"Organization and representation of data as sequences of personal situational contexts for real-time analysis.""","The research is reproducible regarding data, as the Smart University (SU) dataset and its detailed description are available for download at https://livepeople.datascientia.eu/dataset/smartunitn2. There is no information provided about the availability of source code for the project.","The paper introduces a model for representing personal situational context using data streams from sensors and user feedback, enabling real-time, person-centric services.",,,,,,,,,,,,,
The Smart University dataset includes 158 students,139,239 annotations,and about one terabyte of data collected over four weeks.,,,,,,,,,,,,
"No statistical significance (p-values) or quantitative analysis results are reported.""",The primary outcome is a proposed model for representing personal situational context using data streams from sensors and user feedback.,,,,,,,,,,,,,,
The model was validated using the Smart University (SU) dataset: 158 students,4 weeks,139,239 annotations,~1 terabyte of data.,,,,,,,,,,,
"No statistical values or measured effects are reported.""","Data are highly heterogeneous (categorical, numerical, natural language, unstructured) and collected at different time frequencies.",,,,,,,,,,,,,,
Data may exist at different levels of abstraction (e.g.,GPS coordinates vs. place names).,,,,,,,,,,,,,,
"Many challenges remain unsolved for real-time exploitation and representation of these data streams.""",The study proposes a model for representing a person's situational context using data from sensors and user input on mobile devices.,,,,,,,,,,,,,,
This model enables knowledge-level representation of time-based data streams.,,,,,,,,,,,,,,,
"It supports person-centric services
Absence of abstract","such as predicting habits and improving human-machine interaction.""
user-level representations of continuously growing data streams for improved Human-Machine interaction.","Lack of methods for representing and managing heterogeneous, multi-format data streams in real time.",,,,,,,,,,,,,
Need for knowledge-level models,like Knowledge Graphs,"to make personal situational context understandable and usable.""","The study highlights the need for future research on representing and managing heterogeneous data streams in real-time, addressing challenges like varying data types, time frequencies, and abstraction levels. Further investigation is needed into supporting person-centric services using personal situational context during data collection.","Case study; observational study; data collected from 158 university students over four weeks using the iLog app; data includes self-reported answers to closed-ended questions every half-hour; no mention of randomization, blinding, controls, or intervention.",,,,,,,,,,,"The objective of the study is to exploit data streams at run-time, as they are collected, to support person-centric services (such as predicting human habits or improving human-machine interaction) by representing input streams as sequences of personal situational contexts."
Knowledge Graphs: The Future of Data Integration and Insightful Discovery,"Mohamed Saher, Farah Kirollos, Lotfy Abdelrahman, Rizk Kareem, Saeed Abdelrahman, Mohamed Shahenda, Khouriba Ghada, Arafa Tamer",2023,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
The paper highlights that knowledge graphs (KGs) and knowledge graph embeddings (KGEs) are crucial for data integration,explainable AI,"and autonomous driving. Detailed KGs improve semantic capture. Dynamic Knowledge Graphs (DKG) enable real-time conceptual change. New insight: High informational detail in KGs enhances both type and relational understanding.""",No information available,"The paper's main objective is to enhance tourism analytics by integrating climate and tourism data using a knowledge graph-based system; the key method involves extracting and organizing data with language models and ontologies, and the principal finding is improved insights into tourism-climate interactions through structured, reusable data integration.",,"The paper explores how knowledge graphs (KGs) enhance data integration and insight discovery. It examines their role in explainable AI, automatic coding, and situation understanding in automated driving. Using various methodologies, the study finds KGs improve explainability, interdisciplinary research, and classification accuracy, especially in complex scenarios.",,,,,,,"Top-down and bottom-up approaches: The top-down approach defines the ontology or data schema first, then extracts knowledge; the bottom-up approach starts with extracting knowledge from data, then defines the ontology based on extracted information.","How can combining domain-specific and causal commonsense knowledge through knowledge graphs and related techniques improve reasoning, robustness, generalization, and explainability in AI systems, particularly for safety-critical applications like autonomous vehicles?",
Relationship extraction using large language models: Unstructured data is prepared and formatted,then a prompt instructs the model (e.g.,Llama 7b) to extract subject-predicate-object triples,ensuring consistent structured output.,,,,,,,,,,,,
Explainability techniques in AI: Knowledge graphs are used for pre-modeling,in-modeling,"and post-modeling explainability to enhance understanding and transparency in AI models.""",,Knowledge graphs improved accuracy by up to 10% compared to using only original tabular data.,,,,,,,,,,,
In autonomous driving,knowledge graphs with the highest informational detail achieved over 95% accuracy in classifying driving situations,outperforming traditional feature vectors.,,,,,,,,,,,,,
"No explicit p-values or statistical significance measures are reported in the context.""",Up to a 10% increase in accuracy when using knowledge graphs compared to only original tabular data.,,,,,,,,,,,,,,
Main results include improved data gathering and classification on climate change,standardization of database formats,and promotion of data interoperability.,,,,,,,,,,,,,
"Addressed heterogeneity in paleoclimatology research using ontological approaches like SWEET ontology.""",Models learn well on knowledge types present in training data but do not generalize well.,,,,,,,,,,,,,,
Prior work focused mainly on perception-based methods,limiting insights from text-based approaches.,,,,,,,,,,,,,,
Further research is suggested to improve generalization and robustness,"especially for safety-critical applications like autonomous vehicles.""","Knowledge graphs enable structured extraction and explainable clustering of entities, improving data integration and insight discovery.",,,,,,,,,,,,,
The ExCut method enhances interpretability by generating human-comprehensible cluster labels.,,,,,,,,,,,,,,,
Knowledge graphs support explainable AI at pre-,in-,and post-modeling stages,aiding transparency and model understanding.,,,,,,,,,,,,
A global Climate Action Knowledge Graph is recommended to connect diverse climate data,"fostering collaboration and informed decision-making.""","Achieving accurate, complete, and consistent knowledge graphs from unstructured data remains a major challenge; current automated methods still require human supervision and validation.",,,,,,,,,,,,,
"Integrating dynamic knowledge graph tools more seamlessly with collaborative platforms and enabling manual edits is needed for improved conceptual change support.
Developing global",interconnected knowledge graphs (e.g.,"for climate action) to link diverse data sources and enhance collaboration and decision-making is a key future direction.""","Future research should focus on improving integration with existing platforms, enabling manual edits in dynamic knowledge graphs, developing global knowledge graphs (e.g., for climate action), and advancing methods for handling updates without full retraining. Addressing current system limitations and supporting continual knowledge improvement are also recommended.",,,,,,,,,,,,"The objectives are to standardize database formats, promote data interoperability, address heterogeneity in paleoclimatology research, and enhance automated driving systems by using knowledge graphs and ontologies for richer semantic representation, improved classification, prediction, and integration of heterogeneous data sources."
Smart Knowledge Transfer using Google-like Search,"Das Partha Pratim, Majumdar Srijoni",2023,reference-manager,,,"Implementation Insights highlight that SMARTKT integrates multiple knowledge sources—such as source code, runtime traces, comments, version and bug trackers, and design documents—using SPARQL queries and word vector semantics for intelligent responses. The prototype supports entity-based, list, and template queries, addressing fragmented knowledge transfer and program comprehension challenges for developers.",,,,Knowledge Primitive Extraction: Extracts atomic units of knowledge (Primitives) from sources using natural language processing and instrumentation frameworks.,,"How can an integrated search framework like SMARTKT extract, associate, and represent knowledge from multiple software sources to improve program comprehension and support maintenance engineers in addressing software maintenance challenges?","SMARTKT is a search framework designed to improve program comprehension by extracting and integrating knowledge from multiple sources into a semantic graph. Using natural language processing and instrumentation, it supports various query types, helping maintenance engineers answer syntax and semantic questions efficiently. The framework aims to reduce software maintenance costs.","The research goal is to improve program comprehension by proposing SMARTKT, a search framework that extracts and integrates knowledge from multiple software sources into a semantic graph, enabling effective syntax and semantic queries; results show SMARTKT provides direct and intelligent responses to various query types for maintenance engineers.","Program Comprehension, Knowledge Transfer, Machine Learning, Natural Language Processing, Semantic Graph"
Mining Comments and Repositories: Mines code comments and analyzes software repositories to extract project management details,bug history,version changes,and developer/tester relationships.,,,,,,,,,,,,
"Surveys and Interviews: Conducts surveys and personal interviews with developers to analyze comprehension challenges and framework requirements.""",,"SMARTKT is a search framework for C/C++ and Python codebases, supporting entity, list, template, and free-form queries to assist maintenance engineers with syntax and semantics.",,,,,,,,,,,,,
SMARTKT extracts and associates knowledge from multiple sources,representing it as a semantic graph to provide direct and intelligent responses,including additional alerts (e.g.,data race).,,,,,,,,,,,,
Surveys and interviews revealed existing tools use limited sources and lack an integrated,"easy-to-use framework; SMARTKT addresses these gaps. No quantitative results or p-values are provided.""",Primary outcomes:,,,,,,,,,,,,,
Identification of knowledge types and their sources relevant to program comprehension (e.g.,software development,application-oriented,version evolution,defect evolution,,project management,,,,,,,,,business specs.).
Observation that existing assistance tools use limited sources and lack an integrated framework.,,,,,,,,,,,,,,,
Results and measured effects:,,,,,,,,,,,,,,,
"No statistical values or quantitative effects reported.""",Available assistance tools consider only limited sources.,,,,,,,,,,,,,,
There is an absence of an easy-to-use integrated framework based on these sources.,,,,,,,,,,,,,,,
"Resources like experienced team members with broad application knowledge are rarely available due to evolving teams and fragmented task distribution.""",SMARTKT is an integrated search framework that aids program comprehension by extracting and associating knowledge from multiple sources.,,,,,,,,,,,,,,
It supports various query types,providing direct responses and additional insights,such as data race alerts.,,,,,,,,,,,,,
SMARTKT bridges missing information across sources and validates application metadata.,,,,,,,,,,,,,,,
Existing tools are limited; SMARTKT addresses the need for a comprehensive,"easy-to-use framework.""",Existing tools consider only limited sources of information for program comprehension.,,,,,,,,,,,,,
There is a lack of an easy-to-use,integrated framework that combines knowledge from diverse sources.,,,,,,,,,,,,,,
"Future research should focus on developing comprehensive frameworks that integrate multiple knowledge sources to aid program comprehension.""","Future research should address the limitation of current assistance tools that consider only limited sources. There is a need for an easy-to-use integrated framework that combines diverse knowledge sources to better support program comprehension and project management in evolving, fragmented software teams.",,,"The objectives of the study are to propose and design the SMARTKT search framework to assist maintenance engineers in program comprehension by extracting, associating, and representing knowledge from multiple software development sources, supporting various query types, bridging missing information, and validating application metadata.",,,,,,,,,,,
"1+1>2: Programming Know-What and Know-How Knowledge Fusion, Semantic Enrichment and Coherent Application","Huang Qing, Yuan Zhiqiang, Xing Zhenchang, Zuo Zhengkang, Wang Changjing, Xia Xin",2023,reference-manager,10.1109/CONF.2023.12345,,"The implementation uses a large language model (UIE framework) to extract entities and semantic relations, making search results interpretable and supporting development needs. It introduces nine API semantic relations, enriches code search, debugging, and optimization, and uncovers five new API and two new task relations not in existing knowledge graphs.",,,,"Selection and filtering of Stack Overflow questions: 8,667 Java API usage questions were filtered by title/content and answer quality, then 100 were selected to cover 10 topics.",,How can the fusion of programming know-what knowledge (API-KG) and know-how knowledge (Task-KG) facilitate the discovery of fine-grained semantic relations and improve the effectiveness of answering API usage questions?,"The paper investigates whether combining know-what (API-KG) and know-how (Task-KG) knowledge helps novice developers solve Java API usage questions. Using a user study with 12 students and analysis of 100 Stack Overflow questions, the study finds that knowledge fusion reveals new semantic relations, improving recommendation effectiveness.","The research goal is to fuse programming know-what (API) and know-how (task) knowledge into a unified knowledge graph using fine-grained semantic relations; the approach extracts and links task attributes and API entities, and the principal finding is that the resulting fused KG enhances code search, debugging, and optimization.",
Manual annotation and extraction: Two experienced master's students independently extracted answer points and annotated questions,with high agreement (Cohen’s Kappa 0.842–0.864).,,,,,,,,,,,,,,
"User study: 12 novice Java students were divided into two groups to compare the effectiveness of the proposed tool versus baseline tools (API-KG and Task-KG) using controlled tasks and time limits.""","The research provides experimental data for replication and describes detailed sampling and annotation procedures. However, there is no mention of source code availability for the project. Thus, reproducibility is supported for data but not for source code.","Among 100 Stack Overflow questions, 206 answer points were identified; 36% (Q1) could be answered using existing knowledge graphs, while 64% (Q2) could not. Cohen’s Kappa for annotation agreement was 0.864.",,,,,,,,,,,,,
Four new implicit semantic relations were found in 78.3% of Q1 answer points: Function Similarity (23.5%),Function Opposite (9.8%),Behavior Difference (21.6%),and Function Replace (45.1%).,,,,,,,,,,,,
Seven implicit semantic relations were identified for RQ3: Function Collaboration (14.3%),Type Conversion (12.5%),Implement Constraint (10.7%),Logic Constraint (7.1%),Efficiency Comparison (5.3%),,Task Align (33.9%),,,,,,,,"Task phrase identification using verb phrases achieved Precision 89.79%, Recall 89.90%, F1-Score 89.84%, and Accuracy 86.56%, outperforming the baseline (Precision 51.36%, Recall 77.80%, F1-Score 61.88%, Accuracy 61.36%).","and Task Overlap (16.1%). No p-values reported."""
API-packet attribute extraction accuracy: 89.72% (Cohen’s kappa: 0.931).,,,,,,,,,,,,,,,
Task-align relation accuracy: 88.63%; task-overlap relation accuracy: 92.47%.,,,,,,,,,,,,,,,
API semantic relation inference accuracies: Function Opposite 1.000,Function Similarity 0.965,Behavior Difference 0.975,Function Replace 0.958,Function Collaboration 0.951,,Implement Constraint 0.953,,,,,,,Efficiency Comparison 1.000.,Type Conversion 1.000,Logical Constraint 0.952
Effectiveness evaluation: Tool answers positively if its answer includes the accepted answer; inter-rater agreement measured by Cohen’s kappa; disagreements resolved by majority-win.,,,,,,,,,,,,,,,
The constructed API-Task KG contains 8,672 API entities,7,806 task entities,916 API semantic relations,,and 7,,,,,,,,Subjective judgment in data annotation may affect internal validity.,"496 task semantic relations."""
Evaluation limited to Java documentation,affecting external validity.,,,,,,,,,,,,,,
NLP tool errors (e.g.,Spacy POS tagging and sentence splitting).,,,,,,,,,,,,,,
User study limited to 30 programming questions.,,,,,,,,,,,,,,,
Possible loss of API package names due to missing import statements.,,,,,,,,,,,,,,,
"Heuristic extraction of API semantic relations lacks generality.""","The proposed approach identifies and utilizes nine types of implicit API semantic relations, improving the interpretability and precision of code search results.",,,,,,,,,,,,,,
The fused API-Task knowledge graph (KG) benefits code search,debugging,and optimization by providing richer semantic connections.,,,,,,,,,,,,,
The approach outperforms baseline tools (API-KG and Task-KG) in helping novice developers solve questions.,,,,,,,,,,,,,,,
"Recommendations include leveraging large language models to enhance entity and relation extraction for broader applicability.""","Current API-KG and Task-KG lack inherent connections, limiting their ability to solve programming issues.",,,,,,,,,,,,,,
There is a need to enrich knowledge graphs with fine-grained semantic relations beyond existing declaration (syntactic) relations.,,,,,,,,,,,,,,,
"Future work should focus on fusing and mutually enriching know-what (API) and know-how (task) knowledge for more effective knowledge discovery.""",,"The study design is an empirical study involving: selection of 100 Stack Overflow Java API usage questions across 10 topics; independent extraction of answer points by two experienced Master’s students (Cohen’s Kappa 0.842 and 0.880); conflict resolution by a PhD student; and a user study with randomized, parallel groups of novice developers.",,The objectives of the study are to: (1) understand the complementary nature of know-what and know-how knowledge in API-KG and Task-KG; (2) explore combining them to discover more fine-grained semantic relations; (3) evaluate the effectiveness and quality of the constructed API-Task KG.,,,,,,,,,,,
"Knowledge Net: Model and System for Accumulation, Representation, and Use of Knowledge","Tushkanova Olga, Samoylov Vladimir",2019,reference-manager,10.1016/j.ifacol.2019.11.351,,"The paper highlights that Knowledge Net uniquely supports multi-aspect entity descriptions, dynamic evolution of concept structures, faceted search, and versioning. It uses a graph data model and is open source. However, details on expert intervention, ontology technologies, and implementation specifics are missing. No complete analogues exist.",,,,"Use of a graph data model for representing knowledge and data, enabling dynamic expansion and flexible relationships.",,"How can a multidimensional, dynamic, and integrity-preserving knowledge and data model be developed and implemented to support accumulation, presentation, and use of knowledge in a single shared information space of a digital enterprise?","The paper introduces the Knowledge Net semantic specification model and its open-source software, enabling organizations to create and expand knowledge graphs for shared information spaces in intelligent enterprises. The model supports multidimensionality and dynamic structure formation. Findings highlight industry interest but note current solutions are insufficient for full digital transformation.","The paper's main objective is to formalize and manage enterprise knowledge using the Knowledge Net model; its key method is a graph-based, multi-aspect semantic specification approach; and its principal finding is that Knowledge Net enables dynamic, expandable, shared information spaces, supporting digital transformation in intelligent enterprises.","Keywords: single shared information space, knowledge model, knowledge representation, aspect, ontology, knowledge graph, graph data model."
Extraction and comparison of concepts from text documents with enterprise ontologies in the xIRBIS-ML subsystem.,,,,,,,,,,,,,,,
Faceted search and similarity-based ranking for object properties,"supporting complex and unstructured data types.""","The research is supported by open source software development, and a prototype web application was created. However, there is no explicit mention of the availability or location of the source code for the project. Reproducibility details beyond this are not provided.","The Knowledge Net model enables accumulation, presentation, and use of knowledge and data in a shared information space, supporting multidimensionality, dynamic evolution, and faceted search.",,,,,,,,,,,,
No complete analogues to Knowledge Net were found,highlighting its novelty; the open source prototype supports key model features.,,,,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported.""","The Knowledge Net software prototype was tested by creating a knowledge graph fragment for an electric vehicle manufacturer, processing about 50 objects and identifying about 200 aspects.",,,,,,,,,,,,,,
Positive feedback was received from Info Wings LLC engineers and several experts in knowledge formalization.,,,,,,,,,,,,,,,
The prototype needs improvement in user experience and performance.,,,,,,,,,,,,,,,
"No statistical values or quantitative effect measures are reported.""","Prototype needs improvement, including user experience and performance issues.",,,,,,,,,,,,,,
Lack of clarity on required expert intervention for concept extraction and ontology processing.,,,,,,,,,,,,,,,
Unclear which technologies are used for ontology creation and processing.,,,,,,,,,,,,,,,
Uncertainty about support for unique identifiers and dynamic ontology changes.,,,,,,,,,,,,,,,
Unclear if applied software development was conducted for proposed approaches.,,,,,,,,,,,,,,,
"Existing developments are insufficient for effective industry implementation.""","The Knowledge Net model enables organizations to independently or collaboratively create knowledge graphs for a shared information space, essential for intelligent enterprises in Industry 4.0.",,,,,,,,,,,,,,
Key advantages include multidimensionality,expandability,and dynamic aspect/property structuring.,,,,,,,,,,,,,
The open source prototype supports core features and received positive expert feedback.,,,,,,,,,,,,,,,
Future releases aim to enhance subject/property descriptions,introduce template-hints,"and implement fuzzy search.""",Unclear level of expert intervention required for concept extraction and ontology comparison in xIRBIS-ML.,,,,,,,,,,,,
Lack of information on technologies used for ontology creation,unique identifier support,and dynamic ontology scheme changes.,,,,,,,,,,,,,
"Insufficient applied software development and industry implementation for proposed intelligent system models.""","Future research should address the extent of expert intervention required in xIRBIS-ML, clarify technologies for ontology creation and processing, and examine support for unique identifiers and dynamic ontology changes. There is also a need for formal methods and software to support evolving, multidimensional knowledge structures in intelligent enterprises.",,,The objectives are:,,,,,,,,,,,
Enable faceted search for object properties and objects with specific characteristics.,,,,,,,,,,,,,,,
Support search for object analogues and ranking by similarity.,,,,,,,,,,,,,,,
Allow description and storage of complex or unstructured data (e.g.,"CAD-models).""",,,,,,,,,,,,,,
CTGNN: Crystal Transformer Graph Neural Network for Crystal Material Property Prediction,"Du Zijian, Jin Luozhijie, Shu Le, Cen Yan, Xu Yuanfeng, Mei Yongfeng, Zhang Hao",2024,reference-manager,,,"The Implementation Insights highlight that CTGNN combines Transformer structures with traditional Graph Neural Networks (GNNs), using an angular encoder to capture angle features. Dual-Transformer modules model both local and broader atomic interactions. CTGNN outperforms models like CGCNN and MEGNET in predicting formation energy and bandgap, especially for perovskites.",,,,Transformer model: Uses multi-head self-attention to process sequential data and learn relationships between sequences.,,"How can the proposed Crystal Transformer Graph Neural Network (CTGNN), which integrates Transformer architectures and traditional graph neural network inductive biases with angular encoding, improve the prediction of material properties compared to existing models?","The paper proposes the Crystal Transformer Graph Neural Network (CTGNN) to improve materials property prediction. CTGNN combines Transformer-based message passing and traditional graph neural network (GNN) inductive biases, using an angular encoder for angle features. Experiments show CTGNN outperforms other models on widely-used materials databases, highlighting its effectiveness.","The research goal is to improve materials property prediction by proposing the Crystal Transformer Graph Neural Network (CTGNN), which combines Transformer-based message capturing and GNN inductive bias; the approach uses dual-Transformer structures with angular encoding, and results show CTGNN outperforms other models on widely-used materials databases.",
CTGNN model: Combines Transformer encoder layers for atom and neighbor features,followed by CGCNN (Crystal Graph Convolutional Neural Network) convolution layers,pooling,and prediction layers.,,,,,,,,,,,,
"Incorporation of topological features: Includes angular and distance (RBF kernel) information in the model.""
Quantitative results: For perovskite formation energy",CTGNN MAE = 0.013,"CTGNN outperforms existing models (CGCNN, MEGNet) in predicting formation energy and bandgap, with lower MAE and higher R² on both JARVIS-DFT and perovskite datasets.
R² = 0.996; for perovskite bandgap",MAE = 0.156,R² = 0.960.,,,,,,,,,,,
"CTGNN provides a more accurate foundation for material property prediction and new material discovery; no explicit p-values reported.""",Primary outcomes measured: Formation energy (Ef) and bandgap (Eg) prediction accuracy.,,,,,,,,,,,,,,
CTGNN model achieved lowest MAE (Mean Absolute Error) and highest R² on both perovskite and Jarvis datasets.,,,,,,,,,,,,,,,
Pero(Ef): CTGNN MAE = 0.013 eV/atom,R² = 0.996,,,,,,,,,,,,,,
Pero(Eg): CTGNN MAE = 0.156 eV,R² = 0.960,,,,,,,,,,,,,,
Jarvist(Eg): CTGNN MAE = 0.469 eV,R² = 0.910,,,,,,,,,,,,,,
"CTGNN outperformed CGCNN and MEGNet by 51.85–59.38% (Ef) and 45.26–47.30% (Eg) on perovskite dataset.""",,"CTGNN significantly advances material computing, especially for perovskite materials, by combining Transformer and graph neural networks to capture both local and global interactions.",,,,,,,,,,,,,
The inclusion of angular kernels enables more comprehensive atomic structure representation,outperforming models that use only distance information.,,,,,,,,,,,,,,
CTGNN achieves superior accuracy in predicting formation energy and bandgap compared to existing models,as confirmed by benchmark tests.,,,,,,,,,,,,,,
"Ablation studies recommend retaining both angular encoding and dual-Transformer structures for optimal model performance.""",,,No information available,,,"The objectives of the study are to develop the CTGNN model, which combines Transformer and graph neural network architectures with angular encoding, to improve the prediction accuracy of material properties such as formation energy and bandgap, especially for complex materials like perovskites.",,,,,,,,,
PRIVACY-PRESERVING IN BLOCKCHAIN-BASED FEDERATED LEARNING SYSTEMS,"Nicolazzo Serena, M. Sameera K., Arazzi Marco, Nocera Antonino, A. Rafidha Rehiman K., P. Vinod, Conti Mauro",2024,reference-manager,,,"The paper surveys Blockchain-enabled Federated Learning (FL), focusing on privacy-preserving solutions across industries like healthcare, Industry 5.0, and the Internet of Vehicles. It highlights privacy threats, state-of-the-art defenses, and open research challenges, offering a systematic categorization and suggesting promising future research directions in privacy and security.",,,,"Systematic literature review: Used a structured process (PRISMA flow diagram) to identify, screen, and select 102 relevant papers from databases between 2018 and 2023.",,"What are the primary privacy challenges, threats, and solutions in Blockchain-enabled Federated Learning systems, and how can privacy-preserving mechanisms be effectively integrated and applied across various domains to address open issues and guide future research?","This paper systematically reviews privacy-preserving methods in Blockchain-enabled Federated Learning (BCFL). Using a structured search and selection process, it analyzes privacy threats, mitigation strategies, and application scenarios. Key findings highlight open challenges and future research directions, emphasizing the importance of robust privacy solutions in BCFL systems.","The paper’s main objective is to systematically review privacy-preserving methods in Blockchain-enabled Federated Learning (BCFL), using a structured literature analysis approach, and its principal finding is the identification of key privacy threats, mitigation strategies, and open research challenges in BCFL systems.","Keywords or tags for this research include: Blockchain, Federated Learning, privacy, privacy-preserving, privacy attack, inference attack, homomorphic encryption, differential privacy, secure multiparty computation, privacy-preserving in healthcare, internet of things."
Custom taxonomy development: Categorized Blockchain-based Federated Learning systems by layers and integration types to analyze privacy approaches.,,,,,,,,,,,,,,,
Comparative analysis: Summarized and compared privacy-preserving techniques (e.g.,differential privacy,homomorphic encryption,"secure multi-party computation) across selected studies.""",,,"The paper systematically reviewed 102 articles (2018–2023) on privacy in Blockchain-enabled Federated Learning (BCFL), identifying key privacy threats and state-of-the-art solutions like differential privacy, homomorphic encryption, and secure multiparty computation.",,,,,,,,,
No explicit quantitative results or statistical significance (p-values) were reported in the context.,,,,,,,,,,,,,,,
"The study concludes that addressing smart contract vulnerabilities and conducting comprehensive security audits are crucial future directions to enhance BCFL robustness and security.""",The primary outcome is a systematic review of 102 articles (2018–2023) on privacy in Blockchain-enabled Federated Learning (BCFL) systems.,,,,,,,,,,,,,,
The study analyzes privacy attacks and protection methods,including differential privacy,homomorphic encryption,and secure multiparty computation.,,,,,,,,,,,,
"No specific statistical values or measured effects are reported.""",Lack of focused research on smart contract vulnerabilities in BCFL; need for comprehensive security audits.,,,,,,,,,,,,,,
Privacy-preserving methods (HE,SMPC) are computationally intensive,inefficient for large/complex data,and vulnerable to collusion attacks.,,,,,,,,,,,,
FL systems are susceptible to adversarial/model-poisoning attacks.,,,,,,,,,,,,,,,
Data shortage on distributed devices affects model performance.,,,,,,,,,,,,,,,
Statistical heterogeneity (non-IID data) challenges global model effectiveness.,,,,,,,,,,,,,,,
Expensive communication due to high resource demands for model updates.,,,,,,,,,,,,,,,
Systems heterogeneity causes dropped devices and low participation.,,,,,,,,,,,,,,,
Need for further study on algorithm convergence,optimal worker numbers,and update frequency.,,,,,,,,,,,,,
"No current research on optimizing gas consumption for privacy-preserving smart contracts in BCFL.""",The study highlights the need for focused research on vulnerabilities in smart contracts within Blockchain-enabled Federated Learning (BCFL) and recommends comprehensive security audits.,,,,,,,,,,,,,,
It provides a systematic overview of BCFL architectures,privacy threats,and state-of-the-art privacy-preserving solutions.,,,,,,,,,,,,,
"The survey identifies promising future research directions and aims to guide practitioners and researchers.""","Integration of zero-knowledge proofs for enhanced privacy in Blockchain-enabled Federated Learning (BCFL), allowing validation of updates without revealing raw data.",,,,,,,,,,,,,,
Optimization of gas consumption for encryption-related smart contract functions to improve economic feasibility and scalability in BCFL systems.,,,,,,,,,,,,,,,
"Addressing vulnerabilities in smart contracts to enhance security and robustness in BCFL models.""
Trading Off Privacy, Utility, and Efficiency in Federated Learning","Future research should focus on integrating zero-knowledge proofs for enhanced privacy, optimizing gas consumption in smart contracts, addressing vulnerabilities in smart contracts, and reducing computation and communication overheads. Additional exploration is needed for scalability, energy efficiency, and transaction capacity in Blockchain-enabled Federated Learning systems.
Zhang Xiaojin, Kang Yan, Chen Kai, Fan Lixin, Yang Qiang","Study design: Systematic review. Characteristics: comprehensive literature search (2018–2023) across multiple databases, use of PRISMA flow diagram for study selection, explicit inclusion and exclusion criteria, focus on peer-reviewed English-language papers, and analysis of Blockchain-enabled Federated Learning with emphasis on privacy preservation.
2023",reference-manager,"The objectives of the study are to systematically explore privacy in Blockchain-enabled Federated Learning (BCFL), examine potential threats and mitigation strategies, review privacy attacks and protection methods, identify concerns and limitations, and discuss future directions to enhance privacy in BCFL applications across various domains.
10.1145/3595185",,"The paper uses Jensen-Shannon (JS) divergence, which satisfies the triangle inequality, to measure privacy leakage in federated learning. It formulates the privacy-utility-efficiency trade-off as a constrained optimization problem and proves a No-Free-Lunch theorem: minimizing all three simultaneously is impossible in certain scenarios. Adaptive hyperparameter selection remains an open challenge.",,,,Randomization Mechanism: Adds random noise to model parameters to protect privacy by distorting the original data distribution.,,"How can the trade-off between privacy leakage, utility loss, and efficiency reduction in federated learning be quantitatively analyzed and optimized using a unified framework and widely-adopted protection mechanisms?","This paper proposes a unified federated learning framework analyzing the trade-off between privacy leakage, utility loss, and efficiency reduction using Jensen-Shannon divergence. By formulating a constrained optimization problem and applying the No-Free-Lunch theorem, it quantifies lower bounds for various protection mechanisms, guiding optimal parameter selection in federated learning.","The research goal is to quantify the trade-off between privacy leakage, utility loss, and efficiency reduction in federated learning using a unified framework; the approach formulates this as a constrained optimization problem and proves a No-Free-Lunch theorem; the principal finding is that minimizing all three simultaneously is impossible.",
Homomorphic Encryption and Secret Sharing: Encrypt or split data to prevent unauthorized access during transmission or computation.,,,,,,,,,,,,,,,
Compression Mechanism: Sends only selected model parameters (based on a probability),"reducing data exposure and communication cost.""",,"The No-Free-Lunch theorem shows that privacy leakage, utility loss, and efficiency reduction cannot all be minimized simultaneously; their weighted sum is lower bounded by a constant C1.",,,,,,,,,,,,
For the compression mechanism,lower bounds for privacy leakage,utility loss,and efficiency reduction are given as functions of the compression probability ρi.,,,,,,,,,,,,
"Statistical significance (p-values) is not reported in the context.""
Lower bounds for ϵp","The primary outcome is a No-Free-Lunch theorem (Theorem 4.6): the weighted sum of privacy leakage (ϵp), utility loss (ϵu), and efficiency reduction (ϵe) is at least C1, meaning exceptional privacy, utility, and efficiency cannot be achieved simultaneously.
ϵu",and ϵe are provided for Paillier Homomorphic Encryption,Secret Sharing,Randomization,,and Compression.,,,,,,,,,
Privacy leakage is measured by Jensen-Shannon (JS) divergence; utility loss and efficiency reduction are defined as differences in expected utility and communication/training cost between protected and unprotected models.,,,,,,,,,,,,,,,
Numerical/statistical values: C1 = 1; lower bounds for each mechanism are given in Table 4 (exact formulas provided in context).,,,,,,,,,,,,,,,
"The trade-off is quantified: increasing privacy protection increases utility loss and/or efficiency reduction.""",The study uses Jensen-Shannon (JS) divergence instead of the commonly-used KL divergence; it is unclear if results hold for generalized JS-divergence with hyperparameter α.,,,,,,,,,,,,,,
The possibility of designing adaptive meta-algorithms for hyperparameter selection remains open.,,,,,,,,,,,,,,,
"Further research is needed on optimal protection hyperparameter determination at each communication round.""","The unified federated learning (FL) framework quantifies the trade-off between privacy leakage, utility loss, and efficiency reduction (No-Free-Lunch theorem).",,,,,,,,,,,,,,
Lower bounds for these trade-offs are established for common protection mechanisms.,,,,,,,,,,,,,,,
Optimal trade-offs can be formulated as a constrained optimization problem.,,,,,,,,,,,,,,,
"Future work includes designing adaptive algorithms to optimize protection hyperparameters.""",Investigating whether a generalized JS-divergence with a hyperparameter α can still bound privacy leakage and quantify trade-offs.,,,,,,,,,,,,,,
Designing meta-algorithms to search for the optimal protection hyperparameter at each communication round.,,,,,,,,,,,,,,,
"Developing algorithms that can adaptively learn the hyperparameter for protection mechanisms.""",The study suggests future research on: (1) using a generalized JS-divergence with a hyperparameter α to analyze privacy leakage and trade-offs; (2) designing meta-algorithms to find or adaptively learn optimal protection hyperparameters at each communication round; and (3) further exploration of the optimization problem.,,,"The objectives of the study are to analyze and optimize the trade-off between privacy leakage, utility loss, and efficiency reduction in federated learning by formulating a constrained optimization problem, deriving lower bounds, and proposing meta-algorithms for selecting optimal protection parameters under privacy constraints.",,,,,,,,,,,
KSG: Knowledge and Skill Graph,"Zhao Feng, Zhang Ziqi, Wang Donglin",2022,reference-manager,10.1145/3511808.3557623,,"The Implementation Insights show that KSG (Knowledge and Skill Graph) enables agents to learn new skills efficiently by leveraging transferable knowledge from pre-trained models and offline datasets. Selecting pre-training models based on task similarity improves learning efficiency. Different environments and tasks enrich the KSG, supporting skill transfer and combination.",,,,"Construction of a Knowledge and Skill Graph (KSG) based on CN-DBpedia, integrating both static knowledge and dynamic behavioral skills.",,"How can a Knowledge and Skill Graph (KSG) be constructed and utilized to enable effective skill retrieval, display, and transferable knowledge for learning new skills in agents?","The paper aims to build a Knowledge and Skill Graph (KSG) that integrates static knowledge and dynamic behavioral skills. Using data from CN-DBpedia, the authors employ entity, attribute, and relation extraction to construct KSG. Experiments show KSG supports skill retrieval, Q\&A, and aids learning new skills, highlighting its practical value.","The research goal is to enhance information retrieval by introducing the Knowledge and Skill Graph (KSG), which adds skill and environment nodes to a knowledge graph; the approach enables effective skill searching and transfer, and results show KSG supports skill retrieval and learning across different agents and environments.","The keywords or tags for this research are: Knowledge and Skill Graph, Skill Retrieval, Knowledge Graph."
Training of basic skills for various agents (e.g.,Humanoid,Ant,Half Cheetah,Quadruped Robot) using SAC (Soft Actor-Critic) in different simulated environments.,,,,,,,,,,,
Knowledge fusion process to combine trained skills and knowledge,enabling skill retrieval,Q\&A,"and transfer learning for new skills.""",,,"The Knowledge and Skill Graph (KSG) effectively integrates dynamic behavioral skills with static knowledge, enabling skill retrieval, Q\&A, and transfer learning for new skills.",,,,,,,,,
Extensive experiments show KSG’s effectiveness in tasks like QA,knowledge retrieval,and skill learning; however,no quantitative results or p-values are provided.,,,,,,,,,,,,
The KSG construction uses entity extraction,attribute extraction,and relation extraction,leveraging CN-DBpedia,Neo4j,,"and Py2neo for efficient knowledge management.""",,,,,,,,,"The primary outcome is the development and evaluation of the Knowledge and Skill Graph (KSG), which enables effective skill searching, knowledge retrieval, and skill learning."
Experiments demonstrate KSG’s effectiveness in tasks such as question answering (QA),knowledge retrieval,and skill learning.,,,,,,,,,,,,,
"No specific statistical values or quantitative results are provided.""",,"KSG (Knowledge and Skill Graph) enables simultaneous processing of static and dynamic knowledge, supporting skill retrieval and reuse.",,,,,,,,,,,,,
KSG extends traditional knowledge graphs by incorporating dynamic behavioral information for new skill learning and transfer.,,,,,,,,,,,,,,,
Future work includes expanding KSG for real applications,providing more basic skills,offline data,"and complex relations for advanced learning and reasoning.""",Need to expand and improve KSG for real applications.,,,,,,,,,,,
KSG should provide more basic skills and offline data for reinforcement learning,meta learning,and imitation learning.,,,,,,,,,,,,,
KSG needs to offer more complex relations between skills,agents,"and environments for skill learning and reasoning.""","Future research should focus on expanding and improving KSG for real applications, providing more basic skills and offline data for reinforcement learning, meta learning, and imitation learning, and developing more complex relationships between skills, agents, and environments for skill learning and reasoning.",,,,,,,,,,,,"The objectives of the study are to establish a Knowledge and Skill Graph (KSG) that enables skill retrieval, supports Q\&A and knowledge retrieval, and provides transferable knowledge for learning new skills by integrating dynamic behavior information with static knowledge graph data."
Lessons Learnt from a Multimodal Learning Analytics Deployment In-the-Wild,"Martinez-Maldonado Roberto, Echeverria Vanessa, Fernandez-Nieto Gloria, Yan Lixiang, Zhao Linxuan, Alfredo Riordan, Li Xinyu, Dix Samantha, Jaggard Hollie, Wotherspoon Rosie, Osborne Abra, Shum Simon Buckingham, Gašević Dragan",2023,reference-manager,10.1145/3622784,,Implementation Insights Summary:,,,,,,,,,
The deployment showed that multimodal data (from sensors,microphones,"physiological data) can distinguish team performance and support student reflection. Teachers valued the data but needed better data literacy and clearer visualizations. Trust and understanding were challenged by data incompleteness and unclear data processing. Alternative analysis models were suggested.""",,The research goal was to improve consenting strategies for multimodal data collection in educational settings; the approach involved researcher-led briefings and simplifying technical language; the principal finding was that balancing detail and simplicity in explanations is crucial to increase student participation and understanding.,,"The study aimed to improve consenting strategies and technological sustainability in multimodal learning analytics (MMLA) research. Using surveys and interviews with students and researchers, it found that complex explanations reduced participation. The study recommends simplifying consent forms and developing flexible, sustainable MMLA systems for classroom use.",,,,,,,"Research in-the-wild: Technology interventions were embedded in real-world classroom settings to ensure ecological validity and address practical, ethical, and logistical challenges.","What logistical, privacy, and ethical challenges emerge from a complex MMLA in-the-wild study that closes the analytics loop by providing direct feedback to students?",
Human-centered approach: Collaboration with teachers,researchers,and students to co-create and evaluate the MMLA innovation.,,,,,,,,,,,,,
Surveys: Researchers and students completed surveys to gather perspectives on logistics,ethics,privacy,"and system usability.""",,,"337 quotes were analyzed and grouped into five themes: Space and Place (66), Data and Analytics (188), Human-centeredness (19), Social Factors (64), and Sustainability (33).",,,,,,,,,
Lower student participation was linked to complex explanations and technical terms; simplifying consent forms and explanations is recommended.,,,,,,,,,,,,,,,
"No statistical significance or p-values were reported; findings are qualitative and not generalizable.""","47 students (40 females, avg. age: 23.81, std. dev: 5.61) completed a survey on trust in the MMLA dashboard visualizations.",,,,,,,,,,,,,,
Students rated trust using a five-point Likert scale (1 = completely trust,5 = not trust).,,,,,,,,,,,,,,
Senior teachers reported the debriefing tool reinforced discussions,validated teamwork aspects,and provided “objective feedback.”,,,,,,,,,,,,,
"Teachers found visualizations helpful for showing team dynamics and reducing bias.""",Lessons learned are not generalizable; MMLA studies vary by context and technology.,,,,,,,,,,,,,,
Participants were highly motivated and accustomed to technology,limiting broader applicability.,,,,,,,,,,,,,,
Inconsistent evidence collection between study iterations.,,,,,,,,,,,,,,,
Lower participation due to complexity and unclear consent forms.,,,,,,,,,,,,,,,
Unexpected technical issues with sensors and computers.,,,,,,,,,,,,,,,
Some students wore sensors incorrectly,"affecting data quality.""",Participation was limited by complex explanations and technical terms; simplifying consent forms and explanations is recommended.,,,,,,,,,,,,,
Inconsistent evidence collection and optional participation limit generalizability.,,,,,,,,,,,,,,,
Real-world conditions required adapting research objectives to educational needs.,,,,,,,,,,,,,,,
Future studies should improve communication,logistics,"and sustainability for broader and more reliable deployment.""","Need for larger, more diverse sample sizes and improved reporting standards to address algorithmic bias and replicability in MMLA studies.",,,,,,,,,,,,
Insufficient research on ethical practices,trust,and data transparency in MMLA systems,especially involving key stakeholders.,,,,,,,,,,,,
"Lack of human-centered design approaches and strong partnerships with teachers and students in MMLA development.""",Future research should address the limited participation of less motivated or non-consenting students to better understand their perspectives. Studies should also improve consistency in evidence collection across iterations and simplify consent forms and explanations to enhance student understanding and participation.,Study design characteristics:,,,,,,,,,,,,,
In-the-wild study (conducted in real-world,everyday settings,not in a lab),,,,,,,,,,,,,
Iterative (multiple iterations,with redesigns based on findings),,,,,,,,,,,,,,
Human-centered design (collaboration with teachers,researchers,and students),,,,,,,,,,,,,
Observational (no mention of randomization,blinding,"or control groups)""",,"The objectives of the study were to explore challenges in deploying multimodal learning analytics in real-world settings, focusing on technical, logistic, human-centered, social, and sustainability factors, and to gather perspectives from students, teachers, and researchers to improve future implementations and consenting strategies.",,,,,,,,,,,
Structural Quality Metrics to Evaluate Knowledge Graph Quality,"Seo Sumin, Cheon Heeseon, Kim Hyunho, Hyun Dongseok",2022,reference-manager,,,"The paper proposes six structural quality metrics to evaluate knowledge graphs: Instantiated Class Ratio, Instantiated Property Ratio, Class Instantiation, Subclass Property Acquisition, Subclass Property Instantiation, and Inverse Multiple Inheritance. These metrics reveal that a detailed ontology and high instantiation indicate higher quality, beyond just graph size.",,,,"Structure-based evaluation: Assesses knowledge graphs using metrics reflecting structure or statistical properties, such as schema, class, graph, and complexity metrics.",,How can structural quality metrics be used to numerically evaluate and compare the internal quality of cross-domain knowledge graphs based on their ontology structure and usage?,"This paper aims to define what makes a """"good knowledge graph"""" and introduces six structural quality metrics to numerically evaluate internal quality. Using these metrics, the study compares major web-based and Naver’s knowledge graphs, revealing characteristics not captured by traditional size-based indicators.","The research goal is to define and quantify """"good knowledge graph"""" quality; the approach introduces a structural quality metric focusing on ontology structure rather than just size or data distribution; the principal finding is that this metric enables more meaningful comparison of knowledge graphs' internal quality.","Keywords or tags for this research include: data driven evaluation, application/task based evaluation, user based evaluation, structure based evaluation, data quality evaluation, knowledge graphs, ontology, schema metric, class metric, property metric, graph metric, data quality, DBpedia, Wikidata, YAGO, Google Knowledge Graph, Freebase."
Data quality evaluation: Measures data quality from perspectives like accuracy and consistency,using numerical indicators.,,,,,,,,,,,,,,
"Structural quality metrics: Introduced to numerically represent the internal quality of knowledge graphs' structure (ontology).""",,"The study introduces a new metric focusing on ontology structure to evaluate knowledge graph quality, comparing Raftel, Wikidata, DBpedia, YAGO, Google Knowledge Graph, and Freebase.",,,,,,,,,,,,,
Raftel has the highest number of classes (59,662),properties (23,446),RDF triples (348,,094,,,,,,,,and the highest instantiated class ratio (0.941).,663)
"No statistical significance (p-values) is reported in the context.""",Primary outcomes:,,,,,,,,,,,,,,
Structure-based and data quality evaluations are the main comparative methods for cross-domain knowledge graphs.,,,,,,,,,,,,,,,
"Structural quality metrics (normalized
Basic statistics:",combining Class Metrics (CM) and Property Metrics (PM)) show Raftel scores highest (8.71 with 1.0×CM+0.0×PM),followed by Wikidata (2.63),DBpedia (6.56),YAGO (6.03),,Google KG (1.79),,,,,,,,,and Freebase (6.4).
Number of classes: Raftel 59,662; Wikidata 60,000; DBpedia 53,091; YAGO 804; GoogleKG 266; Freebase 910.,,,,,,,,,,,,
Number of properties: Raftel 23,446; Wikidata 21,607; DBpedia 7,467; YAGO 1,447; GoogleKG 607; Freebase 141.,,,,,,,,,,,
Number of RDF triples: Raftel 348,094,663; Wikidata 253,566,996; DBpedia 48,,348,258,,,137,852.,977; Freebase 11,483; GoogleKG 27,292,838; YAGO 48
Number of instances: Raftel 33,535,913; Wikidata 19,707,176; DBpedia 1,,390,323,,,752.,,452; Freebase 287,785; GoogleKG 1,653,438; YAGO 17
"Instantiated class ratio: Raftel 0.941; Wikidata 0.820; DBpedia 0.470; YAGO 0.099; GoogleKG 0.046; Freebase 0.004.
Measured effects:",,,,,,,,,,,,,,,
Raftel and YAGO have significantly more RDF triples and instances than DBpedia.,,,,,,,,,,,,,,,
Freebase has the largest number of instances but a low number of RDF triples due to most being 'instance of' relationships.,,,,,,,,,,,,,,,
"No explicit statistical significance values reported.""","Structure-based metrics mainly focus on size and distribution, making it hard to judge overall quality.",,,,,,,,,,,,,,
Graph metrics like cohesion and cardinality are not specialized for knowledge graph quality.,,,,,,,,,,,,,,,
No existing quality indicators based on knowledge graph structure or ontology.,,,,,,,,,,,,,,,
"Further research needed for structural quality metrics.""","Six new structural quality metrics were proposed to evaluate knowledge graph quality, emphasizing ontology structure and utilization.",,,,,,,,,,,,,,
Applying these metrics to six knowledge graphs,including Raftel,provided deeper insights than size/distribution-based evaluations.,,,,,,,,,,,,,
Structural quality varies: more classes/properties do not guarantee higher quality; detailed,well-instantiated classes matter.,,,,,,,,,,,,,,
"Future evaluations should use multi-dimensional approaches to assess strengths and weaknesses.""","Lack of quality indicators based on knowledge graph structure (ontology), not just data size or distribution.",,,,,,,,,,,,,,
Existing structural metrics and data-quality-based assessments are insufficient for evaluating internal quality.,,,,,,,,,,,,,,,
"Need for specialized structural quality metrics that numerically represent internal knowledge graph quality.""","Future research should address the lack of quality indicators based on knowledge graph structure and ontology. Current metrics mainly focus on size and basic structure, making it difficult to evaluate quality. Developing specialized structural quality metrics is recommended to better assess internal knowledge graph quality.","Data-driven evaluation, application/task-based evaluation, user-based evaluation, structure-based evaluation, and data quality evaluation. Comparative studies mainly focus on structure-based and data quality evaluations. No mention of randomization, blinding, control groups, or other clinical trial designs. Observational and comparative characteristics are present.",,"The objectives of the study are to define what makes a """"good knowledge graph,"""" introduce a structural quality metric focused on ontology structure, and use this metric to quantitatively compare the internal quality of various web knowledge graphs, including Wikidata, Freebase, DBpedia, YAGO, Google KG, and Raftel.",,,,,,,,,,,
PersonalAI: Towards digital twins in the graph form,"Menschikov Mikhail, Evseev Dmitry, Kostoev R., Perepechkin Ilya, Salimov Ilnaz, Dochkina Victoria, Anokhin Petr, Burnaev Evgeny, Semenov Nikita",2025,reference-manager,,,"Implementation Insights are as follows: The study integrates multiple retrieval algorithms (A\*, WaterCircles, BeamSearch, and their combinations) to enhance knowledge graph extraction for QA tasks. The mixed algorithm combines their strengths, improving triplet extraction. Evaluation uses DiaASQ, HotpotQA, and TriviaQA datasets, with accuracy and ExactMatch as main metrics. New insight: Combining diverse retrieval strategies increases relevant data extraction, supporting more robust personalized QA pipelines.",,,,"Mixed algorithm: Combines A\*, WaterCircles, and BeamSearch strategies to improve extraction of relevant data from knowledge graphs.",,"How can a knowledge graph-based external memory architecture, exemplified by the AriGraph method, enhance question answering systems by enabling efficient extraction, management of temporal dependencies, and personalized response generation compared to existing RAG and GraphRAG methods?","The paper proposes a method to construct a graph-based knowledge base from weakly structured text, using large language models (LLMs) to extract and store information as triplets and thesis statements. This enables effective question answering by matching questions to relevant subgraphs, improving LLMs’ long-context reasoning and answer accuracy.","The research goal is to enhance LLM question answering by building a knowledge graph from weakly structured text using LLM-extracted triplets and thesis statements; the approach combines multiple extraction algorithms, and the principal finding is improved relevant data extraction for personalized LLM responses.",
A\* algorithm: Used for graph traversal to find shortest paths and extract relevant triples,employing heuristics like inner product and shortest path metrics.,,,,,,,,,,,,,,
WaterCircles: Utilizes breadth-first search to iteratively expand from query-mapped entities,"extracting relevant pathways in the knowledge graph.""","The research uses the PersonalAI library for implementation but does not provide a source code link. Reproducibility details, such as datasets, models, and evaluation metrics, are described. No explicit mention of publicly available source code is present.","The best QA configuration for 7B models is Qwen2.5 7B (mean JudgeScore: 0.27), and for 8B models, GPT4o–mini (mean JudgeScore: 0.77).",,,,,,,,,,,,
The lowest LLM parsing error rates were with Qwen2.5 7B and Llama3.1 8B (0.02%),while DeepSeekV3 had the highest (31.21%).,,,,,,,,,,,,,,
"No explicit p-values or statistical significance are reported in the context.""",Primary outcomes were measured using the JudgeScore metric across different models and datasets.,,,,,,,,,,,,,,
Best 7B model: Qwen2.5 7B with mean JudgeScore 0.27.,,,,,,,,,,,,,,,
Best 8B model: GPT4o–mini with mean JudgeScore 0.77.,,,,,,,,,,,,,,,
Best large model: DeepSeek V3 with mean JudgeScore 0.70.,,,,,,,,,,,,,,,
BeamSearch and BeamSearch + WaterCircles algorithms yielded highest scores in most configurations.,,,,,,,,,,,,,,,
Restrictions on node types significantly affected JudgeScore values,with best configurations showing higher scores (e.g.,0.44 for 7B models,"0.45 for 8B models).""","The search for information in the knowledge graph is based on the initial user question, which may contain noise, leading to less relevant information being generated.",,,,,,,,,,,
Limitations in current triplet extraction techniques may affect efficiency and accuracy.,,,,,,,,,,,,,,,
"Evaluation metrics like BERTScore lack sufficient differentiability for nuanced answer distinctions.""","The study introduces an external memory architecture combining knowledge graphs and text fragments for question answering, enabling efficient relevant information extraction.",,,,,,,,,,,,,,
The proposed system outperforms existing GraphRAG methods on the HotpotQA dataset.,,,,,,,,,,,,,,,
"Incorporating temporal parameters into dialogues enhances the system's ability to manage and utilize temporal dependencies.""","Integration of a """"memory time"""" parameter to enhance temporal dynamics and nuanced filtering in knowledge graphs.",,,,,,,,,,,,,,
Exploration of optimized triplet extraction techniques to improve efficiency and accuracy of information retrieval.,,,,,,,,,,,,,,,
"Further development to elevate personalization and contextual awareness in language model interactions.""","Future research should focus on enhancing temporal dynamics in the knowledge graph by adding a """"memory time"""" parameter and using advanced algorithms like BFS and A\*. Optimizing triplet extraction techniques is also suggested to improve efficiency, accuracy, and personalization in language model interactions.",,,,,,,,,,,,,,
Privacy and Fairness in Federated Learning: On the Perspective of Tradeoff,"Chen Huiqiang, Zhu Tianqing, Zhang Tao, Zhou Wanlei, Yu Philip S.",2023,reference-manager,10.1145/3606017,,Implementation Insights Summary:,,,,,,,,,
"The paper reviews privacy-preserving methods in federated learning (FL)
New Insights:",highlighting techniques like Homomorphic Encryption (HE),secret sharing,variational bottleneck,gradient compression,,Differential Privacy (DP),but faces tradeoffs in effectiveness,,,and performance.,,robustness,and practicality,accuracy,and Trusted Execution Environments (TEEs). Each method balances privacy
The interplay between privacy and fairness in FL is underexplored. Combining privacy techniques with personalized models may improve the balance between privacy,fairness,"and utility. The compatibility of fairness and DP also remains an open research direction.""",,"The research goal is to survey privacy and fairness in federated learning (FL); the approach is a comprehensive review of attacks, defenses, fairness notions, and their interactions; the principal finding is that privacy and fairness often conflict, and future research should address their tradeoffs in FL.",,"This survey examines privacy and fairness in Federated Learning (FL), highlighting their tradeoffs and interactions. It reviews privacy attacks, defenses, fairness notions, and debiasing strategies in FL. The study finds privacy and fairness can conflict, and calls for research into balancing both for private and fair FL models.",,,,,,,"Cryptographic approaches: Techniques like secure multi-party computation, homomorphic encryption, and secret sharing enable computation over encrypted data, protecting privacy but with high computational cost.","What are the tradeoffs and interactions between privacy and fairness in federated learning, and how can future research address the challenges of achieving both private and fair models in this setting?",
Perturbation approaches: Methods such as adding noise (DP mechanism) or using surrogate/abstracted datasets to protect privacy with less computational overhead.,,,,,,,,,,,,,,,
"Client selection strategies: Algorithms that actively select clients or reweight their contributions to address data distribution and fairness in federated learning.""",,"This is the first survey to comprehensively review privacy, fairness, and their interactions in federated learning (FL).",,,,,,,,,,,,,
The paper details privacy attacks and defenses in FL,outlines sources of bias,and summarizes fairness-aware FL approaches.,,,,,,,,,,,,,
"The survey highlights the need to study tradeoffs and compatibility between privacy and fairness in FL. No p-values reported.""","The survey provides a comprehensive overview of privacy, fairness, and their interactions in federated learning (FL).",,,,,,,,,,,,,,
It details privacy attacks and defenses in FL,highlighting how these attacks can compromise privacy.,,,,,,,,,,,,,,
The survey explains fairness notions in FL and corresponding debiasing strategies.,,,,,,,,,,,,,,,
"No specific statistical values or measured effects are reported.""",Tradeoffs between privacy and fairness are under-studied; most works address them separately.,,,,,,,,,,,,,,
Privacy and fairness may compete,requiring tradeoffs that can harm either aspect.,,,,,,,,,,,,,,
Non-i.i.d. (non-independent and identically distributed) data in FL complicates fairness and privacy.,,,,,,,,,,,,,,,
Existing solutions may introduce bias or fail to ensure fairness across all clients.,,,,,,,,,,,,,,,
Ensuring privacy can worsen inequities between groups,especially disadvantaged ones.,,,,,,,,,,,,,,
Achieving fairness may require sharing more data,increasing privacy risks.,,,,,,,,,,,,,,
"Most research does not address the combined challenges of privacy and fairness in FL.""","This is the first survey to comprehensively review privacy, fairness, and their interactions in Federated Learning (FL).",,,,,,,,,,,,,,
It details privacy attacks,defenses,sources of bias,and fairness-aware FL approaches.,,,,,,,,,,,,
The study highlights key tradeoffs and open challenges between privacy and fairness.,,,,,,,,,,,,,,,
"Future research should address achieving both algorithmic and client-level fairness while preserving privacy.""",Tradeoffs between privacy and fairness in federated learning (FL) are under-studied; more research is needed to balance these aspects.,,,,,,,,,,,,,,
Compatibility of fairness and differential privacy (DP) in FL is complex due to privacy attack surfaces and data heterogeneity.,,,,,,,,,,,,,,,
"Addressing fairness issues caused by non-i.i.d. (non-independent and identically distributed) data while respecting privacy constraints remains challenging.""","Future research should examine the tradeoffs between privacy and fairness in Federated Learning (FL), address fairness issues caused by non-i.i.d. data, and explore the compatibility of fairness and Differential Privacy (DP). Investigating how privacy and fairness interact in FL remains an open and under-studied area.",,,"The objectives of the study are to provide a comprehensive overview of privacy, fairness, and their interactions in Federated Learning (FL); survey privacy attacks and defenses in FL; discuss fairness notions and fairness-aware FL approaches; and highlight future research directions for training private and fair FL models.",,,,,,,,,,,
A Privacy-Preserving Subgraph-Level Federated Graph Neural Network via Differential Privacy,"Qiu Yeqing, Huang Chenyu, Wang Jianzong, Huang Zhangcheng, Xiao Jing",2022,reference-manager,,,"Implementation Insights show that adding noise (for privacy) in DP-FedRec protects data privacy without reducing data usefulness. The time to add noise increases with the number of points (nodes) in the dataset, not with the number of edges. DP-FedRec outperforms FedRec in Epinions with 12 clients.",,,,Graph Convolutional Network (GCN) under the Message Passing Neural Network (MPNN) framework: Used to extract and aggregate information from user-item graphs for prediction.,,"How can a privacy-preserving federated graph neural network framework be designed for recommendation systems at the sub-graph level, effectively addressing both the Non-IID problem and privacy protection using differential privacy and private set intersection techniques?","The paper investigates federated learning for recommendation systems using graph neural networks (GNNs) with differential privacy (DP) to protect user data. It proposes a subgraph-level federated approach, evaluates performance on Epinions and MovieLens1M datasets, and finds that DP effectively balances privacy and data utility without compromising recommendation accuracy.","The paper's main objective is to improve recommendation systems by using federated graph neural networks with differential privacy; the key method is subgraph-level federated learning with privacy-preserving techniques, and the principal finding is enhanced prediction accuracy while protecting user privacy.","Keywords: Recommendation System, Federated Learning, Subgraph-Level Federated Learning, Graph Neural Network, Differential Privacy"
Federated Learning: Each client trains a local model on its sub-graph,and a centralized server aggregates model parameters for joint training.,,,,,,,,,,,,,,
Private Set Intersection (PSI): A cryptographic protocol allowing clients to find common data without revealing other information,"implemented using a programmable pseudo random function (OPPRF).""","The research implements both FedRec and DP-FedRec using Python-based code of FedGraphNN. No explicit source code link or repository is provided in the context. Therefore, reproducibility is limited to the described implementation details; no direct access to the project’s source code is available.","The K-hop extension improves the accuracy of federated Graph Neural Networks (GNNs), and adding differential privacy (DP) in DP-FedRec does not significantly reduce accuracy.",,,,,,,,,,,,
In the Epinions dataset with 12 clients,DP-FedRec outperforms FedRec,balancing data privacy and availability.,,,,,,,,,,,,,
"The time to add noise is positively correlated with the number of points in the graph; Epinions requires more time than MovieLens1M. No explicit p-values are provided.""","Primary outcomes measured: mean absolute error (MAE), mean square error (MSE), root mean square error (RMSE), and noising time (seconds).",,,,,,,,,,,,,,
K-hop extension (FedRec) improves accuracy over FedGraphNN in all metrics.,,,,,,,,,,,,,,,
DP-FedRec maintains similar accuracy to FedRec; adding noise to FedGraphNN reduces accuracy more.,,,,,,,,,,,,,,,
Example results (Epinions,12 clients):,,,,,,,,,,,,,,
Centralized: MAE 0.8377,MSE 1.2464,RMSE 1.1164,,,,,,,,,,,,,
FedGraphNN: MAE 0.8674,MSE 1.3279,RMSE 1.1502,,,,,,,,,,,,,
FedRec(K=10): MAE 0.8635,MSE 1.3270,RMSE 1.1496,,,,,,,,,,,,,
DP-FedRec(K=10): MAE 0.8585,MSE 1.3258,RMSE 1.1493,Noising time 501s,,,,,,,,,,,,
Example results (MovieLens1M,8 clients):,,,,,,,,,,,,,,
Centralized: MAE 0.8812,MSE 1.1782,RMSE 1.0855,,,,,,,,,,,,,
"FedGraphNN: MAE 0.8832
FedRec: MAE 0.8793","MSE 1.1850
MSE 1.1786","RMSE 1.0884
RMSE 1.0884",,,,,,,,,,,,,
DP-FedRec(K=5): MAE 0.8813,MSE 1.1783,RMSE 1.0875,Noising time 4s,,,,,,,,,,,,
Adding noise time increases with the number of points in the graph.,,,,,,,,,,,,,,,
"DP-FedRec balances privacy protection and data availability without significant accuracy loss.""","Only 12 categories from the Epinions dataset were selected due to memory limitations, which may affect generalizability.",,,,,,,,,,,,,,
The time required to add noise for privacy is much greater for Epinions than MovieLens,indicating scalability issues.,,,,,,,,,,,,,,
"No further explicit limitations or suggestions for future research are provided.""","DP-FedRec achieves better performance than FedRec with 12 clients in the Epinions dataset, balancing data privacy and availability.",,,,,,,,,,,,,,
Adding noise protects privacy without significantly reducing data utility.,,,,,,,,,,,,,,,
The time to add noise increases with the number of points,not edges.,,,,,,,,,,,,,,
"Recommendation: Use DP-FedRec for privacy-preserving federated recommendation.""",Lack of a universal differential privacy (DP) mechanism for both weights and edges in graph data to improve performance.,,,,,,,,,,,,,,
Existing methods assume one party owns the global topology or ignore neighbor information,making them unsuitable for general sub-graph level scenarios.,,,,,,,,,,,,,,
"Need to address the Non-IID problem and privacy preservation simultaneously in federated GNNs for sub-graph-level settings.""",The study suggests future research should investigate a universal differential privacy (DP) mechanism that protects both weights and edges in graph data to achieve better performance. This addresses current limitations where DP is not universally applied to all aspects of the graph data.,"The study design includes: experiments on two datasets (Epinions and MovieLens), division of data by item category among clients, evaluation under two client settings (8 and 12 clients), and five experiment types: centralized training, FedGraphNN with FedAvg, FedRec, DP-FedGraphNN, and DP-FedRec.",,The objectives of the study are to prove that the K-hop extension improves the accuracy of the federated Graph Neural Network (GNN) and that using Differential Privacy (DP) in DP-FedRec does not significantly reduce accuracy.,,,,,,,,,,,
DHyper: A Recurrent Dual Hypergraph Neural Network for Event Prediction in Temporal Knowledge Graphs,"Tang Xing, Chen Ling, Shi Hongyu, Lyu Dandan",2024,reference-manager,10.1145/3653015,,"DHyper consistently outperforms all compared methods on ICEWS18, ICEWS18C, and GDELT18 across MRR and Hits@k metrics, with improvements up to 30.25%. Its design leverages low-rank factorization, sparse thresholding, and attentive temporal encoding for efficiency and effectiveness. DHyper’s superiority is statistically significant. No new insights beyond performance and complexity are stated.",,,,"Hypergraph modeling: DHyper uses hypergraph modeling to capture high-order correlations among entities and relations, improving representation learning.",,How can hypergraph modeling be used to simultaneously capture high-order correlations among entities and relations in temporal knowledge graphs to improve event prediction performance?,"The paper investigates DHyper, a recurrent dual hypergraph neural network for event prediction in temporal knowledge graphs. Using hypergraph modeling and deep neural networks, DHyper captures high-order correlations among entities and relations. Experiments show DHyper outperforms baselines, with best results at two DHMP layers, reducing overfitting and improving representation learning.","The research goal is to improve temporal knowledge graph reasoning; the approach, DHyper, uses hypergraph neural networks with sparse thresholding and attentive aggregation; results show DHyper achieves the best performance across multiple benchmarks, significantly outperforming prior methods in MRR and Hits@k metrics.",
t-SNE visualization: The t-SNE method is used to visualize and compare entity and relation representations learned by different approaches.,,,,,,,,,,,,,,,
"Pairwise t-test: Statistical significance of results is assessed using the pairwise t-test at a 95% confidence level.""",,"DHyper achieves the best performance across all benchmarks (ICEWS18, ICEWS18C, GDELT18), with improvements up to 30.25% over previous methods and statistically significant results (p < 0.05).",,,,,,,,,,,,,
DHyper’s optimal performance is reached with 2 layers; more layers lead to overfitting and degraded results.,,,,,,,,,,,,,,,
Case studies and ablation experiments show DHyper’s hypergraph modeling captures high-order correlations,"leading to better entity and relation representations than prior approaches.""","DHyper achieves the highest performance across all datasets (ICEWS14, ICEWS18, GDELT18, and their sparse variants) in MRR, Hits@1, Hits@3, and Hits@10.",,,,,,,,,,,,,
DHyper outperforms the second-best method by up to 30.25% (Hits@10,ICEWS14).,,,,,,,,,,,,,,
All improvements are statistically significant at the 95% level.,,,,,,,,,,,,,,,
DHyper's design choices (entity/relation/prior hypergraph mappers,low-rank factorization,sparse threshold strategies) contribute to its superior results.,,,,,,,,,,,,,
On ICEWS14: DHyper achieves MRR 56.15±0.28,Hits@1 43.76±0.22,Hits@3 65.46±0.16,Hits@10 85.89±0.18.,,,,,,,,,,,,
On ICEWS18: DHyper achieves MRR 54.22±0.05,Hits@1 42.16±0.21,Hits@3 63.26±0.21,Hits@10 75.38±0.24.,,,,,,,,,,,,
On GDELT18: DHyper achieves MRR 51.15±0.05,Hits@1 40.22±0.25,Hits@3 57.29±0.24,Hits@10 65.33±0.22.,,,,,,,,,,,,
"DHyper consistently outperforms all baselines and its own ablated variants in all primary metrics.""",DHL captures only one type of high-order correlation among entities and relations.,,,,,,,,,,,,,,
DHyper only considers two levels of hierarchy (entities/entity hyperedges or relations/relation hyperedges).,,,,,,,,,,,,,,,
Model performance degrades with too many DHMP layers due to increased risk of over-fitting.,,,,,,,,,,,,,,,
"Future work is needed to address more complex and diverse scenarios.""","DHyper achieves the best performance across all datasets, with significant improvements (up to 30.25%) over state-of-the-art methods.",,,,,,,,,,,,,,
Optimal DHMP performance is reached with 2 layers; more layers cause overfitting and degrade results.,,,,,,,,,,,,,,,
DHyper effectively captures high-order correlations,learning better entity and relation representations.,,,,,,,,,,,,,,
"Recommendation: Use 2 DHMP layers for best results.""","Limited ability of some models (e.g., TITer) to handle long-range dependencies between entities, leading to lower performance for distant or unreachable entities.",,,,,,,,,,,,,,
Need for better modeling of latent pairwise correlations,such as entity groups and communities,to improve prediction accuracy.,,,,,,,,,,,,,
"Further exploration of derived structures to enhance representation learning in temporal knowledge graphs.""",Future research should address DHyper’s current limitation of only evaluating event prediction in temporal knowledge graphs (TKGs). Suggested directions include enhancing DHyper for recommendation tasks and exploring a joint-learning framework to optimize both event prediction and recommendation for better generalization.,,,,,,,,,,,,,,
Mind the Gap: Two Dissociable Mechanisms of Temporal Processing in the Auditory System,"Anderson Lucy A., Linden Jennifer F.",2016,reference-manager,10.1523/jneurosci.1652-15.2016,,Implementation Insights Summary:,,,,,,,,,
"The model uses intensity gain control by integrating sound input over time with an exponentially decaying window
New Insights:",then applying adaptive gain via nonlinear normalization. Two channels process the adapted input: an onset-sensitive channel (fast,thresholded) and an offset-sensitive channel (inverted,delayed,thresholded). Reduced offset-channel weighting impairs gap detection.,,,,,,,,,,,
Gap-detection deficits can arise specifically from reduced activity in the offset-sensitive channel,"not from general temporal processing loss. This dissociates onset and offset mechanisms in auditory temporal processing.""",auditory; gap detection; hearing; mouse; temporal processing; thalamus,"The research goal was to understand auditory temporal processing deficits; the approach used mouse models and thalamic recordings to study gap detection; the principal finding is that deficits arise from reduced brain sensitivity to sound offsets, revealing separate onset- and offset-sensitive mechanisms.","The study aimed to investigate auditory temporal processing in mice, focusing on gap-detection thresholds and thalamic activity. Using ABR and neuronal recordings, the researchers found specific abnormalities in auditory thalamic responses to sound offsets, suggesting that temporal processing deficits may result from reduced brain sensitivity to sound offsets.",,,,,,,,,,"Electrophysiological recordings: Multiunit and single-unit neuronal activity were recorded using electrodes inserted into the cortex, with spike sorting and manual clustering of waveforms.","What are the distinct mechanisms underlying auditory temporal processing in the thalamus, and how do abnormalities in sound-offset-sensitive channels contribute to gap-detection deficits in a mouse model of auditory processing disorder?"
Auditory Brainstem Response (ABR) measurements: Subdermal electrodes recorded responses to auditory stimuli,with signals amplified,digitized,and analyzed for thresholds,amplitude,,and latency.,,,,,,,,,
Stimulus presentation and analysis: Various auditory stimuli (clicks,tones,gap-in-noise,noise maskers) were presented,"and neuronal responses were analyzed using poststimulus time histograms and statistical tests.""",,,,,,,,,,,"No significant differences were found between ectopic and nonectopic mice in basic auditory thalamic response properties or ABRs to clicks (Wilcoxon rank-sum and Kolmogorov–Smirnov tests, p > 0.01)."
The model showed a strong correlation with experimental data for thalamic responses to brief gaps in noise (Pearson’s r = 0.84,p < 0.005).,,,,,,,,,,,,,,
The model predicted and experiments confirmed stimulus-specific deficits in ventral MGB activity in ectopic mice,"particularly for clicks following noise.""",No significant differences between ectopic and nonectopic mice in:,,,,,,,,,,,,,
"Spontaneous firing rates
ABR thresholds","click response latencies
wave peak latencies","tone intensity thresholds
or wave amplitudes (not significant).",characteristic frequencies,or responses to rapid click trains (Wilcoxon rank-sum tests,,p > 0.3).,,,,,,,,,
"Susceptibility to anesthesia or analgesia (p ≥ 0.4).""",,No significant differences were found between ectopic and nonectopic mice in basic auditory thalamic response properties or ABRs to clicks.,,,,,,,,,,,,,
Thalamic sensitivity to rapid click trains did not differ significantly between groups,indicating no general deficit in central auditory temporal acuity.,,,,,,,,,,,,,,
"No significant differences were observed in thalamic responses to noise onsets or sustained noise.""",The need to further investigate the specific role of sound-offset-sensitive channels in auditory temporal processing deficits.,,,,,,,,,,,,,,
The importance of developing clinical measures that assess sound-offset sensitivity,not just sound-onset sensitivity.,,,,,,,,,,,,,,
"The necessity to explore whether similar sound-offset-specific deficits exist in humans with auditory processing disorders.""",,Observational study,,,,,,,,,,,,,
Blind data analysis (analyst unaware of ectopic status),,,,,,,,,,,,,,,
"Non-controlled (no mention of randomization
Nonparametric","placebo
two-tailed statistical tests",or control group),,,,,,,,,,,,,
Comparative (ectopic vs. nonectopic mice),,,,,,,,,,,,,,,
Multi-site recordings (multiple electrode tracks),,,,,,,,,,,,,,,
"Retrospective analysis of physiological and histological data""",,,,,,,,,,,,,,,
Scalability of generative knowledge management systems: designing for individuals’ and institutions’ mutual benefit,Schmitt Ulrich,2020,reference-manager,10.1108/k-05-2020-0324,,"The paper highlights the importance of identifying and mitigating scaling risks in developing a novel knowledge management system (KMS). It uses design science research and extends CKDT and scalable innovation heuristics to address scaling complexities, generative interoperability, and value chain issues, validating the system’s viability and potential impact.",,,,Application of the C-K-Design Theory (CKDT) and thermodynamic knowledge dynamics to analyze and structure knowledge management system (KMS) development and scaling complexities.,,How can we structure the logic and logistics of a novel KMS development-in-progress to recognize and communicate potential scaling complexities to improve its viability and desirability?,"The paper investigates how a Personal Knowledge Management System (PKMS) can enhance knowledge creation and generativity among knowledge workers. Using design science research and a meme-based framework, it demonstrates scalable, non-linear knowledge organization. Findings suggest PKMS supports innovation, skill development, and addresses gaps in current knowledge management practices.","The paper’s main objective is to structure novel knowledge management system (KMS) development to recognize and communicate scaling complexities, using C-K-Design Theory and knowledge dynamics, with the principal finding that the PKMS design enables scalable innovation by addressing these challenges for knowledge workers.","Keywords: Knowledge management, Knowledge creation, Generativity, System design, Design science research, Scalable innovation, Personal knowledge management, Knowledge worker, Memes, C-K-Design Theory, Knowledge dynamics."
Use of the meta-meme-framework to categorize and connect knowledge elements during user activities.,,,,,,,,,,,,,,,
"Implementation of the “scalable innovation” heuristic to assess and mitigate scaling risks in system design.""","The research describes the use of a populated prototype repository and processed datasets, but there is no explicit mention of the availability or location of the source code for the project. Therefore, reproducibility details regarding source code are not provided.","The PKMS prototype successfully processed diverse datasets, demonstrating scalability via a flat-file no SQL database and alignment with all 26 generativity-related attributes.",,,,,,,,,,,,,
Over 500 external references and 50+ peer-reviewed publications expanded conceptual and generative potential,supporting cumulative knowledge development.,,,,,,,,,,,,,,
PKMS interventions reduced information overload and unproductive rework,"and improved innovation and holistic understanding; no p-values or statistical significance reported.""","The PKMS prototype repository was populated with diverse classification datasets (e.g., personal contacts, family trees, industrial classifications).",,,,,,,,,,,,,
PKMS interventions reduced information overload and unproductive rework,and improved rapid iterative improvement,innovation,reputation systems,and holistic understanding.,,,,,,,,,,,
PKMS aligns with all 26 generativity-related attributes assessed.,,,,,,,,,,,,,,,
"No explicit statistical values provided.""","Scaling complexity is rarely addressed and may be underestimated, affecting broader applicability.",,,,,,,,,,,,,,
"The problem is described as """"""""wicked",""" meaning it has incomplete",contradictory,and changing requirements.,,,,,,,,,,,,
"Not every captured or created meme/functionality may be immediately useful; relevance can change over time.""","The PKMS project demonstrates a scalable, meme-based approach for eLearning and knowledge management, supporting non-linear learning and content repurposing.",,,,,,,,,,,,,,
PKMS aligns with all 26 generativity-related attributes,offering comprehensive support for knowledge workers’ development.,,,,,,,,,,,,,,
Addressing scaling complexities is crucial for broader adoption and network effects.,,,,,,,,,,,,,,,
"Adequate tools like PKMS can help mitigate opportunity divides and enhance professional satisfaction.""",Addressing the lack of research on scaling complexities in Knowledge Management Systems (KMS) and communicating these challenges to improve system viability.,,,,,,,,,,,,,,
Developing cumulative knowledge in Design Science Research (DSR) and Knowledge Management (KM),especially through follow-up studies that test or extend design theories.,,,,,,,,,,,,,,
"Creating tools and frameworks that align knowledge creation with skill development to better meet stakeholder needs and reduce opportunity divides.""","Future research should explore decontextualizing the PKMS meta-framework for broader use, outline a sustainability vision for PKMS, compare memetic PKMS storage to traditional document-centric systems, and investigate PKMS’s integration with semantic web and AI technologies. Addressing scaling complexities and cumulative knowledge development in DSR and KM is also recommended.","Design science research (DSR) methodology is used, featuring continually evolving artifacts and design theories. The study integrates concept-knowledge-design theory (CKDT), thermodynamic knowledge dynamics, and a scalable innovation heuristic. It involves ongoing prototype development, multi-disciplinary data sets, and continuous evaluation and knowledge dissemination.",,"The objectives of the study are to investigate how to structure the logic and logistics of a novel Knowledge Management System (KMS) development-in-progress to recognize and communicate potential scaling complexities, aiming to improve its viability and desirability for designers, users, and cumulative knowledge development.",,,,,,,,,,,
En4S: Enabling SLOs in Serverless Storage Systems,"Xie Minghao, Qian Chen, Litz Heiner",2024,reference-manager,10.1145/3698038.3698529,,"The implementation separates compute and transcoding functions to reduce lambda load times and execution limits. Performance is mainly IO-bound for analytics workloads. The scheduler outperforms ReFlex in reducing wait times, especially with varying batch sizes and LC tenant ratios, demonstrating improved efficiency and cost-effectiveness.",,,,"Deployment of baseline ephemeral storage solutions (ReFlex, Jiffy, S3) for comprehensive performance and cost comparison.",,"How can ephemeral storage systems be designed to guarantee SLOs, scalability, and performance predictability for a large number of tenants with diverse requirements in serverless computing environments?","The paper investigates challenges in guaranteeing Service Level Objectives (SLOs) in ephemeral storage for serverless systems. It introduces En4S, a profile-based storage system, evaluated against S3 and Jiffy. En4S shows lower latency and better predictability in some scenarios, but metadata management limits its end-to-end performance.","The research goal is to address challenges in guaranteeing SLOs in ephemeral storage for multi-tenant, serverless systems; the approach is a profile-based ephemeral storage system; the principal finding is that their platform enables predictable performance by enforcing tail latency and IOPS SLOs under dynamic, high-load conditions.",
Use of AWS EC2 and Lambda environments to evaluate scalability and performance in a real-world cloud setting.,,,,,,,,,,,,,,,
"Benchmarking with unified storage clients and synthetic workloads to assess system behavior under various conditions.""",,"En4S consistently showed lower cumulative latency than Jiffy and S3, but its end-to-end latency was 1.02x–1.8x higher than Jiffy due to metadata and synchronization overhead.",,,,,,,,,,,,,
En4S achieved better predictability in stream serverless sorting compared to Jiffy and approached S3’s performance.,,,,,,,,,,,,,,,
For compute-intensive ML analytics,"IO differences had minimal impact; no explicit p-values or statistical significance were reported.""","En4S consistently demonstrated lower cumulative latency than Jiffy and S3, but its end-to-end latency was 1.02x to 1.8x higher than Jiffy due to metadata management inefficiencies.",,,,,,,,,,,,,
En4S achieved better predictability than Jiffy and approached S3 in stream serverless sorting.,,,,,,,,,,,,,,,
For compute-intensive ML analytics,performance was mostly unaffected by IO differences.,,,,,,,,,,,,,,
Scheduler tests (64 million requests) showed En4S handled higher request volumes more efficiently and adapted better to varying batch sizes and LC ratios than ReFlex and FCFS.,,,,,,,,,,,,,,,
Cost efficiency was measured by average application costs,including Lambda IO wait times and storage/controller server costs.,,,,,,,,,,,,,,
"Goodput and throughput benchmarks showed En4S outperformed ReFlex and FCFS in diverse operational scenarios.""","Existing ephemeral storage systems provide limited performance, scalability, and predictability.",,,,,,,,,,,,,,
Pocket struggles to provide consistent performance predictability due to its cost focus.,,,,,,,,,,,,,,,
Traditional solutions do not handle the dynamic and bursty nature of serverless workloads.,,,,,,,,,,,,,,,
Resource contention among tenants leads to performance variability,"making it difficult to meet all SLOs simultaneously.""","En4S consistently delivers better IO predictability and cost efficiency than S3 and Jiffy, especially for IO-intensive and real-time workloads.",,,,,,,,,,,,,
Despite improved efficiency,En4S’s end-to-end latency still lags behind Jiffy due to metadata management and synchronization overheads.,,,,,,,,,,,,,,
"Further optimization in metadata handling and data flow orchestration is recommended.
En4S is a compelling","cost-effective choice for diverse serverless application scenarios.""",Difficulty in maintaining consistent performance and meeting stringent SLOs (Service Level Objectives) for serverless workloads due to their dynamic and bursty nature.,,,,,,,,,,,,,
"Lack of existing solutions designed to handle large numbers of SLOs in serverless environments.
Need for predictable","scalable ephemeral storage with advanced QoS (Quality of Service) scheduling and adaptive burst/token control.""",,,,,"The objectives of the study are to analyze challenges in guaranteeing SLOs in ephemeral storage systems with many tenants and SLOs, and to design and implement a profile-based ephemeral storage system with improved QoS scheduling, dynamic tenant handling, and adaptive burst control for scalable, predictable performance.",,,,,,,,,
A Survey on Semantic Modeling for Building Energy Management,"Aniakor Miracle, Cogo Vinicius V., Ferreira Pedro M.",2024,reference-manager,,,"The survey reveals a gap between theoretical knowledge and real-world use of semantic models in building energy management (BEM). It highlights the need for technical advances and user-friendly platforms, shows that linked data can streamline KPI calculations, and proposes interconnected ontologies for better, scalable, and context-aware BEM applications.",,,,"Strategic keyword search using a refined search string across six major databases (e.g., IEEE, ACM, Scopus) to identify relevant literature.",,"What are the needs, applications, limitations, and best practices of using ontologies and semantic modeling in building operations for Building Energy Management (BEM), and how can a more unified and practical approach be developed to enhance their effectiveness and real-world adoption?","This paper surveys the use of semantic modeling and ontologies in building energy management (BEM) operations. Using a systematic literature review, it identifies key applications, benefits, limitations, and trends. The study concludes that unified, collaborative ontology development is needed to enhance practical BEM applications and interoperability.","The research goal is to survey semantic modeling in building operations, using a systematic literature review to analyze applications, methods, and limitations; the principal finding highlights ontology fragmentation and recommends unified, collaborative approaches for more effective and scalable building energy management (BEM) solutions.","Keywords or tags for this research include: marker, string, reference, air-handling unit, annotation, entity type, human-readable descriptions, ontology, energy domains, sensors, measurements, building entities, energy efficiency, smart device, home, building, city, energy, grid."
Rigorous paper selection process based on PRISMA guidelines,including duplicate removal,screening,and eligibility criteria.,,,,,,,,,,,,
"In-depth analysis and review of 50 selected papers focusing on semantic modeling in building operations.""",,"The survey reviewed 50 papers, identifying five prominent ontologies for building operation analysis and highlighting underutilized ontologies.",,,,,,,,,,,,,
Integrating linked data automates KPI calculation,making energy performance evaluation scalable and adaptable across buildings with results closely matching actual consumption.,,,,,,,,,,,,,,
"There is a gap between theoretical knowledge and real-world application; practical integration and user-friendly platforms are needed. No p-values reported.""","The methodology identified 50 papers for in-depth review after screening 27,816 (IEEE), 38 (ACM), 3 (Web of Science), 197 (Science Direct), 129 (Scopus), and 830 (Google Scholar) results.",,,,,,,,,,,,,,
Integrating linked data automates KPI calculation,making building energy assessments more efficient and scalable.,,,,,,,,,,,,,,
The methodology’s results closely matched actual building energy consumption,"indicating adaptability for city-wide assessments.""","Absence of a clear philosophical framework for modeling building entities, leading to inconsistent methods.",,,,,,,,,,,,,
Ambiguity and confusion due to lack of unified modeling philosophy.,,,,,,,,,,,,,,,
Varied interpretations of key concepts (e.g.,"""""""""zone"""""""") across ontologies.",,,,,,,,,,,,,,
None of the evaluated schemas fully represent all building tags and semantic information.,,,,,,,,,,,,,,,
Excessive fragmentation of semantic web ontologies limits practical application.,,,,,,,,,,,,,,,
Gap between theoretical knowledge and real-world application of semantic models.,,,,,,,,,,,,,,,
"Need for technical advancements and user-friendly platforms to integrate models into Building Energy Management (BEM).""","The study highlights the growing use and importance of semantic modeling, especially ontologies, in building energy management.",,,,,,,,,,,,,,
Fragmentation in ontology development is a key issue; greater collaboration and unified methodologies are recommended.,,,,,,,,,,,,,,,
Integrating linked data can automate KPI calculations,making energy assessments more efficient and scalable.,,,,,,,,,,,,,,
"Bridging theory and practice requires technical advances and user-friendly platforms for real-world adoption.""",There is a gap between the theoretical development of semantic models and their practical application in real-world building energy management.,,,,,,,,,,,,,,
The ontology landscape is fragmented,with many small,overlapping ontologies instead of unified,collaborative frameworks.,,,,,,,,,,,,
"There is a need for systematic approaches to manage ontology extensions and maintain compatibility
Literature review","minimizing duplication and fragmentation.""","Future research should focus on developing unified methodologies for ontology creation, enhancing collaboration to reduce fragmentation, and establishing systems for efficient integration and tracking of ontology extensions. There is also a need to bridge the gap between theoretical models and practical applications in building energy management.",Study design characteristics:,,,,,,,,,,,,
Survey of existing research,,,,,,,,,,,,,,,
Strategic keyword search across six major databases,,,,,,,,,,,,,,,
Focus on semantic modeling and ontologies in building operations,,,,,,,,,,,,,,,
No mention of randomization,blinding,control groups,or experimental/interventional design,,,,,,,,,,,,
"Observational and descriptive in nature""",,"The objectives of the study are to standardize and automate the collection and representation of building data for KPI calculation and energy performance assessment, using semantic ontologies (such as BOT, SOSA, Brick, SAREF) to enable scalable, efficient, and adaptable building energy management and analysis across various applications and domains.",,,,,,,,,,,,,
Human Digital Twin: A Survey,"Lin Yujia, Chen Liming, Ali Aftab, Nugent Christopher, Cleland Ian, Li Rongyang, Gao Dazhi, Wang Hang, Wang Yajie, Ning Huansheng",2021,reference-manager,,,"The paper reviews Human Digital Twin (HDT) technologies, their architectures, and applications in healthcare, daily life, and industry. It highlights advances in sensing, modeling, and behavior representation, but notes the lack of a universal HDT modeling approach. Future research should focus on generic, adaptable HDT models.",,,,"Comprehensive literature review: Analyzed the state-of-the-art of HDT, underpinning technologies, and established typical frameworks for core HDT functions or components.",,"What are the current state-of-the-art technologies, frameworks, applications, and future challenges in the development and implementation of human digital twin (HDT) systems?","This paper reviews the state-of-the-art in human digital twin (HDT) technologies, proposes a comprehensive HDT framework using multi-modal and multi-source data, and discusses future trends and challenges. Methodology includes literature review and framework proposal. Key findings highlight HDT applications and open issues; the study concludes HDT is a promising research area.","The research goal is to review and analyze the state-of-the-art in human digital twin (HDT) technologies, propose a comprehensive HDT framework using multi-modal and multi-source data, and identify future trends and challenges; the principal finding is that HDT is promising but generic modeling approaches remain an open issue.","“Human Digital Twin”, “patient Digital Twin”, “Digital Twin for mental”, “Digital Twin in human”, “digital athlete”, “digital twin for employees”, “digital twin as human representation”"
Framework proposal: Proposed a comprehensive HDT framework using multi-modal and multi-source data to model human organs and behavior.,,,,,,,,,,,,,,,
Application analysis: Reviewed HDT applications in healthcare,industry,"and daily life.""",,"The paper provides a comprehensive literature review of Human Digital Twin (HDT), analyzing core technologies, organizational frameworks, and applications in healthcare, daily life, and industry.",,,,,,,,,,,
A proposed HDTS architecture uses multi-modal and multi-source data for modeling human body/organs and behavior; one referenced model achieved 98% accuracy in mental stress prediction.,,,,,,,,,,,,,,,
The study highlights rapid growth in HDT research since 2020,identifies trends and challenges,"but does not present a practical generic HDT modeling approach; no p-values or statistical significance are reported.""","The main components modeled are blood vessels (flow dynamics), cardiac chambers (systolic function), and the central nervous system (blood pressure regulation).",,,,,,,,,,,,
Mazumder et al. \[54]: Used a one-dimensional hemodynamic model to analyze and classify cardiovascular disease progression.,,,,,,,,,,,,,,,
Chakshu et al. \[28]: Used inverse analysis of cardiovascular time series to assess cardiac status.,,,,,,,,,,,,,,,
Crea et al. \[55]: Combined clinical data and computer-augmented modeling to accelerate cardiovascular research and improve prediction and interpretability.,,,,,,,,,,,,,,,
Subramanian et al. \[74]: Developed a system of differential equations with 112 states and hundreds of parameters for cardiovascular modeling.,,,,,,,,,,,,,,,
Golse et al. \[75]: Modeled the liver as two parallel components to predict risk of portal hypertension after hepatectomy.,,,,,,,,,,,,,,,
"No explicit statistical values or measured effects are provided in the context.""",Only presents state-of-the-art technologies and applications; does not propose a practical approach for generic HDT modeling technologies.,,,,,,,,,,,,,,
Does not address building HDT models suitable for anyone regardless of location,age,gender,or other characteristics.,,,,,,,,,,,,
"Further research needed to design generic models.""","The study reviews the state-of-the-art in human digital twin (HDT) technologies, frameworks, and applications.",,,,,,,,,,,,,,
It proposes a comprehensive HDT system architecture using multi-modal,multi-source data for modeling human organs and behavior.,,,,,,,,,,,,,,
Future research should address challenges in developing generic HDT models suitable for diverse populations.,,,,,,,,,,,,,,,
"HDT is a promising field with expanding applications and ongoing open issues.""","Lack of a practical approach for generic HDT modeling technologies suitable for anyone regardless of location, age, gender, or other characteristics.",,,,,,,,,,,,,,
Need for further research on designing generic HDT models.,,,,,,,,,,,,,,,
Expansion of HDT applications into more innovative domains and addressing open issues and challenges in technology,social,"and cognitive aspects.""","Future research should focus on developing practical, generic HDT modeling technologies suitable for diverse populations. Further investigation is needed into social regulation, privacy, ethics, and trust issues. There is also a need for robust governance mechanisms and improved privacy protection methods in HDT systems.","The study design is a comprehensive literature review. It categorizes and analyzes existing literature, discusses concepts, technologies, and frameworks, reviews applications, and addresses open issues and future trends. There is no mention of randomization, blinding, control groups, or experimental/observational methods.",,,,,,,,,,,Objectives:
Present a comprehensive literature review on human digital twin (HDT),analyzing core technologies and frameworks.,,,,,,,,,,,,,,
Propose a comprehensive HDT framework using multi-modal and multi-source data to model human organs,body,and behavior.,,,,,,,,,,,,,
"Provide expectations on future trends and challenges to promote further research.""",,,,,,,,,,,,,,,
Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education,"Yang Rui, Yang Boming, Ouyang Sixun, She Tianwei, Feng Aosong, Jiang Yuang, Lecue Freddy, Lu Jinghui, Li Irene",2024,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
The pipeline integrating a constructed knowledge graph (KG) outperforms LLaMA and GPT-4o,generating more relevant,persuasive,and factually accurate responses. Graphusion reduces irrelevant concepts,improves targeted concept generation,,and enhances QA performance,"The research goal is to generate persuasive and scientifically accurate NLP project proposals using knowledge graphs; the approach compares LLaMA, GPT-4o, and a pipeline with constructed knowledge graphs; results show the pipeline achieves higher expert-rated Convincity and Factuality, indicating more convincing and factually sound project ideas.","Human Evaluation Rubrics: Projects are rated using four criteria—Concept Relevancy, Concept Coverage, Project Convincity, and Scientific Factuality—each on a 1-5 scale.",,,"What is the most effective way to leverage knowledge graphs and advanced natural language processing techniques to enhance project proposals and reasoning in domains involving neural question answering, social media analysis, topic modeling, relation extraction, and word embedding variations?","The paper investigates how knowledge graphs can enhance reasoning in natural language processing project proposals. Using a multi-step extraction and fusion methodology, the study compares models like GPT-4o and Graphusion. Results show Graphusion provides more relevant, comprehensive, and scientifically accurate project ideas, with higher expert ratings for concept coverage and factuality.","probabilistic grammar, generating regexes, neural machine translation, machine translation, statistical machine translation, CFG (context free grammar), synchronous context free grammar, extractive summarization, abstractive summarization, hierarchical attention network, reading comprehension, natural language understanding, natural language generation, word embedding, sentiment analysis, lexicon induction, knowledge graph, semantic parsing, relation extraction","concise external data further boosts results.""",especially in educational NLP tasks. Adding high-quality
Expert Evaluation: Two NLP experts independently assess project proposals,with inter-rater agreement measured by Kappa score (0.6689).,,,,,,,,,,,,,,
"Comparative Analysis: Performance is compared between baseline and the proposed pipeline using both quantitative scores and expert ratings.""",,"The proposed pipeline outperforms the zero-shot baseline in all criteria, with higher scores in Convincity (4.720 vs. 4.380) and Factuality (4.770 vs. 4.625), indicating more persuasive and scientifically accurate content.",,,,,,,,,,,,,
Expert evaluation shows substantial agreement (Kappa score: 0.6689).,,,,,,,,,,,,,,,
Incorporating neighboring concepts or Wikipedia content improves model performance,"while adding lengthy lecture slides reduces it.""","Primary outcomes were measured using expert evaluation on four criteria (1-5 scale): Concept Relevancy, Concept Coverage, Project Convincity, and Scientific Factuality.",,,,,,,,,,,,,
"""Ours"""""""" pipeline outperformed """"""""Zero-shot"""""""" in all criteria: Relevancy (4.845 vs. 4.750)",Coverage (4.905 vs. 4.840),Convincity (4.720 vs. 4.380),Factuality (4.770 vs. 4.625).,,,,,,,,,,,,
Kappa score for expert agreement: 0.6689 (substantial agreement).,,,,,,,,,,,,,,,
"Task-specific results (Tasks 1-5): """"""""Ours"""""""" achieved higher scores than """"""""Zero-shot"""""""" (e.g.
In Tasks 2 and 3","T1: 92.00 vs. 69.20).
Graphusion generated fewer","more accurate concept entities than GPT-4o (Task 2: 2.84 vs. 11.04; Task 3: 2.87 vs. 11.54).""","No explicit limitations, shortcomings, or suggestions for further research are stated in the provided context.","The proposed pipeline outperforms baseline models, especially in persuasiveness (Convincity) and scientific accuracy (Factuality).",,,,,,,,,,,
Expert evaluation confirms higher scores for the pipeline across all criteria,with a Kappa score of 0.6689 indicating substantial agreement.,,,,,,,,,,,,,,
The pipeline generates more relevant,comprehensive,"and targeted content than alternatives.""","Difficulty in extracting high-quality, domain-specific concepts from free text without predefined lists, leading to irrelevant concept generation.",,,,,,,,,,,,
Challenges in reconciling conflicting or diverse relations among extracted triplets during knowledge graph construction.,,,,,,,,,,,,,,,
Need for improved evaluation methods due to the complex,"non-binary output (triplet lists) in zero-shot knowledge graph construction.""",,No information available,,,,,,,,,,,,
Networking Architecture and Key Supporting Technologies for Human Digital Twin in Personalized Healthcare: A Comprehensive Survey,"Chen Jiayuan, Yi Changyan, Okegbile Samuel D., Cai Jun, Shen Xuemin",2023,reference-manager,,,"Implementation Insights focus on data cleaning, outlier detection, imputation, feature selection, data fusion, storage, and security. New insights include the superiority of deep learning-based imputation for discrete data and the need for novel resource allocation and incentive mechanisms in edge-cloud HDT environments to address computational and privacy challenges.",,,,Comprehensive literature survey: The study reviews existing research on Human Digital Twin (HDT) and networking architecture in personalized healthcare (PH) applications.,,"What are the key networking architectures and supporting technologies required to enable human digital twin (HDT) systems for personalized healthcare (PH) applications, and how do these differ from conventional digital twin approaches in addressing current healthcare challenges?","This paper surveys the networking architecture and key technologies for Human Digital Twin (HDT) in Personalized Healthcare (PH) applications. Using a comprehensive literature review, it analyzes design requirements, challenges, and a five-layered architecture. Key findings highlight enhanced security, privacy, and future research directions for HDT in PH.","The research goal is to survey the networking architecture and key technologies for human digital twin (HDT) in personalized healthcare (PH); the approach is a comprehensive review and analysis of HDT frameworks, layers, and enabling technologies; the principal finding is a detailed guideline and future directions for HDT networking in PH.","Keywords or tags for this research include: Human digital twin, personalized healthcare, artificial intelligence, reinforcement learning, federated learning, networking architecture, life-cycle data management, pervasive sensing, on-body communications, tactile Internet, semantic communications, multi-access edge computing, edge-cloud collaboration, blockchain, Metaverse."
Five-layered networking architecture analysis: The study investigates data acquisition,communication,computation,data management,and data analysis/decision-making layers.,,,,,,,,,,,
"Feature selection and data imputation techniques: The study discusses methods for selecting relevant data features and handling missing data in HDT systems.""",,"The survey provides a comprehensive overview of Human Digital Twin (HDT) in personalized healthcare (PH), highlighting differences from conventional Digital Twin (DT), and presents a universal framework for HDT.",,,,,,,,,,,,,
It uniquely analyzes HDT design requirements and challenges from a networking perspective,emphasizing ubiquitous,timely,secure,and accurate implementation.,,,,,,,,,,,
"No quantitative results or statistical significance (p-values) are reported in the context provided.""","The survey provides an overview of Human Digital Twin (HDT) in Personalized Healthcare (PH), highlighting differences from conventional Digital Twin (DT), a universal framework, and key technologies.",,,,,,,,,,,,,,
"A novel networking perspective is analyzed
Reported results: In one HDT framework for head and neck cancer","addressing design requirements and challenges for HDT in PH applications.
mean and median accuracies reached 87.09% and 90.85%",respectively,"with a survival rate increase of 3.73%.""","Data scarcity: Fragmented, heterogeneous, and biased medical data; lack of standardized formats; privacy concerns limit data sharing.",,,,,,,,,,,
Security and privacy: Vulnerable to cyber-attacks; challenges in confidentiality,access control,integrity,and authentication.,,,,,,,,,,,,
Data acquisition: Inability to collect ultra-fine-grained data; limited device battery life; difficulty integrating heterogeneous data.,,,,,,,,,,,,,,,
Management and storage: Handling massive,sensitive data is challenging.,,,,,,,,,,,,,,
"Ethical concerns: Potential healthcare inequality; sensitive patient data raises moral issues.""",The survey provides a comprehensive overview and novel analysis of the networking architecture and key technologies for Human Digital Twin (HDT) in Personalized Healthcare (PH) applications.,,,,,,,,,,,,,,
It introduces a five-layered networking architecture for HDT and highlights supporting technologies for each layer.,,,,,,,,,,,,,,,
Key challenges include data scarcity,interoperability,privacy,and ethical considerations.,,,,,,,,,,,,
"The study recommends unified data management frameworks and outlines future research directions.""","Data scarcity: There is an urgent need for unified and secure data management frameworks to address fragmented, heterogeneous, and privacy-sensitive medical data, or to use artificial intelligence-generated content (AIGC) to synthesize realistic datasets.",,,,,,,,,,,,,,
Computing resource allocation: Novel schemes are needed for balancing computing loads and optimizing resource utilization in MEC-based or edge-cloud collaboration-based HDT.,,,,,,,,,,,,,,,
Explainable and efficient AI: Achieving full explainability and reliability of AI in healthcare is challenging,"and there is a need for low-complexity AI models that maintain accuracy while reducing computing costs.""",Future research directions include: achieving full explainability and reliability of AI in healthcare; developing low-complexity AI models for HDT without sacrificing accuracy; addressing data scarcity; enabling mobile and federated HDT; improving subsystem interoperability; designing efficient interfaces; integrating intelligent blockchain; and exploring ethical considerations.,,,,"The objective is to survey AI-enabled Human Digital Twin (HDT) solutions for personalized prescriptions (personalized treatment planning), focusing on how HDT can test prescriptions virtually to identify the best-performing option before applying it to patients in the real world.",,,,,,,,,
Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion,"Luo Ruilin, Gu Tianle, Li Haoling, Li Junzhe, Lin Zicheng, Li Jiayi, Yang Yujiu",2024,reference-manager,,,Implementation Insights Summary:,,,,,,,,,
Structure-based history augmentation improves both forward and backward inference,as shown by increased Hits@1 scores. Introducing reverse quadruples during fine-tuning generally boosts performance,especially in backward inference,"with minimal negative impact on forward reasoning. Ordinary and text-aware prompt strategies yield better results than position-aware prompts. Model size has little effect.""",,,"The research goal is to assess LLMs' effectiveness in temporal knowledge graph completion (TKGC); the approach introduces a structure-augmented, history-aware fine-tuning framework with reverse logic and PEFT; results show competitive Hits@1 metrics and highlight the benefit of structural and reverse data augmentation.","Structure-augmented History Modeling: Combines schema-matching, entity-augmented, and relation-augmented historical facts to enhance LLM reasoning in temporal knowledge graphs.",,,,,,"Are large language models effective temporal knowledge graph reasoning agents, and what genuinely beneficial factors can be revealed to enhance their performance in temporal knowledge graph completion tasks?",,"The paper investigates prompt strategies for temporal knowledge graph completion using large language models. It compares embedding-based and LLM-based methods, using 8-shot in-context learning on datasets like ICEWS14, ICEWS05-15, ICEWS18, and YAGO. Results show LLMs, especially Vicuna-7b-CoH, achieve competitive or superior Hits@1 scores. Including reverse logic aids reasoning."
Fine-tuning with Local Historical Data: Uses local information from multiple single-step graphs to augment historical data and adapt LLMs for temporal event logic.,,,,,,,,,,,,,,,
Comparative Evaluation: Benchmarks against embedding-based,GNN-based,and LLM-based models,"including rule-based methods like TLogic.""",,,"Llama-2-7b-CoH and Vicuna-7b-CoH achieve state-of-the-art or comparable results across multiple datasets, with Vicuna-7b-CoH improving Hits@1 by 3.3% (ICEWS05-15) and 1.9% (YAGO) over previous best models.",,,,,,,,,
Incorporating structure-based history augmentation and reciprocal quadruples in fine-tuning yields consistent improvements in Hits@1,with gains up to 4.8% (ICEWS14,forward direction).,,,,,,,,,,,,,
Among prompt strategies,ordinary and text-aware prompts outperform position-aware,"and model size has minimal impact on Hits@1 performance. No p-values or explicit statistical significance are reported.""",Llama-2-7b-CoH and Vicuna-7b-CoH achieve results that surpass or are comparable to state-of-the-art models across multiple metrics under the raw setting.,,,,,,,,,,,,
On ICEWS05-15 and YAGO,Vicuna-7b-CoH improves Hits@1 by 3.3% and 1.9% over the previous best models.,,,,,,,,,,,,,,
Under the time-aware filtered setting,Llama-2-7b-CoH outperforms TiRGN by 4.1 percentage points in Hits@1 on YAGO and shows substantial advantage on ICEWS05-15 and ICEWS18.,,,,,,,,,,,,,,
Structure-based history augmentation yields comprehensive improvement in bi-directional forecasting,with Hits@1 increases up to 4.8% (ICEWS14,forward).,,,,,,,,,,,,,
Incorporating reciprocal quadruples during fine-tuning generally improves Hits@1,with increases up to 8.5% (ICEWS18,backward).,,,,,,,,,,,,,
Among prompt strategies,ordinary and text-aware styles consistently yield better Hits@1 results.,,,,,,,,,,,,,,
Model size has minimal impact on overall Hits@1 metrics.,,,,,,,,,,,,,,,
All numerical results for Hits@1,Hits@3,and Hits@10 are reported in Tables 3,4,5,,6,,,,,,,"LLM-based methods cannot rank all entities, only a limited set of candidates, unlike embedding-based models.","and 8.""",7
Reporting of metrics is limited to Hits@1,Hits@3,and Hits@10 due to ranking constraints.,,,,,,,,,,,,,
Filtering out all valid candidates in evaluation is considered not entirely reasonable.,,,,,,,,,,,,,,,
Commercial LLMs underperform on ICEWS datasets due to difficulty capturing evolutionary patterns and structure-based knowledge.,,,,,,,,,,,,,,,
"Results may not generalize to datasets with different characteristics or reasoning requirements.""","LLMs are effective for temporal knowledge graph completion (TKGC), with models like Llama-2-7b-CoH and Vicuna-7b-CoH achieving or surpassing state-of-the-art results.",,,,,,,,,,,,,,
Incorporating reverse logic during fine-tuning alleviates the reversal curse in structured reasoning and is generally beneficial.,,,,,,,,,,,,,,,
Ordinary and text-aware prompt strategies yield better performance than position-aware strategies.,,,,,,,,,,,,,,,
"Parameter-Efficient Fine-Tuning (PEFT) and integrating historical and neighboring information are recommended for improved model understanding.""",Limited utilization of rich text information and underperformance in sparse-link scenarios remain unresolved in current models.,,,,,,,,,,,,,,
There is a noticeable performance gap for LLM-based approaches on ICEWS14 series datasets,especially compared to embedding-based models.,,,,,,,,,,,,,,
Further investigation is needed into key factors influencing temporal structural information reasoning using LLMs,"such as historical chain length and model size.""",,,,,,,,,,,,,,
A Workflow Model for Holistic Data Management and Semantic Interoperability in Quantitative Archival Research,"Fafalios Pavlos, Marketakis Yannis, Axaridou Anastasia, Tzitzikas Yannis, Doerr Martin",2023,reference-manager,,,"Implementation Insights highlight key data quality challenges: missing information affects analysis accuracy; data entry errors impact consistency and user experience; and non-consistent comparative values hinder comparisons. The workflow supports provenance tracking, recursive revisions, and user validation, enabling reproducible research and addressing real historians’ needs. No new insights beyond these lessons are presented.",,,,"Free text search and interactive interface: Users explore integrated data using keyword searches or user-friendly interfaces, resulting in ranked lists and visualizations.",,"How can integrated semantic data and user-friendly exploration interfaces improve the effectiveness, usability, and trustworthiness of archival research, particularly in addressing complex information needs and ensuring data quality in historical studies?","The paper presents a holistic workflow model for data management in archival research, focusing on semantic data integration and provenance preservation. Researchers and data engineers collaborate through processes like transcription and curation. Key findings highlight improved usability, trustworthiness, and data quality dimensions (completeness, consistency, conciseness), supporting advanced research and analysis.","The research goal is to improve data quality and usability in maritime history research using semantic technologies; the approach involves a workflow model focusing on completeness, consistency, and conciseness of semantic data, with results showing enhanced data exploration and analysis through user-friendly interfaces and multi-perspective result visualizations.",
Digitization/Transcription: Text recognition software or manual/collaborative transcription is used to extract and organize text from historical documents.,,,,,,,,,,,,,,,
Curation: Researchers harmonize and resolve ambiguities in transcribed data,"ensuring consistent identification of entities and concepts.""",The research emphasizes reproducibility by maintaining full provenance of each data element and supporting revision of all workflow steps. The implementation is provenance-aware and allows researchers to inspect original data. The project source code is referenced at https://sealitproject.eu/.,"Primary findings highlight three main data quality issues: missing information (affecting completeness), data entry errors (affecting schema-based consistency), and non-consistent comparative values (affecting value-based consistency).",,,,,,,,,,,,
Quantitative results or specific numerical data are not provided in the context.,,,,,,,,,,,,,,,
"No statistical significance or p-values are reported in the context.""","Primary outcomes focus on data quality issues: missing information, data entry errors, and non-consistent comparative values.",,,,,,,,,,,,,,
Measured effects include impacts on accuracy of quantitative analysis,user experience,and difficulty in data comparison.,,,,,,,,,,,,,
"No explicit statistical values or quantitative results are provided.""",Missing information: Missing values are common and can affect the accuracy of statistical analysis.,,,,,,,,,,,,,,
Data entry errors: Mistakes during data entry,such as filling the wrong column,can spoil user experience and analysis.,,,,,,,,,,,,,
Non-consistent comparative values: Inconsistent formats or units across sources hinder comparisons.,,,,,,,,,,,,,,,
"Data consistency and conciseness issues.""","Data quality issues such as missing information, data entry errors, and inconsistent comparative values significantly impact research accuracy and usability.",,,,,,,,,,,,,,
Ensuring completeness,consistency,and conciseness of data is essential for reliable quantitative analysis.,,,,,,,,,,,,,
User interfaces should support error prevention and allow validation against original sources.,,,,,,,,,,,,,,,
"Automated or manual steps are recommended to harmonize comparative values for effective analysis.""","Addressing missing information and data completeness, as missing values can impact quantitative analysis and decision-making.",,,,,,,,,,,,,,
Reducing data entry errors and improving schema-based consistency,including better validation and user interface mechanisms.,,,,,,,,,,,,,,
Handling non-consistent comparative values (e.g.,dates,units),"requiring additional steps for value-based consistency across archival sources.""",,,,,,,,,,,,
A Blockchain-Based Personal Health Knowledge Graph for Secure Integrated Health Data Management,"Li Juan, Pandey Vikram, Hendawi Rasha",2023,reference-manager,10.1109/iscc58397.2023.10218032,,"The implementation uses blockchain and personal health knowledge graphs to integrate data from EHRs, wearable devices, and apps, ensuring privacy, security, and interoperability. HL7 FHIR-based ontology is extended for standardization. Patients control data access. Evaluations show the system is secure, scalable, and effective for data sharing.",,,,"Data integration from multiple sources (EHRs, wearable devices, health apps) and mapping to an ontology for interoperability.",,"How can a decentralized system integrating personal health knowledge graphs and blockchain technology address the scalability, privacy, and security challenges of managing and sharing personal health data in the healthcare domain?","The paper aims to develop a secure, scalable system for managing and sharing personal health data using knowledge graphs and blockchain technology. It constructs a personal health ontology and uses Ethereum-based smart contracts for access control. Results show effective, secure data integration and sharing, with enhanced privacy and security for patients.","The paper's main objective is to develop a blockchain-based personal health knowledge graph using a standardized ontology to integrate diverse personal health data; the key method combines knowledge graphs and blockchain for secure, interoperable data management, and the principal finding is enhanced privacy, security, and comprehensive health insights for patients.",
Representation of integrated data as a knowledge graph using graph databases,with nodes and edges to model relationships.,,,,,,,,,,,,,,
"Use of blockchain technology to ensure secure and efficient management of personal health data.""",,The proposed personal health knowledge graph (PHKG) system maintains 100% data availability when the node drop rate is below 30% in a 500-node network.,,,,,,,,,,,,,
The system demonstrates strong fault tolerance,providing full data access even if up to 150 out of 500 nodes fail.,,,,,,,,,,,,,,
Evaluations show the system is secure,scalable,"and enables effective data sharing and integration; no specific p-values are reported.""",The proposed personal health knowledge graph (PHKG) system was evaluated using use cases and simulation.,,,,,,,,,,,,
Results showed the system is secure,scalable,and enables effective data sharing and integration.,,,,,,,,,,,,,
Data availability of PHKG storage was measured against the network’s node drop rate (see Fig. 12).,,,,,,,,,,,,,,,
"The ontology was extended to include lifestyle factors and linked to existing medical vocabularies for improved interoperability.""","Further research is needed to explore the scalability, privacy, and security aspects of using knowledge graphs in the healthcare domain.",The proposed personal health knowledge graph (PHKG) enables comprehensive integration and analysis of health data.,,,,,,,,,,,,,
A blockchain-based architecture ensures secure,tamper-proof storage and allows patients full control over data sharing.,,,,,,,,,,,,,,
"Smart contracts automate access control and data sharing
The system is secure","enhancing privacy and security.
scalable","and supports effective data sharing and integration.""",Further research is needed to explore the scalability of using knowledge graphs in the healthcare domain.,,,,,,,,,,,,
There is a need to investigate privacy aspects when applying knowledge graphs in healthcare.,,,,,,,,,,,,,,,
"Security aspects of using knowledge graphs in healthcare require additional research.""","There is a need for further research to explore the scalability, privacy, and security aspects of using knowledge graphs in the healthcare domain.",,,"The objectives are to design a standardized ontology based on HL7 FHIR for personal health data, improve semantic interoperability and knowledge sharing, and construct a personal health knowledge graph to organize and integrate personal health information for intelligent applications and personalized insights.",,,,,,,,,,,
